{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [[  2.31800000e+01   2.72720000e+01   4.26000000e+02   7.21250000e+02\n",
      "    4.79298818e-03]\n",
      " [  2.31500000e+01   2.72675000e+01   4.29500000e+02   7.14000000e+02\n",
      "    4.78344095e-03]\n",
      " [  2.31500000e+01   2.72450000e+01   4.26000000e+02   7.13500000e+02\n",
      "    4.77946352e-03]\n",
      " ..., \n",
      " [  2.11000000e+01   3.60950000e+01   4.33000000e+02   7.98500000e+02\n",
      "    5.59563902e-03]\n",
      " [  2.11000000e+01   3.62600000e+01   4.33000000e+02   8.20333333e+02\n",
      "    5.62144937e-03]\n",
      " [  2.11000000e+01   3.62000000e+01   4.47000000e+02   8.21000000e+02\n",
      "    5.61206354e-03]]\n",
      "np.shape(train_data):  (8143, 8)\n",
      "t:  [ 23.18  23.15  23.15 ...,  21.1   21.1   21.1 ]\n",
      "hu:  [ 27.272   27.2675  27.245  ...,  36.095   36.26    36.2   ]\n",
      "lt:  [ 426.   429.5  426.  ...,  433.   433.   447. ]\n",
      "co2:  [ 721.25        714.          713.5        ...,  798.5         820.33333333\n",
      "  821.        ]\n",
      "hu_r:  [ 0.00479299  0.00478344  0.00477946 ...,  0.00559564  0.00562145\n",
      "  0.00561206]\n",
      "o:  [ 1.  1.  1. ...,  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "\n",
    "Note that input layer is noted as layer0, hidden layer is noted as layer1, and output layer is noted is layer2\n",
    "'''\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = np.genfromtxt('../data/assign2_train_data.txt', delimiter=',')\n",
    "#print \"train_data:\", train_data\n",
    "\n",
    "t = train_data[:,2]\n",
    "hu = train_data[:,3]\n",
    "lt = train_data[:,4]\n",
    "co2 = train_data[:,5]\n",
    "hu_r = train_data[:,6]\n",
    "\n",
    "o = train_data[:,7]\n",
    "\n",
    "#data\n",
    "data = np.column_stack((t, hu, lt, co2, hu_r))\n",
    "print \"data:\", data\n",
    "\n",
    "print \"np.shape(train_data): \", np.shape(train_data)\n",
    "#print \"np.shape(t): \", np.shape(t)\n",
    "\n",
    "print \"t: \", t\n",
    "print \"hu: \", hu\n",
    "print \"lt: \", lt\n",
    "print \"co2: \", co2\n",
    "print \"hu_r: \", hu_r\n",
    "print \"o: \", o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    #normalize train data\n",
    "    #normalize t\n",
    "    t = data[:, 0]\n",
    "    t_min = np.min(t)\n",
    "    t_max = np.max(t)\n",
    "    d_t = t_max - t_min\n",
    "    #normalized t: n_t\n",
    "    n_t = []\n",
    "    for each in t:\n",
    "        n_t.append((each - t_min) / d_t)\n",
    "\n",
    "    #normalize hu\n",
    "    hu = data[:, 1]\n",
    "    hu_min = np.min(hu)\n",
    "    hu_max = np.max(hu)\n",
    "    d_hu = hu_max - hu_min\n",
    "    #normalized h: n_hu\n",
    "    n_hu = []\n",
    "    for each in hu:\n",
    "        n_hu.append((each - hu_min) / d_hu)\n",
    "\n",
    "    #normalize lt\n",
    "    lt = data[:, 2]\n",
    "    lt_min = np.min(lt)\n",
    "    lt_max = np.max(lt)\n",
    "    d_lt = lt_max - lt_min\n",
    "    #normalized lt: n_lt\n",
    "    n_lt = []\n",
    "    for each in lt:\n",
    "        n_lt.append((each - lt_min) / d_lt)\n",
    "\n",
    "    #normalize co2\n",
    "    co2 = data[:, 3]\n",
    "    co2_min = np.min(co2)\n",
    "    co2_max = np.max(co2)\n",
    "    d_co2 = co2_max - co2_min\n",
    "    #normalized co2: n_co2\n",
    "    n_co2 = []\n",
    "    for each in co2:\n",
    "        n_co2.append((each - co2_min) / d_co2)\n",
    "\n",
    "    #normalize hu_r\n",
    "    hu_r = data[:, 4]\n",
    "    hu_r_min = np.min(hu_r)\n",
    "    hu_r_max = np.max(hu_r)\n",
    "    d_hu_r = hu_r_max - hu_r_min\n",
    "    #normalized hu_r: n_hu_r\n",
    "    n_hu_r = []\n",
    "    for each in hu_r:\n",
    "        n_hu_r.append((each - hu_r_min) / d_hu_r)\n",
    "\n",
    "    #normalized data: n_data\n",
    "    n_data = np.column_stack((n_t, n_hu, n_lt, n_co2, n_hu_r))\n",
    "    \n",
    "    return n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_data: [[ 1.          0.47053302  0.27549041  0.190933    0.55731842]\n",
      " [ 0.99282297  0.47033188  0.27775383  0.18644592  0.55480723]\n",
      " [ 0.99282297  0.46932618  0.27549041  0.18613647  0.55376106]\n",
      " ..., \n",
      " [ 0.50239234  0.86490111  0.28001725  0.23874362  0.76843752]\n",
      " [ 0.50239234  0.87227623  0.28001725  0.25225643  0.77522635]\n",
      " [ 0.50239234  0.86959437  0.28907092  0.25266904  0.77275762]]\n"
     ]
    }
   ],
   "source": [
    "n_data = normalize(data)\n",
    "print \"n_data:\", n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws:  [0.8159865810101249, 0.8594825563728048, 0.5858482257906332, 0.3747241271369467, 0.13126998785464317, 0.5285616877006225]\n"
     ]
    }
   ],
   "source": [
    "def initialize_weights():\n",
    "    w_t = random.random()\n",
    "    w_hu = random.random()\n",
    "    w_lt = random.random()\n",
    "    w_co2 = random.random()\n",
    "    w_hu_r = random.random()\n",
    "    b = random.random()\n",
    "\n",
    "    #weights\n",
    "    ws = [w_t, w_hu, w_lt, w_co2, w_hu_r, b]\n",
    "    return ws\n",
    "\n",
    "\n",
    "init_ws = initialize_weights()\n",
    "ws = copy.deepcopy(init_ws)\n",
    "print \"ws: \", ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def activation(x): # use sigmoid as activation function\n",
    "    a = sigmoid(x)\n",
    "    if a >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def perceptron(ins, ws):\n",
    "    sum = 0\n",
    "    for x, w in zip(ins, ws):\n",
    "        sum += x * w\n",
    "    return activation(sum)\n",
    "\n",
    "# lr - learning rate\n",
    "# bs - batch size\n",
    "\n",
    "def train(n_data, lr, bs):\n",
    "    print \"Training starts: len of data:\", len(n_data)\n",
    "    \n",
    "    accuracies = []\n",
    "    epoch = 0\n",
    "    n_t = n_data[:, 0]\n",
    "    n_hu = n_data[:, 1]\n",
    "    n_lt = n_data[:, 2]\n",
    "    n_co2 = n_data[:, 3]\n",
    "    n_hu_r = n_data[:, 4]\n",
    "    while epoch <= 10000:\n",
    "        # train\n",
    "        i = 0\n",
    "        for (n_t_i, n_hu_i, n_lt_i, n_co2_i, n_hu_r_i, o_i) in zip(n_t, n_hu, n_lt, n_co2, n_hu_r, o):\n",
    "            sum_error_w_t = 0\n",
    "            sum_error_w_hu = 0\n",
    "            sum_error_w_lt = 0\n",
    "            sum_error_w_co2 = 0\n",
    "            sum_error_w_hu_r = 0\n",
    "            sum_error_b = 0\n",
    "            \n",
    "            error = o_i - (n_t_i * ws[0] + n_hu_i * ws[1] + n_lt_i * ws[2] + n_co2_i * ws[3] + n_hu_r_i * ws[4] + ws[5])\n",
    "            sum_error_w_t += error * n_t_i\n",
    "            sum_error_w_hu += error * n_hu_i\n",
    "            sum_error_w_lt += error * n_lt_i\n",
    "            sum_error_w_co2 += error * n_co2_i\n",
    "            sum_error_w_hu_r += error * n_hu_r_i\n",
    "            sum_error_b += error\n",
    "            \n",
    "            if i >= bs: # only update weights after one batch size\n",
    "                ws[0] += lr * sum_error_w_t\n",
    "                ws[1] += lr * sum_error_w_hu\n",
    "                ws[2] += lr * sum_error_w_lt\n",
    "                ws[3] += lr * sum_error_w_co2\n",
    "                ws[4] += lr * sum_error_w_hu_r\n",
    "                ws[5] += lr * sum_error_b\n",
    "                sum_error_w_t = 0\n",
    "                sum_error_w_hu = 0\n",
    "                sum_error_w_lt = 0\n",
    "                sum_error_w_co2 = 0\n",
    "                sum_error_w_hu_r = 0\n",
    "                sum_error_b = 0\n",
    "                i = 1\n",
    "                continue\n",
    "            \n",
    "            #test every epoch\n",
    "            accuracy = get_one_feedforward_accuracy(n_data, ws, o)\n",
    "            accuracies.append(accuracy)\n",
    "            if epoch % 100 == 0:\n",
    "                print \"epoch:\", epoch, \" accuracy:\",accuracy\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        # count \n",
    "        epoch += 1\n",
    "    print \"Training ends.\"\n",
    "    print \"Initial weights:\", init_ws\n",
    "    print \"Trained weights:\", ws\n",
    "    print \"Trained data accuracy:\", accuracies[-1]\n",
    "    \n",
    "    test_data = np.genfromtxt('../data/assign2_test_data.txt', delimiter=',')\n",
    "    # print \"test_data:\", test_data\n",
    "    test(test_data, o, lr, bs)\n",
    "\n",
    "    plot_accuracy(accuracies, lr, bs, stage=\"train\")\n",
    "    \n",
    "def test(test_data, o, lr, bs):\n",
    "    print \"Testing starts: len of data:\", len(test_data)\n",
    "    #normalize test data\n",
    "    #normalize t\n",
    "    n_test_data = normalize(test_data)\n",
    "    n_t = n_test_data[:, 0]\n",
    "    n_hu = n_test_data[:, 1]\n",
    "    n_lt = n_test_data[:, 2]\n",
    "    n_co2 = n_test_data[:, 3]\n",
    "    n_hu_r = n_test_data[:, 4]\n",
    "    accuracy = get_one_feedforward_accuracy(n_test_data, ws, o)\n",
    "    print \"Test data accuracy:\",accuracy\n",
    "        \n",
    "def get_one_feedforward_accuracy(n_data, ws, o):\n",
    "    \n",
    "    #calculate accuracy\n",
    "    #outputs\n",
    "    os = []\n",
    "    for each in zip(n_data):\n",
    "        for ins in each:\n",
    "            os.append(perceptron(ins, ws))\n",
    "            \n",
    "    #count correct output number\n",
    "    correct_cnt = 0\n",
    "    for o_i, os_i in zip(o, os):\n",
    "        if (o_i == os_i):\n",
    "            correct_cnt += 1\n",
    "    accuracy = float(correct_cnt) / float(len(o))\n",
    "    ##calculate accuracy\n",
    "    return accuracy\n",
    "    \n",
    "def plot_accuracy(accuracies, lr, bs, stage):\n",
    "    accs_y = np.array(accuracies)\n",
    "    epochs = []\n",
    "    for i in range(len(accuracies)):\n",
    "        epochs.append(i)\n",
    "    epochs_x = np.array(epochs)\n",
    "    plt.plot(epochs_x, accs_y)\n",
    "    title = stage\n",
    "    title += ' lr:'\n",
    "    title += str(lr)\n",
    "    title += ' batch_size:'\n",
    "    title += str(bs)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts: len of data: 8143\n",
      "epoch: 0  accuracy: 0.212329608252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-fc998d9a317d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-386b5f57b79c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_data, lr, bs)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum_error_w_co2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum_error_w_hu_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mws\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum_error_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0msum_error_w_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0msum_error_w_hu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(n_data, lr = 0.001, bs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data: [[             nan              nan   2.17600000e+01 ...,   1.02966667e+03\n",
      "    5.02101089e-03   1.00000000e+00]\n",
      " [             nan              nan   2.17900000e+01 ...,   1.00000000e+03\n",
      "    5.00858127e-03   1.00000000e+00]\n",
      " [             nan              nan   2.17675000e+01 ...,   1.00375000e+03\n",
      "    5.02156913e-03   1.00000000e+00]\n",
      " ..., \n",
      " [             nan              nan   2.08900000e+01 ...,   1.52150000e+03\n",
      "    4.23681810e-03   1.00000000e+00]\n",
      " [             nan              nan   2.08900000e+01 ...,   1.63200000e+03\n",
      "    4.27948547e-03   1.00000000e+00]\n",
      " [             nan              nan   2.10000000e+01 ...,   1.86400000e+03\n",
      "    4.32073200e-03   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "test_data = np.genfromtxt('../data/assign2_test_data.txt', delimiter=',')\n",
    "# print \"test_data:\", test_data\n",
    "test(test_data, o, lr, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
