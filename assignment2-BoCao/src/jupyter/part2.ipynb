{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [[  2.31800000e+01   2.72720000e+01   4.26000000e+02   7.21250000e+02\n",
      "    4.79298818e-03]\n",
      " [  2.31500000e+01   2.72675000e+01   4.29500000e+02   7.14000000e+02\n",
      "    4.78344095e-03]\n",
      " [  2.31500000e+01   2.72450000e+01   4.26000000e+02   7.13500000e+02\n",
      "    4.77946352e-03]\n",
      " ..., \n",
      " [  2.11000000e+01   3.60950000e+01   4.33000000e+02   7.98500000e+02\n",
      "    5.59563902e-03]\n",
      " [  2.11000000e+01   3.62600000e+01   4.33000000e+02   8.20333333e+02\n",
      "    5.62144937e-03]\n",
      " [  2.11000000e+01   3.62000000e+01   4.47000000e+02   8.21000000e+02\n",
      "    5.61206354e-03]]\n",
      "np.shape(train_data):  (8143, 8)\n",
      "t:  [ 23.18  23.15  23.15 ...,  21.1   21.1   21.1 ]\n",
      "hu:  [ 27.272   27.2675  27.245  ...,  36.095   36.26    36.2   ]\n",
      "lt:  [ 426.   429.5  426.  ...,  433.   433.   447. ]\n",
      "co2:  [ 721.25        714.          713.5        ...,  798.5         820.33333333\n",
      "  821.        ]\n",
      "hu_r:  [ 0.00479299  0.00478344  0.00477946 ...,  0.00559564  0.00562145\n",
      "  0.00561206]\n",
      "o:  [ 1.  1.  1. ...,  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "\n",
    "Note that input layer is noted as layer0, hidden layer is noted as layer1, and output layer is noted is layer2\n",
    "'''\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = np.genfromtxt('../data/assign2_train_data.txt', delimiter=',')\n",
    "#print \"train_data:\", train_data\n",
    "\n",
    "t = train_data[:,2]\n",
    "hu = train_data[:,3]\n",
    "lt = train_data[:,4]\n",
    "co2 = train_data[:,5]\n",
    "hu_r = train_data[:,6]\n",
    "\n",
    "o = train_data[:,7]\n",
    "\n",
    "#data\n",
    "data = np.column_stack((t, hu, lt, co2, hu_r))\n",
    "print \"data:\", data\n",
    "\n",
    "print \"np.shape(train_data): \", np.shape(train_data)\n",
    "#print \"np.shape(t): \", np.shape(t)\n",
    "\n",
    "print \"t: \", t\n",
    "print \"hu: \", hu\n",
    "print \"lt: \", lt\n",
    "print \"co2: \", co2\n",
    "print \"hu_r: \", hu_r\n",
    "print \"o: \", o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    #normalize train data\n",
    "    #normalize t\n",
    "    t = data[:, 0]\n",
    "    t_min = np.min(t)\n",
    "    t_max = np.max(t)\n",
    "    d_t = t_max - t_min\n",
    "    #normalized t: n_t\n",
    "    n_t = []\n",
    "    for each in t:\n",
    "        n_t.append((each - t_min) / d_t)\n",
    "\n",
    "    #normalize hu\n",
    "    hu = data[:, 1]\n",
    "    hu_min = np.min(hu)\n",
    "    hu_max = np.max(hu)\n",
    "    d_hu = hu_max - hu_min\n",
    "    #normalized h: n_hu\n",
    "    n_hu = []\n",
    "    for each in hu:\n",
    "        n_hu.append((each - hu_min) / d_hu)\n",
    "\n",
    "    #normalize lt\n",
    "    lt = data[:, 2]\n",
    "    lt_min = np.min(lt)\n",
    "    lt_max = np.max(lt)\n",
    "    d_lt = lt_max - lt_min\n",
    "    #normalized lt: n_lt\n",
    "    n_lt = []\n",
    "    for each in lt:\n",
    "        n_lt.append((each - lt_min) / d_lt)\n",
    "\n",
    "    #normalize co2\n",
    "    co2 = data[:, 3]\n",
    "    co2_min = np.min(co2)\n",
    "    co2_max = np.max(co2)\n",
    "    d_co2 = co2_max - co2_min\n",
    "    #normalized co2: n_co2\n",
    "    n_co2 = []\n",
    "    for each in co2:\n",
    "        n_co2.append((each - co2_min) / d_co2)\n",
    "\n",
    "    #normalize hu_r\n",
    "    hu_r = data[:, 4]\n",
    "    hu_r_min = np.min(hu_r)\n",
    "    hu_r_max = np.max(hu_r)\n",
    "    d_hu_r = hu_r_max - hu_r_min\n",
    "    #normalized hu_r: n_hu_r\n",
    "    n_hu_r = []\n",
    "    for each in hu_r:\n",
    "        n_hu_r.append((each - hu_r_min) / d_hu_r)\n",
    "\n",
    "    #normalized data: n_data\n",
    "    n_data = np.column_stack((n_t, n_hu, n_lt, n_co2, n_hu_r))\n",
    "    \n",
    "    return n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_data: [[ 1.          0.47053302  0.27549041  0.190933    0.55731842]\n",
      " [ 0.99282297  0.47033188  0.27775383  0.18644592  0.55480723]\n",
      " [ 0.99282297  0.46932618  0.27549041  0.18613647  0.55376106]\n",
      " ..., \n",
      " [ 0.50239234  0.86490111  0.28001725  0.23874362  0.76843752]\n",
      " [ 0.50239234  0.87227623  0.28001725  0.25225643  0.77522635]\n",
      " [ 0.50239234  0.86959437  0.28907092  0.25266904  0.77275762]]\n"
     ]
    }
   ],
   "source": [
    "n_data = normalize(data)\n",
    "print \"n_data:\", n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ws1:  [0.38901288849555493, 0.2338866188986185, 0.28059545059502233, 0.8459272759191067, 0.03442867197464117, 0.09524491716002914]\n"
     ]
    }
   ],
   "source": [
    "def initialize_weights1():\n",
    "    w_t = random.random()\n",
    "    w_hu = random.random()\n",
    "    w_lt = random.random()\n",
    "    w_co2 = random.random()\n",
    "    w_hu_r = random.random()\n",
    "    b = random.random()\n",
    "\n",
    "    #weights\n",
    "    ws1 = [w_t, w_hu, w_lt, w_co2, w_hu_r, b]\n",
    "    return ws1\n",
    "\n",
    "init_ws1 = initialize_weights1()\n",
    "ws1 = copy.deepcopy(init_ws1)\n",
    "print \"ws1: \", ws1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize random weights from layer with i neurons to layer with j neurons -- fully connected layer including bias\n",
    "# note that each row is the weights list from former layer to jth neuron in the latter layer\n",
    "def initialize_weights(i, j):\n",
    "    ws = np.random.rand(j, i + 1)\n",
    "    return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  8, 12])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test inner \n",
    "a = [1, 2, 3]\n",
    "b = [2, 0, 5]\n",
    "c = 4\n",
    "#np.inner(a, b)\n",
    "np.inner(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Reference: https://www.youtube.com/watch?v=GlcnxUlrtek\n",
    "def sigmoidPrime(x):\n",
    "    #Derivative of Sigmoid Function\n",
    "    return np.exp(-x) / ((1 + np.exp(-x) ** 2))\n",
    "\n",
    "def activation(x): # use sigmoid as activation function\n",
    "    a = sigmoid(x)\n",
    "    if a >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def perceptron(sum_of_inputs):\n",
    "    return activation(sum_of_inputs)\n",
    "\n",
    "# Reference: http://cs229.stanford.edu/notes/cs229-notes1.pdf\n",
    "# squared error\n",
    "def error_function(o, y):\n",
    "    return (o - y) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_data, o, lr, H, bs):\n",
    "    print \"Training starts: len of data:\", len(n_data)\n",
    "    \n",
    "    accuracies = []\n",
    "    epoch = 0\n",
    "    n_t = n_data[:, 0]\n",
    "    n_hu = n_data[:, 1]\n",
    "    n_lt = n_data[:, 2]\n",
    "    n_co2 = n_data[:, 3]\n",
    "    n_hu_r = n_data[:, 4]\n",
    "    n_b = np.ones(len(n_t))\n",
    "    \n",
    "    train_n_data = np.column_stack((n_t, n_hu, n_lt, n_co2, n_hu_r, n_b))\n",
    "    print \"n_t:\", n_t\n",
    "    print \"n_hu:\", n_hu\n",
    "    print \"n_lt:\", n_lt\n",
    "    print \"n_b:\", n_b\n",
    "    print \"train_n_data:\", train_n_data\n",
    "    \n",
    "    #ws0 are the weights from input layer to hidden layer\n",
    "    init_ws0 = initialize_weights(5, H)\n",
    "    ws0 = copy.deepcopy(init_ws0)\n",
    "    print \"ws0 before training:\", ws0\n",
    "\n",
    "    #ws1 are the weights from hidden layer to output layer\n",
    "    init_ws1 = initialize_weights(H, 1)\n",
    "    init_ws1 = init_ws1[0]\n",
    "    ws1 = copy.deepcopy(init_ws1)\n",
    "    print \"ws1 before training:\", ws1\n",
    "    \n",
    "    epoch = 1\n",
    "    \n",
    "    delta2 = 0 # delta from the output layer\n",
    "    delta1 = [] # deltas from hidden layer nodes\n",
    "    while epoch <= 10000:\n",
    "        \n",
    "        i_data = 0 # index in the traning data list\n",
    "        i_bs = 0 # index of batch size\n",
    "        err2 = 0 # output layer error\n",
    "        # one epoch\n",
    "        while i_data < len(n_t):\n",
    "            \n",
    "            # each hidden neuron\n",
    "            hidden_os = [] # hidden layer outputs\n",
    "            for i in range(H):\n",
    "                #print \"i_data:\",i_data\n",
    "                hidden_o_i = sigmoid(np.inner(ws0[i], train_n_data[i_data])) # one output of one hidden layer neuron\n",
    "                hidden_os.append(hidden_o_i)\n",
    "\n",
    "            hidden_os.append(1) # append bias\n",
    "            #print \"hidden_os:\", hidden_os\n",
    "            \n",
    "            sum_to_output_layer = np.inner(hidden_os, ws1) # output layer output\n",
    "            net_o = sigmoid(sum_to_output_layer)\n",
    "            # print \"net_o:\", net_o\n",
    "            \n",
    "            err2 += error_function(net_o, o[i_data]) # output layer error\n",
    "            # print \"err2: \", err2\n",
    "            \n",
    "            # only update weights after bs batch_size\n",
    "            # print \"o:\",o\n",
    "            if i_bs >= bs:\n",
    "                ### update ws1: weights from hidden layer to output layer\n",
    "                # Derivative of err2 with respect to ws1\n",
    "                # Reference: https://www.youtube.com/watch?v=zpykfC4VnpM, https://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf\n",
    "                # d_err2_ws1 = -(net_o - o[i_data]) * sigmoidPrime(net_o) * hidden_os\n",
    "                delta2 = net_o * (1 - net_o) * (net_o - o[i_data])# output layers delta \n",
    "                #ws1 += ws1 + lr * err2 * hidden_os\n",
    "                # print \"delta2:\", delta2\n",
    "                \n",
    "                delta_ws1 = []\n",
    "                for i_hidden_node in range(H):\n",
    "                    # print \"hidden_os[i_hidden_node]:\", hidden_os[i_hidden_node]\n",
    "                    delta1.append(hidden_os[i_hidden_node] * \n",
    "                                  (1 - hidden_os[i_hidden_node]) * \n",
    "                                  delta2 * \n",
    "                                  ws1[i_hidden_node])\n",
    "                    #print \"delta1: \", delta1\n",
    "                    \n",
    "                    delta_ws1.append(-lr * delta2 * hidden_os[i_hidden_node])\n",
    "                \n",
    "                # append bias delta\n",
    "                delta_b1 = -lr * delta2\n",
    "                delta_ws1.append(delta_b1)\n",
    "                # print \"delta_ws1: \", delta_ws1\n",
    "                \n",
    "                # print \"ws1 before updated: \", ws1\n",
    "                # print \"delta_ws1: \", delta_ws1\n",
    "                # update ws1 - weights from hidden layer to output layer\n",
    "                for ii in range(len(ws1)):\n",
    "                    # print \"ws1[ii]: \", ws1[ii]\n",
    "                    # print \"delta_ws1[ii]:\" , delta_ws1[ii]\n",
    "                    ws1[ii] += delta_ws1[ii]\n",
    "                    #ws1[ii] += 100\n",
    "                # print \"ws1 after  updated: \", ws1\n",
    "                # print \"   \"\n",
    "                ### end of updating ws1\n",
    "                \n",
    "                ### update ws0: weights from input layer to hidden layer\n",
    "                delta1 = []\n",
    "                ins = train_n_data[i_data] # one input layer\n",
    "                # print \"ins: \", ins\n",
    "                for ii in range(len(ins)):\n",
    "                    sum_of_delta2_ws0 = 0\n",
    "                    ws0_input_node_i_to_all_hidden_nodes = ws0[:, ii]\n",
    "                    for jj in range(len(ws0_input_node_i_to_all_hidden_nodes)):\n",
    "                        sum_of_delta2_ws0 += delta2 * ws0_input_node_i_to_all_hidden_nodes[jj]\n",
    "                    deltaii = ins[ii] * (1 - ins[ii]) * sum_of_delta2_ws0\n",
    "                    # print \"deltajj:\", deltajj\n",
    "                    delta1.append(deltaii)\n",
    "                \n",
    "                # print \"delta1: \", delta1\n",
    "                # print \"len(ins):\" , len(ins)\n",
    "                # print \"len(delta1):\", len(delta1)\n",
    "                    \n",
    "                ## update ws0 specifically\n",
    "                # print \"ws0 before updated: \", ws0\n",
    "                for ii in range(len(ws0)): # each row is the list of weights from all inputs layer to one hidden layer\n",
    "                    for jj in range(len(ws0[0])):\n",
    "                        delta_w = -lr * delta1[jj] * ins[jj]\n",
    "                        ws0[ii][jj] += delta_w\n",
    "                # print \"ws0 after  updated: \", ws0\n",
    "                # print \"     \"\n",
    "                ## end of updating ws0 specifically\n",
    "                ### end of updating ws0\n",
    "                \n",
    "                delta2 = 0\n",
    "                err2 = 0\n",
    "                i_bs = -1\n",
    "                \n",
    "            hidden_os = []\n",
    "            i_data += 1\n",
    "            i_bs += 1\n",
    "        \n",
    "        \n",
    "        #test every epoch\n",
    "        accuracy = get_one_feedforward_accuracy(train_n_data, H, ws0, ws1, o)\n",
    "        accuracies.append(accuracy)\n",
    "        #if epoch % 100 == 0:\n",
    "        print \"epoch:\", epoch, \" accuracy:\",accuracy\n",
    "        \n",
    "        epoch += 1\n",
    "    \n",
    "    \n",
    "    print \"Training ends.\"\n",
    "    print \"Initial input to hidden layer weights:\", init_ws0\n",
    "    print \"Trained input to hidden layer weights:\", ws0\n",
    "    print \"Initial hidden to output layer weights:\", init_ws1\n",
    "    print \"Trained hidden to output layer weights:\", ws1\n",
    "    plot_accuracy(accuracies, lr, bs, stage=\"train\")\n",
    "       \n",
    "def test(test_data, o, lr, bs):\n",
    "    print \"Testing starts: len of data:\", len(test_data)\n",
    "    #normalize test data\n",
    "    #normalize t\n",
    "    n_test_data = normalize(test_data)\n",
    "    n_t = n_test_data[:, 0]\n",
    "    n_hu = n_test_data[:, 1]\n",
    "    n_lt = n_test_data[:, 2]\n",
    "    n_co2 = n_test_data[:, 3]\n",
    "    n_hu_r = n_test_data[:, 4]\n",
    "    accuracy = get_one_feedforward_accuracy(n_test_data, ws, o)\n",
    "    print \"Test data accuracy:\",accuracy\n",
    "        \n",
    "def get_one_feedforward_accuracy(n_data, H, ws0, ws1, o):\n",
    "    \n",
    "    # print \"ws0: \", ws0\n",
    "    # print \"ws1: \", ws1\n",
    "    # print \"n_data: \", n_data\n",
    "    #calculate accuracy\n",
    "    \n",
    "    os = [] #outputs of output layer\n",
    "    \n",
    "    for each in zip(n_data): \n",
    "        for ins in each: # each sample\n",
    "            # print \"ins: \", ins\n",
    "            hidden_os = [] # hidden layer outputs\n",
    "            for i_hidden in range(H):\n",
    "                sum_input = np.inner(ins, ws0[i_hidden])\n",
    "                output = sigmoid(sum_input)\n",
    "                hidden_os.append(output)\n",
    "            hidden_os.append(1)\n",
    "            # print \"hidden_os: \", hidden_os\n",
    "            sum_input_to_output_layer = np.inner(hidden_os, ws1)\n",
    "            output = perceptron(sum_input_to_output_layer) # predict output of the whole neural network\n",
    "            os.append(output)\n",
    "       \n",
    "    # print \"os: \", os\n",
    "    # print \"o: \", o\n",
    "    #count correct output number\n",
    "    correct_cnt = 0\n",
    "    for o_i, os_i in zip(o, os):\n",
    "        if (o_i == os_i):\n",
    "            correct_cnt += 1\n",
    "    accuracy = float(correct_cnt) / float(len(o))\n",
    "    ##calculate accuracy\n",
    "    return accuracy\n",
    "    \n",
    "def plot_accuracy(accuracies, lr, bs, stage):\n",
    "    accs_y = np.array(accuracies)\n",
    "    epochs = []\n",
    "    for i in range(len(accuracies)):\n",
    "        epochs.append(i)\n",
    "    epochs_x = np.array(epochs)\n",
    "    plt.plot(epochs_x, accs_y)\n",
    "    title = stage\n",
    "    title += ' lr:'\n",
    "    title += str(lr)\n",
    "    title += ' batch_size:'\n",
    "    title += str(bs)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts: len of data: 8143\n",
      "n_t: [ 1.          0.99282297  0.99282297 ...,  0.50239234  0.50239234\n",
      "  0.50239234]\n",
      "n_hu: [ 0.47053302  0.47033188  0.46932618 ...,  0.86490111  0.87227623\n",
      "  0.86959437]\n",
      "n_lt: [ 0.27549041  0.27775383  0.27549041 ...,  0.28001725  0.28001725\n",
      "  0.28907092]\n",
      "n_b: [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "train_n_data: [[ 1.          0.47053302  0.27549041  0.190933    0.55731842  1.        ]\n",
      " [ 0.99282297  0.47033188  0.27775383  0.18644592  0.55480723  1.        ]\n",
      " [ 0.99282297  0.46932618  0.27549041  0.18613647  0.55376106  1.        ]\n",
      " ..., \n",
      " [ 0.50239234  0.86490111  0.28001725  0.23874362  0.76843752  1.        ]\n",
      " [ 0.50239234  0.87227623  0.28001725  0.25225643  0.77522635  1.        ]\n",
      " [ 0.50239234  0.86959437  0.28907092  0.25266904  0.77275762  1.        ]]\n",
      "ws0 before training: [[ 0.79716328  0.97672096  0.05107952  0.32681194  0.64675715  0.59785209]\n",
      " [ 0.43381932  0.00633879  0.37022038  0.99304956  0.52310838  0.42307654]\n",
      " [ 0.04342842  0.15749552  0.24517236  0.83027736  0.35712635  0.65417334]]\n",
      "ws1 before training: [ 0.35051055  0.06163708  0.93139863  0.039582  ]\n",
      "epoch: 1  accuracy: 0.787670391748\n",
      "epoch: 2  accuracy: 0.787670391748\n",
      "epoch: 3  accuracy: 0.787670391748\n",
      "epoch: 4  accuracy: 0.787670391748\n",
      "epoch: 5  accuracy: 0.787670391748\n",
      "epoch: 6  accuracy: 0.787670391748\n",
      "epoch: 7  accuracy: 0.787670391748\n",
      "epoch: 8  accuracy: 0.787670391748\n",
      "epoch: 9  accuracy: 0.787670391748\n",
      "epoch: 10  accuracy: 0.787670391748\n",
      "epoch: 11  accuracy: 0.787670391748\n",
      "epoch: 12  accuracy: 0.787670391748\n",
      "epoch: 13  accuracy: 0.787670391748\n",
      "epoch: 14  accuracy: 0.787670391748\n",
      "epoch: 15  accuracy: 0.787670391748\n",
      "epoch: 16  accuracy: 0.787670391748\n",
      "epoch: 17  accuracy: 0.787670391748\n",
      "epoch: 18  accuracy: 0.787670391748\n",
      "epoch: 19  accuracy: 0.787670391748\n",
      "epoch: 20  accuracy: 0.787670391748\n",
      "epoch: 21  accuracy: 0.787670391748\n",
      "epoch: 22  accuracy: 0.787670391748\n",
      "epoch: 23  accuracy: 0.787670391748\n",
      "epoch: 24  accuracy: 0.787670391748\n",
      "epoch: 25  accuracy: 0.787670391748\n",
      "epoch: 26  accuracy: 0.787670391748\n",
      "epoch: 27  accuracy: 0.787670391748\n",
      "epoch: 28  accuracy: 0.787670391748\n",
      "epoch: 29  accuracy: 0.787670391748\n",
      "epoch: 30  accuracy: 0.787670391748\n",
      "epoch: 31  accuracy: 0.787670391748\n",
      "epoch: 32  accuracy: 0.787670391748\n",
      "epoch: 33  accuracy: 0.787670391748\n",
      "epoch: 34  accuracy: 0.787670391748\n",
      "epoch: 35  accuracy: 0.787670391748\n",
      "epoch: 36  accuracy: 0.787670391748\n",
      "epoch: 37  accuracy: 0.787670391748\n",
      "epoch: 38  accuracy: 0.787670391748\n",
      "epoch: 39  accuracy: 0.787670391748\n",
      "epoch: 40  accuracy: 0.787670391748\n",
      "epoch: 41  accuracy: 0.787670391748\n",
      "epoch: 42  accuracy: 0.787670391748\n",
      "epoch: 43  accuracy: 0.787670391748\n",
      "epoch: 44  accuracy: 0.787670391748\n",
      "epoch: 45  accuracy: 0.787670391748\n",
      "epoch: 46  accuracy: 0.787670391748\n",
      "epoch: 47  accuracy: 0.787670391748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-26c0c5b938e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-40b8e563d53e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_data, o, lr, H, bs)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m#print \"i_data:\",i_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mhidden_o_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_n_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# one output of one hidden layer neuron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mhidden_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_o_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1f7823baeac2>\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Reference: https://www.youtube.com/watch?v=GlcnxUlrtek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoidPrime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(n_data, o, lr = 0.01, H = 3, bs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = np.genfromtxt('../data/assign2_test_data.txt', delimiter=',')\n",
    "# print \"test_data:\", test_data\n",
    "test(test_data, o, lr, bs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
