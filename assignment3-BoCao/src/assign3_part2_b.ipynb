{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gina-husband-Emilio', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      " \n",
      "['Lucia-husband-Marco', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      " \n",
      "Gina-husband-Emilio\n",
      " \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#Reference: https://www.cs.colorado.edu/~mozer/Teaching/syllabi/DeepLearningFall2017/assignments/family_trees/create_dataset.py\n",
    "import numpy as np\n",
    "\n",
    "def familytree():\n",
    "    def bitvec(ix,nbit):\n",
    "        out = []\n",
    "        for i in range(nbit):\n",
    "            out.append((i==ix)+0.0)\n",
    "        return np.array(out)\n",
    "\n",
    "    names = [ \"Christopher\", \"Andrew\", \"Arthur\", \"James\", \"Charles\", \"Colin\", \"Penelope\", \"Christine\", \"Victoria\", \"Jennifer\", \"Margaret\", \"Charlotte\", \"Roberto\", \"Pierro\", \"Emilio\", \"Marco\", \"Tomaso\", \"Alfonso\", \"Maria\", \"Francesca\", \"Lucia\", \"Angela\", \"Gina\", \"Sophia\"]\n",
    "    relations = [ \"husband\", \"wife\", \"son\", \"daughter\", \"father\", \"mother\", \"brother\", \"sister\", \"nephew\", \"niece\", \"uncle\", \"aunt\"]\n",
    "\n",
    "    dataset = []\n",
    "    with open('relations.txt','r') as f:\n",
    "        for line in f:\n",
    "            sline = line.split();\n",
    "            p1 = names.index(sline[0])\n",
    "            r = relations.index(sline[1])\n",
    "            p2 = names.index(sline[2])\n",
    "            d = [ sline[0]+'-'+sline[1]+'-'+sline[2], \n",
    "                  np.concatenate((bitvec(p1,len(names)),bitvec(r,len(relations)))),\n",
    "                  bitvec(p2,len(names)) ]\n",
    "                  #bitvec(p2,len(names)) ]\n",
    "            dataset.append(d)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def familytree():\n",
    "    def bitvec(ix,nbit):\n",
    "        out = []\n",
    "        for i in range(nbit):\n",
    "            out.append((i==ix)+0.0)\n",
    "        return np.array(out)\n",
    "\n",
    "    names = [ \"Christopher\", \"Andrew\", \"Arthur\", \"James\", \"Charles\", \"Colin\", \"Penelope\", \"Christine\", \"Victoria\", \"Jennifer\", \"Margaret\", \"Charlotte\", \"Roberto\", \"Pierro\", \"Emilio\", \"Marco\", \"Tomaso\", \"Alfonso\", \"Maria\", \"Francesca\", \"Lucia\", \"Angela\", \"Gina\", \"Sophia\"]\n",
    "    relations = [ \"husband\", \"wife\", \"son\", \"daughter\", \"father\", \"mother\", \"brother\", \"sister\", \"nephew\", \"niece\", \"uncle\", \"aunt\"]\n",
    "\n",
    "    dataset = []\n",
    "    with open('relations.txt','r') as f:\n",
    "        for line in f:\n",
    "            sline = line.split();\n",
    "            p1 = names.index(sline[0])\n",
    "            r = relations.index(sline[1])\n",
    "            p2 = names.index(sline[2])\n",
    "            d = [ sline[0]+'-'+sline[1]+'-'+sline[2], \n",
    "                  np.concatenate((bitvec(p1,len(names)),bitvec(r,len(relations)))),\n",
    "                  bitvec(p2,len(names)) ]\n",
    "                  #bitvec(p2,len(names)) ]\n",
    "            dataset.append(d)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = familytree()\n",
    "\n",
    "print (dataset[0])\n",
    "print (\" \")\n",
    "print (dataset[1])\n",
    "print (\" \")\n",
    "print (dataset[0][0])\n",
    "print (\" \")\n",
    "print (dataset[0][1])\n",
    "print (\" \")\n",
    "print (dataset[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gina-husband-Emilio', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      " \n",
      "['Lucia-husband-Marco', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      " \n",
      "Gina-husband-Emilio\n",
      " \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print (dataset[0])\n",
    "print (\" \")\n",
    "print (dataset[1])\n",
    "print (\" \")\n",
    "print (dataset[0][0])\n",
    "print (\" \")\n",
    "print (dataset[0][1])\n",
    "print (\" \")\n",
    "print (dataset[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_Xa[1]:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.]\n",
      "train_data_Xb[1]:  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "train_data_Y[1]:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n",
      "len(train_data_Y):  104\n",
      "len(train_data_Y[0]):  24\n",
      "train_data_labels:  [14, 15, 16, 12, 13, 20, 22, 21, 18, 19, 20, 20, 18, 18, 19, 19, 15, 15, 12, 12, 13, 13, 17, 14, 15, 23, 20, 21, 14, 14, 15, 15, 17, 17, 20, 20, 21, 21, 23, 23, 14, 14, 21, 21, 23, 23, 23, 23, 17, 17, 17, 17, 2, 3, 4, 0, 1, 8, 10, 9, 6, 7, 8, 8, 6, 6, 7, 7, 3, 3, 0, 0, 1, 1, 5, 2, 3, 11, 8, 9, 2, 2, 3, 3, 5, 5, 8, 8, 9, 9, 11, 11, 2, 2, 9, 9, 11, 11, 11, 11, 5, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset\n",
    "# get train input and output data\n",
    "train_data_Xa = []\n",
    "train_data_Xb = []\n",
    "train_data_Y = []\n",
    "train_data_labels = []\n",
    "for i in range(len(train_data)):\n",
    "    train_data_Xa.append(train_data[i][1][0:24])\n",
    "    train_data_Xb.append(train_data[i][1][24:])\n",
    "    train_data_Y.append(train_data[i][2])\n",
    "    for j in range(len(train_data[i][2])):\n",
    "        if (train_data[i][2][j] == 1):\n",
    "            train_data_labels.append(j)\n",
    "    \n",
    "print (\"train_data_Xa[1]: \", train_data_Xa[1])\n",
    "print (\"train_data_Xb[1]: \", train_data_Xb[1])\n",
    "print (\"train_data_Y[1]: \", train_data_Y[1])\n",
    "print (\"len(train_data_Y): \", len(train_data_Y))\n",
    "print (\"len(train_data_Y[0]): \", len(train_data_Y[0]))\n",
    "print (\"train_data_labels: \", train_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsXa_H1a: <tf.Variable 'Variable_8:0' shape=(24, 6) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_8:0' shape=(24, 6) dtype=float32_ref>\n",
      "Tensor(\"Relu_4:0\", shape=(?, 6), dtype=float32)\n",
      "Tensor(\"concat_2:0\", shape=(?, 12), dtype=float32)\n",
      "hy:  Tensor(\"Sigmoid_5:0\", shape=(?, 24), dtype=float32)\n",
      "softmax_hy: Tensor(\"Softmax_2:0\", shape=(?, 24), dtype=float32)\n",
      "train_data_labels: [14, 15, 16, 12, 13, 20, 22, 21, 18, 19, 20, 20, 18, 18, 19, 19, 15, 15, 12, 12, 13, 13, 17, 14, 15, 23, 20, 21, 14, 14, 15, 15, 17, 17, 20, 20, 21, 21, 23, 23, 14, 14, 21, 21, 23, 23, 23, 23, 17, 17, 17, 17, 2, 3, 4, 0, 1, 8, 10, 9, 6, 7, 8, 8, 6, 6, 7, 7, 3, 3, 0, 0, 1, 1, 5, 2, 3, 11, 8, 9, 2, 2, 3, 3, 5, 5, 8, 8, 9, 9, 11, 11, 2, 2, 9, 9, 11, 11, 11, 11, 5, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "# build the network architecture\n",
    "lr = 0.05\n",
    "epochs = 10000\n",
    "n_inputa = 24\n",
    "n_inputb = 12\n",
    "n_H1a = 6\n",
    "n_H1b = 6\n",
    "n_H2 = 12\n",
    "n_H3 = 6\n",
    "n_output = 24\n",
    "\n",
    "Xa = tf.placeholder(tf.float32)\n",
    "Xb = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.int32)\n",
    "\n",
    "wsXa_H1a = tf.Variable(tf.random_uniform([n_inputa, n_H1a], -3.0, 3.0))\n",
    "wsXb_H1b = tf.Variable(tf.random_uniform([n_inputb, n_H1b], -3.0, 3.0))\n",
    "print (\"wsXa_H1a:\", wsXa_H1a)\n",
    "wsH2_H3 = tf.Variable(tf.random_uniform([n_H2, n_H3], -3.0, 3.0))\n",
    "wsH3_Y = tf.Variable(tf.random_uniform([n_H3, n_output], -3.0, 3.0))\n",
    "\n",
    "#H1a_outputs = tf.sigmoid(tf.matmul(Xa, wsXa_H1a))\n",
    "H1a_outputs = tf.nn.relu(tf.matmul(Xa, wsXa_H1a))\n",
    "#H1b_outputs = tf.sigmoid(tf.matmul(Xb, wsXb_H1b))\n",
    "H1b_outputs = tf.nn.relu(tf.matmul(Xb, wsXb_H1b))\n",
    "H2_outputs = tf.concat([H1a_outputs, H1b_outputs], 1)\n",
    "print (wsXa_H1a)\n",
    "print (H1a_outputs)\n",
    "print (H2_outputs)\n",
    "H3_outputs = tf.sigmoid(tf.matmul(H2_outputs, wsH2_H3))\n",
    "hy = tf.sigmoid(tf.matmul(H3_outputs, wsH3_Y))\n",
    "print (\"hy: \",hy)\n",
    "#train_data_hy_one = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = train_data_labels, logits = Y)\n",
    "#train_data_hy_one = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = train_data_labels, logits = train_data_Y)\n",
    "softmax_hy = tf.nn.softmax(hy)\n",
    "print (\"softmax_hy:\",softmax_hy)\n",
    "\n",
    "#cost = tf.reduce_mean(tf.square(Y - hy))\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(train_data_hy * tf.log(Y), reduction_indices=[1])) #cross entropy\n",
    "#cost = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = train_data_labels, logits = Y)\n",
    "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = train_data_labels, logits = hy)\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_data_labels, logits=Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "print (\"train_data_labels:\", train_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Step:  0\n",
      "tf.argmax(softmax_hy,1): [14  2  2 14  2 15  3 22 15 14 13 13  9  0 13 15 14 22 11 14 14 15  2  2  2\n",
      " 14 11 15 14 14 14 14 14 14 15 14 14 14 14 15 11 11  2 14 11 14 14 11 14 14\n",
      " 14 11 11 14  2  2 14 22 22 22 22 22  2  2 23  5  2 23 15 15 15 14 15 11 15\n",
      " 14 15 15 11 11 11 14 14 11 14 11 22 13 22 22 14 22 11 11  2  2 11 14 11 11\n",
      " 22 15 11 14]\n",
      "Accuracy:  0.0576923\n",
      " \n",
      "Step:  1000\n",
      "tf.argmax(softmax_hy,1): [23 23 23 23 23 20 12 21 20 15  2  2  2  2 13  7 15  9 12 23 23 21 23 23  2\n",
      " 23 23 21 23 23 15 15 23 23 23 23 15 21 23 23 23 23 23 23 23 23 21 21 23 23\n",
      " 23 21  2 23 23 23 23  8 21  9  9 12 15 15 13 13  7 13 11 11 21 12 21 21 15\n",
      " 23 23 11 21  9 15 15 23 23 23 21  8  8  9  9  9  9 15 15 15 15 21 21 11  5\n",
      " 21 21 11 21]\n",
      "Accuracy:  0.230769\n",
      " \n",
      "Step:  2000\n",
      "tf.argmax(softmax_hy,1): [23 23 23 23 23 20 12 21 20 15  2  2  2  2 15  7 21 11 12 23 23 21 23 23  2\n",
      " 23  3 23 23 23 15 15 23 23 20 20 21 21 23 23 23 23 23 23  3 23  5  5  3 23\n",
      " 23 21  2 23 23 23 23  8 21  9  9  7 15 15  7 15  7 15  5  5 21 21 21 21 15\n",
      " 23 23  5 21 11 15 15 23 23 23  5  8  8  8  8 11  9 15 15 15 15 21  5  5  5\n",
      " 21 21  5  5]\n",
      "Accuracy:  0.230769\n",
      " \n",
      "Step:  3000\n",
      "tf.argmax(softmax_hy,1): [23 23 23 23 23 20 22 21 20 15  2  2  2  2 19 23 21 11 12 23 23 21 23 23  2\n",
      " 23  3 23 23 23 15 15 23 23 20 20 21 21 23 23 23 23 23 23  3 23  3  5  3 23\n",
      " 23 21  2 23 23 23 23  8 21  9  9  7  8  8  7 15 23 15  5  5 21 21 21 11 15\n",
      " 23 23  5 21 11 15 15 23 23 23  5  8  8  8  8 11 11 15 15 15 15 21  5  5  5\n",
      " 21 21  5  5]\n",
      "Accuracy:  0.269231\n",
      " \n",
      "Step:  4000\n",
      "tf.argmax(softmax_hy,1): [23 23 23 23 23 20 22 21 20 15 20 20  2 20  7 23 21 15 12 23 23 21 23 23  2\n",
      " 23  3 23 23 23 15 15 23 23 20 20 21 21 23 23 23 23 23 23  3 23  3  5  3 23\n",
      " 23 21  2 23 23 23 23  9 21  9  9  7  8  8  7  7 23 15  5  5 21 21 21 15 15\n",
      " 23 23  5 21 15 15  2 23 23 23  5  9  8  8  8 15 15 15 15 15 15 21  5  5  5\n",
      " 21 21  5  5]\n",
      "Accuracy:  0.259615\n",
      " \n",
      "Step:  5000\n",
      "tf.argmax(softmax_hy,1): [23 23 23 23 23 20 22 21 20  6 20 20 20 20  7 23 21 15 12 23 23 21 23 23 23\n",
      " 23  3 23 23 23 15 15 23 21 20 20 21 21 23 23 23 23 23 23  3 23  3  5  3 23\n",
      " 23 21  2 23 23 23 23  9 21  9  9  7  8  8  7  6 23  6  5  5 21 21 21 15 15\n",
      "  3 23  5 21 15 15  2 23 23 23  5  9  8  8  8 15 15 15 15 15 15 21  5  5  5\n",
      " 21 21  5  5]\n",
      "Accuracy:  0.269231\n",
      " \n",
      "Step:  6000\n",
      "tf.argmax(softmax_hy,1): [23 23 23 23 23 20 22 21 20  6 20 20 20 20  7 23 21 15 12 23 23 21 23 23 23\n",
      " 23  3 23 23 23 15 15 23 21 20 20 21 21 23 23 23 23 23 23  3 23  3  5  3 23\n",
      " 23 21  2 23 23 23 23  9 21  9  9  7  8  8  7  6 23  6  5  5 21 21 21 15 15\n",
      "  3 23  5 21 15 11  2 21 21 23  5  9  8  8  8 15 15 11 11 11 11 21  5 11  5\n",
      " 21 21  5  5]\n",
      "Accuracy:  0.278846\n",
      " \n",
      "Step:  7000\n",
      "tf.argmax(softmax_hy,1): [23 23 23 23 23 20 22 21 20  6 20 20 20 20 19 23 21 15 12 23 23 21 23 23 23\n",
      " 23  3 23 23 23 15 15 23 21 20 20 21 21 23 23 23 23 23 23  3 23  3  5  3 23\n",
      " 23 21  2 23 23 23 23  9 21  9  9  7  8  8  7  6 23  6  5  5 21 21 21 15 11\n",
      "  3 23  5 21 15 11  2 21 21 23  5  9  8  8  8 15 15 11 11 11 11 21  5 11  5\n",
      " 21 21  5  5]\n",
      "Accuracy:  0.288462\n",
      " \n",
      "Step:  8000\n",
      "tf.argmax(softmax_hy,1): [23 23 23 23 23 20 22 21 20  6 20 20 20 20 19 23 21 15 12 23 23 21 23 23 23\n",
      " 23  3 23 23 23 15 15 23 21 20 20 21 21 23 23 23 23 23 23  3 23  3  5  3 23\n",
      " 23 21  2 23 23 23 23  9 21  9  9  7  8  8 19  6 23  6  5  5 21 21 21 15 11\n",
      "  3 23  5 21 15 11 15 21 21 23  5  9  8  8  8 15 15 11 11 11 11 21  5 11  5\n",
      " 21 21  5  5]\n",
      "Accuracy:  0.278846\n",
      " \n",
      "Step:  9000\n",
      "tf.argmax(softmax_hy,1): [23 23 23 23 23 20 22 21 20  6 20 20 20 20 19 23 21 15 12 23 23 21 23 23 23\n",
      " 23  3 23 23 23 15 15 23 21 20 20 21 21 23 23 23 23 23 23  3 23  3  5  3 23\n",
      " 23 21  2 23 23 23 23  9 21  9  9  7  8  8 19  6 23  6  5  5 21 21 21 15 11\n",
      "  3 23  5 21 15 11 15 21 21 23  5  9  8  8  8 15 15 11 11 11 11 21  5 11  5\n",
      " 21 21  5  5]\n",
      "Accuracy:  0.278846\n",
      "[array([[-10.27754116,  -1.62678552,  -8.86935711,   9.94894218,\n",
      "         -9.21721935,   1.62418604, -11.53180885,  -2.15685534,\n",
      "         -6.65289593,  -9.63973999,   1.99142528,  -9.07083607,\n",
      "          5.18935966,  -7.5057621 ,   9.26380825, -16.16000748,\n",
      "         -1.83092916,   7.83560514,  -0.53880775,  -8.62965298,\n",
      "          4.94722891,   6.54393673,   1.57269943,   8.24590683],\n",
      "       [ -1.27988601,  -2.8622508 ,  -6.71959734,  -5.44499779,\n",
      "         -3.61663032,   9.58339214,  -1.24823439,  -5.23977804,\n",
      "         15.54936028,   8.29474735,  -1.22349   ,   9.41481972,\n",
      "        -15.41916466,  -8.30851078, -16.61982918,   4.15389538,\n",
      "          0.18449405,  -9.56213379, -10.32512188,  -4.12505054,\n",
      "         -7.4706955 ,   9.93349075,  -9.01734638, -11.60669994],\n",
      "       [-11.15537548,  -2.84076691,   8.33649063,  -6.03917313,\n",
      "         -7.17661953,  -8.1163311 ,  -4.14027691,  -7.1603179 ,\n",
      "         -8.3387289 ,   2.16852474,  -9.60168171,  -0.33608603,\n",
      "         -9.01073647, -10.21110535,  -2.59382057,  -1.3517797 ,\n",
      "         -3.40710402,  -7.74793673,   6.84469318,  -7.51471615,\n",
      "          9.10516167, -17.42225075,   0.54748935,  -8.88625717],\n",
      "       [  7.99026871,  -2.70877695,   3.82583618,   5.36703491,\n",
      "          5.01570892,   3.26368999,  -4.39678955, -12.4212532 ,\n",
      "        -11.18405914,  -4.82663345,  -7.31614399,   4.78107834,\n",
      "          2.1453588 ,   5.3642292 ,  -3.06538773,   9.11554241,\n",
      "         -4.20304823,   6.75968361,  -9.31573486,  -8.84244919,\n",
      "        -11.18697357,  -3.09254503,  -6.7856617 ,   9.44600105],\n",
      "       [  9.85691833,  -2.77349854,  -9.18763542, -12.74036026,\n",
      "         -2.9716363 , -14.35976601,  10.72733784,   7.90800953,\n",
      "          6.38679695,   7.06065416,   4.98178387,  -0.98602772,\n",
      "          6.0403924 ,   7.75972605,  -9.14033508,   9.94797516,\n",
      "         -3.05074072, -15.34295273,  -8.95787716,   7.20490122,\n",
      "        -14.13453484,   5.529953  ,   1.75202584, -10.55196476],\n",
      "       [ -4.30945873,  -3.4088974 , -11.49641323, -12.67574883,\n",
      "         -1.59694779,  -9.38441849,  -0.12188659,   4.86839199,\n",
      "         -2.43205309, -14.11803532,  -3.9845829 , -15.97771645,\n",
      "         -4.43948507,  -1.8023988 ,  -0.85929781,  -3.51801515,\n",
      "         -3.63590074,  -0.89304304,  -2.03371263,   5.53602123,\n",
      "         -3.82858658,   1.03481436,  -7.2883625 ,   6.22205067]], dtype=float32)]\n",
      "Accuracy:  0.269231\n",
      "wsXa_H1a:  [[  2.01871777e+00  -2.09621143e+00  -2.01648474e+00  -1.40987492e+00\n",
      "   -9.46328975e-04  -1.40263867e+00]\n",
      " [ -1.02389789e+00  -5.55059910e-02  -4.91580984e-04  -1.46641088e+00\n",
      "    3.13333464e+00  -2.78523445e+00]\n",
      " [ -1.65903497e+00  -1.96835589e+00   3.41642427e+00  -2.59314322e+00\n",
      "    3.84362936e+00  -8.24267149e-01]\n",
      " [ -2.93328738e+00  -2.29739928e+00   3.21980357e+00  -3.83868456e-01\n",
      "   -4.15938266e-05  -9.11992550e-01]\n",
      " [ -5.40867090e-01  -1.03136802e+00  -1.21770215e+00  -1.20041800e+00\n",
      "    1.52684703e-01  -1.35525799e+00]\n",
      " [  3.09551597e+00  -2.56119490e+00  -1.46460271e+00   3.44507694e+00\n",
      "   -2.46536359e-03  -1.45940436e-02]\n",
      " [ -3.27782393e-01  -8.96236897e-01  -1.89475965e+00  -1.03011131e-02\n",
      "   -9.64500189e-01   1.83861351e+00]\n",
      " [ -2.51811504e+00  -4.12291288e-01  -6.97921515e-01  -2.03879070e+00\n",
      "    3.11082053e+00  -2.32374501e+00]\n",
      " [ -6.58090721e-05   3.12084556e+00   5.78459072e+00  -2.62100959e+00\n",
      "    1.21856439e+00  -2.89944641e-04]\n",
      " [ -2.96434816e-02  -2.75819778e-01  -2.88449693e+00   6.93166256e-01\n",
      "    2.94096971e+00   1.83995664e+00]\n",
      " [  4.35738420e+00   1.04022634e+00   2.15806150e+00  -1.00003338e+00\n",
      "    1.03519189e+00  -8.39558601e-01]\n",
      " [  3.76672840e+00  -5.48434909e-05  -1.91637540e+00   2.89448285e+00\n",
      "   -2.81391263e+00  -5.98746166e-03]\n",
      " [ -2.62117696e+00   3.75832486e+00  -1.54888519e-04   1.92840385e+00\n",
      "    2.96042371e+00  -6.68112421e-04]\n",
      " [  2.07325745e+00  -7.24247932e-01   2.15242648e+00  -2.67379045e+00\n",
      "   -7.89563656e-01   3.93866396e+00]\n",
      " [  1.71582234e+00   4.47789383e+00   2.46459889e+00  -2.25037837e+00\n",
      "   -7.79368401e-01  -1.90438771e+00]\n",
      " [ -2.06908321e+00   1.37459373e+00  -1.10179326e-03   4.52489567e+00\n",
      "    3.57026386e+00  -1.49486303e-01]\n",
      " [ -2.80346131e+00  -5.68147659e-01   3.00123882e+00   3.98601937e+00\n",
      "    2.95493221e+00  -4.66439009e-01]\n",
      " [ -1.19711208e+00   3.19230229e-01  -3.15638065e-01  -1.16541338e+00\n",
      "   -1.01658440e+00   2.77756858e+00]\n",
      " [ -6.85747713e-04   2.57658911e+00  -2.79648542e+00  -2.56282568e-01\n",
      "    5.56559515e+00  -7.81487543e-05]\n",
      " [ -2.58997178e+00  -1.21369171e+00   1.07333347e-01   1.47046018e+00\n",
      "   -3.11259270e-01   2.28871346e+00]\n",
      " [  2.93177438e+00   2.89144945e+00  -4.24842596e-01  -2.91079259e+00\n",
      "    2.69436240e+00   4.18566895e+00]\n",
      " [ -9.94219065e-01   1.15803659e+00  -5.07069111e-01  -4.17577744e-01\n",
      "   -6.82877541e-01   6.12102461e+00]\n",
      " [ -2.33261347e+00   3.00992632e+00  -2.52364898e+00  -2.53756857e+00\n",
      "    3.38109708e+00  -7.78442144e-01]\n",
      " [ -2.42170691e-03  -1.15841460e+00  -3.98834469e-04  -1.22067833e+00\n",
      "   -2.67324615e+00  -1.79841042e-01]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in xrange(epochs):\n",
    "        #print (\"train_data_hy[0]:\", sess.run(tf.shape(tf.argmax(Y, 1)), feed_dict={Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "        #train network in batch size = bs\n",
    "        sess.run(optimizer, feed_dict={Xa: train_data_Xa, \n",
    "                                       Xb: train_data_Xb, \n",
    "                                       Y: train_data_Y})\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print (\" \")\n",
    "            print (\"Step: \", step)\n",
    "            #print (\"Cost:\", sess.run(cost, feed_dict={Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "            print (\"tf.argmax(softmax_hy,1):\", sess.run(tf.argmax(softmax_hy,1), feed_dict={Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "            #print (\"softmax_hy:\", sess.run(softmax_hy, feed_dict={Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "            #print (\"hy:\", sess.run(hy, feed_dict={Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "            correct_prediction = tf.equal(tf.argmax(softmax_hy,1), train_data_labels)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            print (\"Accuracy: \", accuracy.eval({Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "                \n",
    "    correct_prediction = tf.equal(tf.argmax(softmax_hy,1), train_data_labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "    print (sess.run([wsH3_Y], feed_dict={Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "    print (\"Accuracy: \", accuracy.eval({Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "    print (\"wsXa_H1a: \", sess.run(wsXa_H1a, feed_dict={Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
