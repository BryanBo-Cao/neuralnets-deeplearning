{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gina-husband-Emilio', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      " \n",
      "['Lucia-husband-Marco', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      " \n",
      "Gina-husband-Emilio\n",
      " \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#Reference: https://www.cs.colorado.edu/~mozer/Teaching/syllabi/DeepLearningFall2017/assignments/family_trees/create_dataset.py\n",
    "import numpy as np\n",
    "\n",
    "def familytree():\n",
    "    def bitvec(ix,nbit):\n",
    "        out = []\n",
    "        for i in range(nbit):\n",
    "            out.append((i==ix)+0.0)\n",
    "        return np.array(out)\n",
    "\n",
    "    names = [ \"Christopher\", \"Andrew\", \"Arthur\", \"James\", \"Charles\", \"Colin\", \"Penelope\", \"Christine\", \"Victoria\", \"Jennifer\", \"Margaret\", \"Charlotte\", \"Roberto\", \"Pierro\", \"Emilio\", \"Marco\", \"Tomaso\", \"Alfonso\", \"Maria\", \"Francesca\", \"Lucia\", \"Angela\", \"Gina\", \"Sophia\"]\n",
    "    relations = [ \"husband\", \"wife\", \"son\", \"daughter\", \"father\", \"mother\", \"brother\", \"sister\", \"nephew\", \"niece\", \"uncle\", \"aunt\"]\n",
    "\n",
    "    dataset = []\n",
    "    with open('relations.txt','r') as f:\n",
    "        for line in f:\n",
    "            sline = line.split();\n",
    "            p1 = names.index(sline[0])\n",
    "            r = relations.index(sline[1])\n",
    "            p2 = names.index(sline[2])\n",
    "            d = [ sline[0]+'-'+sline[1]+'-'+sline[2], \n",
    "                  np.concatenate((bitvec(p1,len(names)),bitvec(r,len(relations)))),\n",
    "                  bitvec(p2,len(names)) ]\n",
    "                  #bitvec(p2,len(names)) ]\n",
    "            dataset.append(d)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def familytree():\n",
    "    def bitvec(ix,nbit):\n",
    "        out = []\n",
    "        for i in range(nbit):\n",
    "            out.append((i==ix)+0.0)\n",
    "        return np.array(out)\n",
    "\n",
    "    names = [ \"Christopher\", \"Andrew\", \"Arthur\", \"James\", \"Charles\", \"Colin\", \"Penelope\", \"Christine\", \"Victoria\", \"Jennifer\", \"Margaret\", \"Charlotte\", \"Roberto\", \"Pierro\", \"Emilio\", \"Marco\", \"Tomaso\", \"Alfonso\", \"Maria\", \"Francesca\", \"Lucia\", \"Angela\", \"Gina\", \"Sophia\"]\n",
    "    relations = [ \"husband\", \"wife\", \"son\", \"daughter\", \"father\", \"mother\", \"brother\", \"sister\", \"nephew\", \"niece\", \"uncle\", \"aunt\"]\n",
    "\n",
    "    dataset = []\n",
    "    with open('relations.txt','r') as f:\n",
    "        for line in f:\n",
    "            sline = line.split();\n",
    "            p1 = names.index(sline[0])\n",
    "            r = relations.index(sline[1])\n",
    "            p2 = names.index(sline[2])\n",
    "            d = [ sline[0]+'-'+sline[1]+'-'+sline[2], \n",
    "                  np.concatenate((bitvec(p1,len(names)),bitvec(r,len(relations)))),\n",
    "                  bitvec(p2,len(names)) ]\n",
    "                  #bitvec(p2,len(names)) ]\n",
    "            dataset.append(d)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = familytree()\n",
    "\n",
    "print (dataset[0])\n",
    "print (\" \")\n",
    "print (dataset[1])\n",
    "print (\" \")\n",
    "print (dataset[0][0])\n",
    "print (\" \")\n",
    "print (dataset[0][1])\n",
    "print (\" \")\n",
    "print (dataset[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arthur-niece-Charlotte', array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n",
      " \n",
      "['Maria-daughter-Lucia', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.])]\n",
      " \n",
      "Arthur-niece-Charlotte\n",
      " \n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " \n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "#randomly shuffle the whole dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "print (dataset[0])\n",
    "print (\" \")\n",
    "print (dataset[1])\n",
    "print (\" \")\n",
    "print (dataset[0][0])\n",
    "print (\" \")\n",
    "print (dataset[0][1])\n",
    "print (\" \")\n",
    "print (dataset[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data[1]:  ['Maria-daughter-Lucia', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.])]\n",
      "test_dat[1]:  ['Sophia-father-Marco', array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
      "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]), array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])]\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset[0:89]\n",
    "test_data = dataset[90:104]\n",
    "print (\"train_data[1]: \", train_data[1])\n",
    "print (\"test_dat[1]: \", test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_Xa[1]:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.]\n",
      "train_data_Xb[1]:  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "train_data_Y[1]:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.]\n",
      "len(train_data_Y):  89\n",
      "len(train_data_Y[0]):  24\n",
      "train_data_labels:  [11, 20, 13, 21, 17, 11, 18, 8, 11, 7, 6, 19, 0, 9, 5, 15, 2, 8, 15, 14, 9, 11, 21, 21, 3, 14, 21, 20, 9, 8, 2, 23, 6, 3, 4, 14, 23, 2, 9, 17, 23, 3, 22, 18, 3, 2, 19, 7, 20, 5, 14, 17, 11, 17, 1, 5, 21, 11, 15, 3, 23, 3, 6, 0, 5, 21, 8, 12, 17, 11, 13, 14, 10, 8, 17, 8, 20, 9, 16, 1, 9, 1, 23, 18, 7, 0, 23, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "# get train input and output data\n",
    "train_data_Xa = []\n",
    "train_data_Xb = []\n",
    "train_data_Y = []\n",
    "train_data_labels = []\n",
    "for i in range(len(train_data)):\n",
    "    train_data_Xa.append(train_data[i][1][0:24])\n",
    "    train_data_Xb.append(train_data[i][1][24:])\n",
    "    train_data_Y.append(train_data[i][2])\n",
    "    for j in range(len(train_data[i][2])):\n",
    "        if (train_data[i][2][j] == 1):\n",
    "            train_data_labels.append(j)\n",
    "    \n",
    "print (\"train_data_Xa[1]: \", train_data_Xa[1])\n",
    "print (\"train_data_Xb[1]: \", train_data_Xb[1])\n",
    "print (\"train_data_Y[1]: \", train_data_Y[1])\n",
    "print (\"len(train_data_Y): \", len(train_data_Y))\n",
    "print (\"len(train_data_Y[0]): \", len(train_data_Y[0]))\n",
    "print (\"train_data_labels: \", train_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data_Xa[1]:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.]\n",
      "test_data_Xb[1]:  [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "test_data_Y[1]:  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.]\n",
      "len(test_data_Y):  14\n",
      "len(test_data_Y[0]):  24\n",
      "test_data_labels:  [5, 15, 15, 20, 5, 2, 19, 2, 14, 12, 15, 20, 23, 13]\n"
     ]
    }
   ],
   "source": [
    "# get train input and output data\n",
    "test_data_Xa = []\n",
    "test_data_Xb = []\n",
    "test_data_Y = []\n",
    "test_data_labels = []\n",
    "for i in range(len(test_data)):\n",
    "    test_data_Xa.append(test_data[i][1][0:24])\n",
    "    test_data_Xb.append(test_data[i][1][24:])\n",
    "    test_data_Y.append(test_data[i][2])\n",
    "    for j in range(len(test_data[i][2])):\n",
    "        if (test_data[i][2][j] == 1):\n",
    "            test_data_labels.append(j)\n",
    "    \n",
    "print (\"test_data_Xa[1]: \", test_data_Xa[1])\n",
    "print (\"test_data_Xb[1]: \", test_data_Xb[1])\n",
    "print (\"test_data_Y[1]: \", test_data_Y[1])\n",
    "print (\"len(test_data_Y): \", len(test_data_Y))\n",
    "print (\"len(test_data_Y[0]): \", len(test_data_Y[0]))\n",
    "print (\"test_data_labels: \", test_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network architecture\n",
    "lr = 0.05\n",
    "epochs = 10000\n",
    "n_inputa = 24\n",
    "n_inputb = 12\n",
    "n_H1a = 6\n",
    "n_H1b = 6\n",
    "n_H2 = 12\n",
    "n_H3 = 6\n",
    "n_output = 24\n",
    "\n",
    "Xa = tf.placeholder(tf.float32)\n",
    "Xb = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "wsXa_H1a = tf.Variable(tf.random_uniform([n_inputa, n_H1a], 0.0, 1.0))\n",
    "wsXb_H1b = tf.Variable(tf.random_uniform([n_inputb, n_H1b], 0.0, 1.0))\n",
    "wsH2_H3 = tf.Variable(tf.random_uniform([n_H2, n_H3], 0.0, 1.0))\n",
    "wsH3_Y = tf.Variable(tf.random_uniform([n_H3, n_output], 0.0, 1.0))\n",
    "\n",
    "H1a_outputs = tf.sigmoid(tf.matmul(Xa, wsXa_H1a))\n",
    "H1b_outputs = tf.sigmoid(tf.matmul(Xb, wsXb_H1b))\n",
    "H2_outputs = tf.concat([H1a_outputs, H1b_outputs], 1)\n",
    "print (wsXa_H1a)\n",
    "print (H1a_outputs)\n",
    "print (H2_outputs)\n",
    "H3_outputs = tf.sigmoid(tf.matmul(H2_outputs, wsH2_H3))\n",
    "hy = tf.sigmoid(tf.matmul(H3_outputs, wsH3_Y))\n",
    "print (\"hy: \",hy)\n",
    "#train_data_hy_one = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = train_data_labels, logits = Y)\n",
    "#train_data_hy_one = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = train_data_labels, logits = train_data_Y)\n",
    "softmax_hy = tf.nn.softmax(hy)\n",
    "print (\"softmax_hy:\",softmax_hy)\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(Y - hy) / 2)\n",
    "#cost = tf.reduce_mean(-tf.reduce_sum(train_data_hy * tf.log(Y), reduction_indices=[1])) #cross entropy\n",
    "#cost = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = train_data_labels, logits = Y)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for step in xrange(epochs):\n",
    "        #print (\"train_data_hy[0]:\", sess.run(tf.shape(tf.argmax(Y, 1)), feed_dict={Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "        #train network in batch size = bs\n",
    "        sess.run(optimizer, feed_dict={Xa: train_data_Xa, \n",
    "                                       Xb: train_data_Xb, \n",
    "                                       Y: train_data_Y})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print (\"Cost:\", sess.run(cost, feed_dict={Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "            print (\"tf.argmax(softmax_hy,0):\", sess.run(tf.argmax(softmax_hy,0), feed_dict={Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "            correct_prediction = tf.equal(tf.argmax(softmax_hy,1), tf.argmax(Y,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            print (\"Accuracy: \", accuracy.eval({Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "            \n",
    "    correct_prediction = tf.equal(tf.argmax(train_data_softmax_hy,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        \n",
    "    print (sess.run([wsH3_Y], feed_dict={Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))\n",
    "    print (\"Accuracy: \", accuracy.eval({Xa: train_data_Xa, Xb: train_data_Xb, Y: train_data_Y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
