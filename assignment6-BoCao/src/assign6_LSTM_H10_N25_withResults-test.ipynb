{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 10\n",
    "N = 25\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.6954, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 20, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.6933, Training Accuracy= 0.495\n",
      "Epoch: 40, Loss= 0.6932, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.499\n",
      "Epoch: 60, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 70, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 130, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 160, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 220, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 240, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 250, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 260, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 270, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 290, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 300, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 310, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 320, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 330, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 340, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 350, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 360, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 370, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 380, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 390, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 400, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 410, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 420, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 430, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 440, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 450, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 460, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 470, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 480, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 490, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 500, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 510, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 520, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 530, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 540, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 550, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 560, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 570, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 580, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 590, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 600, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 610, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 640, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 650, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 660, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 670, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 680, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 700, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 710, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 730, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 740, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 750, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 770, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 780, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 790, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 800, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 810, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 830, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 840, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 850, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 860, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 870, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 880, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 890, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 900, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 910, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 920, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 930, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 940, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 950, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 960, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 970, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 990, Loss= 0.6928, Training Accuracy= 0.506\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.506\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7042, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6981, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6975, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6970, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6967, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6965, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6963, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.6961, Training Accuracy= 0.500\n",
      "Epoch: 90, Loss= 0.6960, Training Accuracy= 0.500\n",
      "Epoch: 100, Loss= 0.6958, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6957, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 140, Loss= 0.6955, Training Accuracy= 0.500\n",
      "Epoch: 150, Loss= 0.6954, Training Accuracy= 0.500\n",
      "Epoch: 160, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 170, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 180, Loss= 0.6952, Training Accuracy= 0.500\n",
      "Epoch: 190, Loss= 0.6952, Training Accuracy= 0.500\n",
      "Epoch: 200, Loss= 0.6951, Training Accuracy= 0.500\n",
      "Epoch: 210, Loss= 0.6951, Training Accuracy= 0.500\n",
      "Epoch: 220, Loss= 0.6950, Training Accuracy= 0.500\n",
      "Epoch: 230, Loss= 0.6950, Training Accuracy= 0.500\n",
      "Epoch: 240, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 250, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 260, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 270, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 280, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 290, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 300, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 310, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 320, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 330, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 340, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 350, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 360, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 370, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 380, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 390, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 400, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 410, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 420, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 430, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 440, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 450, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 460, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 470, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 480, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 490, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 500, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 510, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 520, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 530, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 540, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 550, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 560, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 570, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 580, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 590, Loss= 0.6943, Training Accuracy= 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 610, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 620, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 630, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 640, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 650, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 660, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 670, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 680, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 690, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 700, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 710, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 720, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 730, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 740, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 750, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 760, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 770, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 780, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 790, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 800, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 810, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 820, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 830, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 840, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 850, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 860, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 870, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 880, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 890, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 900, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 910, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 920, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 930, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 940, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 950, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 960, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 970, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 980, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 990, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4923\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 20, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 30, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 40, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 50, Loss= 0.6926, Training Accuracy= 0.508\n",
      "Epoch: 60, Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 70, Loss= 0.6925, Training Accuracy= 0.510\n",
      "Epoch: 80, Loss= 0.6925, Training Accuracy= 0.513\n",
      "Epoch: 90, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 100, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 110, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 120, Loss= 0.6923, Training Accuracy= 0.515\n",
      "Epoch: 130, Loss= 0.6923, Training Accuracy= 0.515\n",
      "Epoch: 140, Loss= 0.6923, Training Accuracy= 0.515\n",
      "Epoch: 150, Loss= 0.6923, Training Accuracy= 0.515\n",
      "Epoch: 160, Loss= 0.6922, Training Accuracy= 0.513\n",
      "Epoch: 170, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 180, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 190, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 200, Loss= 0.6922, Training Accuracy= 0.511\n",
      "Epoch: 210, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 220, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 230, Loss= 0.6922, Training Accuracy= 0.513\n",
      "Epoch: 240, Loss= 0.6921, Training Accuracy= 0.512\n",
      "Epoch: 250, Loss= 0.6921, Training Accuracy= 0.512\n",
      "Epoch: 260, Loss= 0.6921, Training Accuracy= 0.513\n",
      "Epoch: 270, Loss= 0.6921, Training Accuracy= 0.515\n",
      "Epoch: 280, Loss= 0.6921, Training Accuracy= 0.516\n",
      "Epoch: 290, Loss= 0.6921, Training Accuracy= 0.516\n",
      "Epoch: 300, Loss= 0.6921, Training Accuracy= 0.516\n",
      "Epoch: 310, Loss= 0.6921, Training Accuracy= 0.516\n",
      "Epoch: 320, Loss= 0.6921, Training Accuracy= 0.516\n",
      "Epoch: 330, Loss= 0.6921, Training Accuracy= 0.516\n",
      "Epoch: 340, Loss= 0.6921, Training Accuracy= 0.517\n",
      "Epoch: 350, Loss= 0.6920, Training Accuracy= 0.518\n",
      "Epoch: 360, Loss= 0.6920, Training Accuracy= 0.519\n",
      "Epoch: 370, Loss= 0.6920, Training Accuracy= 0.519\n",
      "Epoch: 380, Loss= 0.6920, Training Accuracy= 0.519\n",
      "Epoch: 390, Loss= 0.6920, Training Accuracy= 0.518\n",
      "Epoch: 400, Loss= 0.6920, Training Accuracy= 0.518\n",
      "Epoch: 410, Loss= 0.6919, Training Accuracy= 0.519\n",
      "Epoch: 420, Loss= 0.6919, Training Accuracy= 0.519\n",
      "Epoch: 430, Loss= 0.6919, Training Accuracy= 0.518\n",
      "Epoch: 440, Loss= 0.6919, Training Accuracy= 0.519\n",
      "Epoch: 450, Loss= 0.6918, Training Accuracy= 0.519\n",
      "Epoch: 460, Loss= 0.6918, Training Accuracy= 0.519\n",
      "Epoch: 470, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 480, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 490, Loss= 0.6916, Training Accuracy= 0.520\n",
      "Epoch: 500, Loss= 0.6916, Training Accuracy= 0.520\n",
      "Epoch: 510, Loss= 0.6915, Training Accuracy= 0.521\n",
      "Epoch: 520, Loss= 0.6915, Training Accuracy= 0.521\n",
      "Epoch: 530, Loss= 0.6915, Training Accuracy= 0.522\n",
      "Epoch: 540, Loss= 0.6914, Training Accuracy= 0.522\n",
      "Epoch: 550, Loss= 0.6914, Training Accuracy= 0.521\n",
      "Epoch: 560, Loss= 0.6914, Training Accuracy= 0.521\n",
      "Epoch: 570, Loss= 0.6914, Training Accuracy= 0.522\n",
      "Epoch: 580, Loss= 0.6913, Training Accuracy= 0.522\n",
      "Epoch: 590, Loss= 0.6913, Training Accuracy= 0.522\n",
      "Epoch: 600, Loss= 0.6913, Training Accuracy= 0.523\n",
      "Epoch: 610, Loss= 0.6913, Training Accuracy= 0.524\n",
      "Epoch: 620, Loss= 0.6913, Training Accuracy= 0.522\n",
      "Epoch: 630, Loss= 0.6912, Training Accuracy= 0.525\n",
      "Epoch: 640, Loss= 0.6912, Training Accuracy= 0.524\n",
      "Epoch: 650, Loss= 0.6912, Training Accuracy= 0.525\n",
      "Epoch: 660, Loss= 0.6912, Training Accuracy= 0.524\n",
      "Epoch: 670, Loss= 0.6912, Training Accuracy= 0.522\n",
      "Epoch: 680, Loss= 0.6912, Training Accuracy= 0.523\n",
      "Epoch: 690, Loss= 0.6911, Training Accuracy= 0.520\n",
      "Epoch: 700, Loss= 0.6911, Training Accuracy= 0.519\n",
      "Epoch: 710, Loss= 0.6911, Training Accuracy= 0.520\n",
      "Epoch: 720, Loss= 0.6911, Training Accuracy= 0.521\n",
      "Epoch: 730, Loss= 0.6911, Training Accuracy= 0.522\n",
      "Epoch: 740, Loss= 0.6910, Training Accuracy= 0.521\n",
      "Epoch: 750, Loss= 0.6910, Training Accuracy= 0.521\n",
      "Epoch: 760, Loss= 0.6910, Training Accuracy= 0.522\n",
      "Epoch: 770, Loss= 0.6910, Training Accuracy= 0.522\n",
      "Epoch: 780, Loss= 0.6910, Training Accuracy= 0.522\n",
      "Epoch: 790, Loss= 0.6910, Training Accuracy= 0.522\n",
      "Epoch: 800, Loss= 0.6909, Training Accuracy= 0.523\n",
      "Epoch: 810, Loss= 0.6909, Training Accuracy= 0.524\n",
      "Epoch: 820, Loss= 0.6909, Training Accuracy= 0.525\n",
      "Epoch: 830, Loss= 0.6909, Training Accuracy= 0.524\n",
      "Epoch: 840, Loss= 0.6909, Training Accuracy= 0.525\n",
      "Epoch: 850, Loss= 0.6908, Training Accuracy= 0.524\n",
      "Epoch: 860, Loss= 0.6908, Training Accuracy= 0.524\n",
      "Epoch: 870, Loss= 0.6908, Training Accuracy= 0.524\n",
      "Epoch: 880, Loss= 0.6908, Training Accuracy= 0.524\n",
      "Epoch: 890, Loss= 0.6908, Training Accuracy= 0.525\n",
      "Epoch: 900, Loss= 0.6907, Training Accuracy= 0.524\n",
      "Epoch: 910, Loss= 0.6907, Training Accuracy= 0.525\n",
      "Epoch: 920, Loss= 0.6907, Training Accuracy= 0.526\n",
      "Epoch: 930, Loss= 0.6906, Training Accuracy= 0.526\n",
      "Epoch: 940, Loss= 0.6906, Training Accuracy= 0.527\n",
      "Epoch: 950, Loss= 0.6906, Training Accuracy= 0.525\n",
      "Epoch: 960, Loss= 0.6906, Training Accuracy= 0.526\n",
      "Epoch: 970, Loss= 0.6905, Training Accuracy= 0.528\n",
      "Epoch: 980, Loss= 0.6905, Training Accuracy= 0.528\n",
      "Epoch: 990, Loss= 0.6905, Training Accuracy= 0.528\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5043\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.6953, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 20, Loss= 0.6931, Training Accuracy= 0.514\n",
      "Epoch: 30, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 40, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 50, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 60, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 70, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 80, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 90, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 100, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 110, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 120, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 130, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 140, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 150, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 160, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 170, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 180, Loss= 0.6926, Training Accuracy= 0.513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 200, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 210, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 220, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 230, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 240, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 250, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 260, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 270, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 280, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 290, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 300, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 310, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 320, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 330, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 340, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 350, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 360, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 370, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 380, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 390, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 400, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 410, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 420, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 430, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 440, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 450, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 460, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 470, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 480, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 490, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 500, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 510, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 520, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 530, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 540, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 550, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 560, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 570, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 580, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 590, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 600, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 610, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 620, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 630, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 640, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 650, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 660, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 670, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 680, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 690, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 700, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 710, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 720, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 730, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 740, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 750, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 760, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 770, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 780, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 790, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 800, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 810, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 820, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 830, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 840, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 850, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 860, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 870, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 880, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 890, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 900, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 910, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 920, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 930, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 940, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 950, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 960, Loss= 0.6924, Training Accuracy= 0.512\n",
      "Epoch: 970, Loss= 0.6924, Training Accuracy= 0.512\n",
      "Epoch: 980, Loss= 0.6924, Training Accuracy= 0.512\n",
      "Epoch: 990, Loss= 0.6924, Training Accuracy= 0.512\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.49\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.6975, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.6968, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6964, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.6960, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6958, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6956, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6955, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6953, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6952, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6948, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.6948, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 190, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 200, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 210, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 220, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 230, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 270, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 280, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 290, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 300, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 310, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 320, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 330, Loss= 0.6943, Training Accuracy= 0.504\n",
      "Epoch: 340, Loss= 0.6943, Training Accuracy= 0.504\n",
      "Epoch: 350, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 370, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 380, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 400, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 410, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 420, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 430, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 440, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 450, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 460, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 480, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 490, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 500, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 510, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 520, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 530, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 540, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 550, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 560, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 570, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 580, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 590, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 600, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 610, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 620, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 630, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 640, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 650, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 660, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 670, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 680, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 690, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 700, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 710, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 720, Loss= 0.6940, Training Accuracy= 0.504\n",
      "Epoch: 730, Loss= 0.6940, Training Accuracy= 0.504\n",
      "Epoch: 740, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 750, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 760, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 770, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 780, Loss= 0.6939, Training Accuracy= 0.505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 800, Loss= 0.6939, Training Accuracy= 0.506\n",
      "Epoch: 810, Loss= 0.6939, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 830, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 840, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 850, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 860, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 870, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 880, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 890, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 900, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 910, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 920, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 930, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 940, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 950, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 960, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 970, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 980, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Epoch: 990, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4942\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.6956, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6946, Training Accuracy= 0.499\n",
      "Epoch: 20, Loss= 0.6942, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.6939, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.6937, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.6935, Training Accuracy= 0.499\n",
      "Epoch: 60, Loss= 0.6935, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 160, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 210, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 220, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 230, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 240, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 250, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 260, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 270, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 280, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 290, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 300, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 310, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 320, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 330, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 340, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 350, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 360, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 370, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 380, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 390, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 400, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 410, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 420, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 430, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 440, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 450, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 460, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 470, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 480, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 490, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 500, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 510, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 520, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 530, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 540, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 550, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 560, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 570, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 580, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 590, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 600, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 610, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 620, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 630, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 640, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 650, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 660, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 670, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 680, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 690, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 700, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 710, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 720, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 730, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 740, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 750, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 760, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 770, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 780, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 790, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 800, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 810, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 820, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 830, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 840, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 850, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 860, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 870, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 880, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 890, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 900, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 910, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 920, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 930, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 940, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 950, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 960, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 970, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 980, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 990, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5068\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.7108, Training Accuracy= 0.504\n",
      "Epoch: 10, Loss= 0.7000, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6976, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6965, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.6959, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6955, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6952, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.6950, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6948, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6947, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.6946, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.6945, Training Accuracy= 0.502\n",
      "Epoch: 120, Loss= 0.6944, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.6943, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.6943, Training Accuracy= 0.502\n",
      "Epoch: 150, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 160, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 170, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 180, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 190, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 200, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 210, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 220, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 230, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 240, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 250, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 260, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 270, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 280, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 290, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 300, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 310, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 320, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 330, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 340, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 350, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 360, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 370, Loss= 0.6937, Training Accuracy= 0.502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 390, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 400, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 410, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 420, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 430, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 440, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 450, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 460, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 470, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 490, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 500, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 510, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 520, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 530, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 540, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 550, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 560, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 570, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 580, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 590, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 600, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 610, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 640, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 650, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 660, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 670, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 680, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 690, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 700, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 710, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 740, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 750, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 760, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 770, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 780, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 790, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 800, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 810, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 820, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 830, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 840, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 850, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 860, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 870, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 880, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 890, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 900, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 910, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 920, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 930, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 940, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 950, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 960, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 970, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 980, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 990, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.49\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.6974, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6944, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 60, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 70, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 100, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 110, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 120, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 130, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 140, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 150, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 160, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 170, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 180, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 190, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 200, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 210, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 220, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 230, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 240, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 250, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 260, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 270, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 280, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 290, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 300, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 310, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 320, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 330, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 340, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 350, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 360, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 370, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 380, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 390, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 400, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 410, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 420, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 430, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 440, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 450, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 460, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 470, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 480, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 490, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 500, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 510, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 520, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 530, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 540, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 550, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 560, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 570, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 580, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 590, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 600, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 610, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 620, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 630, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 640, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 650, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 660, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 670, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 680, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 690, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 700, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 710, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 720, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 730, Loss= 0.6925, Training Accuracy= 0.519\n",
      "Epoch: 740, Loss= 0.6925, Training Accuracy= 0.519\n",
      "Epoch: 750, Loss= 0.6925, Training Accuracy= 0.519\n",
      "Epoch: 760, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 770, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 780, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 790, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 800, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 810, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 820, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 830, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 840, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 850, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 860, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 870, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 880, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 890, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 900, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 910, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 920, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 930, Loss= 0.6924, Training Accuracy= 0.520\n",
      "Epoch: 940, Loss= 0.6924, Training Accuracy= 0.520\n",
      "Epoch: 950, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 960, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 970, Loss= 0.6923, Training Accuracy= 0.520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 990, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.6986, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6965, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6960, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6958, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6954, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6952, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.6951, Training Accuracy= 0.500\n",
      "Epoch: 90, Loss= 0.6950, Training Accuracy= 0.500\n",
      "Epoch: 100, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.6947, Training Accuracy= 0.501\n",
      "Epoch: 130, Loss= 0.6947, Training Accuracy= 0.501\n",
      "Epoch: 140, Loss= 0.6946, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.6946, Training Accuracy= 0.501\n",
      "Epoch: 160, Loss= 0.6945, Training Accuracy= 0.501\n",
      "Epoch: 170, Loss= 0.6945, Training Accuracy= 0.501\n",
      "Epoch: 180, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 190, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 200, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 210, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 220, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 230, Loss= 0.6942, Training Accuracy= 0.501\n",
      "Epoch: 240, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 250, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 260, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 270, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 280, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 290, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 300, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 310, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 320, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 330, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 340, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 350, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 360, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 370, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 380, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 390, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 400, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 410, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 420, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 430, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 450, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 460, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 470, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 490, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 500, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 510, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 520, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 530, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 540, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 550, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 560, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 570, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 580, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 590, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 600, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 610, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 620, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 630, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 640, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 650, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 660, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 670, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 680, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 690, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 700, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 710, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 720, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 730, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 740, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 750, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 760, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 770, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 790, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 810, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 830, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 840, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 850, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 860, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 870, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 880, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 890, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 900, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 910, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 920, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 930, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 940, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 950, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 960, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 970, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 980, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 990, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5096\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.6944, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.6939, Training Accuracy= 0.498\n",
      "Epoch: 50, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 60, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6936, Training Accuracy= 0.501\n",
      "Epoch: 100, Loss= 0.6935, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6935, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 130, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 140, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 150, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6933, Training Accuracy= 0.502\n",
      "Epoch: 190, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 200, Loss= 0.6933, Training Accuracy= 0.502\n",
      "Epoch: 210, Loss= 0.6933, Training Accuracy= 0.502\n",
      "Epoch: 220, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 230, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 250, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 270, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 280, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 290, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 300, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 310, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 320, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 330, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 340, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 350, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 360, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 370, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 380, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 400, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 410, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 420, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 430, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 440, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 450, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 460, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 480, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 490, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 500, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 510, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 520, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 530, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 540, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 550, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 560, Loss= 0.6930, Training Accuracy= 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 580, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 590, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 600, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 610, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 620, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 630, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 680, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 700, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 710, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 720, Loss= 0.6929, Training Accuracy= 0.504\n",
      "Epoch: 730, Loss= 0.6929, Training Accuracy= 0.504\n",
      "Epoch: 740, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 750, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 760, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 770, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 790, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 800, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 810, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 820, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 830, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 840, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 850, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 860, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 870, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 880, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 890, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 900, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 910, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 920, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 930, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 940, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 950, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 960, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 970, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 990, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5124\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.06\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.50599998, 0.4923, 0.5043, 0.49000001, 0.49419999, 0.5068, 0.49000001, 0.5, 0.50959998, 0.51239997]\n",
      "mean of test_accuracies_10replications:  0.50056\n",
      "standard deviation of test_accuracies_10replications_std_mean:  7.97773431987e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmZWFicOMzMrixOHmZmV5f8DcrXRVTJWgPQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f502dd150b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
