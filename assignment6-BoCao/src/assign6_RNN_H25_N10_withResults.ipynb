{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 25\n",
    "N = 10\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 1.0421, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 1.0628, Training Accuracy= 0.504\n",
      "Epoch: 20, Loss= 1.0578, Training Accuracy= 0.504\n",
      "Epoch: 30, Loss= 0.9425, Training Accuracy= 0.504\n",
      "Epoch: 40, Loss= 0.9421, Training Accuracy= 0.504\n",
      "Epoch: 50, Loss= 0.9413, Training Accuracy= 0.504\n",
      "Epoch: 60, Loss= 0.9406, Training Accuracy= 0.504\n",
      "Epoch: 70, Loss= 0.9399, Training Accuracy= 0.504\n",
      "Epoch: 80, Loss= 0.9405, Training Accuracy= 0.504\n",
      "Epoch: 90, Loss= 0.9357, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.9164, Training Accuracy= 0.504\n",
      "Epoch: 110, Loss= 0.8226, Training Accuracy= 0.504\n",
      "Epoch: 120, Loss= 0.8210, Training Accuracy= 0.504\n",
      "Epoch: 130, Loss= 0.7768, Training Accuracy= 0.504\n",
      "Epoch: 140, Loss= 0.7753, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.7684, Training Accuracy= 0.504\n",
      "Epoch: 160, Loss= 0.7653, Training Accuracy= 0.504\n",
      "Epoch: 170, Loss= 0.7653, Training Accuracy= 0.504\n",
      "Epoch: 180, Loss= 0.7658, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.7741, Training Accuracy= 0.504\n",
      "Epoch: 200, Loss= 0.7703, Training Accuracy= 0.504\n",
      "Epoch: 210, Loss= 0.7508, Training Accuracy= 0.504\n",
      "Epoch: 220, Loss= 0.7422, Training Accuracy= 0.504\n",
      "Epoch: 230, Loss= 0.7405, Training Accuracy= 0.504\n",
      "Epoch: 240, Loss= 0.7402, Training Accuracy= 0.504\n",
      "Epoch: 250, Loss= 0.7392, Training Accuracy= 0.504\n",
      "Epoch: 260, Loss= 0.7374, Training Accuracy= 0.504\n",
      "Epoch: 270, Loss= 0.7334, Training Accuracy= 0.504\n",
      "Epoch: 280, Loss= 0.7289, Training Accuracy= 0.504\n",
      "Epoch: 290, Loss= 0.7283, Training Accuracy= 0.504\n",
      "Epoch: 300, Loss= 0.7282, Training Accuracy= 0.504\n",
      "Epoch: 310, Loss= 0.7286, Training Accuracy= 0.504\n",
      "Epoch: 320, Loss= 0.7292, Training Accuracy= 0.504\n",
      "Epoch: 330, Loss= 0.7296, Training Accuracy= 0.504\n",
      "Epoch: 340, Loss= 0.7295, Training Accuracy= 0.504\n",
      "Epoch: 350, Loss= 0.7302, Training Accuracy= 0.504\n",
      "Epoch: 360, Loss= 0.7305, Training Accuracy= 0.504\n",
      "Epoch: 370, Loss= 0.7293, Training Accuracy= 0.504\n",
      "Epoch: 380, Loss= 0.7267, Training Accuracy= 0.504\n",
      "Epoch: 390, Loss= 0.7336, Training Accuracy= 0.504\n",
      "Epoch: 400, Loss= 0.7321, Training Accuracy= 0.504\n",
      "Epoch: 410, Loss= 0.7332, Training Accuracy= 0.504\n",
      "Epoch: 420, Loss= 0.7310, Training Accuracy= 0.504\n",
      "Epoch: 430, Loss= 0.7214, Training Accuracy= 0.504\n",
      "Epoch: 440, Loss= 0.7211, Training Accuracy= 0.504\n",
      "Epoch: 450, Loss= 0.7202, Training Accuracy= 0.504\n",
      "Epoch: 460, Loss= 0.7198, Training Accuracy= 0.505\n",
      "Epoch: 470, Loss= 0.7218, Training Accuracy= 0.505\n",
      "Epoch: 480, Loss= 0.7226, Training Accuracy= 0.507\n",
      "Epoch: 490, Loss= 0.7131, Training Accuracy= 0.505\n",
      "Epoch: 500, Loss= 0.7529, Training Accuracy= 0.525\n",
      "Epoch: 510, Loss= 0.7566, Training Accuracy= 0.503\n",
      "Epoch: 520, Loss= 0.7319, Training Accuracy= 0.503\n",
      "Epoch: 530, Loss= 0.7187, Training Accuracy= 0.507\n",
      "Epoch: 540, Loss= 0.6464, Training Accuracy= 0.597\n",
      "Epoch: 550, Loss= 0.8550, Training Accuracy= 0.489\n",
      "Epoch: 560, Loss= 0.9024, Training Accuracy= 0.527\n",
      "Epoch: 570, Loss= 0.8204, Training Accuracy= 0.505\n",
      "Epoch: 580, Loss= 0.4337, Training Accuracy= 0.834\n",
      "Epoch: 590, Loss= 0.8695, Training Accuracy= 0.508\n",
      "Epoch: 600, Loss= 1.0572, Training Accuracy= 0.506\n",
      "Epoch: 610, Loss= 0.7957, Training Accuracy= 0.501\n",
      "Epoch: 620, Loss= 0.7939, Training Accuracy= 0.508\n",
      "Epoch: 630, Loss= 0.7921, Training Accuracy= 0.504\n",
      "Epoch: 640, Loss= 0.7718, Training Accuracy= 0.552\n",
      "Epoch: 650, Loss= 0.7820, Training Accuracy= 0.505\n",
      "Epoch: 660, Loss= 0.7694, Training Accuracy= 0.504\n",
      "Epoch: 670, Loss= 0.8000, Training Accuracy= 0.501\n",
      "Epoch: 680, Loss= 0.7744, Training Accuracy= 0.505\n",
      "Epoch: 690, Loss= 0.7499, Training Accuracy= 0.517\n",
      "Epoch: 700, Loss= 0.8731, Training Accuracy= 0.523\n",
      "Epoch: 710, Loss= 0.7239, Training Accuracy= 0.506\n",
      "Epoch: 720, Loss= 0.7179, Training Accuracy= 0.507\n",
      "Epoch: 730, Loss= 0.7126, Training Accuracy= 0.520\n",
      "Epoch: 740, Loss= 0.7218, Training Accuracy= 0.517\n",
      "Epoch: 750, Loss= 0.7232, Training Accuracy= 0.510\n",
      "Epoch: 760, Loss= 0.7252, Training Accuracy= 0.515\n",
      "Epoch: 770, Loss= 0.8537, Training Accuracy= 0.510\n",
      "Epoch: 780, Loss= 0.7068, Training Accuracy= 0.513\n",
      "Epoch: 790, Loss= 0.7434, Training Accuracy= 0.496\n",
      "Epoch: 800, Loss= 0.6910, Training Accuracy= 0.547\n",
      "Epoch: 810, Loss= 0.7394, Training Accuracy= 0.502\n",
      "Epoch: 820, Loss= 0.7352, Training Accuracy= 0.518\n",
      "Epoch: 830, Loss= 0.7468, Training Accuracy= 0.511\n",
      "Epoch: 840, Loss= 0.7201, Training Accuracy= 0.522\n",
      "Epoch: 850, Loss= 0.7267, Training Accuracy= 0.525\n",
      "Epoch: 860, Loss= 0.7324, Training Accuracy= 0.499\n",
      "Epoch: 870, Loss= 0.7052, Training Accuracy= 0.533\n",
      "Epoch: 880, Loss= 0.7367, Training Accuracy= 0.511\n",
      "Epoch: 890, Loss= 0.7489, Training Accuracy= 0.510\n",
      "Epoch: 900, Loss= 0.7134, Training Accuracy= 0.520\n",
      "Epoch: 910, Loss= 0.7242, Training Accuracy= 0.520\n",
      "Epoch: 920, Loss= 0.7040, Training Accuracy= 0.521\n",
      "Epoch: 930, Loss= 0.7750, Training Accuracy= 0.509\n",
      "Epoch: 940, Loss= 0.7192, Training Accuracy= 0.528\n",
      "Epoch: 950, Loss= 0.7060, Training Accuracy= 0.537\n",
      "Epoch: 960, Loss= 0.7064, Training Accuracy= 0.530\n",
      "Epoch: 970, Loss= 0.7346, Training Accuracy= 0.506\n",
      "Epoch: 980, Loss= 0.7481, Training Accuracy= 0.507\n",
      "Epoch: 990, Loss= 0.6898, Training Accuracy= 0.545\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5156\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.9535, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 0.7772, Training Accuracy= 0.496\n",
      "Epoch: 20, Loss= 0.7555, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.7472, Training Accuracy= 0.496\n",
      "Epoch: 40, Loss= 0.7378, Training Accuracy= 0.496\n",
      "Epoch: 50, Loss= 0.7358, Training Accuracy= 0.495\n",
      "Epoch: 60, Loss= 0.7351, Training Accuracy= 0.494\n",
      "Epoch: 70, Loss= 0.7026, Training Accuracy= 0.513\n",
      "Epoch: 80, Loss= 0.7058, Training Accuracy= 0.517\n",
      "Epoch: 90, Loss= 0.6742, Training Accuracy= 0.596\n",
      "Epoch: 100, Loss= 0.6629, Training Accuracy= 0.606\n",
      "Epoch: 110, Loss= 0.7734, Training Accuracy= 0.496\n",
      "Epoch: 120, Loss= 0.7795, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.7570, Training Accuracy= 0.497\n",
      "Epoch: 140, Loss= 0.7545, Training Accuracy= 0.510\n",
      "Epoch: 150, Loss= 0.9066, Training Accuracy= 0.504\n",
      "Epoch: 160, Loss= 0.7840, Training Accuracy= 0.501\n",
      "Epoch: 170, Loss= 0.8333, Training Accuracy= 0.501\n",
      "Epoch: 180, Loss= 0.8121, Training Accuracy= 0.497\n",
      "Epoch: 190, Loss= 0.7988, Training Accuracy= 0.512\n",
      "Epoch: 200, Loss= 0.8222, Training Accuracy= 0.498\n",
      "Epoch: 210, Loss= 0.8147, Training Accuracy= 0.534\n",
      "Epoch: 220, Loss= 0.5907, Training Accuracy= 0.707\n",
      "Epoch: 230, Loss= 0.9282, Training Accuracy= 0.514\n",
      "Epoch: 240, Loss= 0.7238, Training Accuracy= 0.609\n",
      "Epoch: 250, Loss= 0.7558, Training Accuracy= 0.494\n",
      "Epoch: 260, Loss= 0.8802, Training Accuracy= 0.494\n",
      "Epoch: 270, Loss= 1.0961, Training Accuracy= 0.496\n",
      "Epoch: 280, Loss= 1.0776, Training Accuracy= 0.496\n",
      "Epoch: 290, Loss= 1.0599, Training Accuracy= 0.496\n",
      "Epoch: 300, Loss= 1.0433, Training Accuracy= 0.496\n",
      "Epoch: 310, Loss= 0.9852, Training Accuracy= 0.497\n",
      "Epoch: 320, Loss= 0.7722, Training Accuracy= 0.576\n",
      "Epoch: 330, Loss= 1.0924, Training Accuracy= 0.502\n",
      "Epoch: 340, Loss= 1.1360, Training Accuracy= 0.496\n",
      "Epoch: 350, Loss= 0.9265, Training Accuracy= 0.496\n",
      "Epoch: 360, Loss= 0.9113, Training Accuracy= 0.496\n",
      "Epoch: 370, Loss= 0.9031, Training Accuracy= 0.496\n",
      "Epoch: 380, Loss= 0.8736, Training Accuracy= 0.585\n",
      "Epoch: 390, Loss= 0.7523, Training Accuracy= 0.497\n",
      "Epoch: 400, Loss= 0.7668, Training Accuracy= 0.508\n",
      "Epoch: 410, Loss= 0.7428, Training Accuracy= 0.527\n",
      "Epoch: 420, Loss= 0.6312, Training Accuracy= 0.667\n",
      "Epoch: 430, Loss= 0.7884, Training Accuracy= 0.531\n",
      "Epoch: 440, Loss= 0.7363, Training Accuracy= 0.496\n",
      "Epoch: 450, Loss= 0.7291, Training Accuracy= 0.498\n",
      "Epoch: 460, Loss= 0.7729, Training Accuracy= 0.496\n",
      "Epoch: 470, Loss= 0.7766, Training Accuracy= 0.496\n",
      "Epoch: 480, Loss= 0.7875, Training Accuracy= 0.497\n",
      "Epoch: 490, Loss= 0.7935, Training Accuracy= 0.496\n",
      "Epoch: 500, Loss= 0.7567, Training Accuracy= 0.530\n",
      "Epoch: 510, Loss= 0.8439, Training Accuracy= 0.507\n",
      "Epoch: 520, Loss= 0.7353, Training Accuracy= 0.500\n",
      "Epoch: 530, Loss= 0.7329, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.6565, Training Accuracy= 0.580\n",
      "Epoch: 550, Loss= 0.7215, Training Accuracy= 0.489\n",
      "Epoch: 560, Loss= 1.7426, Training Accuracy= 0.492\n",
      "Epoch: 570, Loss= 0.8395, Training Accuracy= 0.500\n",
      "Epoch: 580, Loss= 0.8368, Training Accuracy= 0.499\n",
      "Epoch: 590, Loss= 0.7621, Training Accuracy= 0.499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.8171, Training Accuracy= 0.504\n",
      "Epoch: 610, Loss= 0.7373, Training Accuracy= 0.504\n",
      "Epoch: 620, Loss= 0.7857, Training Accuracy= 0.498\n",
      "Epoch: 630, Loss= 0.7309, Training Accuracy= 0.504\n",
      "Epoch: 640, Loss= 0.7549, Training Accuracy= 0.498\n",
      "Epoch: 650, Loss= 0.7484, Training Accuracy= 0.492\n",
      "Epoch: 660, Loss= 0.8634, Training Accuracy= 0.503\n",
      "Epoch: 670, Loss= 0.7535, Training Accuracy= 0.513\n",
      "Epoch: 680, Loss= 0.7636, Training Accuracy= 0.517\n",
      "Epoch: 690, Loss= 0.8291, Training Accuracy= 0.496\n",
      "Epoch: 700, Loss= 0.7663, Training Accuracy= 0.505\n",
      "Epoch: 710, Loss= 0.7750, Training Accuracy= 0.493\n",
      "Epoch: 720, Loss= 0.7701, Training Accuracy= 0.500\n",
      "Epoch: 730, Loss= 0.7736, Training Accuracy= 0.496\n",
      "Epoch: 740, Loss= 0.7625, Training Accuracy= 0.497\n",
      "Epoch: 750, Loss= 0.7756, Training Accuracy= 0.502\n",
      "Epoch: 760, Loss= 0.7573, Training Accuracy= 0.501\n",
      "Epoch: 770, Loss= 0.8023, Training Accuracy= 0.494\n",
      "Epoch: 780, Loss= 0.7448, Training Accuracy= 0.506\n",
      "Epoch: 790, Loss= 0.7588, Training Accuracy= 0.502\n",
      "Epoch: 800, Loss= 0.7533, Training Accuracy= 0.532\n",
      "Epoch: 810, Loss= 0.7661, Training Accuracy= 0.498\n",
      "Epoch: 820, Loss= 0.7434, Training Accuracy= 0.500\n",
      "Epoch: 830, Loss= 0.7497, Training Accuracy= 0.514\n",
      "Epoch: 840, Loss= 0.7669, Training Accuracy= 0.499\n",
      "Epoch: 850, Loss= 0.7888, Training Accuracy= 0.500\n",
      "Epoch: 860, Loss= 0.7568, Training Accuracy= 0.506\n",
      "Epoch: 870, Loss= 0.8517, Training Accuracy= 0.498\n",
      "Epoch: 880, Loss= 0.7883, Training Accuracy= 0.498\n",
      "Epoch: 890, Loss= 0.7522, Training Accuracy= 0.498\n",
      "Epoch: 900, Loss= 0.7892, Training Accuracy= 0.499\n",
      "Epoch: 910, Loss= 0.7742, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.7708, Training Accuracy= 0.496\n",
      "Epoch: 930, Loss= 0.7936, Training Accuracy= 0.495\n",
      "Epoch: 940, Loss= 0.7471, Training Accuracy= 0.506\n",
      "Epoch: 950, Loss= 0.7852, Training Accuracy= 0.501\n",
      "Epoch: 960, Loss= 0.7786, Training Accuracy= 0.496\n",
      "Epoch: 970, Loss= 0.7715, Training Accuracy= 0.496\n",
      "Epoch: 980, Loss= 0.7485, Training Accuracy= 0.494\n",
      "Epoch: 990, Loss= 0.7536, Training Accuracy= 0.498\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5084\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 1.6931, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.9704, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.8957, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.8586, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.8485, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.8180, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.8101, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.8075, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.8040, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.8005, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.7984, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.7942, Training Accuracy= 0.502\n",
      "Epoch: 120, Loss= 0.7818, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.7683, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.7702, Training Accuracy= 0.502\n",
      "Epoch: 150, Loss= 0.7719, Training Accuracy= 0.502\n",
      "Epoch: 160, Loss= 0.7720, Training Accuracy= 0.502\n",
      "Epoch: 170, Loss= 0.7693, Training Accuracy= 0.502\n",
      "Epoch: 180, Loss= 0.7650, Training Accuracy= 0.502\n",
      "Epoch: 190, Loss= 0.8720, Training Accuracy= 0.498\n",
      "Epoch: 200, Loss= 0.7473, Training Accuracy= 0.628\n",
      "Epoch: 210, Loss= 1.0752, Training Accuracy= 0.503\n",
      "Epoch: 220, Loss= 1.0216, Training Accuracy= 0.525\n",
      "Epoch: 230, Loss= 0.9750, Training Accuracy= 0.504\n",
      "Epoch: 240, Loss= 1.2245, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 1.1479, Training Accuracy= 0.502\n",
      "Epoch: 260, Loss= 1.3180, Training Accuracy= 0.501\n",
      "Epoch: 270, Loss= 1.0755, Training Accuracy= 0.502\n",
      "Epoch: 280, Loss= 1.1232, Training Accuracy= 0.510\n",
      "Epoch: 290, Loss= 0.9788, Training Accuracy= 0.530\n",
      "Epoch: 300, Loss= 0.9207, Training Accuracy= 0.516\n",
      "Epoch: 310, Loss= 0.9446, Training Accuracy= 0.501\n",
      "Epoch: 320, Loss= 1.2423, Training Accuracy= 0.507\n",
      "Epoch: 330, Loss= 0.7933, Training Accuracy= 0.549\n",
      "Epoch: 340, Loss= 0.8581, Training Accuracy= 0.515\n",
      "Epoch: 350, Loss= 0.8875, Training Accuracy= 0.502\n",
      "Epoch: 360, Loss= 0.9281, Training Accuracy= 0.502\n",
      "Epoch: 370, Loss= 0.7124, Training Accuracy= 0.576\n",
      "Epoch: 380, Loss= 1.1528, Training Accuracy= 0.502\n",
      "Epoch: 390, Loss= 1.0745, Training Accuracy= 0.502\n",
      "Epoch: 400, Loss= 1.1574, Training Accuracy= 0.502\n",
      "Epoch: 410, Loss= 1.1285, Training Accuracy= 0.502\n",
      "Epoch: 420, Loss= 1.1212, Training Accuracy= 0.502\n",
      "Epoch: 430, Loss= 1.0872, Training Accuracy= 0.502\n",
      "Epoch: 440, Loss= 1.0995, Training Accuracy= 0.502\n",
      "Epoch: 450, Loss= 1.0464, Training Accuracy= 0.502\n",
      "Epoch: 460, Loss= 0.9172, Training Accuracy= 0.502\n",
      "Epoch: 470, Loss= 0.8742, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.9889, Training Accuracy= 0.502\n",
      "Epoch: 490, Loss= 1.0072, Training Accuracy= 0.505\n",
      "Epoch: 500, Loss= 0.9062, Training Accuracy= 0.503\n",
      "Epoch: 510, Loss= 0.8780, Training Accuracy= 0.505\n",
      "Epoch: 520, Loss= 0.9896, Training Accuracy= 0.503\n",
      "Epoch: 530, Loss= 1.0784, Training Accuracy= 0.502\n",
      "Epoch: 540, Loss= 0.9805, Training Accuracy= 0.502\n",
      "Epoch: 550, Loss= 0.9778, Training Accuracy= 0.502\n",
      "Epoch: 560, Loss= 0.9410, Training Accuracy= 0.505\n",
      "Epoch: 570, Loss= 0.9251, Training Accuracy= 0.507\n",
      "Epoch: 580, Loss= 0.9640, Training Accuracy= 0.504\n",
      "Epoch: 590, Loss= 1.0691, Training Accuracy= 0.502\n",
      "Epoch: 600, Loss= 1.0545, Training Accuracy= 0.502\n",
      "Epoch: 610, Loss= 1.0408, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 1.0613, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 1.1443, Training Accuracy= 0.502\n",
      "Epoch: 640, Loss= 0.9984, Training Accuracy= 0.502\n",
      "Epoch: 650, Loss= 0.9646, Training Accuracy= 0.502\n",
      "Epoch: 660, Loss= 0.9348, Training Accuracy= 0.502\n",
      "Epoch: 670, Loss= 0.9195, Training Accuracy= 0.502\n",
      "Epoch: 680, Loss= 0.9442, Training Accuracy= 0.502\n",
      "Epoch: 690, Loss= 0.9451, Training Accuracy= 0.502\n",
      "Epoch: 700, Loss= 0.9382, Training Accuracy= 0.502\n",
      "Epoch: 710, Loss= 0.9346, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.9050, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 1.0124, Training Accuracy= 0.502\n",
      "Epoch: 740, Loss= 0.8859, Training Accuracy= 0.502\n",
      "Epoch: 750, Loss= 0.8554, Training Accuracy= 0.502\n",
      "Epoch: 760, Loss= 0.8463, Training Accuracy= 0.502\n",
      "Epoch: 770, Loss= 0.9095, Training Accuracy= 0.502\n",
      "Epoch: 780, Loss= 0.9290, Training Accuracy= 0.502\n",
      "Epoch: 790, Loss= 0.9105, Training Accuracy= 0.502\n",
      "Epoch: 800, Loss= 0.8943, Training Accuracy= 0.502\n",
      "Epoch: 810, Loss= 0.9420, Training Accuracy= 0.502\n",
      "Epoch: 820, Loss= 0.9567, Training Accuracy= 0.502\n",
      "Epoch: 830, Loss= 1.0183, Training Accuracy= 0.502\n",
      "Epoch: 840, Loss= 0.8895, Training Accuracy= 0.504\n",
      "Epoch: 850, Loss= 1.0125, Training Accuracy= 0.502\n",
      "Epoch: 860, Loss= 0.9903, Training Accuracy= 0.504\n",
      "Epoch: 870, Loss= 1.0471, Training Accuracy= 0.502\n",
      "Epoch: 880, Loss= 0.8924, Training Accuracy= 0.502\n",
      "Epoch: 890, Loss= 0.8828, Training Accuracy= 0.502\n",
      "Epoch: 900, Loss= 0.8805, Training Accuracy= 0.502\n",
      "Epoch: 910, Loss= 0.8123, Training Accuracy= 0.505\n",
      "Epoch: 920, Loss= 0.8976, Training Accuracy= 0.502\n",
      "Epoch: 930, Loss= 0.7739, Training Accuracy= 0.502\n",
      "Epoch: 940, Loss= 0.7757, Training Accuracy= 0.502\n",
      "Epoch: 950, Loss= 0.8009, Training Accuracy= 0.513\n",
      "Epoch: 960, Loss= 0.7863, Training Accuracy= 0.502\n",
      "Epoch: 970, Loss= 0.7743, Training Accuracy= 0.502\n",
      "Epoch: 980, Loss= 0.7696, Training Accuracy= 0.502\n",
      "Epoch: 990, Loss= 0.8104, Training Accuracy= 0.502\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5012\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 1.4222, Training Accuracy= 0.499\n",
      "Epoch: 10, Loss= 0.9104, Training Accuracy= 0.499\n",
      "Epoch: 20, Loss= 0.7641, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.7441, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.7431, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.7419, Training Accuracy= 0.501\n",
      "Epoch: 60, Loss= 0.7331, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.7342, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.7590, Training Accuracy= 0.495\n",
      "Epoch: 90, Loss= 0.7458, Training Accuracy= 0.505\n",
      "Epoch: 100, Loss= 0.7315, Training Accuracy= 0.539\n",
      "Epoch: 110, Loss= 0.5692, Training Accuracy= 0.660\n",
      "Epoch: 120, Loss= 0.9239, Training Accuracy= 0.499\n",
      "Epoch: 130, Loss= 0.7903, Training Accuracy= 0.526\n",
      "Epoch: 140, Loss= 0.9252, Training Accuracy= 0.499\n",
      "Epoch: 150, Loss= 0.7584, Training Accuracy= 0.506\n",
      "Epoch: 160, Loss= 1.2745, Training Accuracy= 0.514\n",
      "Epoch: 170, Loss= 0.7498, Training Accuracy= 0.568\n",
      "Epoch: 180, Loss= 0.7861, Training Accuracy= 0.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.7893, Training Accuracy= 0.518\n",
      "Epoch: 200, Loss= 0.7941, Training Accuracy= 0.512\n",
      "Epoch: 210, Loss= 1.0309, Training Accuracy= 0.499\n",
      "Epoch: 220, Loss= 0.8225, Training Accuracy= 0.499\n",
      "Epoch: 230, Loss= 0.9191, Training Accuracy= 0.499\n",
      "Epoch: 240, Loss= 0.8733, Training Accuracy= 0.500\n",
      "Epoch: 250, Loss= 0.8078, Training Accuracy= 0.505\n",
      "Epoch: 260, Loss= 0.7260, Training Accuracy= 0.511\n",
      "Epoch: 270, Loss= 1.0771, Training Accuracy= 0.499\n",
      "Epoch: 280, Loss= 0.6453, Training Accuracy= 0.695\n",
      "Epoch: 290, Loss= 1.9043, Training Accuracy= 0.495\n",
      "Epoch: 300, Loss= 1.0239, Training Accuracy= 0.499\n",
      "Epoch: 310, Loss= 0.9244, Training Accuracy= 0.502\n",
      "Epoch: 320, Loss= 0.7815, Training Accuracy= 0.502\n",
      "Epoch: 330, Loss= 0.9534, Training Accuracy= 0.502\n",
      "Epoch: 340, Loss= 0.8133, Training Accuracy= 0.499\n",
      "Epoch: 350, Loss= 0.7246, Training Accuracy= 0.497\n",
      "Epoch: 360, Loss= 0.7356, Training Accuracy= 0.504\n",
      "Epoch: 370, Loss= 0.8329, Training Accuracy= 0.499\n",
      "Epoch: 380, Loss= 0.7437, Training Accuracy= 0.520\n",
      "Epoch: 390, Loss= 0.7276, Training Accuracy= 0.515\n",
      "Epoch: 400, Loss= 0.7337, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.7450, Training Accuracy= 0.500\n",
      "Epoch: 420, Loss= 0.7390, Training Accuracy= 0.528\n",
      "Epoch: 430, Loss= 0.7159, Training Accuracy= 0.521\n",
      "Epoch: 440, Loss= 0.7278, Training Accuracy= 0.492\n",
      "Epoch: 450, Loss= 0.7228, Training Accuracy= 0.521\n",
      "Epoch: 460, Loss= 0.7624, Training Accuracy= 0.520\n",
      "Epoch: 470, Loss= 0.7676, Training Accuracy= 0.498\n",
      "Epoch: 480, Loss= 0.7467, Training Accuracy= 0.502\n",
      "Epoch: 490, Loss= 0.7439, Training Accuracy= 0.502\n",
      "Epoch: 500, Loss= 0.7236, Training Accuracy= 0.511\n",
      "Epoch: 510, Loss= 0.7229, Training Accuracy= 0.515\n",
      "Epoch: 520, Loss= 0.7750, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.8251, Training Accuracy= 0.499\n",
      "Epoch: 540, Loss= 0.7851, Training Accuracy= 0.503\n",
      "Epoch: 550, Loss= 0.7774, Training Accuracy= 0.506\n",
      "Epoch: 560, Loss= 0.7624, Training Accuracy= 0.495\n",
      "Epoch: 570, Loss= 0.7581, Training Accuracy= 0.503\n",
      "Epoch: 580, Loss= 0.8100, Training Accuracy= 0.500\n",
      "Epoch: 590, Loss= 0.7598, Training Accuracy= 0.510\n",
      "Epoch: 600, Loss= 0.7441, Training Accuracy= 0.498\n",
      "Epoch: 610, Loss= 0.7174, Training Accuracy= 0.505\n",
      "Epoch: 620, Loss= 0.7165, Training Accuracy= 0.504\n",
      "Epoch: 630, Loss= 0.7229, Training Accuracy= 0.506\n",
      "Epoch: 640, Loss= 0.7243, Training Accuracy= 0.513\n",
      "Epoch: 650, Loss= 0.7114, Training Accuracy= 0.549\n",
      "Epoch: 660, Loss= 0.7155, Training Accuracy= 0.529\n",
      "Epoch: 670, Loss= 0.7556, Training Accuracy= 0.535\n",
      "Epoch: 680, Loss= 0.7122, Training Accuracy= 0.513\n",
      "Epoch: 690, Loss= 0.7493, Training Accuracy= 0.516\n",
      "Epoch: 700, Loss= 0.7132, Training Accuracy= 0.515\n",
      "Epoch: 710, Loss= 0.7324, Training Accuracy= 0.507\n",
      "Epoch: 720, Loss= 0.6999, Training Accuracy= 0.526\n",
      "Epoch: 730, Loss= 0.7060, Training Accuracy= 0.548\n",
      "Epoch: 740, Loss= 0.7009, Training Accuracy= 0.530\n",
      "Epoch: 750, Loss= 0.7298, Training Accuracy= 0.515\n",
      "Epoch: 760, Loss= 0.7364, Training Accuracy= 0.527\n",
      "Epoch: 770, Loss= 0.7184, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.7145, Training Accuracy= 0.520\n",
      "Epoch: 790, Loss= 0.7221, Training Accuracy= 0.508\n",
      "Epoch: 800, Loss= 0.7133, Training Accuracy= 0.504\n",
      "Epoch: 810, Loss= 0.7041, Training Accuracy= 0.510\n",
      "Epoch: 820, Loss= 0.6756, Training Accuracy= 0.560\n",
      "Epoch: 830, Loss= 0.7184, Training Accuracy= 0.520\n",
      "Epoch: 840, Loss= 0.7050, Training Accuracy= 0.530\n",
      "Epoch: 850, Loss= 0.7140, Training Accuracy= 0.521\n",
      "Epoch: 860, Loss= 0.7253, Training Accuracy= 0.510\n",
      "Epoch: 870, Loss= 0.7410, Training Accuracy= 0.499\n",
      "Epoch: 880, Loss= 0.7138, Training Accuracy= 0.500\n",
      "Epoch: 890, Loss= 0.7167, Training Accuracy= 0.508\n",
      "Epoch: 900, Loss= 0.7138, Training Accuracy= 0.507\n",
      "Epoch: 910, Loss= 0.7335, Training Accuracy= 0.506\n",
      "Epoch: 920, Loss= 0.7230, Training Accuracy= 0.510\n",
      "Epoch: 930, Loss= 0.7377, Training Accuracy= 0.504\n",
      "Epoch: 940, Loss= 0.7083, Training Accuracy= 0.504\n",
      "Epoch: 950, Loss= 0.7380, Training Accuracy= 0.505\n",
      "Epoch: 960, Loss= 0.7066, Training Accuracy= 0.510\n",
      "Epoch: 970, Loss= 0.7310, Training Accuracy= 0.497\n",
      "Epoch: 980, Loss= 0.7249, Training Accuracy= 0.499\n",
      "Epoch: 990, Loss= 0.7177, Training Accuracy= 0.499\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5027\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 1.8747, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.9891, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.9695, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.7139, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.7131, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.7120, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6945, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6935, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.6933, Training Accuracy= 0.501\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 130, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 140, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 150, Loss= 0.6964, Training Accuracy= 0.498\n",
      "Epoch: 160, Loss= 0.6985, Training Accuracy= 0.498\n",
      "Epoch: 170, Loss= 0.6982, Training Accuracy= 0.498\n",
      "Epoch: 180, Loss= 0.6981, Training Accuracy= 0.498\n",
      "Epoch: 190, Loss= 0.6983, Training Accuracy= 0.498\n",
      "Epoch: 200, Loss= 0.6986, Training Accuracy= 0.498\n",
      "Epoch: 210, Loss= 0.6988, Training Accuracy= 0.498\n",
      "Epoch: 220, Loss= 0.6992, Training Accuracy= 0.498\n",
      "Epoch: 230, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 240, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 250, Loss= 0.6983, Training Accuracy= 0.497\n",
      "Epoch: 260, Loss= 0.6985, Training Accuracy= 0.498\n",
      "Epoch: 270, Loss= 0.6993, Training Accuracy= 0.495\n",
      "Epoch: 280, Loss= 0.6991, Training Accuracy= 0.495\n",
      "Epoch: 290, Loss= 0.6990, Training Accuracy= 0.491\n",
      "Epoch: 300, Loss= 0.6996, Training Accuracy= 0.498\n",
      "Epoch: 310, Loss= 0.7011, Training Accuracy= 0.506\n",
      "Epoch: 320, Loss= 0.7019, Training Accuracy= 0.506\n",
      "Epoch: 330, Loss= 0.6976, Training Accuracy= 0.513\n",
      "Epoch: 340, Loss= 0.6670, Training Accuracy= 0.630\n",
      "Epoch: 350, Loss= 0.7355, Training Accuracy= 0.491\n",
      "Epoch: 360, Loss= 0.7274, Training Accuracy= 0.524\n",
      "Epoch: 370, Loss= 0.7123, Training Accuracy= 0.508\n",
      "Epoch: 380, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.6982, Training Accuracy= 0.515\n",
      "Epoch: 400, Loss= 0.6973, Training Accuracy= 0.510\n",
      "Epoch: 410, Loss= 0.6973, Training Accuracy= 0.521\n",
      "Epoch: 420, Loss= 0.7001, Training Accuracy= 0.510\n",
      "Epoch: 430, Loss= 0.7107, Training Accuracy= 0.506\n",
      "Epoch: 440, Loss= 0.7185, Training Accuracy= 0.506\n",
      "Epoch: 450, Loss= 0.7354, Training Accuracy= 0.515\n",
      "Epoch: 460, Loss= 0.7080, Training Accuracy= 0.547\n",
      "Epoch: 470, Loss= 0.7330, Training Accuracy= 0.490\n",
      "Epoch: 480, Loss= 0.7512, Training Accuracy= 0.493\n",
      "Epoch: 490, Loss= 0.7271, Training Accuracy= 0.498\n",
      "Epoch: 500, Loss= 0.7255, Training Accuracy= 0.491\n",
      "Epoch: 510, Loss= 0.7193, Training Accuracy= 0.512\n",
      "Epoch: 520, Loss= 0.7406, Training Accuracy= 0.505\n",
      "Epoch: 530, Loss= 0.7289, Training Accuracy= 0.490\n",
      "Epoch: 540, Loss= 0.7236, Training Accuracy= 0.532\n",
      "Epoch: 550, Loss= 0.7266, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.7193, Training Accuracy= 0.505\n",
      "Epoch: 570, Loss= 0.7086, Training Accuracy= 0.518\n",
      "Epoch: 580, Loss= 0.7308, Training Accuracy= 0.510\n",
      "Epoch: 590, Loss= 0.7514, Training Accuracy= 0.511\n",
      "Epoch: 600, Loss= 0.6882, Training Accuracy= 0.533\n",
      "Epoch: 610, Loss= 0.7589, Training Accuracy= 0.501\n",
      "Epoch: 620, Loss= 0.7148, Training Accuracy= 0.528\n",
      "Epoch: 630, Loss= 0.7060, Training Accuracy= 0.503\n",
      "Epoch: 640, Loss= 0.6943, Training Accuracy= 0.514\n",
      "Epoch: 650, Loss= 0.6980, Training Accuracy= 0.516\n",
      "Epoch: 660, Loss= 0.7054, Training Accuracy= 0.512\n",
      "Epoch: 670, Loss= 0.7009, Training Accuracy= 0.516\n",
      "Epoch: 680, Loss= 0.7043, Training Accuracy= 0.520\n",
      "Epoch: 690, Loss= 0.7373, Training Accuracy= 0.503\n",
      "Epoch: 700, Loss= 0.7171, Training Accuracy= 0.501\n",
      "Epoch: 710, Loss= 0.6990, Training Accuracy= 0.522\n",
      "Epoch: 720, Loss= 0.7496, Training Accuracy= 0.490\n",
      "Epoch: 730, Loss= 0.6915, Training Accuracy= 0.527\n",
      "Epoch: 740, Loss= 0.6927, Training Accuracy= 0.528\n",
      "Epoch: 750, Loss= 0.7000, Training Accuracy= 0.515\n",
      "Epoch: 760, Loss= 0.7053, Training Accuracy= 0.566\n",
      "Epoch: 770, Loss= 0.7997, Training Accuracy= 0.502\n",
      "Epoch: 780, Loss= 0.7458, Training Accuracy= 0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.7401, Training Accuracy= 0.505\n",
      "Epoch: 800, Loss= 0.7139, Training Accuracy= 0.503\n",
      "Epoch: 810, Loss= 0.7590, Training Accuracy= 0.487\n",
      "Epoch: 820, Loss= 0.7105, Training Accuracy= 0.516\n",
      "Epoch: 830, Loss= 0.7251, Training Accuracy= 0.507\n",
      "Epoch: 840, Loss= 0.7406, Training Accuracy= 0.509\n",
      "Epoch: 850, Loss= 0.7513, Training Accuracy= 0.511\n",
      "Epoch: 860, Loss= 0.7407, Training Accuracy= 0.514\n",
      "Epoch: 870, Loss= 0.7692, Training Accuracy= 0.523\n",
      "Epoch: 880, Loss= 0.6982, Training Accuracy= 0.518\n",
      "Epoch: 890, Loss= 0.7148, Training Accuracy= 0.523\n",
      "Epoch: 900, Loss= 0.7516, Training Accuracy= 0.513\n",
      "Epoch: 910, Loss= 0.6997, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.7784, Training Accuracy= 0.493\n",
      "Epoch: 930, Loss= 0.7075, Training Accuracy= 0.501\n",
      "Epoch: 940, Loss= 0.6920, Training Accuracy= 0.529\n",
      "Epoch: 950, Loss= 0.7244, Training Accuracy= 0.500\n",
      "Epoch: 960, Loss= 0.7300, Training Accuracy= 0.505\n",
      "Epoch: 970, Loss= 0.6980, Training Accuracy= 0.528\n",
      "Epoch: 980, Loss= 0.7355, Training Accuracy= 0.497\n",
      "Epoch: 990, Loss= 0.7147, Training Accuracy= 0.535\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5065\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 1.2920, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.9154, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.7224, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.7041, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.7003, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6998, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.7018, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.7025, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.7030, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.7050, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.7032, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.7031, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.7033, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.7042, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.7002, Training Accuracy= 0.511\n",
      "Epoch: 150, Loss= 0.6994, Training Accuracy= 0.508\n",
      "Epoch: 160, Loss= 0.6985, Training Accuracy= 0.504\n",
      "Epoch: 170, Loss= 0.6982, Training Accuracy= 0.502\n",
      "Epoch: 180, Loss= 0.6982, Training Accuracy= 0.505\n",
      "Epoch: 190, Loss= 0.6980, Training Accuracy= 0.513\n",
      "Epoch: 200, Loss= 0.6976, Training Accuracy= 0.510\n",
      "Epoch: 210, Loss= 0.6971, Training Accuracy= 0.498\n",
      "Epoch: 220, Loss= 0.6968, Training Accuracy= 0.513\n",
      "Epoch: 230, Loss= 0.6957, Training Accuracy= 0.518\n",
      "Epoch: 240, Loss= 0.6967, Training Accuracy= 0.518\n",
      "Epoch: 250, Loss= 0.6931, Training Accuracy= 0.525\n",
      "Epoch: 260, Loss= 0.7008, Training Accuracy= 0.522\n",
      "Epoch: 270, Loss= 0.8331, Training Accuracy= 0.515\n",
      "Epoch: 280, Loss= 0.6818, Training Accuracy= 0.617\n",
      "Epoch: 290, Loss= 0.7283, Training Accuracy= 0.540\n",
      "Epoch: 300, Loss= 0.7577, Training Accuracy= 0.511\n",
      "Epoch: 310, Loss= 0.7151, Training Accuracy= 0.553\n",
      "Epoch: 320, Loss= 0.7009, Training Accuracy= 0.570\n",
      "Epoch: 330, Loss= 0.6882, Training Accuracy= 0.521\n",
      "Epoch: 340, Loss= 0.7035, Training Accuracy= 0.510\n",
      "Epoch: 350, Loss= 0.6637, Training Accuracy= 0.604\n",
      "Epoch: 360, Loss= 0.7196, Training Accuracy= 0.558\n",
      "Epoch: 370, Loss= 0.7301, Training Accuracy= 0.518\n",
      "Epoch: 380, Loss= 0.7078, Training Accuracy= 0.502\n",
      "Epoch: 390, Loss= 0.7272, Training Accuracy= 0.508\n",
      "Epoch: 400, Loss= 0.7391, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6980, Training Accuracy= 0.519\n",
      "Epoch: 420, Loss= 0.7144, Training Accuracy= 0.505\n",
      "Epoch: 430, Loss= 0.7074, Training Accuracy= 0.492\n",
      "Epoch: 440, Loss= 0.7140, Training Accuracy= 0.501\n",
      "Epoch: 450, Loss= 0.7264, Training Accuracy= 0.498\n",
      "Epoch: 460, Loss= 0.7399, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.7138, Training Accuracy= 0.507\n",
      "Epoch: 480, Loss= 0.7199, Training Accuracy= 0.507\n",
      "Epoch: 490, Loss= 0.6943, Training Accuracy= 0.548\n",
      "Epoch: 500, Loss= 0.7135, Training Accuracy= 0.537\n",
      "Epoch: 510, Loss= 0.7189, Training Accuracy= 0.508\n",
      "Epoch: 520, Loss= 0.7955, Training Accuracy= 0.503\n",
      "Epoch: 530, Loss= 0.7005, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.7148, Training Accuracy= 0.519\n",
      "Epoch: 550, Loss= 0.7262, Training Accuracy= 0.501\n",
      "Epoch: 560, Loss= 0.7210, Training Accuracy= 0.500\n",
      "Epoch: 570, Loss= 0.7219, Training Accuracy= 0.500\n",
      "Epoch: 580, Loss= 0.7223, Training Accuracy= 0.502\n",
      "Epoch: 590, Loss= 0.7251, Training Accuracy= 0.501\n",
      "Epoch: 600, Loss= 0.7262, Training Accuracy= 0.502\n",
      "Epoch: 610, Loss= 0.7264, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 0.7257, Training Accuracy= 0.505\n",
      "Epoch: 630, Loss= 0.7257, Training Accuracy= 0.503\n",
      "Epoch: 640, Loss= 0.7164, Training Accuracy= 0.506\n",
      "Epoch: 650, Loss= 0.7257, Training Accuracy= 0.505\n",
      "Epoch: 660, Loss= 0.7128, Training Accuracy= 0.510\n",
      "Epoch: 670, Loss= 0.7281, Training Accuracy= 0.508\n",
      "Epoch: 680, Loss= 0.7113, Training Accuracy= 0.512\n",
      "Epoch: 690, Loss= 0.7124, Training Accuracy= 0.511\n",
      "Epoch: 700, Loss= 0.7190, Training Accuracy= 0.521\n",
      "Epoch: 710, Loss= 0.7445, Training Accuracy= 0.497\n",
      "Epoch: 720, Loss= 0.7105, Training Accuracy= 0.500\n",
      "Epoch: 730, Loss= 0.7242, Training Accuracy= 0.504\n",
      "Epoch: 740, Loss= 0.7202, Training Accuracy= 0.506\n",
      "Epoch: 750, Loss= 0.7051, Training Accuracy= 0.531\n",
      "Epoch: 760, Loss= 0.7041, Training Accuracy= 0.504\n",
      "Epoch: 770, Loss= 0.7003, Training Accuracy= 0.512\n",
      "Epoch: 780, Loss= 0.7090, Training Accuracy= 0.504\n",
      "Epoch: 790, Loss= 0.7014, Training Accuracy= 0.507\n",
      "Epoch: 800, Loss= 0.7063, Training Accuracy= 0.530\n",
      "Epoch: 810, Loss= 0.6883, Training Accuracy= 0.555\n",
      "Epoch: 820, Loss= 0.7071, Training Accuracy= 0.504\n",
      "Epoch: 830, Loss= 0.7012, Training Accuracy= 0.505\n",
      "Epoch: 840, Loss= 0.7041, Training Accuracy= 0.511\n",
      "Epoch: 850, Loss= 0.7475, Training Accuracy= 0.515\n",
      "Epoch: 860, Loss= 0.7030, Training Accuracy= 0.504\n",
      "Epoch: 870, Loss= 0.7016, Training Accuracy= 0.504\n",
      "Epoch: 880, Loss= 0.7030, Training Accuracy= 0.505\n",
      "Epoch: 890, Loss= 0.6858, Training Accuracy= 0.541\n",
      "Epoch: 900, Loss= 0.6953, Training Accuracy= 0.509\n",
      "Epoch: 910, Loss= 0.6815, Training Accuracy= 0.577\n",
      "Epoch: 920, Loss= 0.7581, Training Accuracy= 0.503\n",
      "Epoch: 930, Loss= 0.7008, Training Accuracy= 0.507\n",
      "Epoch: 940, Loss= 0.7131, Training Accuracy= 0.498\n",
      "Epoch: 950, Loss= 0.7194, Training Accuracy= 0.517\n",
      "Epoch: 960, Loss= 0.7049, Training Accuracy= 0.515\n",
      "Epoch: 970, Loss= 0.7172, Training Accuracy= 0.511\n",
      "Epoch: 980, Loss= 0.7083, Training Accuracy= 0.511\n",
      "Epoch: 990, Loss= 0.6997, Training Accuracy= 0.510\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4949\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.8007, Training Accuracy= 0.508\n",
      "Epoch: 10, Loss= 0.6996, Training Accuracy= 0.508\n",
      "Epoch: 20, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6945, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.6948, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.6952, Training Accuracy= 0.505\n",
      "Epoch: 60, Loss= 0.6964, Training Accuracy= 0.499\n",
      "Epoch: 70, Loss= 0.6977, Training Accuracy= 0.495\n",
      "Epoch: 80, Loss= 0.6981, Training Accuracy= 0.504\n",
      "Epoch: 90, Loss= 0.6977, Training Accuracy= 0.494\n",
      "Epoch: 100, Loss= 0.6971, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.6976, Training Accuracy= 0.501\n",
      "Epoch: 120, Loss= 0.6987, Training Accuracy= 0.509\n",
      "Epoch: 130, Loss= 0.7059, Training Accuracy= 0.504\n",
      "Epoch: 140, Loss= 0.7074, Training Accuracy= 0.505\n",
      "Epoch: 150, Loss= 0.7055, Training Accuracy= 0.512\n",
      "Epoch: 160, Loss= 0.7050, Training Accuracy= 0.516\n",
      "Epoch: 170, Loss= 0.6994, Training Accuracy= 0.499\n",
      "Epoch: 180, Loss= 0.7351, Training Accuracy= 0.509\n",
      "Epoch: 190, Loss= 0.7264, Training Accuracy= 0.513\n",
      "Epoch: 200, Loss= 0.7005, Training Accuracy= 0.512\n",
      "Epoch: 210, Loss= 0.7083, Training Accuracy= 0.497\n",
      "Epoch: 220, Loss= 0.7047, Training Accuracy= 0.499\n",
      "Epoch: 230, Loss= 0.6900, Training Accuracy= 0.545\n",
      "Epoch: 240, Loss= 0.7073, Training Accuracy= 0.515\n",
      "Epoch: 250, Loss= 0.7151, Training Accuracy= 0.490\n",
      "Epoch: 260, Loss= 0.7136, Training Accuracy= 0.498\n",
      "Epoch: 270, Loss= 0.7102, Training Accuracy= 0.505\n",
      "Epoch: 280, Loss= 0.7088, Training Accuracy= 0.500\n",
      "Epoch: 290, Loss= 0.7853, Training Accuracy= 0.496\n",
      "Epoch: 300, Loss= 0.6405, Training Accuracy= 0.651\n",
      "Epoch: 310, Loss= 0.6539, Training Accuracy= 0.680\n",
      "Epoch: 320, Loss= 0.6240, Training Accuracy= 0.632\n",
      "Epoch: 330, Loss= 0.7085, Training Accuracy= 0.496\n",
      "Epoch: 340, Loss= 0.6236, Training Accuracy= 0.662\n",
      "Epoch: 350, Loss= 0.6942, Training Accuracy= 0.586\n",
      "Epoch: 360, Loss= 0.7360, Training Accuracy= 0.507\n",
      "Epoch: 370, Loss= 0.6887, Training Accuracy= 0.535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6950, Training Accuracy= 0.521\n",
      "Epoch: 390, Loss= 0.6977, Training Accuracy= 0.553\n",
      "Epoch: 400, Loss= 0.6885, Training Accuracy= 0.558\n",
      "Epoch: 410, Loss= 0.7270, Training Accuracy= 0.530\n",
      "Epoch: 420, Loss= 0.7032, Training Accuracy= 0.550\n",
      "Epoch: 430, Loss= 0.7927, Training Accuracy= 0.504\n",
      "Epoch: 440, Loss= 0.7093, Training Accuracy= 0.532\n",
      "Epoch: 450, Loss= 0.7469, Training Accuracy= 0.496\n",
      "Epoch: 460, Loss= 0.6965, Training Accuracy= 0.547\n",
      "Epoch: 470, Loss= 0.6990, Training Accuracy= 0.517\n",
      "Epoch: 480, Loss= 0.7112, Training Accuracy= 0.526\n",
      "Epoch: 490, Loss= 0.6884, Training Accuracy= 0.536\n",
      "Epoch: 500, Loss= 0.7143, Training Accuracy= 0.539\n",
      "Epoch: 510, Loss= 0.6995, Training Accuracy= 0.522\n",
      "Epoch: 520, Loss= 0.7055, Training Accuracy= 0.545\n",
      "Epoch: 530, Loss= 0.6893, Training Accuracy= 0.531\n",
      "Epoch: 540, Loss= 0.7016, Training Accuracy= 0.509\n",
      "Epoch: 550, Loss= 0.6996, Training Accuracy= 0.529\n",
      "Epoch: 560, Loss= 0.6977, Training Accuracy= 0.510\n",
      "Epoch: 570, Loss= 0.7288, Training Accuracy= 0.505\n",
      "Epoch: 580, Loss= 0.7255, Training Accuracy= 0.521\n",
      "Epoch: 590, Loss= 0.6917, Training Accuracy= 0.541\n",
      "Epoch: 600, Loss= 0.7080, Training Accuracy= 0.516\n",
      "Epoch: 610, Loss= 0.7158, Training Accuracy= 0.506\n",
      "Epoch: 620, Loss= 0.7013, Training Accuracy= 0.528\n",
      "Epoch: 630, Loss= 0.6935, Training Accuracy= 0.529\n",
      "Epoch: 640, Loss= 0.7073, Training Accuracy= 0.512\n",
      "Epoch: 650, Loss= 0.7022, Training Accuracy= 0.517\n",
      "Epoch: 660, Loss= 0.7197, Training Accuracy= 0.514\n",
      "Epoch: 670, Loss= 0.7001, Training Accuracy= 0.524\n",
      "Epoch: 680, Loss= 0.7019, Training Accuracy= 0.533\n",
      "Epoch: 690, Loss= 0.7121, Training Accuracy= 0.513\n",
      "Epoch: 700, Loss= 0.7126, Training Accuracy= 0.510\n",
      "Epoch: 710, Loss= 0.7081, Training Accuracy= 0.533\n",
      "Epoch: 720, Loss= 0.6988, Training Accuracy= 0.512\n",
      "Epoch: 730, Loss= 0.6983, Training Accuracy= 0.529\n",
      "Epoch: 740, Loss= 0.7105, Training Accuracy= 0.517\n",
      "Epoch: 750, Loss= 0.7076, Training Accuracy= 0.532\n",
      "Epoch: 760, Loss= 0.7265, Training Accuracy= 0.524\n",
      "Epoch: 770, Loss= 0.7281, Training Accuracy= 0.500\n",
      "Epoch: 780, Loss= 0.7088, Training Accuracy= 0.500\n",
      "Epoch: 790, Loss= 0.7079, Training Accuracy= 0.519\n",
      "Epoch: 800, Loss= 0.6934, Training Accuracy= 0.521\n",
      "Epoch: 810, Loss= 0.6994, Training Accuracy= 0.522\n",
      "Epoch: 820, Loss= 0.7252, Training Accuracy= 0.506\n",
      "Epoch: 830, Loss= 0.7046, Training Accuracy= 0.534\n",
      "Epoch: 840, Loss= 0.7108, Training Accuracy= 0.540\n",
      "Epoch: 850, Loss= 0.7103, Training Accuracy= 0.518\n",
      "Epoch: 860, Loss= 0.7233, Training Accuracy= 0.514\n",
      "Epoch: 870, Loss= 0.7480, Training Accuracy= 0.513\n",
      "Epoch: 880, Loss= 0.7072, Training Accuracy= 0.509\n",
      "Epoch: 890, Loss= 0.7322, Training Accuracy= 0.513\n",
      "Epoch: 900, Loss= 0.7070, Training Accuracy= 0.512\n",
      "Epoch: 910, Loss= 0.7219, Training Accuracy= 0.501\n",
      "Epoch: 920, Loss= 0.7575, Training Accuracy= 0.512\n",
      "Epoch: 930, Loss= 0.7333, Training Accuracy= 0.497\n",
      "Epoch: 940, Loss= 0.7178, Training Accuracy= 0.511\n",
      "Epoch: 950, Loss= 0.6941, Training Accuracy= 0.525\n",
      "Epoch: 960, Loss= 0.7045, Training Accuracy= 0.498\n",
      "Epoch: 970, Loss= 0.7057, Training Accuracy= 0.506\n",
      "Epoch: 980, Loss= 0.7277, Training Accuracy= 0.511\n",
      "Epoch: 990, Loss= 0.7022, Training Accuracy= 0.531\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5174\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.7103, Training Accuracy= 0.507\n",
      "Epoch: 10, Loss= 0.8026, Training Accuracy= 0.493\n",
      "Epoch: 20, Loss= 0.7997, Training Accuracy= 0.493\n",
      "Epoch: 30, Loss= 0.7960, Training Accuracy= 0.493\n",
      "Epoch: 40, Loss= 0.7855, Training Accuracy= 0.493\n",
      "Epoch: 50, Loss= 0.7731, Training Accuracy= 0.493\n",
      "Epoch: 60, Loss= 0.7616, Training Accuracy= 0.493\n",
      "Epoch: 70, Loss= 0.7548, Training Accuracy= 0.493\n",
      "Epoch: 80, Loss= 0.7540, Training Accuracy= 0.493\n",
      "Epoch: 90, Loss= 0.7525, Training Accuracy= 0.493\n",
      "Epoch: 100, Loss= 0.7486, Training Accuracy= 0.540\n",
      "Epoch: 110, Loss= 0.7946, Training Accuracy= 0.493\n",
      "Epoch: 120, Loss= 0.7770, Training Accuracy= 0.508\n",
      "Epoch: 130, Loss= 0.8040, Training Accuracy= 0.490\n",
      "Epoch: 140, Loss= 0.8709, Training Accuracy= 0.521\n",
      "Epoch: 150, Loss= 0.7210, Training Accuracy= 0.590\n",
      "Epoch: 160, Loss= 0.7793, Training Accuracy= 0.507\n",
      "Epoch: 170, Loss= 0.7450, Training Accuracy= 0.542\n",
      "Epoch: 180, Loss= 0.8127, Training Accuracy= 0.500\n",
      "Epoch: 190, Loss= 0.0089, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.0038, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.0024, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.0017, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.0013, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.0011, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.0009, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0000, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0000, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0000, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0000, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.0000, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0000, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.9918, Training Accuracy= 0.493\n",
      "Epoch: 10, Loss= 0.8471, Training Accuracy= 0.493\n",
      "Epoch: 20, Loss= 0.7855, Training Accuracy= 0.493\n",
      "Epoch: 30, Loss= 0.7487, Training Accuracy= 0.493\n",
      "Epoch: 40, Loss= 0.7303, Training Accuracy= 0.493\n",
      "Epoch: 50, Loss= 0.7213, Training Accuracy= 0.496\n",
      "Epoch: 60, Loss= 0.7157, Training Accuracy= 0.494\n",
      "Epoch: 70, Loss= 0.7123, Training Accuracy= 0.493\n",
      "Epoch: 80, Loss= 0.7105, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.7101, Training Accuracy= 0.500\n",
      "Epoch: 100, Loss= 0.7101, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.7095, Training Accuracy= 0.504\n",
      "Epoch: 120, Loss= 0.7097, Training Accuracy= 0.506\n",
      "Epoch: 130, Loss= 0.7123, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.7039, Training Accuracy= 0.555\n",
      "Epoch: 150, Loss= 0.7819, Training Accuracy= 0.495\n",
      "Epoch: 160, Loss= 0.8122, Training Accuracy= 0.495\n",
      "Epoch: 170, Loss= 0.7994, Training Accuracy= 0.509\n",
      "Epoch: 180, Loss= 0.7782, Training Accuracy= 0.528\n",
      "Epoch: 190, Loss= 0.8269, Training Accuracy= 0.500\n",
      "Epoch: 200, Loss= 0.8600, Training Accuracy= 0.495\n",
      "Epoch: 210, Loss= 0.8446, Training Accuracy= 0.495\n",
      "Epoch: 220, Loss= 0.8500, Training Accuracy= 0.493\n",
      "Epoch: 230, Loss= 0.8258, Training Accuracy= 0.496\n",
      "Epoch: 240, Loss= 0.8203, Training Accuracy= 0.498\n",
      "Epoch: 250, Loss= 0.8119, Training Accuracy= 0.504\n",
      "Epoch: 260, Loss= 0.8948, Training Accuracy= 0.496\n",
      "Epoch: 270, Loss= 0.9702, Training Accuracy= 0.495\n",
      "Epoch: 280, Loss= 0.8035, Training Accuracy= 0.493\n",
      "Epoch: 290, Loss= 0.7671, Training Accuracy= 0.569\n",
      "Epoch: 300, Loss= 1.0603, Training Accuracy= 0.486\n",
      "Epoch: 310, Loss= 0.9025, Training Accuracy= 0.504\n",
      "Epoch: 320, Loss= 0.9239, Training Accuracy= 0.555\n",
      "Epoch: 330, Loss= 0.7786, Training Accuracy= 0.641\n",
      "Epoch: 340, Loss= 0.8228, Training Accuracy= 0.552\n",
      "Epoch: 350, Loss= 0.8907, Training Accuracy= 0.494\n",
      "Epoch: 360, Loss= 0.8905, Training Accuracy= 0.492\n",
      "Epoch: 370, Loss= 0.7127, Training Accuracy= 0.605\n",
      "Epoch: 380, Loss= 0.9508, Training Accuracy= 0.498\n",
      "Epoch: 390, Loss= 0.9112, Training Accuracy= 0.495\n",
      "Epoch: 400, Loss= 0.9576, Training Accuracy= 0.514\n",
      "Epoch: 410, Loss= 0.9327, Training Accuracy= 0.517\n",
      "Epoch: 420, Loss= 1.0040, Training Accuracy= 0.509\n",
      "Epoch: 430, Loss= 1.2891, Training Accuracy= 0.493\n",
      "Epoch: 440, Loss= 1.2348, Training Accuracy= 0.493\n",
      "Epoch: 450, Loss= 1.2174, Training Accuracy= 0.493\n",
      "Epoch: 460, Loss= 1.1491, Training Accuracy= 0.493\n",
      "Epoch: 470, Loss= 1.1089, Training Accuracy= 0.495\n",
      "Epoch: 480, Loss= 1.0518, Training Accuracy= 0.507\n",
      "Epoch: 490, Loss= 0.9202, Training Accuracy= 0.493\n",
      "Epoch: 500, Loss= 1.5375, Training Accuracy= 0.493\n",
      "Epoch: 510, Loss= 1.3154, Training Accuracy= 0.493\n",
      "Epoch: 520, Loss= 1.1596, Training Accuracy= 0.493\n",
      "Epoch: 530, Loss= 1.2284, Training Accuracy= 0.493\n",
      "Epoch: 540, Loss= 1.1498, Training Accuracy= 0.494\n",
      "Epoch: 550, Loss= 1.0519, Training Accuracy= 0.493\n",
      "Epoch: 560, Loss= 1.0236, Training Accuracy= 0.493\n",
      "Epoch: 570, Loss= 1.0728, Training Accuracy= 0.499\n",
      "Epoch: 580, Loss= 1.0401, Training Accuracy= 0.493\n",
      "Epoch: 590, Loss= 1.0336, Training Accuracy= 0.493\n",
      "Epoch: 600, Loss= 1.1056, Training Accuracy= 0.493\n",
      "Epoch: 610, Loss= 1.0639, Training Accuracy= 0.493\n",
      "Epoch: 620, Loss= 1.1055, Training Accuracy= 0.493\n",
      "Epoch: 630, Loss= 0.9741, Training Accuracy= 0.493\n",
      "Epoch: 640, Loss= 0.9926, Training Accuracy= 0.493\n",
      "Epoch: 650, Loss= 1.0323, Training Accuracy= 0.493\n",
      "Epoch: 660, Loss= 1.0129, Training Accuracy= 0.512\n",
      "Epoch: 670, Loss= 0.9533, Training Accuracy= 0.508\n",
      "Epoch: 680, Loss= 1.0783, Training Accuracy= 0.493\n",
      "Epoch: 690, Loss= 1.0785, Training Accuracy= 0.493\n",
      "Epoch: 700, Loss= 0.8947, Training Accuracy= 0.498\n",
      "Epoch: 710, Loss= 0.9292, Training Accuracy= 0.494\n",
      "Epoch: 720, Loss= 0.9585, Training Accuracy= 0.493\n",
      "Epoch: 730, Loss= 0.9023, Training Accuracy= 0.492\n",
      "Epoch: 740, Loss= 0.8863, Training Accuracy= 0.493\n",
      "Epoch: 750, Loss= 0.9435, Training Accuracy= 0.493\n",
      "Epoch: 760, Loss= 0.9500, Training Accuracy= 0.493\n",
      "Epoch: 770, Loss= 0.9494, Training Accuracy= 0.493\n",
      "Epoch: 780, Loss= 0.9411, Training Accuracy= 0.493\n",
      "Epoch: 790, Loss= 0.9544, Training Accuracy= 0.493\n",
      "Epoch: 800, Loss= 1.0148, Training Accuracy= 0.493\n",
      "Epoch: 810, Loss= 0.9716, Training Accuracy= 0.493\n",
      "Epoch: 820, Loss= 0.9605, Training Accuracy= 0.493\n",
      "Epoch: 830, Loss= 0.9105, Training Accuracy= 0.493\n",
      "Epoch: 840, Loss= 0.9170, Training Accuracy= 0.493\n",
      "Epoch: 850, Loss= 0.9039, Training Accuracy= 0.493\n",
      "Epoch: 860, Loss= 0.8815, Training Accuracy= 0.493\n",
      "Epoch: 870, Loss= 0.9247, Training Accuracy= 0.495\n",
      "Epoch: 880, Loss= 0.8587, Training Accuracy= 0.493\n",
      "Epoch: 890, Loss= 0.8167, Training Accuracy= 0.505\n",
      "Epoch: 900, Loss= 0.9529, Training Accuracy= 0.496\n",
      "Epoch: 910, Loss= 0.9109, Training Accuracy= 0.491\n",
      "Epoch: 920, Loss= 1.1224, Training Accuracy= 0.493\n",
      "Epoch: 930, Loss= 1.0273, Training Accuracy= 0.494\n",
      "Epoch: 940, Loss= 1.0035, Training Accuracy= 0.495\n",
      "Epoch: 950, Loss= 0.8745, Training Accuracy= 0.495\n",
      "Epoch: 960, Loss= 0.8923, Training Accuracy= 0.499\n",
      "Epoch: 970, Loss= 1.1310, Training Accuracy= 0.493\n",
      "Epoch: 980, Loss= 0.9727, Training Accuracy= 0.493\n",
      "Epoch: 990, Loss= 0.9887, Training Accuracy= 0.493\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5048\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 2.3250, Training Accuracy= 0.494\n",
      "Epoch: 10, Loss= 1.7819, Training Accuracy= 0.494\n",
      "Epoch: 20, Loss= 1.4898, Training Accuracy= 0.494\n",
      "Epoch: 30, Loss= 0.7063, Training Accuracy= 0.506\n",
      "Epoch: 40, Loss= 0.7062, Training Accuracy= 0.506\n",
      "Epoch: 50, Loss= 0.7062, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.7060, Training Accuracy= 0.506\n",
      "Epoch: 70, Loss= 0.6943, Training Accuracy= 0.506\n",
      "Epoch: 80, Loss= 0.6942, Training Accuracy= 0.506\n",
      "Epoch: 90, Loss= 0.6985, Training Accuracy= 0.494\n",
      "Epoch: 100, Loss= 0.6985, Training Accuracy= 0.494\n",
      "Epoch: 110, Loss= 0.6985, Training Accuracy= 0.494\n",
      "Epoch: 120, Loss= 0.6985, Training Accuracy= 0.494\n",
      "Epoch: 130, Loss= 0.6985, Training Accuracy= 0.494\n",
      "Epoch: 140, Loss= 0.6985, Training Accuracy= 0.494\n",
      "Epoch: 150, Loss= 0.6985, Training Accuracy= 0.494\n",
      "Epoch: 160, Loss= 0.6985, Training Accuracy= 0.494\n",
      "Epoch: 170, Loss= 0.6986, Training Accuracy= 0.494\n",
      "Epoch: 180, Loss= 0.6986, Training Accuracy= 0.494\n",
      "Epoch: 190, Loss= 0.6986, Training Accuracy= 0.494\n",
      "Epoch: 200, Loss= 0.6986, Training Accuracy= 0.494\n",
      "Epoch: 210, Loss= 0.6987, Training Accuracy= 0.494\n",
      "Epoch: 220, Loss= 0.6988, Training Accuracy= 0.494\n",
      "Epoch: 230, Loss= 0.6991, Training Accuracy= 0.494\n",
      "Epoch: 240, Loss= 0.6993, Training Accuracy= 0.494\n",
      "Epoch: 250, Loss= 0.7000, Training Accuracy= 0.494\n",
      "Epoch: 260, Loss= 0.7318, Training Accuracy= 0.494\n",
      "Epoch: 270, Loss= 0.7320, Training Accuracy= 0.494\n",
      "Epoch: 280, Loss= 0.7325, Training Accuracy= 0.494\n",
      "Epoch: 290, Loss= 0.7337, Training Accuracy= 0.494\n",
      "Epoch: 300, Loss= 0.7370, Training Accuracy= 0.494\n",
      "Epoch: 310, Loss= 0.7525, Training Accuracy= 0.494\n",
      "Epoch: 320, Loss= 0.7528, Training Accuracy= 0.494\n",
      "Epoch: 330, Loss= 0.7534, Training Accuracy= 0.494\n",
      "Epoch: 340, Loss= 0.7558, Training Accuracy= 0.494\n",
      "Epoch: 350, Loss= 0.7558, Training Accuracy= 0.494\n",
      "Epoch: 360, Loss= 0.7558, Training Accuracy= 0.494\n",
      "Epoch: 370, Loss= 0.7558, Training Accuracy= 0.494\n",
      "Epoch: 380, Loss= 0.7562, Training Accuracy= 0.494\n",
      "Epoch: 390, Loss= 0.7517, Training Accuracy= 0.494\n",
      "Epoch: 400, Loss= 0.7512, Training Accuracy= 0.494\n",
      "Epoch: 410, Loss= 0.7510, Training Accuracy= 0.494\n",
      "Epoch: 420, Loss= 0.7508, Training Accuracy= 0.494\n",
      "Epoch: 430, Loss= 0.7506, Training Accuracy= 0.494\n",
      "Epoch: 440, Loss= 0.7506, Training Accuracy= 0.494\n",
      "Epoch: 450, Loss= 0.7506, Training Accuracy= 0.494\n",
      "Epoch: 460, Loss= 0.7507, Training Accuracy= 0.494\n",
      "Epoch: 470, Loss= 0.7508, Training Accuracy= 0.494\n",
      "Epoch: 480, Loss= 0.7512, Training Accuracy= 0.494\n",
      "Epoch: 490, Loss= 0.7548, Training Accuracy= 0.494\n",
      "Epoch: 500, Loss= 0.7522, Training Accuracy= 0.494\n",
      "Epoch: 510, Loss= 0.7507, Training Accuracy= 0.492\n",
      "Epoch: 520, Loss= 0.7580, Training Accuracy= 0.492\n",
      "Epoch: 530, Loss= 0.8108, Training Accuracy= 0.500\n",
      "Epoch: 540, Loss= 0.7791, Training Accuracy= 0.500\n",
      "Epoch: 550, Loss= 0.7636, Training Accuracy= 0.500\n",
      "Epoch: 560, Loss= 0.7617, Training Accuracy= 0.498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.7530, Training Accuracy= 0.500\n",
      "Epoch: 580, Loss= 0.7479, Training Accuracy= 0.496\n",
      "Epoch: 590, Loss= 0.7477, Training Accuracy= 0.495\n",
      "Epoch: 600, Loss= 0.7627, Training Accuracy= 0.494\n",
      "Epoch: 610, Loss= 0.7555, Training Accuracy= 0.494\n",
      "Epoch: 620, Loss= 0.7679, Training Accuracy= 0.512\n",
      "Epoch: 630, Loss= 0.7900, Training Accuracy= 0.449\n",
      "Epoch: 640, Loss= 0.7736, Training Accuracy= 0.494\n",
      "Epoch: 650, Loss= 0.6937, Training Accuracy= 0.528\n",
      "Epoch: 660, Loss= 0.7412, Training Accuracy= 0.495\n",
      "Epoch: 670, Loss= 0.7397, Training Accuracy= 0.495\n",
      "Epoch: 680, Loss= 0.7508, Training Accuracy= 0.495\n",
      "Epoch: 690, Loss= 0.7457, Training Accuracy= 0.495\n",
      "Epoch: 700, Loss= 0.7347, Training Accuracy= 0.495\n",
      "Epoch: 710, Loss= 0.7328, Training Accuracy= 0.495\n",
      "Epoch: 720, Loss= 0.9086, Training Accuracy= 0.501\n",
      "Epoch: 730, Loss= 0.8383, Training Accuracy= 0.500\n",
      "Epoch: 740, Loss= 0.8269, Training Accuracy= 0.500\n",
      "Epoch: 750, Loss= 0.8266, Training Accuracy= 0.502\n",
      "Epoch: 760, Loss= 0.8151, Training Accuracy= 0.502\n",
      "Epoch: 770, Loss= 0.8115, Training Accuracy= 0.508\n",
      "Epoch: 780, Loss= 0.8052, Training Accuracy= 0.507\n",
      "Epoch: 790, Loss= 0.8524, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.8188, Training Accuracy= 0.524\n",
      "Epoch: 810, Loss= 0.7920, Training Accuracy= 0.510\n",
      "Epoch: 820, Loss= 0.7964, Training Accuracy= 0.540\n",
      "Epoch: 830, Loss= 0.8001, Training Accuracy= 0.514\n",
      "Epoch: 840, Loss= 0.8330, Training Accuracy= 0.502\n",
      "Epoch: 850, Loss= 0.8229, Training Accuracy= 0.503\n",
      "Epoch: 860, Loss= 0.9069, Training Accuracy= 0.500\n",
      "Epoch: 870, Loss= 0.8067, Training Accuracy= 0.535\n",
      "Epoch: 880, Loss= 0.8036, Training Accuracy= 0.505\n",
      "Epoch: 890, Loss= 0.8249, Training Accuracy= 0.532\n",
      "Epoch: 900, Loss= 0.7789, Training Accuracy= 0.484\n",
      "Epoch: 910, Loss= 0.8719, Training Accuracy= 0.500\n",
      "Epoch: 920, Loss= 0.7850, Training Accuracy= 0.522\n",
      "Epoch: 930, Loss= 0.8384, Training Accuracy= 0.500\n",
      "Epoch: 940, Loss= 0.8506, Training Accuracy= 0.498\n",
      "Epoch: 950, Loss= 0.8302, Training Accuracy= 0.500\n",
      "Epoch: 960, Loss= 0.8016, Training Accuracy= 0.502\n",
      "Epoch: 970, Loss= 0.8526, Training Accuracy= 0.496\n",
      "Epoch: 980, Loss= 0.8019, Training Accuracy= 0.502\n",
      "Epoch: 990, Loss= 0.8353, Training Accuracy= 0.500\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4973\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.275\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.51560003, 0.50840002, 0.50120002, 0.50269997, 0.50650001, 0.49489999, 0.51740003, 1.0, 0.50480002, 0.4973]\n",
      "mean of test_accuracies_10replications:  0.55488\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.00148528054357\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4nRz3449zAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f937d2f9f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
