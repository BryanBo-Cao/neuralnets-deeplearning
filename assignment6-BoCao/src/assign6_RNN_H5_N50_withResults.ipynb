{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 5\n",
    "N = 50\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.6991, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6946, Training Accuracy= 0.505\n",
      "Epoch: 20, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 30, Loss= 0.6934, Training Accuracy= 0.511\n",
      "Epoch: 40, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 50, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 60, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 70, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 80, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 90, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 100, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 110, Loss= 0.6924, Training Accuracy= 0.512\n",
      "Epoch: 120, Loss= 0.6923, Training Accuracy= 0.513\n",
      "Epoch: 130, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 140, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 150, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 160, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 170, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 180, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 190, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 200, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 210, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 220, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 230, Loss= 0.6923, Training Accuracy= 0.515\n",
      "Epoch: 240, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 250, Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 260, Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 270, Loss= 0.6922, Training Accuracy= 0.517\n",
      "Epoch: 280, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 290, Loss= 0.6921, Training Accuracy= 0.516\n",
      "Epoch: 300, Loss= 0.6922, Training Accuracy= 0.517\n",
      "Epoch: 310, Loss= 0.6922, Training Accuracy= 0.513\n",
      "Epoch: 320, Loss= 0.6923, Training Accuracy= 0.512\n",
      "Epoch: 330, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 340, Loss= 0.6921, Training Accuracy= 0.513\n",
      "Epoch: 350, Loss= 0.6921, Training Accuracy= 0.514\n",
      "Epoch: 360, Loss= 0.6921, Training Accuracy= 0.514\n",
      "Epoch: 370, Loss= 0.6917, Training Accuracy= 0.517\n",
      "Epoch: 380, Loss= 0.6916, Training Accuracy= 0.519\n",
      "Epoch: 390, Loss= 0.6918, Training Accuracy= 0.516\n",
      "Epoch: 400, Loss= 0.6918, Training Accuracy= 0.512\n",
      "Epoch: 410, Loss= 0.6940, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 430, Loss= 0.6943, Training Accuracy= 0.502\n",
      "Epoch: 440, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 450, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 460, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 480, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 490, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 500, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 520, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 530, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 540, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 550, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 560, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 570, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 580, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 590, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 600, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 610, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 620, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 630, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 640, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 650, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 660, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 670, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 680, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 690, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 700, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 710, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 720, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 730, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 740, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 750, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 770, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 780, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 790, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 800, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 810, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 820, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 830, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 840, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 850, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 860, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 870, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 880, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 890, Loss= 0.6926, Training Accuracy= 0.509\n",
      "Epoch: 900, Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 910, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 920, Loss= 0.6925, Training Accuracy= 0.508\n",
      "Epoch: 930, Loss= 0.6925, Training Accuracy= 0.510\n",
      "Epoch: 940, Loss= 0.6922, Training Accuracy= 0.513\n",
      "Epoch: 950, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 960, Loss= 0.6921, Training Accuracy= 0.513\n",
      "Epoch: 970, Loss= 0.6923, Training Accuracy= 0.515\n",
      "Epoch: 980, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 990, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5136\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7023, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.7074, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.7067, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.7046, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.7025, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.7006, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6992, Training Accuracy= 0.499\n",
      "Epoch: 70, Loss= 0.6981, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.6973, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.6966, Training Accuracy= 0.497\n",
      "Epoch: 100, Loss= 0.6961, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 120, Loss= 0.6954, Training Accuracy= 0.498\n",
      "Epoch: 130, Loss= 0.6952, Training Accuracy= 0.498\n",
      "Epoch: 140, Loss= 0.6951, Training Accuracy= 0.499\n",
      "Epoch: 150, Loss= 0.6950, Training Accuracy= 0.500\n",
      "Epoch: 160, Loss= 0.6950, Training Accuracy= 0.501\n",
      "Epoch: 170, Loss= 0.6949, Training Accuracy= 0.501\n",
      "Epoch: 180, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 190, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 200, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 210, Loss= 0.6948, Training Accuracy= 0.501\n",
      "Epoch: 220, Loss= 0.6947, Training Accuracy= 0.501\n",
      "Epoch: 230, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 240, Loss= 0.6946, Training Accuracy= 0.499\n",
      "Epoch: 250, Loss= 0.6945, Training Accuracy= 0.499\n",
      "Epoch: 260, Loss= 0.6943, Training Accuracy= 0.502\n",
      "Epoch: 270, Loss= 0.6941, Training Accuracy= 0.509\n",
      "Epoch: 280, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 290, Loss= 0.6937, Training Accuracy= 0.511\n",
      "Epoch: 300, Loss= 0.6938, Training Accuracy= 0.516\n",
      "Epoch: 310, Loss= 0.6938, Training Accuracy= 0.516\n",
      "Epoch: 320, Loss= 0.6939, Training Accuracy= 0.518\n",
      "Epoch: 330, Loss= 0.6935, Training Accuracy= 0.521\n",
      "Epoch: 340, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 350, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 360, Loss= 0.6960, Training Accuracy= 0.499\n",
      "Epoch: 370, Loss= 0.6951, Training Accuracy= 0.507\n",
      "Epoch: 380, Loss= 0.6949, Training Accuracy= 0.508\n",
      "Epoch: 390, Loss= 0.6946, Training Accuracy= 0.509\n",
      "Epoch: 400, Loss= 0.6962, Training Accuracy= 0.496\n",
      "Epoch: 410, Loss= 0.6979, Training Accuracy= 0.495\n",
      "Epoch: 420, Loss= 0.6975, Training Accuracy= 0.495\n",
      "Epoch: 430, Loss= 0.6974, Training Accuracy= 0.495\n",
      "Epoch: 440, Loss= 0.6972, Training Accuracy= 0.495\n",
      "Epoch: 450, Loss= 0.6971, Training Accuracy= 0.495\n",
      "Epoch: 460, Loss= 0.6970, Training Accuracy= 0.495\n",
      "Epoch: 470, Loss= 0.6968, Training Accuracy= 0.495\n",
      "Epoch: 480, Loss= 0.6967, Training Accuracy= 0.495\n",
      "Epoch: 490, Loss= 0.6966, Training Accuracy= 0.495\n",
      "Epoch: 500, Loss= 0.6965, Training Accuracy= 0.495\n",
      "Epoch: 510, Loss= 0.6965, Training Accuracy= 0.495\n",
      "Epoch: 520, Loss= 0.6964, Training Accuracy= 0.495\n",
      "Epoch: 530, Loss= 0.6964, Training Accuracy= 0.495\n",
      "Epoch: 540, Loss= 0.6963, Training Accuracy= 0.495\n",
      "Epoch: 550, Loss= 0.6963, Training Accuracy= 0.495\n",
      "Epoch: 560, Loss= 0.6963, Training Accuracy= 0.495\n",
      "Epoch: 570, Loss= 0.6962, Training Accuracy= 0.495\n",
      "Epoch: 580, Loss= 0.6962, Training Accuracy= 0.495\n",
      "Epoch: 590, Loss= 0.6961, Training Accuracy= 0.495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6961, Training Accuracy= 0.495\n",
      "Epoch: 610, Loss= 0.6960, Training Accuracy= 0.495\n",
      "Epoch: 620, Loss= 0.6960, Training Accuracy= 0.495\n",
      "Epoch: 630, Loss= 0.6959, Training Accuracy= 0.495\n",
      "Epoch: 640, Loss= 0.6958, Training Accuracy= 0.495\n",
      "Epoch: 650, Loss= 0.6958, Training Accuracy= 0.495\n",
      "Epoch: 660, Loss= 0.6958, Training Accuracy= 0.495\n",
      "Epoch: 670, Loss= 0.6957, Training Accuracy= 0.495\n",
      "Epoch: 680, Loss= 0.6957, Training Accuracy= 0.495\n",
      "Epoch: 690, Loss= 0.6956, Training Accuracy= 0.495\n",
      "Epoch: 700, Loss= 0.6956, Training Accuracy= 0.495\n",
      "Epoch: 710, Loss= 0.6955, Training Accuracy= 0.495\n",
      "Epoch: 720, Loss= 0.6955, Training Accuracy= 0.495\n",
      "Epoch: 730, Loss= 0.6955, Training Accuracy= 0.495\n",
      "Epoch: 740, Loss= 0.6954, Training Accuracy= 0.495\n",
      "Epoch: 750, Loss= 0.6953, Training Accuracy= 0.495\n",
      "Epoch: 760, Loss= 0.6954, Training Accuracy= 0.495\n",
      "Epoch: 770, Loss= 0.6953, Training Accuracy= 0.496\n",
      "Epoch: 780, Loss= 0.6956, Training Accuracy= 0.495\n",
      "Epoch: 790, Loss= 0.6976, Training Accuracy= 0.495\n",
      "Epoch: 800, Loss= 0.6976, Training Accuracy= 0.495\n",
      "Epoch: 810, Loss= 0.6975, Training Accuracy= 0.495\n",
      "Epoch: 820, Loss= 0.6974, Training Accuracy= 0.495\n",
      "Epoch: 830, Loss= 0.6972, Training Accuracy= 0.495\n",
      "Epoch: 840, Loss= 0.6971, Training Accuracy= 0.495\n",
      "Epoch: 850, Loss= 0.6970, Training Accuracy= 0.495\n",
      "Epoch: 860, Loss= 0.6969, Training Accuracy= 0.495\n",
      "Epoch: 870, Loss= 0.6968, Training Accuracy= 0.495\n",
      "Epoch: 880, Loss= 0.6967, Training Accuracy= 0.495\n",
      "Epoch: 890, Loss= 0.6967, Training Accuracy= 0.495\n",
      "Epoch: 900, Loss= 0.6966, Training Accuracy= 0.495\n",
      "Epoch: 910, Loss= 0.6966, Training Accuracy= 0.495\n",
      "Epoch: 920, Loss= 0.6966, Training Accuracy= 0.495\n",
      "Epoch: 930, Loss= 0.6966, Training Accuracy= 0.495\n",
      "Epoch: 940, Loss= 0.6966, Training Accuracy= 0.495\n",
      "Epoch: 950, Loss= 0.6966, Training Accuracy= 0.495\n",
      "Epoch: 960, Loss= 0.6966, Training Accuracy= 0.495\n",
      "Epoch: 970, Loss= 0.6965, Training Accuracy= 0.495\n",
      "Epoch: 980, Loss= 0.6965, Training Accuracy= 0.495\n",
      "Epoch: 990, Loss= 0.6965, Training Accuracy= 0.495\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5056\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.6975, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.6946, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 70, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 80, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 100, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 110, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 120, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 130, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 140, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 150, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 160, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 170, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 180, Loss= 0.6923, Training Accuracy= 0.515\n",
      "Epoch: 190, Loss= 0.6921, Training Accuracy= 0.518\n",
      "Epoch: 200, Loss= 0.6920, Training Accuracy= 0.519\n",
      "Epoch: 210, Loss= 0.6919, Training Accuracy= 0.522\n",
      "Epoch: 220, Loss= 0.6920, Training Accuracy= 0.522\n",
      "Epoch: 230, Loss= 0.6919, Training Accuracy= 0.521\n",
      "Epoch: 240, Loss= 0.6919, Training Accuracy= 0.523\n",
      "Epoch: 250, Loss= 0.6920, Training Accuracy= 0.524\n",
      "Epoch: 260, Loss= 0.6919, Training Accuracy= 0.525\n",
      "Epoch: 270, Loss= 0.6913, Training Accuracy= 0.524\n",
      "Epoch: 280, Loss= 0.6914, Training Accuracy= 0.524\n",
      "Epoch: 290, Loss= 0.6916, Training Accuracy= 0.527\n",
      "Epoch: 300, Loss= 0.6912, Training Accuracy= 0.525\n",
      "Epoch: 310, Loss= 0.6934, Training Accuracy= 0.513\n",
      "Epoch: 320, Loss= 0.6910, Training Accuracy= 0.527\n",
      "Epoch: 330, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 340, Loss= 0.6916, Training Accuracy= 0.524\n",
      "Epoch: 350, Loss= 0.6909, Training Accuracy= 0.528\n",
      "Epoch: 360, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 370, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 380, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 390, Loss= 0.6956, Training Accuracy= 0.497\n",
      "Epoch: 400, Loss= 0.6956, Training Accuracy= 0.497\n",
      "Epoch: 410, Loss= 0.6955, Training Accuracy= 0.497\n",
      "Epoch: 420, Loss= 0.6954, Training Accuracy= 0.497\n",
      "Epoch: 430, Loss= 0.6953, Training Accuracy= 0.497\n",
      "Epoch: 440, Loss= 0.6952, Training Accuracy= 0.497\n",
      "Epoch: 450, Loss= 0.6951, Training Accuracy= 0.497\n",
      "Epoch: 460, Loss= 0.6951, Training Accuracy= 0.497\n",
      "Epoch: 470, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 480, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 490, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 500, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 510, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 520, Loss= 0.6946, Training Accuracy= 0.497\n",
      "Epoch: 530, Loss= 0.6946, Training Accuracy= 0.497\n",
      "Epoch: 540, Loss= 0.6946, Training Accuracy= 0.497\n",
      "Epoch: 550, Loss= 0.6945, Training Accuracy= 0.497\n",
      "Epoch: 560, Loss= 0.6945, Training Accuracy= 0.497\n",
      "Epoch: 570, Loss= 0.6944, Training Accuracy= 0.497\n",
      "Epoch: 580, Loss= 0.6944, Training Accuracy= 0.497\n",
      "Epoch: 590, Loss= 0.6943, Training Accuracy= 0.497\n",
      "Epoch: 600, Loss= 0.6943, Training Accuracy= 0.497\n",
      "Epoch: 610, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 620, Loss= 0.6942, Training Accuracy= 0.501\n",
      "Epoch: 630, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 640, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 650, Loss= 0.6942, Training Accuracy= 0.499\n",
      "Epoch: 660, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 670, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 680, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 690, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 700, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 710, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 720, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 730, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 740, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 750, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 760, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 770, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 780, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 790, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 800, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 810, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 820, Loss= 0.6954, Training Accuracy= 0.497\n",
      "Epoch: 830, Loss= 0.6960, Training Accuracy= 0.500\n",
      "Epoch: 840, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 850, Loss= 0.6969, Training Accuracy= 0.501\n",
      "Epoch: 860, Loss= 0.6969, Training Accuracy= 0.501\n",
      "Epoch: 870, Loss= 0.6968, Training Accuracy= 0.500\n",
      "Epoch: 880, Loss= 0.6967, Training Accuracy= 0.500\n",
      "Epoch: 890, Loss= 0.6966, Training Accuracy= 0.500\n",
      "Epoch: 900, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 910, Loss= 0.6965, Training Accuracy= 0.500\n",
      "Epoch: 920, Loss= 0.6966, Training Accuracy= 0.500\n",
      "Epoch: 930, Loss= 0.6966, Training Accuracy= 0.500\n",
      "Epoch: 940, Loss= 0.6966, Training Accuracy= 0.500\n",
      "Epoch: 950, Loss= 0.6966, Training Accuracy= 0.500\n",
      "Epoch: 960, Loss= 0.6966, Training Accuracy= 0.499\n",
      "Epoch: 970, Loss= 0.6966, Training Accuracy= 0.502\n",
      "Epoch: 980, Loss= 0.6965, Training Accuracy= 0.503\n",
      "Epoch: 990, Loss= 0.6966, Training Accuracy= 0.504\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5031\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.6987, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.6936, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.6971, Training Accuracy= 0.493\n",
      "Epoch: 50, Loss= 0.6969, Training Accuracy= 0.493\n",
      "Epoch: 60, Loss= 0.6967, Training Accuracy= 0.493\n",
      "Epoch: 70, Loss= 0.6966, Training Accuracy= 0.493\n",
      "Epoch: 80, Loss= 0.6965, Training Accuracy= 0.493\n",
      "Epoch: 90, Loss= 0.6964, Training Accuracy= 0.493\n",
      "Epoch: 100, Loss= 0.6964, Training Accuracy= 0.493\n",
      "Epoch: 110, Loss= 0.6964, Training Accuracy= 0.493\n",
      "Epoch: 120, Loss= 0.6963, Training Accuracy= 0.493\n",
      "Epoch: 130, Loss= 0.6963, Training Accuracy= 0.493\n",
      "Epoch: 140, Loss= 0.6963, Training Accuracy= 0.493\n",
      "Epoch: 150, Loss= 0.6963, Training Accuracy= 0.493\n",
      "Epoch: 160, Loss= 0.6962, Training Accuracy= 0.493\n",
      "Epoch: 170, Loss= 0.6962, Training Accuracy= 0.493\n",
      "Epoch: 180, Loss= 0.6962, Training Accuracy= 0.493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.6962, Training Accuracy= 0.493\n",
      "Epoch: 200, Loss= 0.6962, Training Accuracy= 0.493\n",
      "Epoch: 210, Loss= 0.6962, Training Accuracy= 0.493\n",
      "Epoch: 220, Loss= 0.6962, Training Accuracy= 0.493\n",
      "Epoch: 230, Loss= 0.6962, Training Accuracy= 0.493\n",
      "Epoch: 240, Loss= 0.6962, Training Accuracy= 0.493\n",
      "Epoch: 250, Loss= 0.6962, Training Accuracy= 0.493\n",
      "Epoch: 260, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 270, Loss= 0.6962, Training Accuracy= 0.493\n",
      "Epoch: 280, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 290, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 300, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 310, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 320, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 330, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 340, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 350, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 360, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 370, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 380, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 390, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 400, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 410, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 420, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 430, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 440, Loss= 0.6961, Training Accuracy= 0.493\n",
      "Epoch: 450, Loss= 0.6960, Training Accuracy= 0.493\n",
      "Epoch: 460, Loss= 0.6960, Training Accuracy= 0.493\n",
      "Epoch: 470, Loss= 0.6960, Training Accuracy= 0.493\n",
      "Epoch: 480, Loss= 0.6960, Training Accuracy= 0.493\n",
      "Epoch: 490, Loss= 0.6960, Training Accuracy= 0.493\n",
      "Epoch: 500, Loss= 0.6960, Training Accuracy= 0.493\n",
      "Epoch: 510, Loss= 0.6960, Training Accuracy= 0.493\n",
      "Epoch: 520, Loss= 0.6959, Training Accuracy= 0.493\n",
      "Epoch: 530, Loss= 0.6959, Training Accuracy= 0.493\n",
      "Epoch: 540, Loss= 0.6959, Training Accuracy= 0.493\n",
      "Epoch: 550, Loss= 0.6959, Training Accuracy= 0.493\n",
      "Epoch: 560, Loss= 0.6958, Training Accuracy= 0.493\n",
      "Epoch: 570, Loss= 0.6958, Training Accuracy= 0.493\n",
      "Epoch: 580, Loss= 0.6958, Training Accuracy= 0.493\n",
      "Epoch: 590, Loss= 0.6957, Training Accuracy= 0.493\n",
      "Epoch: 600, Loss= 0.6957, Training Accuracy= 0.493\n",
      "Epoch: 610, Loss= 0.6956, Training Accuracy= 0.493\n",
      "Epoch: 620, Loss= 0.6956, Training Accuracy= 0.493\n",
      "Epoch: 630, Loss= 0.6955, Training Accuracy= 0.493\n",
      "Epoch: 640, Loss= 0.6954, Training Accuracy= 0.493\n",
      "Epoch: 650, Loss= 0.6953, Training Accuracy= 0.493\n",
      "Epoch: 660, Loss= 0.6952, Training Accuracy= 0.493\n",
      "Epoch: 670, Loss= 0.6951, Training Accuracy= 0.493\n",
      "Epoch: 680, Loss= 0.6950, Training Accuracy= 0.493\n",
      "Epoch: 690, Loss= 0.6949, Training Accuracy= 0.493\n",
      "Epoch: 700, Loss= 0.6949, Training Accuracy= 0.493\n",
      "Epoch: 710, Loss= 0.6949, Training Accuracy= 0.493\n",
      "Epoch: 720, Loss= 0.6948, Training Accuracy= 0.493\n",
      "Epoch: 730, Loss= 0.6948, Training Accuracy= 0.493\n",
      "Epoch: 740, Loss= 0.6948, Training Accuracy= 0.493\n",
      "Epoch: 750, Loss= 0.6948, Training Accuracy= 0.493\n",
      "Epoch: 760, Loss= 0.6948, Training Accuracy= 0.493\n",
      "Epoch: 770, Loss= 0.6948, Training Accuracy= 0.493\n",
      "Epoch: 780, Loss= 0.6948, Training Accuracy= 0.493\n",
      "Epoch: 790, Loss= 0.6948, Training Accuracy= 0.493\n",
      "Epoch: 800, Loss= 0.6948, Training Accuracy= 0.493\n",
      "Epoch: 810, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 820, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 830, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 840, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 850, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 860, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 870, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 880, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 890, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 900, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 910, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 920, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 930, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 940, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 950, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 960, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 970, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 980, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Epoch: 990, Loss= 0.6947, Training Accuracy= 0.493\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5003\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.6941, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 20, Loss= 0.6936, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.6935, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.6933, Training Accuracy= 0.501\n",
      "Epoch: 80, Loss= 0.6933, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 160, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 190, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 200, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 210, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 230, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 270, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 280, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 290, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 320, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 330, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 340, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 350, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 360, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 400, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 420, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 430, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 470, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 480, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 490, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 500, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 510, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 520, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 530, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 540, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 550, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 560, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 570, Loss= 0.6929, Training Accuracy= 0.504\n",
      "Epoch: 580, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 590, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 600, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 610, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 620, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 630, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 640, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 650, Loss= 0.6928, Training Accuracy= 0.516\n",
      "Epoch: 660, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 670, Loss= 0.6927, Training Accuracy= 0.520\n",
      "Epoch: 680, Loss= 0.6926, Training Accuracy= 0.519\n",
      "Epoch: 690, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 700, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 710, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 730, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 740, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 750, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 760, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 770, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 780, Loss= 0.6926, Training Accuracy= 0.519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 800, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 810, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 820, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 830, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 840, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 850, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 860, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 870, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 880, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 890, Loss= 0.6925, Training Accuracy= 0.521\n",
      "Epoch: 900, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 910, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 920, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 930, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 940, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 950, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 960, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 970, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 980, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 990, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5002\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.6974, Training Accuracy= 0.499\n",
      "Epoch: 10, Loss= 0.6959, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6947, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.6944, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 70, Loss= 0.6940, Training Accuracy= 0.509\n",
      "Epoch: 80, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6935, Training Accuracy= 0.510\n",
      "Epoch: 100, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 110, Loss= 0.6934, Training Accuracy= 0.514\n",
      "Epoch: 120, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 130, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 180, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 200, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 210, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 220, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 230, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 240, Loss= 0.6943, Training Accuracy= 0.496\n",
      "Epoch: 250, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 260, Loss= 0.6951, Training Accuracy= 0.496\n",
      "Epoch: 270, Loss= 0.6951, Training Accuracy= 0.496\n",
      "Epoch: 280, Loss= 0.6950, Training Accuracy= 0.496\n",
      "Epoch: 290, Loss= 0.6950, Training Accuracy= 0.496\n",
      "Epoch: 300, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 310, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 320, Loss= 0.6946, Training Accuracy= 0.497\n",
      "Epoch: 330, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 340, Loss= 0.6944, Training Accuracy= 0.501\n",
      "Epoch: 350, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 370, Loss= 0.6953, Training Accuracy= 0.502\n",
      "Epoch: 380, Loss= 0.6972, Training Accuracy= 0.504\n",
      "Epoch: 390, Loss= 0.6974, Training Accuracy= 0.504\n",
      "Epoch: 400, Loss= 0.6971, Training Accuracy= 0.504\n",
      "Epoch: 410, Loss= 0.6963, Training Accuracy= 0.505\n",
      "Epoch: 420, Loss= 0.6960, Training Accuracy= 0.503\n",
      "Epoch: 430, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 440, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 450, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 460, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 470, Loss= 0.6958, Training Accuracy= 0.496\n",
      "Epoch: 480, Loss= 0.6956, Training Accuracy= 0.496\n",
      "Epoch: 490, Loss= 0.6953, Training Accuracy= 0.496\n",
      "Epoch: 500, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 510, Loss= 0.6948, Training Accuracy= 0.496\n",
      "Epoch: 520, Loss= 0.6947, Training Accuracy= 0.496\n",
      "Epoch: 530, Loss= 0.6946, Training Accuracy= 0.497\n",
      "Epoch: 540, Loss= 0.6945, Training Accuracy= 0.497\n",
      "Epoch: 550, Loss= 0.6944, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.6944, Training Accuracy= 0.499\n",
      "Epoch: 570, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 580, Loss= 0.6943, Training Accuracy= 0.499\n",
      "Epoch: 590, Loss= 0.6943, Training Accuracy= 0.499\n",
      "Epoch: 600, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 610, Loss= 0.6943, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 0.6943, Training Accuracy= 0.504\n",
      "Epoch: 630, Loss= 0.6943, Training Accuracy= 0.504\n",
      "Epoch: 640, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 650, Loss= 0.6946, Training Accuracy= 0.501\n",
      "Epoch: 660, Loss= 0.6946, Training Accuracy= 0.502\n",
      "Epoch: 670, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 680, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 690, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 700, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 710, Loss= 0.6946, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.6946, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 740, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 750, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 760, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 770, Loss= 0.6945, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.6944, Training Accuracy= 0.504\n",
      "Epoch: 790, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 800, Loss= 0.6943, Training Accuracy= 0.505\n",
      "Epoch: 810, Loss= 0.6940, Training Accuracy= 0.507\n",
      "Epoch: 820, Loss= 0.6941, Training Accuracy= 0.507\n",
      "Epoch: 830, Loss= 0.6942, Training Accuracy= 0.507\n",
      "Epoch: 840, Loss= 0.6942, Training Accuracy= 0.508\n",
      "Epoch: 850, Loss= 0.6941, Training Accuracy= 0.509\n",
      "Epoch: 860, Loss= 0.6941, Training Accuracy= 0.509\n",
      "Epoch: 870, Loss= 0.6941, Training Accuracy= 0.510\n",
      "Epoch: 880, Loss= 0.6941, Training Accuracy= 0.508\n",
      "Epoch: 890, Loss= 0.6940, Training Accuracy= 0.507\n",
      "Epoch: 900, Loss= 0.6941, Training Accuracy= 0.507\n",
      "Epoch: 910, Loss= 0.6941, Training Accuracy= 0.510\n",
      "Epoch: 920, Loss= 0.6942, Training Accuracy= 0.507\n",
      "Epoch: 930, Loss= 0.6941, Training Accuracy= 0.509\n",
      "Epoch: 940, Loss= 0.6941, Training Accuracy= 0.509\n",
      "Epoch: 950, Loss= 0.6941, Training Accuracy= 0.507\n",
      "Epoch: 960, Loss= 0.6941, Training Accuracy= 0.510\n",
      "Epoch: 970, Loss= 0.6941, Training Accuracy= 0.507\n",
      "Epoch: 980, Loss= 0.6942, Training Accuracy= 0.509\n",
      "Epoch: 990, Loss= 0.6941, Training Accuracy= 0.510\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4944\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.6978, Training Accuracy= 0.492\n",
      "Epoch: 10, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 20, Loss= 0.6934, Training Accuracy= 0.511\n",
      "Epoch: 30, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 40, Loss= 0.6934, Training Accuracy= 0.510\n",
      "Epoch: 50, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 60, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 70, Loss= 0.6935, Training Accuracy= 0.510\n",
      "Epoch: 80, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 90, Loss= 0.6934, Training Accuracy= 0.513\n",
      "Epoch: 100, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 110, Loss= 0.6931, Training Accuracy= 0.514\n",
      "Epoch: 120, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 130, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 140, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 150, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 160, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 170, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 180, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 190, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 200, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 210, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 220, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 230, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 240, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 250, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 260, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 270, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 280, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 290, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 300, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 310, Loss= 0.6924, Training Accuracy= 0.520\n",
      "Epoch: 320, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 330, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 340, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 350, Loss= 0.6921, Training Accuracy= 0.520\n",
      "Epoch: 360, Loss= 0.6919, Training Accuracy= 0.522\n",
      "Epoch: 370, Loss= 0.6921, Training Accuracy= 0.516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 390, Loss= 0.6940, Training Accuracy= 0.509\n",
      "Epoch: 400, Loss= 0.6939, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 420, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 430, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 440, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 450, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 460, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 470, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 480, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 490, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 500, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 510, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 520, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 530, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Epoch: 550, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 560, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Epoch: 570, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 580, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 590, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 600, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 610, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 620, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 630, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 640, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 660, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 670, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 680, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 690, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 700, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 710, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 720, Loss= 0.6923, Training Accuracy= 0.513\n",
      "Epoch: 730, Loss= 0.6924, Training Accuracy= 0.511\n",
      "Epoch: 740, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 750, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 760, Loss= 0.6944, Training Accuracy= 0.507\n",
      "Epoch: 770, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 780, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 790, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 800, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 810, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 820, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 830, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 840, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 850, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 860, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 870, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 880, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 890, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 900, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 910, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 920, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 930, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 940, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 950, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 960, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 970, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 980, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 990, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4969\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.6948, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 80, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 110, Loss= 0.6940, Training Accuracy= 0.506\n",
      "Epoch: 120, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6933, Training Accuracy= 0.502\n",
      "Epoch: 150, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 160, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 170, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 180, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 190, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 200, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 210, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 230, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 240, Loss= 0.6918, Training Accuracy= 0.521\n",
      "Epoch: 250, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 260, Loss= 0.6921, Training Accuracy= 0.521\n",
      "Epoch: 270, Loss= 0.6922, Training Accuracy= 0.520\n",
      "Epoch: 280, Loss= 0.6923, Training Accuracy= 0.515\n",
      "Epoch: 290, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 300, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 310, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 320, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 330, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 340, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 350, Loss= 0.6923, Training Accuracy= 0.510\n",
      "Epoch: 360, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 370, Loss= 0.6920, Training Accuracy= 0.517\n",
      "Epoch: 380, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 390, Loss= 0.6918, Training Accuracy= 0.517\n",
      "Epoch: 400, Loss= 0.6922, Training Accuracy= 0.515\n",
      "Epoch: 410, Loss= 0.6917, Training Accuracy= 0.518\n",
      "Epoch: 420, Loss= 0.6918, Training Accuracy= 0.518\n",
      "Epoch: 430, Loss= 0.6916, Training Accuracy= 0.521\n",
      "Epoch: 440, Loss= 0.6916, Training Accuracy= 0.517\n",
      "Epoch: 450, Loss= 0.6921, Training Accuracy= 0.512\n",
      "Epoch: 460, Loss= 0.6918, Training Accuracy= 0.515\n",
      "Epoch: 470, Loss= 0.6920, Training Accuracy= 0.516\n",
      "Epoch: 480, Loss= 0.6920, Training Accuracy= 0.517\n",
      "Epoch: 490, Loss= 0.6917, Training Accuracy= 0.521\n",
      "Epoch: 500, Loss= 0.6916, Training Accuracy= 0.519\n",
      "Epoch: 510, Loss= 0.6919, Training Accuracy= 0.518\n",
      "Epoch: 520, Loss= 0.6916, Training Accuracy= 0.520\n",
      "Epoch: 530, Loss= 0.6914, Training Accuracy= 0.522\n",
      "Epoch: 540, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 550, Loss= 0.6921, Training Accuracy= 0.518\n",
      "Epoch: 560, Loss= 0.6917, Training Accuracy= 0.519\n",
      "Epoch: 570, Loss= 0.6917, Training Accuracy= 0.517\n",
      "Epoch: 580, Loss= 0.6918, Training Accuracy= 0.520\n",
      "Epoch: 590, Loss= 0.6916, Training Accuracy= 0.520\n",
      "Epoch: 600, Loss= 0.6918, Training Accuracy= 0.515\n",
      "Epoch: 610, Loss= 0.6916, Training Accuracy= 0.521\n",
      "Epoch: 620, Loss= 0.6918, Training Accuracy= 0.520\n",
      "Epoch: 630, Loss= 0.6916, Training Accuracy= 0.522\n",
      "Epoch: 640, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 650, Loss= 0.6941, Training Accuracy= 0.509\n",
      "Epoch: 660, Loss= 0.6955, Training Accuracy= 0.508\n",
      "Epoch: 670, Loss= 0.6946, Training Accuracy= 0.502\n",
      "Epoch: 680, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 690, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 700, Loss= 0.6940, Training Accuracy= 0.504\n",
      "Epoch: 710, Loss= 0.6939, Training Accuracy= 0.504\n",
      "Epoch: 720, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 730, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 740, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 750, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 760, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 770, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 780, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 790, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 800, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 810, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 830, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 840, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 850, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 860, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 870, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 880, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 890, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 900, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 910, Loss= 0.6931, Training Accuracy= 0.513\n",
      "Epoch: 920, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 930, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 940, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 950, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 960, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 970, Loss= 0.6932, Training Accuracy= 0.504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 990, Loss= 0.6933, Training Accuracy= 0.501\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4999\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.6950, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.6933, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 70, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 80, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 100, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 110, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 120, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 130, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 140, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 150, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 160, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 170, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 180, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 190, Loss= 0.6925, Training Accuracy= 0.513\n",
      "Epoch: 200, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 210, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 220, Loss= 0.6922, Training Accuracy= 0.521\n",
      "Epoch: 230, Loss= 0.6921, Training Accuracy= 0.523\n",
      "Epoch: 240, Loss= 0.6920, Training Accuracy= 0.523\n",
      "Epoch: 250, Loss= 0.6919, Training Accuracy= 0.523\n",
      "Epoch: 260, Loss= 0.6919, Training Accuracy= 0.525\n",
      "Epoch: 270, Loss= 0.6919, Training Accuracy= 0.523\n",
      "Epoch: 280, Loss= 0.6920, Training Accuracy= 0.523\n",
      "Epoch: 290, Loss= 0.6922, Training Accuracy= 0.520\n",
      "Epoch: 300, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 310, Loss= 0.6920, Training Accuracy= 0.522\n",
      "Epoch: 320, Loss= 0.6920, Training Accuracy= 0.522\n",
      "Epoch: 330, Loss= 0.6921, Training Accuracy= 0.521\n",
      "Epoch: 340, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 350, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 370, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 380, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 390, Loss= 0.6926, Training Accuracy= 0.518\n",
      "Epoch: 400, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6926, Training Accuracy= 0.517\n",
      "Epoch: 420, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 430, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 440, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 450, Loss= 0.6928, Training Accuracy= 0.516\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 470, Loss= 0.6934, Training Accuracy= 0.510\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 490, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 500, Loss= 0.6926, Training Accuracy= 0.519\n",
      "Epoch: 510, Loss= 0.6926, Training Accuracy= 0.517\n",
      "Epoch: 520, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 530, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 540, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 550, Loss= 0.6925, Training Accuracy= 0.521\n",
      "Epoch: 560, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 570, Loss= 0.6926, Training Accuracy= 0.517\n",
      "Epoch: 580, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 590, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 600, Loss= 0.6921, Training Accuracy= 0.520\n",
      "Epoch: 610, Loss= 0.6921, Training Accuracy= 0.521\n",
      "Epoch: 620, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 630, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 640, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 650, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 660, Loss= 0.6921, Training Accuracy= 0.519\n",
      "Epoch: 670, Loss= 0.6919, Training Accuracy= 0.522\n",
      "Epoch: 680, Loss= 0.6937, Training Accuracy= 0.506\n",
      "Epoch: 690, Loss= 0.6921, Training Accuracy= 0.515\n",
      "Epoch: 700, Loss= 0.6920, Training Accuracy= 0.514\n",
      "Epoch: 710, Loss= 0.6919, Training Accuracy= 0.520\n",
      "Epoch: 720, Loss= 0.6919, Training Accuracy= 0.517\n",
      "Epoch: 730, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 740, Loss= 0.6917, Training Accuracy= 0.521\n",
      "Epoch: 750, Loss= 0.6919, Training Accuracy= 0.519\n",
      "Epoch: 760, Loss= 0.6916, Training Accuracy= 0.520\n",
      "Epoch: 770, Loss= 0.6916, Training Accuracy= 0.518\n",
      "Epoch: 780, Loss= 0.6960, Training Accuracy= 0.499\n",
      "Epoch: 790, Loss= 0.6933, Training Accuracy= 0.501\n",
      "Epoch: 800, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 810, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 830, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 840, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 850, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 860, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 870, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 880, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 890, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 900, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 910, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 920, Loss= 0.6925, Training Accuracy= 0.513\n",
      "Epoch: 930, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 940, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 950, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 960, Loss= 0.6944, Training Accuracy= 0.501\n",
      "Epoch: 970, Loss= 0.6933, Training Accuracy= 0.501\n",
      "Epoch: 980, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 990, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.498\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7599, Training Accuracy= 0.506\n",
      "Epoch: 10, Loss= 0.7213, Training Accuracy= 0.510\n",
      "Epoch: 20, Loss= 0.7118, Training Accuracy= 0.509\n",
      "Epoch: 30, Loss= 0.7064, Training Accuracy= 0.510\n",
      "Epoch: 40, Loss= 0.7030, Training Accuracy= 0.512\n",
      "Epoch: 50, Loss= 0.7010, Training Accuracy= 0.509\n",
      "Epoch: 60, Loss= 0.6996, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.6986, Training Accuracy= 0.508\n",
      "Epoch: 80, Loss= 0.6979, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.6974, Training Accuracy= 0.509\n",
      "Epoch: 100, Loss= 0.6970, Training Accuracy= 0.509\n",
      "Epoch: 110, Loss= 0.6967, Training Accuracy= 0.510\n",
      "Epoch: 120, Loss= 0.6965, Training Accuracy= 0.510\n",
      "Epoch: 130, Loss= 0.6963, Training Accuracy= 0.510\n",
      "Epoch: 140, Loss= 0.6961, Training Accuracy= 0.511\n",
      "Epoch: 150, Loss= 0.6960, Training Accuracy= 0.511\n",
      "Epoch: 160, Loss= 0.6959, Training Accuracy= 0.510\n",
      "Epoch: 170, Loss= 0.6958, Training Accuracy= 0.511\n",
      "Epoch: 180, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 190, Loss= 0.6956, Training Accuracy= 0.511\n",
      "Epoch: 200, Loss= 0.6955, Training Accuracy= 0.510\n",
      "Epoch: 210, Loss= 0.6955, Training Accuracy= 0.511\n",
      "Epoch: 220, Loss= 0.6954, Training Accuracy= 0.511\n",
      "Epoch: 230, Loss= 0.6953, Training Accuracy= 0.511\n",
      "Epoch: 240, Loss= 0.6953, Training Accuracy= 0.511\n",
      "Epoch: 250, Loss= 0.6952, Training Accuracy= 0.511\n",
      "Epoch: 260, Loss= 0.6951, Training Accuracy= 0.511\n",
      "Epoch: 270, Loss= 0.6951, Training Accuracy= 0.511\n",
      "Epoch: 280, Loss= 0.6950, Training Accuracy= 0.510\n",
      "Epoch: 290, Loss= 0.6950, Training Accuracy= 0.511\n",
      "Epoch: 300, Loss= 0.6949, Training Accuracy= 0.510\n",
      "Epoch: 310, Loss= 0.6948, Training Accuracy= 0.511\n",
      "Epoch: 320, Loss= 0.6947, Training Accuracy= 0.511\n",
      "Epoch: 330, Loss= 0.6945, Training Accuracy= 0.511\n",
      "Epoch: 340, Loss= 0.6943, Training Accuracy= 0.511\n",
      "Epoch: 350, Loss= 0.6940, Training Accuracy= 0.515\n",
      "Epoch: 360, Loss= 0.6938, Training Accuracy= 0.515\n",
      "Epoch: 370, Loss= 0.6936, Training Accuracy= 0.517\n",
      "Epoch: 380, Loss= 0.6935, Training Accuracy= 0.516\n",
      "Epoch: 390, Loss= 0.6934, Training Accuracy= 0.516\n",
      "Epoch: 400, Loss= 0.6933, Training Accuracy= 0.518\n",
      "Epoch: 410, Loss= 0.6933, Training Accuracy= 0.519\n",
      "Epoch: 420, Loss= 0.6932, Training Accuracy= 0.519\n",
      "Epoch: 430, Loss= 0.6931, Training Accuracy= 0.520\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.519\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.521\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.524\n",
      "Epoch: 470, Loss= 0.6944, Training Accuracy= 0.521\n",
      "Epoch: 480, Loss= 0.6927, Training Accuracy= 0.524\n",
      "Epoch: 490, Loss= 0.7043, Training Accuracy= 0.506\n",
      "Epoch: 500, Loss= 0.7031, Training Accuracy= 0.506\n",
      "Epoch: 510, Loss= 0.7024, Training Accuracy= 0.506\n",
      "Epoch: 520, Loss= 0.7020, Training Accuracy= 0.506\n",
      "Epoch: 530, Loss= 0.7018, Training Accuracy= 0.506\n",
      "Epoch: 540, Loss= 0.7017, Training Accuracy= 0.506\n",
      "Epoch: 550, Loss= 0.7016, Training Accuracy= 0.506\n",
      "Epoch: 560, Loss= 0.7015, Training Accuracy= 0.506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.7015, Training Accuracy= 0.506\n",
      "Epoch: 580, Loss= 0.7014, Training Accuracy= 0.506\n",
      "Epoch: 590, Loss= 0.7014, Training Accuracy= 0.506\n",
      "Epoch: 600, Loss= 0.7013, Training Accuracy= 0.506\n",
      "Epoch: 610, Loss= 0.7012, Training Accuracy= 0.506\n",
      "Epoch: 620, Loss= 0.7011, Training Accuracy= 0.506\n",
      "Epoch: 630, Loss= 0.7010, Training Accuracy= 0.506\n",
      "Epoch: 640, Loss= 0.7009, Training Accuracy= 0.506\n",
      "Epoch: 650, Loss= 0.7007, Training Accuracy= 0.506\n",
      "Epoch: 660, Loss= 0.7004, Training Accuracy= 0.506\n",
      "Epoch: 670, Loss= 0.7000, Training Accuracy= 0.506\n",
      "Epoch: 680, Loss= 0.6994, Training Accuracy= 0.506\n",
      "Epoch: 690, Loss= 0.6987, Training Accuracy= 0.506\n",
      "Epoch: 700, Loss= 0.7095, Training Accuracy= 0.506\n",
      "Epoch: 710, Loss= 0.7079, Training Accuracy= 0.506\n",
      "Epoch: 720, Loss= 0.7104, Training Accuracy= 0.506\n",
      "Epoch: 730, Loss= 0.7094, Training Accuracy= 0.506\n",
      "Epoch: 740, Loss= 0.7080, Training Accuracy= 0.506\n",
      "Epoch: 750, Loss= 0.7072, Training Accuracy= 0.506\n",
      "Epoch: 760, Loss= 0.7072, Training Accuracy= 0.506\n",
      "Epoch: 770, Loss= 0.7070, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 790, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 810, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 830, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 840, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 850, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 860, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 870, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 880, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 890, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 900, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 910, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 920, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 930, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 940, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 950, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 960, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 970, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 980, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Epoch: 990, Loss= 0.7069, Training Accuracy= 0.506\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4963\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.2\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.51359999, 0.50559998, 0.50309998, 0.50029999, 0.50019997, 0.49439999, 0.49689999, 0.49990001, 0.498, 0.49630001]\n",
      "mean of test_accuracies_10replications:  0.50083\n",
      "standard deviation of test_accuracies_10replications_std_mean:  5.27105806395e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmJlZSZw4zMysJE4cZmZWEicOMzMryf8H5JThYGitgAwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe4f8835710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
