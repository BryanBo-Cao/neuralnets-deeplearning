{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 10\n",
    "N = 25\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.6952, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6943, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 80, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 90, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 100, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 130, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 140, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 150, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 160, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 170, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 180, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 190, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 200, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 210, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 220, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 230, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 270, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 280, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 290, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 300, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 310, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 320, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 330, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 340, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 350, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 360, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 370, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 380, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 390, Loss= 0.6936, Training Accuracy= 0.508\n",
      "Epoch: 400, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 410, Loss= 0.6936, Training Accuracy= 0.508\n",
      "Epoch: 420, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 430, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 440, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 450, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 460, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 470, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 480, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 490, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 500, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 520, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 530, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 540, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 550, Loss= 0.6934, Training Accuracy= 0.510\n",
      "Epoch: 560, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 570, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 580, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 590, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 600, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 610, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 620, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 630, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 640, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 650, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 660, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 670, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 680, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 690, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 700, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 710, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 720, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 730, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 740, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 750, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 760, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 770, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 780, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 790, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 800, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 810, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 820, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 830, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 840, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 850, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 860, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 870, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 880, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 890, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 900, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 910, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 920, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 930, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 940, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 950, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 960, Loss= 0.6928, Training Accuracy= 0.503\n",
      "Epoch: 970, Loss= 0.6927, Training Accuracy= 0.502\n",
      "Epoch: 980, Loss= 0.6927, Training Accuracy= 0.500\n",
      "Epoch: 990, Loss= 0.6926, Training Accuracy= 0.501\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4994\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.6956, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 20, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 30, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 40, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 70, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 90, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 100, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 110, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 130, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 160, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 230, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 270, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 280, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 290, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 320, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 330, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 340, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 350, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 360, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 400, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 420, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 430, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 440, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 450, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 460, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 470, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 480, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 490, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 500, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 510, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 520, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 530, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 540, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 550, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 560, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 570, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 580, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 590, Loss= 0.6929, Training Accuracy= 0.510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 610, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 620, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 630, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 640, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 650, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 660, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 670, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 680, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 690, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 700, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 710, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 720, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 730, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 740, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 750, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 760, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 770, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 780, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 790, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 800, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 810, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 820, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 830, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 840, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 850, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 860, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 870, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 880, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 890, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 900, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 910, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 920, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 930, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 940, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 950, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 960, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 970, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 990, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4933\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 10, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 20, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 30, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 40, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 60, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 80, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 120, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 150, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 230, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 260, Loss= 0.6929, Training Accuracy= 0.504\n",
      "Epoch: 270, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 280, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 290, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 300, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 310, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 320, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 330, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 340, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 350, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 360, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 370, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 380, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 390, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 400, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 410, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 420, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 430, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 440, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 450, Loss= 0.6928, Training Accuracy= 0.516\n",
      "Epoch: 460, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 470, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 480, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 490, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 500, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 510, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 520, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 530, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 540, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 550, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 560, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 570, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 580, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 590, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 600, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 610, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 620, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 630, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 640, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 650, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 660, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 670, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 680, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 690, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 700, Loss= 0.6923, Training Accuracy= 0.515\n",
      "Epoch: 710, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 720, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 730, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 740, Loss= 0.6922, Training Accuracy= 0.517\n",
      "Epoch: 750, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 760, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 770, Loss= 0.6921, Training Accuracy= 0.519\n",
      "Epoch: 780, Loss= 0.6921, Training Accuracy= 0.519\n",
      "Epoch: 790, Loss= 0.6921, Training Accuracy= 0.519\n",
      "Epoch: 800, Loss= 0.6921, Training Accuracy= 0.520\n",
      "Epoch: 810, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 820, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 830, Loss= 0.6920, Training Accuracy= 0.522\n",
      "Epoch: 840, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 850, Loss= 0.6919, Training Accuracy= 0.521\n",
      "Epoch: 860, Loss= 0.6919, Training Accuracy= 0.520\n",
      "Epoch: 870, Loss= 0.6919, Training Accuracy= 0.518\n",
      "Epoch: 880, Loss= 0.6919, Training Accuracy= 0.517\n",
      "Epoch: 890, Loss= 0.6918, Training Accuracy= 0.517\n",
      "Epoch: 900, Loss= 0.6918, Training Accuracy= 0.516\n",
      "Epoch: 910, Loss= 0.6918, Training Accuracy= 0.517\n",
      "Epoch: 920, Loss= 0.6918, Training Accuracy= 0.517\n",
      "Epoch: 930, Loss= 0.6917, Training Accuracy= 0.517\n",
      "Epoch: 940, Loss= 0.6917, Training Accuracy= 0.517\n",
      "Epoch: 950, Loss= 0.6917, Training Accuracy= 0.519\n",
      "Epoch: 960, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 970, Loss= 0.6916, Training Accuracy= 0.520\n",
      "Epoch: 980, Loss= 0.6916, Training Accuracy= 0.520\n",
      "Epoch: 990, Loss= 0.6916, Training Accuracy= 0.520\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5055\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6933, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 40, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 70, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 80, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 90, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 130, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 160, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 170, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6932, Training Accuracy= 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 200, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 210, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 220, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 230, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 240, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 250, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 260, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 270, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 280, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 290, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 300, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 310, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 320, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 330, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 340, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 350, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 360, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 370, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 380, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 390, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 400, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 420, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 430, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 440, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 450, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 460, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 470, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 480, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 490, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 500, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 520, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 530, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 540, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 550, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 560, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 570, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 580, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 590, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 600, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 610, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 620, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 630, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 640, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 650, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 660, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 670, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 680, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 690, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 700, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 710, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 720, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 730, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 740, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 750, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 760, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 770, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 780, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 790, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 800, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 810, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 820, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 830, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 840, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 850, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 860, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 870, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 880, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 890, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 900, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 910, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 920, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 930, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 940, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 950, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 960, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 970, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 980, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 990, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5082\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.6984, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6936, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.6933, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.6933, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 80, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 90, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 100, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 110, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 190, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 200, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 210, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 220, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 270, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 280, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 290, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 320, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 330, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 340, Loss= 0.6930, Training Accuracy= 0.500\n",
      "Epoch: 350, Loss= 0.6930, Training Accuracy= 0.500\n",
      "Epoch: 360, Loss= 0.6930, Training Accuracy= 0.500\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.500\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 390, Loss= 0.6929, Training Accuracy= 0.500\n",
      "Epoch: 400, Loss= 0.6929, Training Accuracy= 0.500\n",
      "Epoch: 410, Loss= 0.6929, Training Accuracy= 0.500\n",
      "Epoch: 420, Loss= 0.6929, Training Accuracy= 0.502\n",
      "Epoch: 430, Loss= 0.6929, Training Accuracy= 0.502\n",
      "Epoch: 440, Loss= 0.6929, Training Accuracy= 0.502\n",
      "Epoch: 450, Loss= 0.6929, Training Accuracy= 0.502\n",
      "Epoch: 460, Loss= 0.6929, Training Accuracy= 0.501\n",
      "Epoch: 470, Loss= 0.6929, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.6929, Training Accuracy= 0.502\n",
      "Epoch: 490, Loss= 0.6929, Training Accuracy= 0.502\n",
      "Epoch: 500, Loss= 0.6929, Training Accuracy= 0.503\n",
      "Epoch: 510, Loss= 0.6929, Training Accuracy= 0.503\n",
      "Epoch: 520, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 530, Loss= 0.6928, Training Accuracy= 0.505\n",
      "Epoch: 540, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 550, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 560, Loss= 0.6928, Training Accuracy= 0.503\n",
      "Epoch: 570, Loss= 0.6928, Training Accuracy= 0.502\n",
      "Epoch: 580, Loss= 0.6928, Training Accuracy= 0.503\n",
      "Epoch: 590, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 600, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 610, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 620, Loss= 0.6928, Training Accuracy= 0.505\n",
      "Epoch: 630, Loss= 0.6928, Training Accuracy= 0.505\n",
      "Epoch: 640, Loss= 0.6928, Training Accuracy= 0.505\n",
      "Epoch: 650, Loss= 0.6928, Training Accuracy= 0.505\n",
      "Epoch: 660, Loss= 0.6928, Training Accuracy= 0.505\n",
      "Epoch: 670, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 680, Loss= 0.6928, Training Accuracy= 0.503\n",
      "Epoch: 690, Loss= 0.6928, Training Accuracy= 0.503\n",
      "Epoch: 700, Loss= 0.6928, Training Accuracy= 0.503\n",
      "Epoch: 710, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 720, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 730, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 740, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 750, Loss= 0.6927, Training Accuracy= 0.504\n",
      "Epoch: 760, Loss= 0.6927, Training Accuracy= 0.504\n",
      "Epoch: 770, Loss= 0.6927, Training Accuracy= 0.504\n",
      "Epoch: 780, Loss= 0.6927, Training Accuracy= 0.505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.6927, Training Accuracy= 0.505\n",
      "Epoch: 800, Loss= 0.6927, Training Accuracy= 0.505\n",
      "Epoch: 810, Loss= 0.6927, Training Accuracy= 0.505\n",
      "Epoch: 820, Loss= 0.6927, Training Accuracy= 0.505\n",
      "Epoch: 830, Loss= 0.6927, Training Accuracy= 0.506\n",
      "Epoch: 840, Loss= 0.6927, Training Accuracy= 0.505\n",
      "Epoch: 850, Loss= 0.6927, Training Accuracy= 0.506\n",
      "Epoch: 860, Loss= 0.6927, Training Accuracy= 0.506\n",
      "Epoch: 870, Loss= 0.6927, Training Accuracy= 0.506\n",
      "Epoch: 880, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 890, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 900, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 910, Loss= 0.6927, Training Accuracy= 0.506\n",
      "Epoch: 920, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 930, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 940, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 950, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 960, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 970, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 980, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 990, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4982\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.6991, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.6981, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.6977, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.6973, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.6970, Training Accuracy= 0.498\n",
      "Epoch: 50, Loss= 0.6967, Training Accuracy= 0.498\n",
      "Epoch: 60, Loss= 0.6964, Training Accuracy= 0.498\n",
      "Epoch: 70, Loss= 0.6962, Training Accuracy= 0.498\n",
      "Epoch: 80, Loss= 0.6960, Training Accuracy= 0.498\n",
      "Epoch: 90, Loss= 0.6959, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.6957, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.6956, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.6955, Training Accuracy= 0.498\n",
      "Epoch: 130, Loss= 0.6954, Training Accuracy= 0.498\n",
      "Epoch: 140, Loss= 0.6953, Training Accuracy= 0.498\n",
      "Epoch: 150, Loss= 0.6952, Training Accuracy= 0.498\n",
      "Epoch: 160, Loss= 0.6951, Training Accuracy= 0.498\n",
      "Epoch: 170, Loss= 0.6950, Training Accuracy= 0.498\n",
      "Epoch: 180, Loss= 0.6950, Training Accuracy= 0.498\n",
      "Epoch: 190, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 200, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 210, Loss= 0.6948, Training Accuracy= 0.498\n",
      "Epoch: 220, Loss= 0.6948, Training Accuracy= 0.498\n",
      "Epoch: 230, Loss= 0.6947, Training Accuracy= 0.498\n",
      "Epoch: 240, Loss= 0.6947, Training Accuracy= 0.498\n",
      "Epoch: 250, Loss= 0.6946, Training Accuracy= 0.498\n",
      "Epoch: 260, Loss= 0.6946, Training Accuracy= 0.498\n",
      "Epoch: 270, Loss= 0.6946, Training Accuracy= 0.498\n",
      "Epoch: 280, Loss= 0.6945, Training Accuracy= 0.498\n",
      "Epoch: 290, Loss= 0.6945, Training Accuracy= 0.498\n",
      "Epoch: 300, Loss= 0.6945, Training Accuracy= 0.498\n",
      "Epoch: 310, Loss= 0.6944, Training Accuracy= 0.498\n",
      "Epoch: 320, Loss= 0.6944, Training Accuracy= 0.498\n",
      "Epoch: 330, Loss= 0.6944, Training Accuracy= 0.498\n",
      "Epoch: 340, Loss= 0.6944, Training Accuracy= 0.498\n",
      "Epoch: 350, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 360, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 370, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 380, Loss= 0.6943, Training Accuracy= 0.497\n",
      "Epoch: 390, Loss= 0.6943, Training Accuracy= 0.497\n",
      "Epoch: 400, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 410, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 420, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 430, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 440, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 450, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 460, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 470, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 480, Loss= 0.6941, Training Accuracy= 0.497\n",
      "Epoch: 490, Loss= 0.6941, Training Accuracy= 0.497\n",
      "Epoch: 500, Loss= 0.6941, Training Accuracy= 0.497\n",
      "Epoch: 510, Loss= 0.6941, Training Accuracy= 0.497\n",
      "Epoch: 520, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 550, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 570, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 580, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 590, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 600, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 610, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 620, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 630, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 640, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 650, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 660, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 670, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 680, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 690, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 700, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 710, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 740, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 750, Loss= 0.6939, Training Accuracy= 0.504\n",
      "Epoch: 760, Loss= 0.6939, Training Accuracy= 0.504\n",
      "Epoch: 770, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 780, Loss= 0.6939, Training Accuracy= 0.506\n",
      "Epoch: 790, Loss= 0.6939, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.6939, Training Accuracy= 0.506\n",
      "Epoch: 810, Loss= 0.6939, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 830, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 840, Loss= 0.6938, Training Accuracy= 0.510\n",
      "Epoch: 850, Loss= 0.6938, Training Accuracy= 0.511\n",
      "Epoch: 860, Loss= 0.6938, Training Accuracy= 0.511\n",
      "Epoch: 870, Loss= 0.6938, Training Accuracy= 0.511\n",
      "Epoch: 880, Loss= 0.6938, Training Accuracy= 0.511\n",
      "Epoch: 890, Loss= 0.6938, Training Accuracy= 0.512\n",
      "Epoch: 900, Loss= 0.6938, Training Accuracy= 0.513\n",
      "Epoch: 910, Loss= 0.6938, Training Accuracy= 0.513\n",
      "Epoch: 920, Loss= 0.6938, Training Accuracy= 0.512\n",
      "Epoch: 930, Loss= 0.6938, Training Accuracy= 0.513\n",
      "Epoch: 940, Loss= 0.6938, Training Accuracy= 0.514\n",
      "Epoch: 950, Loss= 0.6938, Training Accuracy= 0.515\n",
      "Epoch: 960, Loss= 0.6938, Training Accuracy= 0.516\n",
      "Epoch: 970, Loss= 0.6938, Training Accuracy= 0.516\n",
      "Epoch: 980, Loss= 0.6937, Training Accuracy= 0.515\n",
      "Epoch: 990, Loss= 0.6937, Training Accuracy= 0.514\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.513\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.6945, Training Accuracy= 0.505\n",
      "Epoch: 10, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 20, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 30, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 40, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 60, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 70, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 90, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 100, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 110, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 120, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 130, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 160, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 210, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 230, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 270, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 280, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 290, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 320, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 330, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 340, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 350, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 360, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 400, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 430, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 470, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 500, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 510, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 520, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 530, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 540, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 550, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 560, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 570, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 580, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 590, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 600, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 610, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 620, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 630, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 640, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 650, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 660, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 670, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 680, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 690, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 700, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 710, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 720, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 730, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 740, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 750, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 760, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 770, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 780, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 790, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 800, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 810, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 820, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 830, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 840, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 850, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 860, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 870, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 880, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 890, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 900, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 910, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 920, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 930, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 940, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 950, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 960, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 970, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 980, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 990, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4961\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.6983, Training Accuracy= 0.491\n",
      "Epoch: 10, Loss= 0.6960, Training Accuracy= 0.491\n",
      "Epoch: 20, Loss= 0.6952, Training Accuracy= 0.491\n",
      "Epoch: 30, Loss= 0.6947, Training Accuracy= 0.491\n",
      "Epoch: 40, Loss= 0.6944, Training Accuracy= 0.491\n",
      "Epoch: 50, Loss= 0.6941, Training Accuracy= 0.491\n",
      "Epoch: 60, Loss= 0.6940, Training Accuracy= 0.491\n",
      "Epoch: 70, Loss= 0.6938, Training Accuracy= 0.491\n",
      "Epoch: 80, Loss= 0.6937, Training Accuracy= 0.491\n",
      "Epoch: 90, Loss= 0.6936, Training Accuracy= 0.490\n",
      "Epoch: 100, Loss= 0.6936, Training Accuracy= 0.490\n",
      "Epoch: 110, Loss= 0.6935, Training Accuracy= 0.490\n",
      "Epoch: 120, Loss= 0.6935, Training Accuracy= 0.490\n",
      "Epoch: 130, Loss= 0.6934, Training Accuracy= 0.491\n",
      "Epoch: 140, Loss= 0.6934, Training Accuracy= 0.491\n",
      "Epoch: 150, Loss= 0.6934, Training Accuracy= 0.491\n",
      "Epoch: 160, Loss= 0.6933, Training Accuracy= 0.491\n",
      "Epoch: 170, Loss= 0.6933, Training Accuracy= 0.492\n",
      "Epoch: 180, Loss= 0.6933, Training Accuracy= 0.493\n",
      "Epoch: 190, Loss= 0.6933, Training Accuracy= 0.494\n",
      "Epoch: 200, Loss= 0.6932, Training Accuracy= 0.497\n",
      "Epoch: 210, Loss= 0.6932, Training Accuracy= 0.499\n",
      "Epoch: 220, Loss= 0.6932, Training Accuracy= 0.497\n",
      "Epoch: 230, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 240, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 250, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 260, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 270, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 290, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 300, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 310, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 320, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 330, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 340, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 350, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 360, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 400, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 420, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 430, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 470, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 500, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 520, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 530, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 540, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 550, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 560, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 570, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 580, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 590, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 600, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 610, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 620, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 630, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 640, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 650, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 660, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 670, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 680, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 690, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 700, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 710, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 720, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 730, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 740, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 750, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 760, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 770, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 780, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 790, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 800, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 810, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 820, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 830, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 840, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 850, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 860, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 870, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 880, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 890, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 900, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 910, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 920, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 930, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 940, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 950, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 960, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 970, Loss= 0.6927, Training Accuracy= 0.516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 990, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5053\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.6982, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6953, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.6946, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.6945, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.6943, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 100, Loss= 0.6941, Training Accuracy= 0.497\n",
      "Epoch: 110, Loss= 0.6941, Training Accuracy= 0.497\n",
      "Epoch: 120, Loss= 0.6940, Training Accuracy= 0.497\n",
      "Epoch: 130, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 140, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 150, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 160, Loss= 0.6937, Training Accuracy= 0.497\n",
      "Epoch: 170, Loss= 0.6937, Training Accuracy= 0.497\n",
      "Epoch: 180, Loss= 0.6937, Training Accuracy= 0.497\n",
      "Epoch: 190, Loss= 0.6936, Training Accuracy= 0.497\n",
      "Epoch: 200, Loss= 0.6936, Training Accuracy= 0.498\n",
      "Epoch: 210, Loss= 0.6936, Training Accuracy= 0.498\n",
      "Epoch: 220, Loss= 0.6935, Training Accuracy= 0.498\n",
      "Epoch: 230, Loss= 0.6935, Training Accuracy= 0.498\n",
      "Epoch: 240, Loss= 0.6935, Training Accuracy= 0.498\n",
      "Epoch: 250, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 260, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 270, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 280, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 290, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 300, Loss= 0.6933, Training Accuracy= 0.498\n",
      "Epoch: 310, Loss= 0.6933, Training Accuracy= 0.498\n",
      "Epoch: 320, Loss= 0.6933, Training Accuracy= 0.499\n",
      "Epoch: 330, Loss= 0.6933, Training Accuracy= 0.498\n",
      "Epoch: 340, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 350, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 360, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 370, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 380, Loss= 0.6932, Training Accuracy= 0.499\n",
      "Epoch: 390, Loss= 0.6932, Training Accuracy= 0.500\n",
      "Epoch: 400, Loss= 0.6932, Training Accuracy= 0.500\n",
      "Epoch: 410, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 420, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 430, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 450, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 460, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 490, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 500, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 510, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 520, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 530, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 540, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 550, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 560, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 570, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 580, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 590, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 600, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 610, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 620, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 630, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 640, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 650, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 680, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 700, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 710, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 730, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 740, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 750, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 770, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 780, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 790, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 800, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 810, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 820, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 830, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 840, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 850, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 860, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 870, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 880, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 890, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 900, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 910, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 920, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 930, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 940, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 950, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 960, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 970, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 980, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 990, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4937\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7106, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.7007, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6991, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.6982, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6976, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6972, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6968, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6965, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6963, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6961, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6960, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6958, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6957, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6956, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6955, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.6954, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6953, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6952, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 190, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 200, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 210, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 220, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 230, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6948, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6948, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 270, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 280, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 290, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 300, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 310, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 320, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 330, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 340, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 350, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 370, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 380, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 400, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 410, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 420, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 430, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 440, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 450, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 460, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 480, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 490, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 500, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 510, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 520, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 530, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 540, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 550, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 560, Loss= 0.6941, Training Accuracy= 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 580, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 590, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 600, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 610, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 640, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 650, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 660, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 670, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 680, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 690, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 700, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 710, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 740, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 750, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 760, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 770, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 780, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 790, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 800, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 810, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 820, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 830, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 840, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 850, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 860, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 870, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 880, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 890, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 900, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 910, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 920, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 930, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 940, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 950, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 960, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 970, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 980, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 990, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.502\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.06\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.49939999, 0.49329999, 0.50550002, 0.50819999, 0.4982, 0.51300001, 0.49610001, 0.50529999, 0.4937, 0.50199997]\n",
      "mean of test_accuracies_10replications:  0.50147\n",
      "standard deviation of test_accuracies_10replications_std_mean:  6.16734381765e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee16b536d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
