{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 25\n",
    "N = 2\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.8765, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.6887, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6789, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6726, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.6677, Training Accuracy= 0.754\n",
      "Epoch: 50, Loss= 0.6632, Training Accuracy= 1.000\n",
      "Epoch: 60, Loss= 0.6586, Training Accuracy= 1.000\n",
      "Epoch: 70, Loss= 0.6537, Training Accuracy= 1.000\n",
      "Epoch: 80, Loss= 0.6482, Training Accuracy= 1.000\n",
      "Epoch: 90, Loss= 0.6420, Training Accuracy= 1.000\n",
      "Epoch: 100, Loss= 0.6350, Training Accuracy= 1.000\n",
      "Epoch: 110, Loss= 0.6271, Training Accuracy= 1.000\n",
      "Epoch: 120, Loss= 0.6183, Training Accuracy= 1.000\n",
      "Epoch: 130, Loss= 0.6083, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.5972, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.5848, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.5712, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.5564, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.5404, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.5233, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.5052, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.4864, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.4670, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.4472, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.4272, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.4073, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.3876, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.3684, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.3498, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.3319, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.3147, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.2985, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.2831, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.2686, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.2550, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.2422, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.2303, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.2192, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.2088, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.1991, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.1901, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.1816, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.1738, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.1664, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.1595, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.1531, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.1471, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.1414, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.1361, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.1311, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.1265, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.1220, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.1179, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.1140, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.1103, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.1067, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.1034, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.1003, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0973, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0945, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0918, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0892, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0868, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0844, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0822, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0801, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0781, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0762, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0743, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0725, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0708, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0692, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0676, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0661, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0647, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0633, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0620, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0607, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0594, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0582, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0571, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0560, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0549, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0539, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0529, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0519, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0510, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0501, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0492, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0483, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0475, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0467, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0459, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0452, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0444, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0437, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0430, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0424, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0417, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0411, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7058, Training Accuracy= 0.506\n",
      "Epoch: 10, Loss= 0.6903, Training Accuracy= 0.754\n",
      "Epoch: 20, Loss= 0.6845, Training Accuracy= 0.507\n",
      "Epoch: 30, Loss= 0.6796, Training Accuracy= 0.753\n",
      "Epoch: 40, Loss= 0.6745, Training Accuracy= 0.753\n",
      "Epoch: 50, Loss= 0.6688, Training Accuracy= 0.753\n",
      "Epoch: 60, Loss= 0.6623, Training Accuracy= 0.753\n",
      "Epoch: 70, Loss= 0.6547, Training Accuracy= 0.753\n",
      "Epoch: 80, Loss= 0.6457, Training Accuracy= 0.753\n",
      "Epoch: 90, Loss= 0.6352, Training Accuracy= 0.753\n",
      "Epoch: 100, Loss= 0.6228, Training Accuracy= 1.000\n",
      "Epoch: 110, Loss= 0.6087, Training Accuracy= 1.000\n",
      "Epoch: 120, Loss= 0.5927, Training Accuracy= 1.000\n",
      "Epoch: 130, Loss= 0.5750, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.5560, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.5359, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.5151, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.4937, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.4721, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.4505, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.4289, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.4076, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.3866, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.3662, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.3464, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.3272, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.3089, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.2915, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.2749, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.2593, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.2446, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.2309, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.2180, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.2060, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.1949, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.1845, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.1749, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.1660, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.1577, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.1500, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.1428, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.1362, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.1300, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.1242, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.1189, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.1139, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.1092, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.1048, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.1007, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0969, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.0933, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.0899, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0867, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0837, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0809, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0782, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0757, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.0733, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0711, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0689, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.0669, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0650, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0631, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0614, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0597, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0581, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0566, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0552, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0538, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0525, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0512, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0500, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0488, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0477, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0467, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0456, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0446, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0437, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0428, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0419, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0410, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0402, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0394, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0387, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0379, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0372, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0365, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0359, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0352, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0346, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0340, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0334, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0329, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0323, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0318, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0313, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0308, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0303, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0298, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0293, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.8289, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.7111, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6794, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6653, Training Accuracy= 0.748\n",
      "Epoch: 40, Loss= 0.6548, Training Accuracy= 1.000\n",
      "Epoch: 50, Loss= 0.6445, Training Accuracy= 1.000\n",
      "Epoch: 60, Loss= 0.6336, Training Accuracy= 1.000\n",
      "Epoch: 70, Loss= 0.6216, Training Accuracy= 1.000\n",
      "Epoch: 80, Loss= 0.6083, Training Accuracy= 1.000\n",
      "Epoch: 90, Loss= 0.5936, Training Accuracy= 1.000\n",
      "Epoch: 100, Loss= 0.5774, Training Accuracy= 1.000\n",
      "Epoch: 110, Loss= 0.5597, Training Accuracy= 1.000\n",
      "Epoch: 120, Loss= 0.5403, Training Accuracy= 1.000\n",
      "Epoch: 130, Loss= 0.5195, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.4974, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.4744, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.4508, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.4272, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.4039, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.3811, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.3593, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.3385, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.3188, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.3004, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.2832, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.2671, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.2522, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.2385, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.2257, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.2138, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.2028, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.1927, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.1832, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.1745, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.1663, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.1587, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.1516, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.1450, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.1388, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.1330, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.1276, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.1225, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.1178, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.1133, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.1090, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.1050, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.1013, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0977, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0944, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0912, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.0882, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.0853, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0826, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0800, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0776, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0752, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0730, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.0709, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0689, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0669, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0651, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0633, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0616, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0600, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0585, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0570, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0556, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0542, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0529, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0516, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0504, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0493, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0481, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0471, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0460, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0450, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0441, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0431, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0422, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0414, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0405, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0397, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0389, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0382, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0374, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0367, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0360, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0354, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0347, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0341, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0335, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0329, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0323, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0318, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0313, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0307, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0302, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0297, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0293, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0288, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 1.2692, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.7445, Training Accuracy= 0.509\n",
      "Epoch: 20, Loss= 0.7106, Training Accuracy= 0.509\n",
      "Epoch: 30, Loss= 0.6947, Training Accuracy= 0.509\n",
      "Epoch: 40, Loss= 0.6861, Training Accuracy= 0.509\n",
      "Epoch: 50, Loss= 0.6803, Training Accuracy= 0.509\n",
      "Epoch: 60, Loss= 0.6755, Training Accuracy= 0.509\n",
      "Epoch: 70, Loss= 0.6708, Training Accuracy= 0.756\n",
      "Epoch: 80, Loss= 0.6659, Training Accuracy= 0.756\n",
      "Epoch: 90, Loss= 0.6605, Training Accuracy= 1.000\n",
      "Epoch: 100, Loss= 0.6544, Training Accuracy= 1.000\n",
      "Epoch: 110, Loss= 0.6475, Training Accuracy= 1.000\n",
      "Epoch: 120, Loss= 0.6394, Training Accuracy= 1.000\n",
      "Epoch: 130, Loss= 0.6300, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.6191, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.6065, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.5920, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.5758, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.5577, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.5381, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.5170, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.4948, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.4718, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.4483, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.4248, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.4014, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.3786, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.3565, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.3354, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.3153, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.2964, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.2787, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.2623, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.2470, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.2328, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.2197, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.2075, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.1963, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.1859, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.1763, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.1675, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.1592, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.1516, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.1445, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.1380, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.1319, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.1262, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.1209, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.1160, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.1113, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.1070, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.1030, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0992, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0956, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0922, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0891, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0861, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.0833, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0806, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0781, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0757, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0735, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0713, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0693, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0673, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0655, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0638, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0621, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0605, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0590, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0575, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0561, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0548, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0535, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0523, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0511, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0500, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0489, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0478, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0468, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0458, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0449, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0440, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0432, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0423, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0415, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0407, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0400, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0393, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0385, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0379, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0372, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0366, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0360, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0353, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0348, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0342, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0337, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0331, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0326, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.7426, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.6968, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6925, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6903, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6880, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6856, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6829, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.6800, Training Accuracy= 0.749\n",
      "Epoch: 90, Loss= 0.6768, Training Accuracy= 0.749\n",
      "Epoch: 100, Loss= 0.6732, Training Accuracy= 0.749\n",
      "Epoch: 110, Loss= 0.6692, Training Accuracy= 0.749\n",
      "Epoch: 120, Loss= 0.6646, Training Accuracy= 0.749\n",
      "Epoch: 130, Loss= 0.6592, Training Accuracy= 0.749\n",
      "Epoch: 140, Loss= 0.6530, Training Accuracy= 0.749\n",
      "Epoch: 150, Loss= 0.6458, Training Accuracy= 0.749\n",
      "Epoch: 160, Loss= 0.6375, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.6279, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.6170, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.6047, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.5910, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.5760, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.5599, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.5427, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.5246, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.5059, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.4866, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.4670, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.4473, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.4277, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.4082, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.3892, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.3706, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.3527, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.3355, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.3191, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.3035, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.2888, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.2749, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.2618, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.2496, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.2381, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.2273, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.2172, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.2078, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.1990, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.1907, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.1829, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.1757, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.1689, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.1625, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.1565, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.1508, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.1455, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.1405, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.1357, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.1312, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.1270, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.1230, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.1192, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.1156, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.1122, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.1089, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.1059, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.1029, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.1001, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0975, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0949, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0925, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0902, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0879, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0858, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0838, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0818, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0800, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0782, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0765, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0748, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0732, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.0717, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0702, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0688, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0674, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0661, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0648, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0636, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0624, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0613, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0602, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0591, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0580, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0570, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0561, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0551, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0542, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0533, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0525, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0516, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0508, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0500, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 1.3232, Training Accuracy= 0.505\n",
      "Epoch: 10, Loss= 0.7194, Training Accuracy= 0.494\n",
      "Epoch: 20, Loss= 0.7022, Training Accuracy= 0.494\n",
      "Epoch: 30, Loss= 0.6942, Training Accuracy= 0.494\n",
      "Epoch: 40, Loss= 0.6898, Training Accuracy= 0.754\n",
      "Epoch: 50, Loss= 0.6868, Training Accuracy= 0.754\n",
      "Epoch: 60, Loss= 0.6843, Training Accuracy= 0.509\n",
      "Epoch: 70, Loss= 0.6819, Training Accuracy= 0.509\n",
      "Epoch: 80, Loss= 0.6793, Training Accuracy= 0.509\n",
      "Epoch: 90, Loss= 0.6765, Training Accuracy= 0.509\n",
      "Epoch: 100, Loss= 0.6734, Training Accuracy= 0.509\n",
      "Epoch: 110, Loss= 0.6698, Training Accuracy= 0.509\n",
      "Epoch: 120, Loss= 0.6658, Training Accuracy= 0.509\n",
      "Epoch: 130, Loss= 0.6612, Training Accuracy= 0.509\n",
      "Epoch: 140, Loss= 0.6560, Training Accuracy= 0.509\n",
      "Epoch: 150, Loss= 0.6500, Training Accuracy= 0.755\n",
      "Epoch: 160, Loss= 0.6432, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.6356, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.6271, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.6178, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.6078, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.5969, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.5854, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.5733, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.5605, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.5473, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.5336, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.5194, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.5049, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.4900, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.4749, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.4596, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.4442, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.4288, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.4135, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.3982, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.3833, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.3685, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.3542, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.3402, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.3267, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.3136, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.3010, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.2889, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.2774, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.2663, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.2557, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.2456, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.2360, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.2268, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.2180, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.2097, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.2018, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.1942, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.1871, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.1802, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.1737, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.1676, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.1617, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.1561, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.1508, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.1457, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.1408, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.1362, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.1318, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.1277, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.1237, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.1199, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.1162, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.1128, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.1094, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.1063, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.1032, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.1004, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0976, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0949, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0924, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0899, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0876, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0854, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0832, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0811, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0792, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0773, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0754, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0737, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0720, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0703, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0688, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0672, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0658, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0644, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0630, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0617, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0604, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0592, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0580, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0569, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0558, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0547, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.7983, Training Accuracy= 0.504\n",
      "Epoch: 10, Loss= 0.7129, Training Accuracy= 0.504\n",
      "Epoch: 20, Loss= 0.6927, Training Accuracy= 0.755\n",
      "Epoch: 30, Loss= 0.6834, Training Accuracy= 0.755\n",
      "Epoch: 40, Loss= 0.6777, Training Accuracy= 0.755\n",
      "Epoch: 50, Loss= 0.6733, Training Accuracy= 0.755\n",
      "Epoch: 60, Loss= 0.6692, Training Accuracy= 0.755\n",
      "Epoch: 70, Loss= 0.6648, Training Accuracy= 0.755\n",
      "Epoch: 80, Loss= 0.6599, Training Accuracy= 0.755\n",
      "Epoch: 90, Loss= 0.6542, Training Accuracy= 0.755\n",
      "Epoch: 100, Loss= 0.6477, Training Accuracy= 0.755\n",
      "Epoch: 110, Loss= 0.6403, Training Accuracy= 0.755\n",
      "Epoch: 120, Loss= 0.6320, Training Accuracy= 0.755\n",
      "Epoch: 130, Loss= 0.6227, Training Accuracy= 0.755\n",
      "Epoch: 140, Loss= 0.6125, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.6014, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.5896, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.5770, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.5637, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.5498, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.5352, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.5200, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.5043, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.4881, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.4716, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.4547, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.4376, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.4205, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.4034, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.3865, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.3700, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.3538, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.3381, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.3230, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.3086, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.2947, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.2816, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.2691, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.2573, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.2461, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.2356, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.2256, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.2163, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.2074, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.1992, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.1913, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.1840, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.1771, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.1705, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.1644, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.1585, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.1530, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.1478, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.1429, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.1383, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.1339, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.1297, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.1257, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.1220, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.1184, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.1150, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.1117, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.1086, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.1057, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.1028, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.1002, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0976, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0951, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0928, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0905, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0884, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0863, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0843, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0824, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0805, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0788, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0771, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0754, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0739, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0723, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0709, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0695, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0681, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0668, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0655, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0643, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0631, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0619, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0608, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0597, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0587, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0577, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0567, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0557, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0548, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0539, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0531, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0522, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0514, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0506, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.8241, Training Accuracy= 0.499\n",
      "Epoch: 10, Loss= 0.6902, Training Accuracy= 0.752\n",
      "Epoch: 20, Loss= 0.6796, Training Accuracy= 0.752\n",
      "Epoch: 30, Loss= 0.6728, Training Accuracy= 0.752\n",
      "Epoch: 40, Loss= 0.6658, Training Accuracy= 1.000\n",
      "Epoch: 50, Loss= 0.6578, Training Accuracy= 1.000\n",
      "Epoch: 60, Loss= 0.6483, Training Accuracy= 1.000\n",
      "Epoch: 70, Loss= 0.6369, Training Accuracy= 1.000\n",
      "Epoch: 80, Loss= 0.6232, Training Accuracy= 1.000\n",
      "Epoch: 90, Loss= 0.6073, Training Accuracy= 1.000\n",
      "Epoch: 100, Loss= 0.5890, Training Accuracy= 1.000\n",
      "Epoch: 110, Loss= 0.5686, Training Accuracy= 1.000\n",
      "Epoch: 120, Loss= 0.5464, Training Accuracy= 1.000\n",
      "Epoch: 130, Loss= 0.5227, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.4979, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.4722, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.4461, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.4198, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.3937, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.3681, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.3432, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.3194, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.2968, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.2756, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.2559, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.2376, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.2207, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.2053, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.1912, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.1783, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.1666, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.1559, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.1462, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.1374, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.1294, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.1220, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.1153, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.1092, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.1036, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0985, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0937, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0894, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0853, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0816, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0782, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0750, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0720, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0692, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0666, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0642, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.0619, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.0598, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0578, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0559, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0541, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0524, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0508, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.0493, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0479, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0466, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0453, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0441, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0429, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0418, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0408, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0398, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0388, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0379, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0370, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0362, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0354, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0346, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0338, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0331, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0324, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0318, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0312, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0305, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0300, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0294, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0288, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0283, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0278, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0273, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0268, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0264, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0259, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0255, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0250, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0246, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0242, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0239, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0235, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0231, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0228, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0224, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0221, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0218, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.0215, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0212, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.8212, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 0.7443, Training Accuracy= 0.496\n",
      "Epoch: 20, Loss= 0.7128, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.7004, Training Accuracy= 0.496\n",
      "Epoch: 40, Loss= 0.6954, Training Accuracy= 0.496\n",
      "Epoch: 50, Loss= 0.6931, Training Accuracy= 0.496\n",
      "Epoch: 60, Loss= 0.6918, Training Accuracy= 0.496\n",
      "Epoch: 70, Loss= 0.6909, Training Accuracy= 0.496\n",
      "Epoch: 80, Loss= 0.6900, Training Accuracy= 0.750\n",
      "Epoch: 90, Loss= 0.6892, Training Accuracy= 0.750\n",
      "Epoch: 100, Loss= 0.6883, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6873, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6863, Training Accuracy= 0.750\n",
      "Epoch: 130, Loss= 0.6852, Training Accuracy= 0.750\n",
      "Epoch: 140, Loss= 0.6840, Training Accuracy= 0.750\n",
      "Epoch: 150, Loss= 0.6826, Training Accuracy= 0.750\n",
      "Epoch: 160, Loss= 0.6812, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.6796, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.6778, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.6758, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.6736, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.6712, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.6684, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.6654, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.6620, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.6582, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.6539, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.6492, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.6440, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.6382, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.6319, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.6250, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.6174, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.6092, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.6004, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.5908, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.5806, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.5698, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.5583, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.5462, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.5334, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.5201, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.5063, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.4920, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.4773, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.4622, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.4468, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.4312, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.4155, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.3998, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.3841, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.3685, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.3532, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.3381, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.3234, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.3090, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.2951, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.2817, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.2687, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.2563, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.2445, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.2331, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.2223, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.2121, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.2023, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.1931, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.1843, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.1761, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.1683, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.1609, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.1539, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.1474, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.1412, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.1353, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.1298, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.1246, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.1197, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.1151, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.1107, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.1066, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.1027, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0990, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0955, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0922, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0891, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0861, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0833, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0806, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0781, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0757, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0734, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0712, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0692, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0672, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0653, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0635, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0618, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0601, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0586, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0570, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 1.4519, Training Accuracy= 0.499\n",
      "Epoch: 10, Loss= 0.7344, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.7100, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6985, Training Accuracy= 0.509\n",
      "Epoch: 40, Loss= 0.6920, Training Accuracy= 0.509\n",
      "Epoch: 50, Loss= 0.6874, Training Accuracy= 0.509\n",
      "Epoch: 60, Loss= 0.6837, Training Accuracy= 0.509\n",
      "Epoch: 70, Loss= 0.6802, Training Accuracy= 0.509\n",
      "Epoch: 80, Loss= 0.6768, Training Accuracy= 0.509\n",
      "Epoch: 90, Loss= 0.6732, Training Accuracy= 1.000\n",
      "Epoch: 100, Loss= 0.6695, Training Accuracy= 1.000\n",
      "Epoch: 110, Loss= 0.6656, Training Accuracy= 1.000\n",
      "Epoch: 120, Loss= 0.6615, Training Accuracy= 1.000\n",
      "Epoch: 130, Loss= 0.6570, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.6522, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.6471, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.6416, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.6357, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.6294, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.6227, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.6155, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.6078, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.5997, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.5910, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.5818, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.5721, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.5619, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.5512, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.5401, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.5285, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.5165, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.5042, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.4915, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.4787, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.4657, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.4525, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.4394, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.4262, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.4131, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.4001, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.3873, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.3746, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.3622, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.3500, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.3382, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.3266, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.3154, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.3044, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.2938, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.2836, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.2737, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.2641, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.2549, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.2460, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.2375, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.2293, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.2214, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.2138, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.2065, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.1995, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.1929, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.1864, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.1803, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.1744, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.1688, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.1634, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.1582, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.1533, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.1485, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.1440, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.1396, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.1355, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.1315, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.1277, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.1240, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.1205, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.1171, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.1139, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.1108, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.1078, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.1050, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.1023, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0996, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0971, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0947, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0923, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0901, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0879, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0858, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0838, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0819, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0801, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0783, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0765, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0749, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0732, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0717, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0702, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0687, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0673, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.000275\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "mean of test_accuracies_10replications:  1.0\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.0\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a285273d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
