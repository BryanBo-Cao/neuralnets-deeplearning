{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 10\n",
    "N = 10\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.7103, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6984, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6962, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 60, Loss= 0.6936, Training Accuracy= 0.516\n",
      "Epoch: 70, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 80, Loss= 0.6931, Training Accuracy= 0.514\n",
      "Epoch: 90, Loss= 0.6929, Training Accuracy= 0.517\n",
      "Epoch: 100, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 110, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 120, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 130, Loss= 0.6923, Training Accuracy= 0.521\n",
      "Epoch: 140, Loss= 0.6917, Training Accuracy= 0.531\n",
      "Epoch: 150, Loss= 0.6755, Training Accuracy= 0.579\n",
      "Epoch: 160, Loss= 0.6257, Training Accuracy= 0.632\n",
      "Epoch: 170, Loss= 0.5022, Training Accuracy= 0.786\n",
      "Epoch: 180, Loss= 0.4676, Training Accuracy= 0.795\n",
      "Epoch: 190, Loss= 0.3395, Training Accuracy= 0.869\n",
      "Epoch: 200, Loss= 0.2715, Training Accuracy= 0.893\n",
      "Epoch: 210, Loss= 0.1907, Training Accuracy= 0.924\n",
      "Epoch: 220, Loss= 0.1488, Training Accuracy= 0.942\n",
      "Epoch: 230, Loss= 0.2180, Training Accuracy= 0.887\n",
      "Epoch: 240, Loss= 0.1065, Training Accuracy= 0.964\n",
      "Epoch: 250, Loss= 0.1058, Training Accuracy= 0.961\n",
      "Epoch: 260, Loss= 0.0902, Training Accuracy= 0.968\n",
      "Epoch: 270, Loss= 0.0835, Training Accuracy= 0.970\n",
      "Epoch: 280, Loss= 0.0800, Training Accuracy= 0.968\n",
      "Epoch: 290, Loss= 0.0841, Training Accuracy= 0.964\n",
      "Epoch: 300, Loss= 0.7443, Training Accuracy= 0.504\n",
      "Epoch: 310, Loss= 0.7143, Training Accuracy= 0.509\n",
      "Epoch: 320, Loss= 0.6960, Training Accuracy= 0.520\n",
      "Epoch: 330, Loss= 0.6826, Training Accuracy= 0.547\n",
      "Epoch: 340, Loss= 0.6506, Training Accuracy= 0.581\n",
      "Epoch: 350, Loss= 0.6140, Training Accuracy= 0.672\n",
      "Epoch: 360, Loss= 0.5639, Training Accuracy= 0.705\n",
      "Epoch: 370, Loss= 0.3040, Training Accuracy= 0.887\n",
      "Epoch: 380, Loss= 0.2698, Training Accuracy= 0.906\n",
      "Epoch: 390, Loss= 0.2448, Training Accuracy= 0.913\n",
      "Epoch: 400, Loss= 0.2373, Training Accuracy= 0.920\n",
      "Epoch: 410, Loss= 0.2240, Training Accuracy= 0.925\n",
      "Epoch: 420, Loss= 0.7968, Training Accuracy= 0.525\n",
      "Epoch: 430, Loss= 0.6720, Training Accuracy= 0.658\n",
      "Epoch: 440, Loss= 0.6514, Training Accuracy= 0.663\n",
      "Epoch: 450, Loss= 0.7087, Training Accuracy= 0.655\n",
      "Epoch: 460, Loss= 0.7848, Training Accuracy= 0.519\n",
      "Epoch: 470, Loss= 0.6567, Training Accuracy= 0.585\n",
      "Epoch: 480, Loss= 0.6134, Training Accuracy= 0.623\n",
      "Epoch: 490, Loss= 0.6122, Training Accuracy= 0.642\n",
      "Epoch: 500, Loss= 0.6478, Training Accuracy= 0.650\n",
      "Epoch: 510, Loss= 0.6233, Training Accuracy= 0.688\n",
      "Epoch: 520, Loss= 0.6117, Training Accuracy= 0.698\n",
      "Epoch: 530, Loss= 0.5850, Training Accuracy= 0.714\n",
      "Epoch: 540, Loss= 0.5547, Training Accuracy= 0.731\n",
      "Epoch: 550, Loss= 0.5211, Training Accuracy= 0.768\n",
      "Epoch: 560, Loss= 0.4921, Training Accuracy= 0.775\n",
      "Epoch: 570, Loss= 0.6887, Training Accuracy= 0.539\n",
      "Epoch: 580, Loss= 0.6171, Training Accuracy= 0.683\n",
      "Epoch: 590, Loss= 0.5804, Training Accuracy= 0.729\n",
      "Epoch: 600, Loss= 0.5555, Training Accuracy= 0.743\n",
      "Epoch: 610, Loss= 0.5330, Training Accuracy= 0.756\n",
      "Epoch: 620, Loss= 0.5091, Training Accuracy= 0.775\n",
      "Epoch: 630, Loss= 0.4817, Training Accuracy= 0.794\n",
      "Epoch: 640, Loss= 0.4741, Training Accuracy= 0.805\n",
      "Epoch: 650, Loss= 0.4684, Training Accuracy= 0.809\n",
      "Epoch: 660, Loss= 0.4673, Training Accuracy= 0.811\n",
      "Epoch: 670, Loss= 0.5310, Training Accuracy= 0.760\n",
      "Epoch: 680, Loss= 0.4587, Training Accuracy= 0.813\n",
      "Epoch: 690, Loss= 0.4979, Training Accuracy= 0.782\n",
      "Epoch: 700, Loss= 0.5065, Training Accuracy= 0.788\n",
      "Epoch: 710, Loss= 0.7578, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.7113, Training Accuracy= 0.498\n",
      "Epoch: 730, Loss= 0.6986, Training Accuracy= 0.510\n",
      "Epoch: 740, Loss= 0.6986, Training Accuracy= 0.506\n",
      "Epoch: 750, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 760, Loss= 0.7052, Training Accuracy= 0.493\n",
      "Epoch: 770, Loss= 0.6948, Training Accuracy= 0.511\n",
      "Epoch: 780, Loss= 0.6959, Training Accuracy= 0.505\n",
      "Epoch: 790, Loss= 0.6964, Training Accuracy= 0.514\n",
      "Epoch: 800, Loss= 0.6984, Training Accuracy= 0.511\n",
      "Epoch: 810, Loss= 0.6955, Training Accuracy= 0.509\n",
      "Epoch: 820, Loss= 0.6925, Training Accuracy= 0.508\n",
      "Epoch: 830, Loss= 0.6919, Training Accuracy= 0.513\n",
      "Epoch: 840, Loss= 0.6908, Training Accuracy= 0.517\n",
      "Epoch: 850, Loss= 0.6921, Training Accuracy= 0.512\n",
      "Epoch: 860, Loss= 0.6905, Training Accuracy= 0.523\n",
      "Epoch: 870, Loss= 0.6895, Training Accuracy= 0.520\n",
      "Epoch: 880, Loss= 0.6917, Training Accuracy= 0.518\n",
      "Epoch: 890, Loss= 0.6888, Training Accuracy= 0.529\n",
      "Epoch: 900, Loss= 0.6878, Training Accuracy= 0.535\n",
      "Epoch: 910, Loss= 0.6867, Training Accuracy= 0.539\n",
      "Epoch: 920, Loss= 0.6858, Training Accuracy= 0.536\n",
      "Epoch: 930, Loss= 0.6903, Training Accuracy= 0.518\n",
      "Epoch: 940, Loss= 0.6829, Training Accuracy= 0.531\n",
      "Epoch: 950, Loss= 0.6871, Training Accuracy= 0.523\n",
      "Epoch: 960, Loss= 0.6828, Training Accuracy= 0.526\n",
      "Epoch: 970, Loss= 0.6825, Training Accuracy= 0.536\n",
      "Epoch: 980, Loss= 0.6808, Training Accuracy= 0.536\n",
      "Epoch: 990, Loss= 0.6830, Training Accuracy= 0.524\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5124\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7084, Training Accuracy= 0.485\n",
      "Epoch: 10, Loss= 0.6955, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.6947, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 50, Loss= 0.6939, Training Accuracy= 0.499\n",
      "Epoch: 60, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6934, Training Accuracy= 0.515\n",
      "Epoch: 100, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 130, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 150, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 160, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 170, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 180, Loss= 0.6928, Training Accuracy= 0.505\n",
      "Epoch: 190, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6928, Training Accuracy= 0.506\n",
      "Epoch: 210, Loss= 0.6927, Training Accuracy= 0.498\n",
      "Epoch: 220, Loss= 0.6927, Training Accuracy= 0.500\n",
      "Epoch: 230, Loss= 0.6926, Training Accuracy= 0.500\n",
      "Epoch: 240, Loss= 0.6926, Training Accuracy= 0.505\n",
      "Epoch: 250, Loss= 0.6926, Training Accuracy= 0.509\n",
      "Epoch: 260, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 270, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 280, Loss= 0.6924, Training Accuracy= 0.512\n",
      "Epoch: 290, Loss= 0.6924, Training Accuracy= 0.512\n",
      "Epoch: 300, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 310, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 320, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 330, Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 340, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 350, Loss= 0.6922, Training Accuracy= 0.515\n",
      "Epoch: 360, Loss= 0.6921, Training Accuracy= 0.514\n",
      "Epoch: 370, Loss= 0.6921, Training Accuracy= 0.515\n",
      "Epoch: 380, Loss= 0.6920, Training Accuracy= 0.516\n",
      "Epoch: 390, Loss= 0.6920, Training Accuracy= 0.519\n",
      "Epoch: 400, Loss= 0.6920, Training Accuracy= 0.517\n",
      "Epoch: 410, Loss= 0.6919, Training Accuracy= 0.516\n",
      "Epoch: 420, Loss= 0.6919, Training Accuracy= 0.515\n",
      "Epoch: 430, Loss= 0.6919, Training Accuracy= 0.514\n",
      "Epoch: 440, Loss= 0.6918, Training Accuracy= 0.518\n",
      "Epoch: 450, Loss= 0.6918, Training Accuracy= 0.520\n",
      "Epoch: 460, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 470, Loss= 0.6917, Training Accuracy= 0.525\n",
      "Epoch: 480, Loss= 0.6917, Training Accuracy= 0.524\n",
      "Epoch: 490, Loss= 0.6916, Training Accuracy= 0.525\n",
      "Epoch: 500, Loss= 0.6916, Training Accuracy= 0.520\n",
      "Epoch: 510, Loss= 0.6915, Training Accuracy= 0.529\n",
      "Epoch: 520, Loss= 0.6915, Training Accuracy= 0.528\n",
      "Epoch: 530, Loss= 0.6915, Training Accuracy= 0.519\n",
      "Epoch: 540, Loss= 0.6914, Training Accuracy= 0.517\n",
      "Epoch: 550, Loss= 0.6914, Training Accuracy= 0.515\n",
      "Epoch: 560, Loss= 0.6914, Training Accuracy= 0.516\n",
      "Epoch: 570, Loss= 0.6914, Training Accuracy= 0.518\n",
      "Epoch: 580, Loss= 0.6914, Training Accuracy= 0.516\n",
      "Epoch: 590, Loss= 0.6913, Training Accuracy= 0.516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6913, Training Accuracy= 0.516\n",
      "Epoch: 610, Loss= 0.6913, Training Accuracy= 0.515\n",
      "Epoch: 620, Loss= 0.6913, Training Accuracy= 0.516\n",
      "Epoch: 630, Loss= 0.6912, Training Accuracy= 0.516\n",
      "Epoch: 640, Loss= 0.6912, Training Accuracy= 0.518\n",
      "Epoch: 650, Loss= 0.6912, Training Accuracy= 0.518\n",
      "Epoch: 660, Loss= 0.6911, Training Accuracy= 0.522\n",
      "Epoch: 670, Loss= 0.6911, Training Accuracy= 0.522\n",
      "Epoch: 680, Loss= 0.6911, Training Accuracy= 0.521\n",
      "Epoch: 690, Loss= 0.6910, Training Accuracy= 0.525\n",
      "Epoch: 700, Loss= 0.6910, Training Accuracy= 0.526\n",
      "Epoch: 710, Loss= 0.6910, Training Accuracy= 0.528\n",
      "Epoch: 720, Loss= 0.6909, Training Accuracy= 0.526\n",
      "Epoch: 730, Loss= 0.6909, Training Accuracy= 0.529\n",
      "Epoch: 740, Loss= 0.6908, Training Accuracy= 0.527\n",
      "Epoch: 750, Loss= 0.6908, Training Accuracy= 0.528\n",
      "Epoch: 760, Loss= 0.6908, Training Accuracy= 0.532\n",
      "Epoch: 770, Loss= 0.6907, Training Accuracy= 0.531\n",
      "Epoch: 780, Loss= 0.6907, Training Accuracy= 0.529\n",
      "Epoch: 790, Loss= 0.6906, Training Accuracy= 0.528\n",
      "Epoch: 800, Loss= 0.6906, Training Accuracy= 0.525\n",
      "Epoch: 810, Loss= 0.6905, Training Accuracy= 0.523\n",
      "Epoch: 820, Loss= 0.6905, Training Accuracy= 0.525\n",
      "Epoch: 830, Loss= 0.6904, Training Accuracy= 0.525\n",
      "Epoch: 840, Loss= 0.6904, Training Accuracy= 0.528\n",
      "Epoch: 850, Loss= 0.6904, Training Accuracy= 0.530\n",
      "Epoch: 860, Loss= 0.6903, Training Accuracy= 0.528\n",
      "Epoch: 870, Loss= 0.6903, Training Accuracy= 0.530\n",
      "Epoch: 880, Loss= 0.6902, Training Accuracy= 0.533\n",
      "Epoch: 890, Loss= 0.6902, Training Accuracy= 0.534\n",
      "Epoch: 900, Loss= 0.6901, Training Accuracy= 0.535\n",
      "Epoch: 910, Loss= 0.6901, Training Accuracy= 0.539\n",
      "Epoch: 920, Loss= 0.6901, Training Accuracy= 0.535\n",
      "Epoch: 930, Loss= 0.6901, Training Accuracy= 0.534\n",
      "Epoch: 940, Loss= 0.6900, Training Accuracy= 0.534\n",
      "Epoch: 950, Loss= 0.6900, Training Accuracy= 0.531\n",
      "Epoch: 960, Loss= 0.6900, Training Accuracy= 0.529\n",
      "Epoch: 970, Loss= 0.6900, Training Accuracy= 0.524\n",
      "Epoch: 980, Loss= 0.6900, Training Accuracy= 0.524\n",
      "Epoch: 990, Loss= 0.6900, Training Accuracy= 0.527\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4908\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.7015, Training Accuracy= 0.519\n",
      "Epoch: 10, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 60, Loss= 0.6931, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6930, Training Accuracy= 0.499\n",
      "Epoch: 80, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 100, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 110, Loss= 0.6926, Training Accuracy= 0.509\n",
      "Epoch: 120, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 130, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 140, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 150, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 160, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 170, Loss= 0.6922, Training Accuracy= 0.513\n",
      "Epoch: 180, Loss= 0.6921, Training Accuracy= 0.508\n",
      "Epoch: 190, Loss= 0.6920, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6920, Training Accuracy= 0.513\n",
      "Epoch: 210, Loss= 0.6919, Training Accuracy= 0.511\n",
      "Epoch: 220, Loss= 0.6918, Training Accuracy= 0.515\n",
      "Epoch: 230, Loss= 0.6918, Training Accuracy= 0.512\n",
      "Epoch: 240, Loss= 0.6917, Training Accuracy= 0.511\n",
      "Epoch: 250, Loss= 0.6916, Training Accuracy= 0.517\n",
      "Epoch: 260, Loss= 0.6916, Training Accuracy= 0.517\n",
      "Epoch: 270, Loss= 0.6915, Training Accuracy= 0.520\n",
      "Epoch: 280, Loss= 0.6914, Training Accuracy= 0.519\n",
      "Epoch: 290, Loss= 0.6914, Training Accuracy= 0.518\n",
      "Epoch: 300, Loss= 0.6913, Training Accuracy= 0.517\n",
      "Epoch: 310, Loss= 0.6912, Training Accuracy= 0.522\n",
      "Epoch: 320, Loss= 0.6911, Training Accuracy= 0.526\n",
      "Epoch: 330, Loss= 0.6910, Training Accuracy= 0.523\n",
      "Epoch: 340, Loss= 0.6909, Training Accuracy= 0.522\n",
      "Epoch: 350, Loss= 0.6908, Training Accuracy= 0.522\n",
      "Epoch: 360, Loss= 0.6907, Training Accuracy= 0.521\n",
      "Epoch: 370, Loss= 0.6906, Training Accuracy= 0.518\n",
      "Epoch: 380, Loss= 0.6905, Training Accuracy= 0.523\n",
      "Epoch: 390, Loss= 0.6904, Training Accuracy= 0.524\n",
      "Epoch: 400, Loss= 0.6902, Training Accuracy= 0.535\n",
      "Epoch: 410, Loss= 0.6901, Training Accuracy= 0.530\n",
      "Epoch: 420, Loss= 0.6900, Training Accuracy= 0.528\n",
      "Epoch: 430, Loss= 0.6898, Training Accuracy= 0.532\n",
      "Epoch: 440, Loss= 0.6897, Training Accuracy= 0.531\n",
      "Epoch: 450, Loss= 0.6895, Training Accuracy= 0.535\n",
      "Epoch: 460, Loss= 0.6893, Training Accuracy= 0.532\n",
      "Epoch: 470, Loss= 0.6891, Training Accuracy= 0.536\n",
      "Epoch: 480, Loss= 0.6888, Training Accuracy= 0.540\n",
      "Epoch: 490, Loss= 0.6883, Training Accuracy= 0.540\n",
      "Epoch: 500, Loss= 0.6875, Training Accuracy= 0.542\n",
      "Epoch: 510, Loss= 0.6858, Training Accuracy= 0.544\n",
      "Epoch: 520, Loss= 0.6818, Training Accuracy= 0.592\n",
      "Epoch: 530, Loss= 0.6696, Training Accuracy= 0.615\n",
      "Epoch: 540, Loss= 0.6668, Training Accuracy= 0.604\n",
      "Epoch: 550, Loss= 0.5608, Training Accuracy= 0.722\n",
      "Epoch: 560, Loss= 0.4786, Training Accuracy= 0.751\n",
      "Epoch: 570, Loss= 0.4177, Training Accuracy= 0.763\n",
      "Epoch: 580, Loss= 0.3709, Training Accuracy= 0.814\n",
      "Epoch: 590, Loss= 0.3328, Training Accuracy= 0.864\n",
      "Epoch: 600, Loss= 0.3084, Training Accuracy= 0.877\n",
      "Epoch: 610, Loss= 0.2919, Training Accuracy= 0.882\n",
      "Epoch: 620, Loss= 0.4102, Training Accuracy= 0.799\n",
      "Epoch: 630, Loss= 0.3463, Training Accuracy= 0.822\n",
      "Epoch: 640, Loss= 0.2965, Training Accuracy= 0.837\n",
      "Epoch: 650, Loss= 0.2546, Training Accuracy= 0.841\n",
      "Epoch: 660, Loss= 0.2319, Training Accuracy= 0.850\n",
      "Epoch: 670, Loss= 0.2172, Training Accuracy= 0.877\n",
      "Epoch: 680, Loss= 0.2049, Training Accuracy= 0.880\n",
      "Epoch: 690, Loss= 0.1947, Training Accuracy= 0.886\n",
      "Epoch: 700, Loss= 0.1862, Training Accuracy= 0.889\n",
      "Epoch: 710, Loss= 0.1791, Training Accuracy= 0.896\n",
      "Epoch: 720, Loss= 0.1715, Training Accuracy= 0.889\n",
      "Epoch: 730, Loss= 0.1631, Training Accuracy= 0.884\n",
      "Epoch: 740, Loss= 0.1553, Training Accuracy= 0.893\n",
      "Epoch: 750, Loss= 0.1457, Training Accuracy= 0.896\n",
      "Epoch: 760, Loss= 0.1363, Training Accuracy= 0.912\n",
      "Epoch: 770, Loss= 0.1106, Training Accuracy= 0.939\n",
      "Epoch: 780, Loss= 0.0907, Training Accuracy= 0.963\n",
      "Epoch: 790, Loss= 0.0751, Training Accuracy= 0.977\n",
      "Epoch: 800, Loss= 0.0636, Training Accuracy= 0.980\n",
      "Epoch: 810, Loss= 0.0561, Training Accuracy= 0.982\n",
      "Epoch: 820, Loss= 0.0494, Training Accuracy= 0.985\n",
      "Epoch: 830, Loss= 0.0432, Training Accuracy= 0.987\n",
      "Epoch: 840, Loss= 0.0269, Training Accuracy= 0.996\n",
      "Epoch: 850, Loss= 0.0191, Training Accuracy= 0.995\n",
      "Epoch: 860, Loss= 0.0144, Training Accuracy= 0.999\n",
      "Epoch: 870, Loss= 0.0116, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0098, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0085, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0076, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0068, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0062, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0057, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0053, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0049, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0046, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0043, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0041, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0038, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.7062, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6962, Training Accuracy= 0.496\n",
      "Epoch: 20, Loss= 0.6952, Training Accuracy= 0.495\n",
      "Epoch: 30, Loss= 0.6949, Training Accuracy= 0.496\n",
      "Epoch: 40, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6945, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 80, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 90, Loss= 0.6941, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 120, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6936, Training Accuracy= 0.497\n",
      "Epoch: 170, Loss= 0.6935, Training Accuracy= 0.495\n",
      "Epoch: 180, Loss= 0.6935, Training Accuracy= 0.499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.6934, Training Accuracy= 0.495\n",
      "Epoch: 200, Loss= 0.6933, Training Accuracy= 0.499\n",
      "Epoch: 210, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 220, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 240, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 250, Loss= 0.6924, Training Accuracy= 0.510\n",
      "Epoch: 260, Loss= 0.6918, Training Accuracy= 0.517\n",
      "Epoch: 270, Loss= 0.6911, Training Accuracy= 0.527\n",
      "Epoch: 280, Loss= 0.6901, Training Accuracy= 0.539\n",
      "Epoch: 290, Loss= 0.6888, Training Accuracy= 0.549\n",
      "Epoch: 300, Loss= 0.6878, Training Accuracy= 0.547\n",
      "Epoch: 310, Loss= 0.6575, Training Accuracy= 0.622\n",
      "Epoch: 320, Loss= 0.6916, Training Accuracy= 0.482\n",
      "Epoch: 330, Loss= 0.6436, Training Accuracy= 0.610\n",
      "Epoch: 340, Loss= 0.4542, Training Accuracy= 0.822\n",
      "Epoch: 350, Loss= 0.4798, Training Accuracy= 0.760\n",
      "Epoch: 360, Loss= 0.1655, Training Accuracy= 0.983\n",
      "Epoch: 370, Loss= 0.1202, Training Accuracy= 0.982\n",
      "Epoch: 380, Loss= 0.0883, Training Accuracy= 0.985\n",
      "Epoch: 390, Loss= 0.0686, Training Accuracy= 0.987\n",
      "Epoch: 400, Loss= 0.0546, Training Accuracy= 0.990\n",
      "Epoch: 410, Loss= 0.0450, Training Accuracy= 0.992\n",
      "Epoch: 420, Loss= 0.0380, Training Accuracy= 0.993\n",
      "Epoch: 430, Loss= 0.0333, Training Accuracy= 0.995\n",
      "Epoch: 440, Loss= 0.0537, Training Accuracy= 0.991\n",
      "Epoch: 450, Loss= 0.0287, Training Accuracy= 0.998\n",
      "Epoch: 460, Loss= 0.0308, Training Accuracy= 0.998\n",
      "Epoch: 470, Loss= 0.0450, Training Accuracy= 0.992\n",
      "Epoch: 480, Loss= 0.0238, Training Accuracy= 0.998\n",
      "Epoch: 490, Loss= 0.0226, Training Accuracy= 0.998\n",
      "Epoch: 500, Loss= 0.0223, Training Accuracy= 0.997\n",
      "Epoch: 510, Loss= 0.0221, Training Accuracy= 0.998\n",
      "Epoch: 520, Loss= 0.0145, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0172, Training Accuracy= 0.999\n",
      "Epoch: 540, Loss= 0.0239, Training Accuracy= 0.996\n",
      "Epoch: 550, Loss= 0.0127, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0139, Training Accuracy= 0.999\n",
      "Epoch: 570, Loss= 0.0092, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0081, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0074, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0068, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0063, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0059, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0055, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0052, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0049, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0046, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0044, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0042, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0040, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0038, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0037, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0035, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0034, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0033, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0032, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0031, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0030, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0029, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0028, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0027, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0026, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0025, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0025, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0024, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0023, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0023, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0022, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0022, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0021, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0021, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0020, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0020, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0019, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0019, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0019, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0018, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0018, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0018, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0017, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.7082, Training Accuracy= 0.504\n",
      "Epoch: 10, Loss= 0.7038, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.7014, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.7000, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6990, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6984, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6978, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6974, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6971, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6968, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6965, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6963, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6961, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6960, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6958, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.6957, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6955, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6954, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6953, Training Accuracy= 0.503\n",
      "Epoch: 190, Loss= 0.6952, Training Accuracy= 0.504\n",
      "Epoch: 200, Loss= 0.6951, Training Accuracy= 0.506\n",
      "Epoch: 210, Loss= 0.6950, Training Accuracy= 0.508\n",
      "Epoch: 220, Loss= 0.6949, Training Accuracy= 0.506\n",
      "Epoch: 230, Loss= 0.6948, Training Accuracy= 0.508\n",
      "Epoch: 240, Loss= 0.6947, Training Accuracy= 0.509\n",
      "Epoch: 250, Loss= 0.6946, Training Accuracy= 0.507\n",
      "Epoch: 260, Loss= 0.6945, Training Accuracy= 0.509\n",
      "Epoch: 270, Loss= 0.6944, Training Accuracy= 0.510\n",
      "Epoch: 280, Loss= 0.6944, Training Accuracy= 0.512\n",
      "Epoch: 290, Loss= 0.6943, Training Accuracy= 0.511\n",
      "Epoch: 300, Loss= 0.6942, Training Accuracy= 0.508\n",
      "Epoch: 310, Loss= 0.6941, Training Accuracy= 0.508\n",
      "Epoch: 320, Loss= 0.6940, Training Accuracy= 0.506\n",
      "Epoch: 330, Loss= 0.6939, Training Accuracy= 0.499\n",
      "Epoch: 340, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 350, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 360, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 370, Loss= 0.6934, Training Accuracy= 0.521\n",
      "Epoch: 380, Loss= 0.6932, Training Accuracy= 0.519\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 400, Loss= 0.6928, Training Accuracy= 0.518\n",
      "Epoch: 410, Loss= 0.6927, Training Accuracy= 0.520\n",
      "Epoch: 420, Loss= 0.6926, Training Accuracy= 0.523\n",
      "Epoch: 430, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 440, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 450, Loss= 0.6922, Training Accuracy= 0.521\n",
      "Epoch: 460, Loss= 0.6921, Training Accuracy= 0.525\n",
      "Epoch: 470, Loss= 0.6920, Training Accuracy= 0.528\n",
      "Epoch: 480, Loss= 0.6918, Training Accuracy= 0.530\n",
      "Epoch: 490, Loss= 0.6917, Training Accuracy= 0.530\n",
      "Epoch: 500, Loss= 0.6915, Training Accuracy= 0.532\n",
      "Epoch: 510, Loss= 0.6913, Training Accuracy= 0.534\n",
      "Epoch: 520, Loss= 0.6911, Training Accuracy= 0.534\n",
      "Epoch: 530, Loss= 0.6909, Training Accuracy= 0.528\n",
      "Epoch: 540, Loss= 0.6908, Training Accuracy= 0.529\n",
      "Epoch: 550, Loss= 0.6906, Training Accuracy= 0.522\n",
      "Epoch: 560, Loss= 0.6904, Training Accuracy= 0.523\n",
      "Epoch: 570, Loss= 0.6903, Training Accuracy= 0.525\n",
      "Epoch: 580, Loss= 0.6901, Training Accuracy= 0.525\n",
      "Epoch: 590, Loss= 0.6900, Training Accuracy= 0.528\n",
      "Epoch: 600, Loss= 0.6898, Training Accuracy= 0.535\n",
      "Epoch: 610, Loss= 0.6897, Training Accuracy= 0.538\n",
      "Epoch: 620, Loss= 0.6895, Training Accuracy= 0.540\n",
      "Epoch: 630, Loss= 0.6894, Training Accuracy= 0.543\n",
      "Epoch: 640, Loss= 0.6892, Training Accuracy= 0.544\n",
      "Epoch: 650, Loss= 0.6890, Training Accuracy= 0.548\n",
      "Epoch: 660, Loss= 0.6889, Training Accuracy= 0.544\n",
      "Epoch: 670, Loss= 0.6887, Training Accuracy= 0.552\n",
      "Epoch: 680, Loss= 0.6886, Training Accuracy= 0.553\n",
      "Epoch: 690, Loss= 0.6884, Training Accuracy= 0.553\n",
      "Epoch: 700, Loss= 0.6883, Training Accuracy= 0.556\n",
      "Epoch: 710, Loss= 0.6881, Training Accuracy= 0.550\n",
      "Epoch: 720, Loss= 0.6880, Training Accuracy= 0.548\n",
      "Epoch: 730, Loss= 0.6878, Training Accuracy= 0.547\n",
      "Epoch: 740, Loss= 0.6877, Training Accuracy= 0.545\n",
      "Epoch: 750, Loss= 0.6875, Training Accuracy= 0.543\n",
      "Epoch: 760, Loss= 0.6873, Training Accuracy= 0.546\n",
      "Epoch: 770, Loss= 0.6871, Training Accuracy= 0.546\n",
      "Epoch: 780, Loss= 0.6869, Training Accuracy= 0.546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.6867, Training Accuracy= 0.553\n",
      "Epoch: 800, Loss= 0.6863, Training Accuracy= 0.549\n",
      "Epoch: 810, Loss= 0.6859, Training Accuracy= 0.555\n",
      "Epoch: 820, Loss= 0.6851, Training Accuracy= 0.561\n",
      "Epoch: 830, Loss= 0.6833, Training Accuracy= 0.563\n",
      "Epoch: 840, Loss= 0.6788, Training Accuracy= 0.570\n",
      "Epoch: 850, Loss= 0.6582, Training Accuracy= 0.633\n",
      "Epoch: 860, Loss= 0.5154, Training Accuracy= 0.790\n",
      "Epoch: 870, Loss= 0.3597, Training Accuracy= 0.857\n",
      "Epoch: 880, Loss= 0.4220, Training Accuracy= 0.848\n",
      "Epoch: 890, Loss= 0.1773, Training Accuracy= 0.972\n",
      "Epoch: 900, Loss= 0.0548, Training Accuracy= 0.999\n",
      "Epoch: 910, Loss= 0.0294, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0207, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0159, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0129, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0108, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0081, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0073, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0065, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.7018, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6985, Training Accuracy= 0.506\n",
      "Epoch: 20, Loss= 0.6973, Training Accuracy= 0.507\n",
      "Epoch: 30, Loss= 0.6965, Training Accuracy= 0.506\n",
      "Epoch: 40, Loss= 0.6959, Training Accuracy= 0.509\n",
      "Epoch: 50, Loss= 0.6955, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6948, Training Accuracy= 0.507\n",
      "Epoch: 80, Loss= 0.6946, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.6944, Training Accuracy= 0.506\n",
      "Epoch: 100, Loss= 0.6943, Training Accuracy= 0.504\n",
      "Epoch: 110, Loss= 0.6942, Training Accuracy= 0.511\n",
      "Epoch: 120, Loss= 0.6941, Training Accuracy= 0.512\n",
      "Epoch: 130, Loss= 0.6940, Training Accuracy= 0.513\n",
      "Epoch: 140, Loss= 0.6939, Training Accuracy= 0.511\n",
      "Epoch: 150, Loss= 0.6938, Training Accuracy= 0.510\n",
      "Epoch: 160, Loss= 0.6938, Training Accuracy= 0.512\n",
      "Epoch: 170, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 180, Loss= 0.6938, Training Accuracy= 0.511\n",
      "Epoch: 190, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 200, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 210, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 220, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 230, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 240, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 250, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 260, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 270, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 280, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 290, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 300, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 310, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 320, Loss= 0.6936, Training Accuracy= 0.510\n",
      "Epoch: 330, Loss= 0.6936, Training Accuracy= 0.513\n",
      "Epoch: 340, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 350, Loss= 0.6936, Training Accuracy= 0.514\n",
      "Epoch: 360, Loss= 0.6935, Training Accuracy= 0.514\n",
      "Epoch: 370, Loss= 0.6935, Training Accuracy= 0.513\n",
      "Epoch: 380, Loss= 0.6935, Training Accuracy= 0.517\n",
      "Epoch: 390, Loss= 0.6935, Training Accuracy= 0.516\n",
      "Epoch: 400, Loss= 0.6935, Training Accuracy= 0.515\n",
      "Epoch: 410, Loss= 0.6935, Training Accuracy= 0.515\n",
      "Epoch: 420, Loss= 0.6934, Training Accuracy= 0.516\n",
      "Epoch: 430, Loss= 0.6934, Training Accuracy= 0.514\n",
      "Epoch: 440, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 450, Loss= 0.6934, Training Accuracy= 0.515\n",
      "Epoch: 460, Loss= 0.6933, Training Accuracy= 0.515\n",
      "Epoch: 470, Loss= 0.6933, Training Accuracy= 0.519\n",
      "Epoch: 480, Loss= 0.6933, Training Accuracy= 0.518\n",
      "Epoch: 490, Loss= 0.6933, Training Accuracy= 0.519\n",
      "Epoch: 500, Loss= 0.6933, Training Accuracy= 0.514\n",
      "Epoch: 510, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 520, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 530, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 540, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 550, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 560, Loss= 0.6931, Training Accuracy= 0.514\n",
      "Epoch: 570, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 580, Loss= 0.6931, Training Accuracy= 0.514\n",
      "Epoch: 590, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 600, Loss= 0.6931, Training Accuracy= 0.513\n",
      "Epoch: 610, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 620, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 630, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.519\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.520\n",
      "Epoch: 660, Loss= 0.6929, Training Accuracy= 0.518\n",
      "Epoch: 670, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 680, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 690, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 700, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 710, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 720, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 730, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 740, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 750, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 760, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 770, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 780, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 790, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 800, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 810, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 820, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 830, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 840, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 850, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 860, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 870, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 880, Loss= 0.6925, Training Accuracy= 0.513\n",
      "Epoch: 890, Loss= 0.6924, Training Accuracy= 0.511\n",
      "Epoch: 900, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 910, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 920, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 930, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 940, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 950, Loss= 0.6922, Training Accuracy= 0.523\n",
      "Epoch: 960, Loss= 0.6922, Training Accuracy= 0.525\n",
      "Epoch: 970, Loss= 0.6921, Training Accuracy= 0.528\n",
      "Epoch: 980, Loss= 0.6921, Training Accuracy= 0.532\n",
      "Epoch: 990, Loss= 0.6920, Training Accuracy= 0.531\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5128\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.7010, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 20, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 30, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 40, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 50, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 70, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 80, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 90, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 100, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 110, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 120, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 130, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 140, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 150, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 160, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 180, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 230, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 240, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 250, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 260, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 270, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 280, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 290, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 300, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 310, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 320, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 330, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 340, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 350, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 360, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 370, Loss= 0.6933, Training Accuracy= 0.506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 390, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 400, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 410, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 420, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 430, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 440, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 450, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 460, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 470, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 480, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 490, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 500, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 520, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 530, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 540, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 550, Loss= 0.6933, Training Accuracy= 0.514\n",
      "Epoch: 560, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 570, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 580, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 590, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 600, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 610, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 620, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 630, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 640, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 650, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 660, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 670, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 680, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 690, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 700, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 710, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 720, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 730, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 740, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 750, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 760, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 770, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 780, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 790, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 800, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 810, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 820, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 830, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 840, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 850, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 860, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 870, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 880, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 890, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 900, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 910, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 920, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 930, Loss= 0.6931, Training Accuracy= 0.513\n",
      "Epoch: 940, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 950, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 960, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 970, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 980, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 990, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.498\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.8231, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.7102, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.7032, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.7001, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.6983, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.6972, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6963, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6957, Training Accuracy= 0.504\n",
      "Epoch: 80, Loss= 0.6952, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6948, Training Accuracy= 0.506\n",
      "Epoch: 100, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 110, Loss= 0.6941, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 130, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 160, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 180, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 190, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 200, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 210, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 220, Loss= 0.6924, Training Accuracy= 0.520\n",
      "Epoch: 230, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 240, Loss= 0.6922, Training Accuracy= 0.525\n",
      "Epoch: 250, Loss= 0.6921, Training Accuracy= 0.525\n",
      "Epoch: 260, Loss= 0.6921, Training Accuracy= 0.524\n",
      "Epoch: 270, Loss= 0.6920, Training Accuracy= 0.527\n",
      "Epoch: 280, Loss= 0.6920, Training Accuracy= 0.529\n",
      "Epoch: 290, Loss= 0.6919, Training Accuracy= 0.526\n",
      "Epoch: 300, Loss= 0.6919, Training Accuracy= 0.526\n",
      "Epoch: 310, Loss= 0.6918, Training Accuracy= 0.525\n",
      "Epoch: 320, Loss= 0.6918, Training Accuracy= 0.523\n",
      "Epoch: 330, Loss= 0.6917, Training Accuracy= 0.519\n",
      "Epoch: 340, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 350, Loss= 0.6916, Training Accuracy= 0.517\n",
      "Epoch: 360, Loss= 0.6916, Training Accuracy= 0.516\n",
      "Epoch: 370, Loss= 0.6915, Training Accuracy= 0.519\n",
      "Epoch: 380, Loss= 0.6915, Training Accuracy= 0.520\n",
      "Epoch: 390, Loss= 0.6915, Training Accuracy= 0.516\n",
      "Epoch: 400, Loss= 0.6914, Training Accuracy= 0.514\n",
      "Epoch: 410, Loss= 0.6914, Training Accuracy= 0.517\n",
      "Epoch: 420, Loss= 0.6913, Training Accuracy= 0.516\n",
      "Epoch: 430, Loss= 0.6913, Training Accuracy= 0.514\n",
      "Epoch: 440, Loss= 0.6912, Training Accuracy= 0.512\n",
      "Epoch: 450, Loss= 0.6911, Training Accuracy= 0.512\n",
      "Epoch: 460, Loss= 0.6911, Training Accuracy= 0.519\n",
      "Epoch: 470, Loss= 0.6910, Training Accuracy= 0.522\n",
      "Epoch: 480, Loss= 0.6910, Training Accuracy= 0.529\n",
      "Epoch: 490, Loss= 0.6909, Training Accuracy= 0.532\n",
      "Epoch: 500, Loss= 0.6908, Training Accuracy= 0.534\n",
      "Epoch: 510, Loss= 0.6907, Training Accuracy= 0.541\n",
      "Epoch: 520, Loss= 0.6905, Training Accuracy= 0.549\n",
      "Epoch: 530, Loss= 0.6903, Training Accuracy= 0.545\n",
      "Epoch: 540, Loss= 0.6900, Training Accuracy= 0.538\n",
      "Epoch: 550, Loss= 0.6890, Training Accuracy= 0.544\n",
      "Epoch: 560, Loss= 0.6859, Training Accuracy= 0.548\n",
      "Epoch: 570, Loss= 0.6925, Training Accuracy= 0.539\n",
      "Epoch: 580, Loss= 0.6999, Training Accuracy= 0.527\n",
      "Epoch: 590, Loss= 0.6681, Training Accuracy= 0.576\n",
      "Epoch: 600, Loss= 0.6175, Training Accuracy= 0.659\n",
      "Epoch: 610, Loss= 0.7351, Training Accuracy= 0.512\n",
      "Epoch: 620, Loss= 0.3033, Training Accuracy= 0.919\n",
      "Epoch: 630, Loss= 0.0654, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0340, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0220, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0160, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0100, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0084, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0072, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0063, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0055, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0049, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0045, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0041, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0037, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0034, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0032, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0030, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0028, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0026, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0025, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0023, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0022, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0021, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0020, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0019, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0018, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0017, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0017, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0016, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0015, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0015, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0014, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0014, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0013, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0013, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.0012, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0012, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6934, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 80, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.6931, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 120, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 160, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 220, Loss= 0.6929, Training Accuracy= 0.520\n",
      "Epoch: 230, Loss= 0.6929, Training Accuracy= 0.521\n",
      "Epoch: 240, Loss= 0.6929, Training Accuracy= 0.523\n",
      "Epoch: 250, Loss= 0.6929, Training Accuracy= 0.524\n",
      "Epoch: 260, Loss= 0.6929, Training Accuracy= 0.523\n",
      "Epoch: 270, Loss= 0.6929, Training Accuracy= 0.523\n",
      "Epoch: 280, Loss= 0.6929, Training Accuracy= 0.517\n",
      "Epoch: 290, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 300, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 310, Loss= 0.6929, Training Accuracy= 0.523\n",
      "Epoch: 320, Loss= 0.6928, Training Accuracy= 0.523\n",
      "Epoch: 330, Loss= 0.6928, Training Accuracy= 0.518\n",
      "Epoch: 340, Loss= 0.6928, Training Accuracy= 0.518\n",
      "Epoch: 350, Loss= 0.6928, Training Accuracy= 0.518\n",
      "Epoch: 360, Loss= 0.6928, Training Accuracy= 0.517\n",
      "Epoch: 370, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 380, Loss= 0.6928, Training Accuracy= 0.517\n",
      "Epoch: 390, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 400, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 410, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 420, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 430, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 440, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 450, Loss= 0.6926, Training Accuracy= 0.519\n",
      "Epoch: 460, Loss= 0.6926, Training Accuracy= 0.521\n",
      "Epoch: 470, Loss= 0.6926, Training Accuracy= 0.523\n",
      "Epoch: 480, Loss= 0.6926, Training Accuracy= 0.522\n",
      "Epoch: 490, Loss= 0.6926, Training Accuracy= 0.520\n",
      "Epoch: 500, Loss= 0.6926, Training Accuracy= 0.518\n",
      "Epoch: 510, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 520, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 530, Loss= 0.6925, Training Accuracy= 0.519\n",
      "Epoch: 540, Loss= 0.6925, Training Accuracy= 0.524\n",
      "Epoch: 550, Loss= 0.6925, Training Accuracy= 0.520\n",
      "Epoch: 560, Loss= 0.6925, Training Accuracy= 0.522\n",
      "Epoch: 570, Loss= 0.6925, Training Accuracy= 0.521\n",
      "Epoch: 580, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 590, Loss= 0.6924, Training Accuracy= 0.521\n",
      "Epoch: 600, Loss= 0.6924, Training Accuracy= 0.520\n",
      "Epoch: 610, Loss= 0.6924, Training Accuracy= 0.522\n",
      "Epoch: 620, Loss= 0.6924, Training Accuracy= 0.522\n",
      "Epoch: 630, Loss= 0.6924, Training Accuracy= 0.521\n",
      "Epoch: 640, Loss= 0.6924, Training Accuracy= 0.523\n",
      "Epoch: 650, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 660, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 670, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 680, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 690, Loss= 0.6923, Training Accuracy= 0.512\n",
      "Epoch: 700, Loss= 0.6923, Training Accuracy= 0.513\n",
      "Epoch: 710, Loss= 0.6922, Training Accuracy= 0.515\n",
      "Epoch: 720, Loss= 0.6922, Training Accuracy= 0.515\n",
      "Epoch: 730, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 740, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 750, Loss= 0.6922, Training Accuracy= 0.510\n",
      "Epoch: 760, Loss= 0.6922, Training Accuracy= 0.511\n",
      "Epoch: 770, Loss= 0.6921, Training Accuracy= 0.514\n",
      "Epoch: 780, Loss= 0.6921, Training Accuracy= 0.514\n",
      "Epoch: 790, Loss= 0.6921, Training Accuracy= 0.515\n",
      "Epoch: 800, Loss= 0.6921, Training Accuracy= 0.517\n",
      "Epoch: 810, Loss= 0.6921, Training Accuracy= 0.518\n",
      "Epoch: 820, Loss= 0.6921, Training Accuracy= 0.519\n",
      "Epoch: 830, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 840, Loss= 0.6920, Training Accuracy= 0.517\n",
      "Epoch: 850, Loss= 0.6920, Training Accuracy= 0.524\n",
      "Epoch: 860, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 870, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 880, Loss= 0.6919, Training Accuracy= 0.519\n",
      "Epoch: 890, Loss= 0.6919, Training Accuracy= 0.517\n",
      "Epoch: 900, Loss= 0.6919, Training Accuracy= 0.517\n",
      "Epoch: 910, Loss= 0.6919, Training Accuracy= 0.513\n",
      "Epoch: 920, Loss= 0.6919, Training Accuracy= 0.513\n",
      "Epoch: 930, Loss= 0.6918, Training Accuracy= 0.512\n",
      "Epoch: 940, Loss= 0.6918, Training Accuracy= 0.519\n",
      "Epoch: 950, Loss= 0.6918, Training Accuracy= 0.520\n",
      "Epoch: 960, Loss= 0.6917, Training Accuracy= 0.522\n",
      "Epoch: 970, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 980, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 990, Loss= 0.6916, Training Accuracy= 0.523\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4995\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7064, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.7017, Training Accuracy= 0.499\n",
      "Epoch: 20, Loss= 0.7007, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.6999, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.6993, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.6989, Training Accuracy= 0.499\n",
      "Epoch: 60, Loss= 0.6985, Training Accuracy= 0.499\n",
      "Epoch: 70, Loss= 0.6982, Training Accuracy= 0.499\n",
      "Epoch: 80, Loss= 0.6980, Training Accuracy= 0.499\n",
      "Epoch: 90, Loss= 0.6978, Training Accuracy= 0.499\n",
      "Epoch: 100, Loss= 0.6976, Training Accuracy= 0.499\n",
      "Epoch: 110, Loss= 0.6974, Training Accuracy= 0.499\n",
      "Epoch: 120, Loss= 0.6973, Training Accuracy= 0.499\n",
      "Epoch: 130, Loss= 0.6972, Training Accuracy= 0.499\n",
      "Epoch: 140, Loss= 0.6971, Training Accuracy= 0.499\n",
      "Epoch: 150, Loss= 0.6970, Training Accuracy= 0.499\n",
      "Epoch: 160, Loss= 0.6969, Training Accuracy= 0.499\n",
      "Epoch: 170, Loss= 0.6968, Training Accuracy= 0.499\n",
      "Epoch: 180, Loss= 0.6967, Training Accuracy= 0.499\n",
      "Epoch: 190, Loss= 0.6967, Training Accuracy= 0.499\n",
      "Epoch: 200, Loss= 0.6966, Training Accuracy= 0.499\n",
      "Epoch: 210, Loss= 0.6965, Training Accuracy= 0.499\n",
      "Epoch: 220, Loss= 0.6965, Training Accuracy= 0.499\n",
      "Epoch: 230, Loss= 0.6964, Training Accuracy= 0.499\n",
      "Epoch: 240, Loss= 0.6964, Training Accuracy= 0.499\n",
      "Epoch: 250, Loss= 0.6963, Training Accuracy= 0.499\n",
      "Epoch: 260, Loss= 0.6963, Training Accuracy= 0.499\n",
      "Epoch: 270, Loss= 0.6962, Training Accuracy= 0.499\n",
      "Epoch: 280, Loss= 0.6962, Training Accuracy= 0.499\n",
      "Epoch: 290, Loss= 0.6961, Training Accuracy= 0.499\n",
      "Epoch: 300, Loss= 0.6961, Training Accuracy= 0.499\n",
      "Epoch: 310, Loss= 0.6960, Training Accuracy= 0.499\n",
      "Epoch: 320, Loss= 0.6960, Training Accuracy= 0.499\n",
      "Epoch: 330, Loss= 0.6959, Training Accuracy= 0.499\n",
      "Epoch: 340, Loss= 0.6959, Training Accuracy= 0.499\n",
      "Epoch: 350, Loss= 0.6958, Training Accuracy= 0.499\n",
      "Epoch: 360, Loss= 0.6958, Training Accuracy= 0.499\n",
      "Epoch: 370, Loss= 0.6958, Training Accuracy= 0.499\n",
      "Epoch: 380, Loss= 0.6957, Training Accuracy= 0.499\n",
      "Epoch: 390, Loss= 0.6957, Training Accuracy= 0.499\n",
      "Epoch: 400, Loss= 0.6956, Training Accuracy= 0.499\n",
      "Epoch: 410, Loss= 0.6956, Training Accuracy= 0.499\n",
      "Epoch: 420, Loss= 0.6955, Training Accuracy= 0.499\n",
      "Epoch: 430, Loss= 0.6955, Training Accuracy= 0.499\n",
      "Epoch: 440, Loss= 0.6954, Training Accuracy= 0.499\n",
      "Epoch: 450, Loss= 0.6954, Training Accuracy= 0.499\n",
      "Epoch: 460, Loss= 0.6953, Training Accuracy= 0.499\n",
      "Epoch: 470, Loss= 0.6953, Training Accuracy= 0.499\n",
      "Epoch: 480, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 490, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 500, Loss= 0.6951, Training Accuracy= 0.499\n",
      "Epoch: 510, Loss= 0.6951, Training Accuracy= 0.499\n",
      "Epoch: 520, Loss= 0.6950, Training Accuracy= 0.499\n",
      "Epoch: 530, Loss= 0.6950, Training Accuracy= 0.499\n",
      "Epoch: 540, Loss= 0.6950, Training Accuracy= 0.499\n",
      "Epoch: 550, Loss= 0.6949, Training Accuracy= 0.499\n",
      "Epoch: 560, Loss= 0.6949, Training Accuracy= 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.6949, Training Accuracy= 0.499\n",
      "Epoch: 580, Loss= 0.6949, Training Accuracy= 0.499\n",
      "Epoch: 590, Loss= 0.6948, Training Accuracy= 0.498\n",
      "Epoch: 600, Loss= 0.6948, Training Accuracy= 0.498\n",
      "Epoch: 610, Loss= 0.6948, Training Accuracy= 0.499\n",
      "Epoch: 620, Loss= 0.6948, Training Accuracy= 0.501\n",
      "Epoch: 630, Loss= 0.6948, Training Accuracy= 0.499\n",
      "Epoch: 640, Loss= 0.6947, Training Accuracy= 0.496\n",
      "Epoch: 650, Loss= 0.6947, Training Accuracy= 0.498\n",
      "Epoch: 660, Loss= 0.6947, Training Accuracy= 0.498\n",
      "Epoch: 670, Loss= 0.6947, Training Accuracy= 0.499\n",
      "Epoch: 680, Loss= 0.6947, Training Accuracy= 0.498\n",
      "Epoch: 690, Loss= 0.6947, Training Accuracy= 0.498\n",
      "Epoch: 700, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 710, Loss= 0.6946, Training Accuracy= 0.501\n",
      "Epoch: 720, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 730, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 740, Loss= 0.6946, Training Accuracy= 0.499\n",
      "Epoch: 750, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 760, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 770, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 780, Loss= 0.6945, Training Accuracy= 0.499\n",
      "Epoch: 790, Loss= 0.6945, Training Accuracy= 0.499\n",
      "Epoch: 800, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 810, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 820, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 830, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 840, Loss= 0.6944, Training Accuracy= 0.499\n",
      "Epoch: 850, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 860, Loss= 0.6944, Training Accuracy= 0.499\n",
      "Epoch: 870, Loss= 0.6944, Training Accuracy= 0.499\n",
      "Epoch: 880, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 890, Loss= 0.6943, Training Accuracy= 0.502\n",
      "Epoch: 900, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 910, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 920, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 930, Loss= 0.6943, Training Accuracy= 0.506\n",
      "Epoch: 940, Loss= 0.6943, Training Accuracy= 0.508\n",
      "Epoch: 950, Loss= 0.6943, Training Accuracy= 0.508\n",
      "Epoch: 960, Loss= 0.6942, Training Accuracy= 0.508\n",
      "Epoch: 970, Loss= 0.6942, Training Accuracy= 0.508\n",
      "Epoch: 980, Loss= 0.6942, Training Accuracy= 0.505\n",
      "Epoch: 990, Loss= 0.6942, Training Accuracy= 0.504\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4978\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.03\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.51239997, 0.49079999, 1.0, 1.0, 1.0, 0.51279998, 0.498, 1.0, 0.49950001, 0.49779999]\n",
      "mean of test_accuracies_10replications:  0.70113\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.00244106248021\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGF/+H983OZV0Y4+JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13ab632090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
