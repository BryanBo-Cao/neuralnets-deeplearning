{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 25\n",
    "N = 25\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.8094, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.7193, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.7132, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.7146, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.7168, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.7172, Training Accuracy= 0.507\n",
      "Epoch: 60, Loss= 0.7161, Training Accuracy= 0.509\n",
      "Epoch: 70, Loss= 0.7153, Training Accuracy= 0.511\n",
      "Epoch: 80, Loss= 0.7137, Training Accuracy= 0.515\n",
      "Epoch: 90, Loss= 0.7113, Training Accuracy= 0.520\n",
      "Epoch: 100, Loss= 0.7089, Training Accuracy= 0.526\n",
      "Epoch: 110, Loss= 0.7047, Training Accuracy= 0.533\n",
      "Epoch: 120, Loss= 0.7015, Training Accuracy= 0.532\n",
      "Epoch: 130, Loss= 0.6995, Training Accuracy= 0.538\n",
      "Epoch: 140, Loss= 0.6954, Training Accuracy= 0.542\n",
      "Epoch: 150, Loss= 0.6924, Training Accuracy= 0.548\n",
      "Epoch: 160, Loss= 0.6955, Training Accuracy= 0.538\n",
      "Epoch: 170, Loss= 0.6929, Training Accuracy= 0.552\n",
      "Epoch: 180, Loss= 0.6935, Training Accuracy= 0.548\n",
      "Epoch: 190, Loss= 0.7067, Training Accuracy= 0.538\n",
      "Epoch: 200, Loss= 0.6992, Training Accuracy= 0.547\n",
      "Epoch: 210, Loss= 0.6965, Training Accuracy= 0.543\n",
      "Epoch: 220, Loss= 0.7018, Training Accuracy= 0.544\n",
      "Epoch: 230, Loss= 0.7062, Training Accuracy= 0.536\n",
      "Epoch: 240, Loss= 0.6902, Training Accuracy= 0.565\n",
      "Epoch: 250, Loss= 0.6819, Training Accuracy= 0.572\n",
      "Epoch: 260, Loss= 0.7020, Training Accuracy= 0.550\n",
      "Epoch: 270, Loss= 0.6799, Training Accuracy= 0.571\n",
      "Epoch: 280, Loss= 0.6873, Training Accuracy= 0.563\n",
      "Epoch: 290, Loss= 0.6969, Training Accuracy= 0.552\n",
      "Epoch: 300, Loss= 0.6843, Training Accuracy= 0.564\n",
      "Epoch: 310, Loss= 0.6899, Training Accuracy= 0.557\n",
      "Epoch: 320, Loss= 0.6798, Training Accuracy= 0.578\n",
      "Epoch: 330, Loss= 0.6877, Training Accuracy= 0.563\n",
      "Epoch: 340, Loss= 0.6762, Training Accuracy= 0.576\n",
      "Epoch: 350, Loss= 0.6745, Training Accuracy= 0.574\n",
      "Epoch: 360, Loss= 0.6820, Training Accuracy= 0.569\n",
      "Epoch: 370, Loss= 0.6744, Training Accuracy= 0.582\n",
      "Epoch: 380, Loss= 0.6814, Training Accuracy= 0.577\n",
      "Epoch: 390, Loss= 0.6783, Training Accuracy= 0.576\n",
      "Epoch: 400, Loss= 0.6672, Training Accuracy= 0.593\n",
      "Epoch: 410, Loss= 0.6863, Training Accuracy= 0.570\n",
      "Epoch: 420, Loss= 0.6826, Training Accuracy= 0.569\n",
      "Epoch: 430, Loss= 0.6924, Training Accuracy= 0.550\n",
      "Epoch: 440, Loss= 0.6755, Training Accuracy= 0.575\n",
      "Epoch: 450, Loss= 0.6690, Training Accuracy= 0.590\n",
      "Epoch: 460, Loss= 0.6673, Training Accuracy= 0.587\n",
      "Epoch: 470, Loss= 0.6722, Training Accuracy= 0.580\n",
      "Epoch: 480, Loss= 0.6656, Training Accuracy= 0.591\n",
      "Epoch: 490, Loss= 0.6633, Training Accuracy= 0.594\n",
      "Epoch: 500, Loss= 0.6663, Training Accuracy= 0.592\n",
      "Epoch: 510, Loss= 0.6875, Training Accuracy= 0.562\n",
      "Epoch: 520, Loss= 0.6722, Training Accuracy= 0.577\n",
      "Epoch: 530, Loss= 0.6788, Training Accuracy= 0.572\n",
      "Epoch: 540, Loss= 0.6801, Training Accuracy= 0.571\n",
      "Epoch: 550, Loss= 0.6636, Training Accuracy= 0.601\n",
      "Epoch: 560, Loss= 0.6713, Training Accuracy= 0.582\n",
      "Epoch: 570, Loss= 0.6807, Training Accuracy= 0.567\n",
      "Epoch: 580, Loss= 0.6692, Training Accuracy= 0.585\n",
      "Epoch: 590, Loss= 0.6766, Training Accuracy= 0.573\n",
      "Epoch: 600, Loss= 0.6686, Training Accuracy= 0.587\n",
      "Epoch: 610, Loss= 0.6694, Training Accuracy= 0.589\n",
      "Epoch: 620, Loss= 0.6692, Training Accuracy= 0.592\n",
      "Epoch: 630, Loss= 0.6632, Training Accuracy= 0.602\n",
      "Epoch: 640, Loss= 0.6766, Training Accuracy= 0.575\n",
      "Epoch: 650, Loss= 0.6610, Training Accuracy= 0.603\n",
      "Epoch: 660, Loss= 0.6680, Training Accuracy= 0.590\n",
      "Epoch: 670, Loss= 0.6675, Training Accuracy= 0.592\n",
      "Epoch: 680, Loss= 0.6653, Training Accuracy= 0.588\n",
      "Epoch: 690, Loss= 0.6761, Training Accuracy= 0.585\n",
      "Epoch: 700, Loss= 0.6744, Training Accuracy= 0.587\n",
      "Epoch: 710, Loss= 0.6676, Training Accuracy= 0.589\n",
      "Epoch: 720, Loss= 0.6711, Training Accuracy= 0.586\n",
      "Epoch: 730, Loss= 0.6717, Training Accuracy= 0.582\n",
      "Epoch: 740, Loss= 0.6562, Training Accuracy= 0.603\n",
      "Epoch: 750, Loss= 0.6677, Training Accuracy= 0.595\n",
      "Epoch: 760, Loss= 0.6763, Training Accuracy= 0.578\n",
      "Epoch: 770, Loss= 0.6858, Training Accuracy= 0.550\n",
      "Epoch: 780, Loss= 0.6629, Training Accuracy= 0.601\n",
      "Epoch: 790, Loss= 0.6675, Training Accuracy= 0.590\n",
      "Epoch: 800, Loss= 0.6572, Training Accuracy= 0.605\n",
      "Epoch: 810, Loss= 0.6705, Training Accuracy= 0.588\n",
      "Epoch: 820, Loss= 0.6620, Training Accuracy= 0.601\n",
      "Epoch: 830, Loss= 0.6730, Training Accuracy= 0.587\n",
      "Epoch: 840, Loss= 0.6776, Training Accuracy= 0.576\n",
      "Epoch: 850, Loss= 0.6859, Training Accuracy= 0.558\n",
      "Epoch: 860, Loss= 0.6768, Training Accuracy= 0.577\n",
      "Epoch: 870, Loss= 0.6679, Training Accuracy= 0.589\n",
      "Epoch: 880, Loss= 0.6681, Training Accuracy= 0.592\n",
      "Epoch: 890, Loss= 0.6599, Training Accuracy= 0.605\n",
      "Epoch: 900, Loss= 0.6661, Training Accuracy= 0.595\n",
      "Epoch: 910, Loss= 0.6831, Training Accuracy= 0.562\n",
      "Epoch: 920, Loss= 0.6704, Training Accuracy= 0.588\n",
      "Epoch: 930, Loss= 0.6868, Training Accuracy= 0.556\n",
      "Epoch: 940, Loss= 0.6666, Training Accuracy= 0.594\n",
      "Epoch: 950, Loss= 0.6646, Training Accuracy= 0.594\n",
      "Epoch: 960, Loss= 0.6864, Training Accuracy= 0.552\n",
      "Epoch: 970, Loss= 0.6844, Training Accuracy= 0.560\n",
      "Epoch: 980, Loss= 0.6816, Training Accuracy= 0.563\n",
      "Epoch: 990, Loss= 0.6802, Training Accuracy= 0.570\n",
      "Epoch: 1000, Loss= 0.6685, Training Accuracy= 0.589\n",
      "Epoch: 1010, Loss= 0.6705, Training Accuracy= 0.590\n",
      "Epoch: 1020, Loss= 0.6730, Training Accuracy= 0.584\n",
      "Epoch: 1030, Loss= 0.6790, Training Accuracy= 0.576\n",
      "Epoch: 1040, Loss= 0.6786, Training Accuracy= 0.575\n",
      "Epoch: 1050, Loss= 0.6791, Training Accuracy= 0.577\n",
      "Epoch: 1060, Loss= 0.6786, Training Accuracy= 0.567\n",
      "Epoch: 1070, Loss= 0.6742, Training Accuracy= 0.580\n",
      "Epoch: 1080, Loss= 0.6746, Training Accuracy= 0.586\n",
      "Epoch: 1090, Loss= 0.6637, Training Accuracy= 0.598\n",
      "Epoch: 1100, Loss= 0.6775, Training Accuracy= 0.579\n",
      "Epoch: 1110, Loss= 0.6763, Training Accuracy= 0.578\n",
      "Epoch: 1120, Loss= 0.6705, Training Accuracy= 0.589\n",
      "Epoch: 1130, Loss= 0.6585, Training Accuracy= 0.608\n",
      "Epoch: 1140, Loss= 0.6685, Training Accuracy= 0.590\n",
      "Epoch: 1150, Loss= 0.6776, Training Accuracy= 0.574\n",
      "Epoch: 1160, Loss= 0.6747, Training Accuracy= 0.584\n",
      "Epoch: 1170, Loss= 0.6683, Training Accuracy= 0.588\n",
      "Epoch: 1180, Loss= 0.6607, Training Accuracy= 0.605\n",
      "Epoch: 1190, Loss= 0.6672, Training Accuracy= 0.593\n",
      "Epoch: 1200, Loss= 0.6687, Training Accuracy= 0.592\n",
      "Epoch: 1210, Loss= 0.6705, Training Accuracy= 0.587\n",
      "Epoch: 1220, Loss= 0.6762, Training Accuracy= 0.583\n",
      "Epoch: 1230, Loss= 0.6773, Training Accuracy= 0.574\n",
      "Epoch: 1240, Loss= 0.6722, Training Accuracy= 0.580\n",
      "Epoch: 1250, Loss= 0.6780, Training Accuracy= 0.566\n",
      "Epoch: 1260, Loss= 0.6624, Training Accuracy= 0.599\n",
      "Epoch: 1270, Loss= 0.6725, Training Accuracy= 0.585\n",
      "Epoch: 1280, Loss= 0.6687, Training Accuracy= 0.589\n",
      "Epoch: 1290, Loss= 0.6680, Training Accuracy= 0.593\n",
      "Epoch: 1300, Loss= 0.6716, Training Accuracy= 0.587\n",
      "Epoch: 1310, Loss= 0.6741, Training Accuracy= 0.578\n",
      "Epoch: 1320, Loss= 0.6735, Training Accuracy= 0.581\n",
      "Epoch: 1330, Loss= 0.6584, Training Accuracy= 0.609\n",
      "Epoch: 1340, Loss= 0.6822, Training Accuracy= 0.568\n",
      "Epoch: 1350, Loss= 0.6635, Training Accuracy= 0.598\n",
      "Epoch: 1360, Loss= 0.6668, Training Accuracy= 0.596\n",
      "Epoch: 1370, Loss= 0.6738, Training Accuracy= 0.585\n",
      "Epoch: 1380, Loss= 0.6795, Training Accuracy= 0.567\n",
      "Epoch: 1390, Loss= 0.6691, Training Accuracy= 0.590\n",
      "Epoch: 1400, Loss= 0.6653, Training Accuracy= 0.592\n",
      "Epoch: 1410, Loss= 0.6737, Training Accuracy= 0.572\n",
      "Epoch: 1420, Loss= 0.6855, Training Accuracy= 0.552\n",
      "Epoch: 1430, Loss= 0.6860, Training Accuracy= 0.554\n",
      "Epoch: 1440, Loss= 0.6733, Training Accuracy= 0.577\n",
      "Epoch: 1450, Loss= 0.6770, Training Accuracy= 0.573\n",
      "Epoch: 1460, Loss= 0.6877, Training Accuracy= 0.554\n",
      "Epoch: 1470, Loss= 0.6649, Training Accuracy= 0.597\n",
      "Epoch: 1480, Loss= 0.6895, Training Accuracy= 0.555\n",
      "Epoch: 1490, Loss= 0.6640, Training Accuracy= 0.596\n",
      "Epoch: 1500, Loss= 0.6718, Training Accuracy= 0.582\n",
      "Epoch: 1510, Loss= 0.6843, Training Accuracy= 0.560\n",
      "Epoch: 1520, Loss= 0.6812, Training Accuracy= 0.567\n",
      "Epoch: 1530, Loss= 0.6715, Training Accuracy= 0.585\n",
      "Epoch: 1540, Loss= 0.6813, Training Accuracy= 0.567\n",
      "Epoch: 1550, Loss= 0.6771, Training Accuracy= 0.579\n",
      "Epoch: 1560, Loss= 0.7114, Training Accuracy= 0.520\n",
      "Epoch: 1570, Loss= 0.6852, Training Accuracy= 0.547\n",
      "Epoch: 1580, Loss= 0.6764, Training Accuracy= 0.568\n",
      "Epoch: 1590, Loss= 0.6750, Training Accuracy= 0.581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1600, Loss= 0.6730, Training Accuracy= 0.581\n",
      "Epoch: 1610, Loss= 0.6703, Training Accuracy= 0.589\n",
      "Epoch: 1620, Loss= 0.6764, Training Accuracy= 0.572\n",
      "Epoch: 1630, Loss= 0.6709, Training Accuracy= 0.580\n",
      "Epoch: 1640, Loss= 0.6656, Training Accuracy= 0.594\n",
      "Epoch: 1650, Loss= 0.6611, Training Accuracy= 0.604\n",
      "Epoch: 1660, Loss= 0.6723, Training Accuracy= 0.582\n",
      "Epoch: 1670, Loss= 0.6684, Training Accuracy= 0.591\n",
      "Epoch: 1680, Loss= 0.6875, Training Accuracy= 0.557\n",
      "Epoch: 1690, Loss= 0.6777, Training Accuracy= 0.572\n",
      "Epoch: 1700, Loss= 0.6851, Training Accuracy= 0.560\n",
      "Epoch: 1710, Loss= 0.6587, Training Accuracy= 0.603\n",
      "Epoch: 1720, Loss= 0.6821, Training Accuracy= 0.568\n",
      "Epoch: 1730, Loss= 0.6648, Training Accuracy= 0.597\n",
      "Epoch: 1740, Loss= 0.6629, Training Accuracy= 0.596\n",
      "Epoch: 1750, Loss= 0.6845, Training Accuracy= 0.570\n",
      "Epoch: 1760, Loss= 0.6680, Training Accuracy= 0.589\n",
      "Epoch: 1770, Loss= 0.6831, Training Accuracy= 0.563\n",
      "Epoch: 1780, Loss= 0.6677, Training Accuracy= 0.586\n",
      "Epoch: 1790, Loss= 0.6620, Training Accuracy= 0.601\n",
      "Epoch: 1800, Loss= 0.6627, Training Accuracy= 0.598\n",
      "Epoch: 1810, Loss= 0.6809, Training Accuracy= 0.571\n",
      "Epoch: 1820, Loss= 0.6792, Training Accuracy= 0.577\n",
      "Epoch: 1830, Loss= 0.6615, Training Accuracy= 0.604\n",
      "Epoch: 1840, Loss= 0.6779, Training Accuracy= 0.573\n",
      "Epoch: 1850, Loss= 0.6751, Training Accuracy= 0.581\n",
      "Epoch: 1860, Loss= 0.6657, Training Accuracy= 0.595\n",
      "Epoch: 1870, Loss= 0.6746, Training Accuracy= 0.579\n",
      "Epoch: 1880, Loss= 0.6561, Training Accuracy= 0.609\n",
      "Epoch: 1890, Loss= 0.6640, Training Accuracy= 0.598\n",
      "Epoch: 1900, Loss= 0.6654, Training Accuracy= 0.594\n",
      "Epoch: 1910, Loss= 0.6640, Training Accuracy= 0.595\n",
      "Epoch: 1920, Loss= 0.6710, Training Accuracy= 0.585\n",
      "Epoch: 1930, Loss= 0.7023, Training Accuracy= 0.504\n",
      "Epoch: 1940, Loss= 0.6993, Training Accuracy= 0.511\n",
      "Epoch: 1950, Loss= 0.6822, Training Accuracy= 0.560\n",
      "Epoch: 1960, Loss= 0.6761, Training Accuracy= 0.577\n",
      "Epoch: 1970, Loss= 0.6945, Training Accuracy= 0.540\n",
      "Epoch: 1980, Loss= 0.6842, Training Accuracy= 0.562\n",
      "Epoch: 1990, Loss= 0.6672, Training Accuracy= 0.590\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4878\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.9121, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 0.7994, Training Accuracy= 0.496\n",
      "Epoch: 20, Loss= 0.7659, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.7479, Training Accuracy= 0.496\n",
      "Epoch: 40, Loss= 0.7363, Training Accuracy= 0.496\n",
      "Epoch: 50, Loss= 0.7282, Training Accuracy= 0.496\n",
      "Epoch: 60, Loss= 0.7215, Training Accuracy= 0.496\n",
      "Epoch: 70, Loss= 0.7157, Training Accuracy= 0.499\n",
      "Epoch: 80, Loss= 0.7100, Training Accuracy= 0.501\n",
      "Epoch: 90, Loss= 0.7064, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.7041, Training Accuracy= 0.507\n",
      "Epoch: 110, Loss= 0.7024, Training Accuracy= 0.510\n",
      "Epoch: 120, Loss= 0.7011, Training Accuracy= 0.512\n",
      "Epoch: 130, Loss= 0.6992, Training Accuracy= 0.516\n",
      "Epoch: 140, Loss= 0.6958, Training Accuracy= 0.525\n",
      "Epoch: 150, Loss= 0.6929, Training Accuracy= 0.534\n",
      "Epoch: 160, Loss= 0.6929, Training Accuracy= 0.535\n",
      "Epoch: 170, Loss= 0.6958, Training Accuracy= 0.531\n",
      "Epoch: 180, Loss= 0.7135, Training Accuracy= 0.508\n",
      "Epoch: 190, Loss= 0.6994, Training Accuracy= 0.529\n",
      "Epoch: 200, Loss= 0.6974, Training Accuracy= 0.530\n",
      "Epoch: 210, Loss= 0.6913, Training Accuracy= 0.541\n",
      "Epoch: 220, Loss= 0.6948, Training Accuracy= 0.540\n",
      "Epoch: 230, Loss= 0.6862, Training Accuracy= 0.553\n",
      "Epoch: 240, Loss= 0.6880, Training Accuracy= 0.549\n",
      "Epoch: 250, Loss= 0.6913, Training Accuracy= 0.547\n",
      "Epoch: 260, Loss= 0.6797, Training Accuracy= 0.564\n",
      "Epoch: 270, Loss= 0.6926, Training Accuracy= 0.543\n",
      "Epoch: 280, Loss= 0.6859, Training Accuracy= 0.558\n",
      "Epoch: 290, Loss= 0.6814, Training Accuracy= 0.567\n",
      "Epoch: 300, Loss= 0.6875, Training Accuracy= 0.558\n",
      "Epoch: 310, Loss= 0.6703, Training Accuracy= 0.585\n",
      "Epoch: 320, Loss= 0.6729, Training Accuracy= 0.581\n",
      "Epoch: 330, Loss= 0.6722, Training Accuracy= 0.582\n",
      "Epoch: 340, Loss= 0.6940, Training Accuracy= 0.550\n",
      "Epoch: 350, Loss= 0.6728, Training Accuracy= 0.576\n",
      "Epoch: 360, Loss= 0.6781, Training Accuracy= 0.571\n",
      "Epoch: 370, Loss= 0.6722, Training Accuracy= 0.579\n",
      "Epoch: 380, Loss= 0.6978, Training Accuracy= 0.536\n",
      "Epoch: 390, Loss= 0.6733, Training Accuracy= 0.575\n",
      "Epoch: 400, Loss= 0.6783, Training Accuracy= 0.567\n",
      "Epoch: 410, Loss= 0.6689, Training Accuracy= 0.582\n",
      "Epoch: 420, Loss= 0.6709, Training Accuracy= 0.582\n",
      "Epoch: 430, Loss= 0.6675, Training Accuracy= 0.586\n",
      "Epoch: 440, Loss= 0.6755, Training Accuracy= 0.576\n",
      "Epoch: 450, Loss= 0.7107, Training Accuracy= 0.508\n",
      "Epoch: 460, Loss= 0.6989, Training Accuracy= 0.519\n",
      "Epoch: 470, Loss= 0.6985, Training Accuracy= 0.527\n",
      "Epoch: 480, Loss= 0.6978, Training Accuracy= 0.532\n",
      "Epoch: 490, Loss= 0.7037, Training Accuracy= 0.520\n",
      "Epoch: 500, Loss= 0.6916, Training Accuracy= 0.540\n",
      "Epoch: 510, Loss= 0.6922, Training Accuracy= 0.544\n",
      "Epoch: 520, Loss= 0.6882, Training Accuracy= 0.546\n",
      "Epoch: 530, Loss= 0.6909, Training Accuracy= 0.543\n",
      "Epoch: 540, Loss= 0.6886, Training Accuracy= 0.548\n",
      "Epoch: 550, Loss= 0.6856, Training Accuracy= 0.554\n",
      "Epoch: 560, Loss= 0.6883, Training Accuracy= 0.550\n",
      "Epoch: 570, Loss= 0.6947, Training Accuracy= 0.539\n",
      "Epoch: 580, Loss= 0.6996, Training Accuracy= 0.523\n",
      "Epoch: 590, Loss= 0.6834, Training Accuracy= 0.558\n",
      "Epoch: 600, Loss= 0.6843, Training Accuracy= 0.559\n",
      "Epoch: 610, Loss= 0.6820, Training Accuracy= 0.568\n",
      "Epoch: 620, Loss= 0.6791, Training Accuracy= 0.569\n",
      "Epoch: 630, Loss= 0.6864, Training Accuracy= 0.559\n",
      "Epoch: 640, Loss= 0.6813, Training Accuracy= 0.567\n",
      "Epoch: 650, Loss= 0.6835, Training Accuracy= 0.558\n",
      "Epoch: 660, Loss= 0.6861, Training Accuracy= 0.555\n",
      "Epoch: 670, Loss= 0.6902, Training Accuracy= 0.546\n",
      "Epoch: 680, Loss= 0.6965, Training Accuracy= 0.541\n",
      "Epoch: 690, Loss= 0.6969, Training Accuracy= 0.529\n",
      "Epoch: 700, Loss= 0.6899, Training Accuracy= 0.549\n",
      "Epoch: 710, Loss= 0.6824, Training Accuracy= 0.560\n",
      "Epoch: 720, Loss= 0.6878, Training Accuracy= 0.555\n",
      "Epoch: 730, Loss= 0.6844, Training Accuracy= 0.559\n",
      "Epoch: 740, Loss= 0.6885, Training Accuracy= 0.545\n",
      "Epoch: 750, Loss= 0.6858, Training Accuracy= 0.556\n",
      "Epoch: 760, Loss= 0.6808, Training Accuracy= 0.568\n",
      "Epoch: 770, Loss= 0.6787, Training Accuracy= 0.567\n",
      "Epoch: 780, Loss= 0.6957, Training Accuracy= 0.539\n",
      "Epoch: 790, Loss= 0.6835, Training Accuracy= 0.558\n",
      "Epoch: 800, Loss= 0.6892, Training Accuracy= 0.544\n",
      "Epoch: 810, Loss= 0.6836, Training Accuracy= 0.557\n",
      "Epoch: 820, Loss= 0.6885, Training Accuracy= 0.544\n",
      "Epoch: 830, Loss= 0.6923, Training Accuracy= 0.545\n",
      "Epoch: 840, Loss= 0.6959, Training Accuracy= 0.543\n",
      "Epoch: 850, Loss= 0.7046, Training Accuracy= 0.525\n",
      "Epoch: 860, Loss= 0.6831, Training Accuracy= 0.563\n",
      "Epoch: 870, Loss= 0.6929, Training Accuracy= 0.539\n",
      "Epoch: 880, Loss= 0.6812, Training Accuracy= 0.566\n",
      "Epoch: 890, Loss= 0.6909, Training Accuracy= 0.540\n",
      "Epoch: 900, Loss= 0.6892, Training Accuracy= 0.546\n",
      "Epoch: 910, Loss= 0.6866, Training Accuracy= 0.551\n",
      "Epoch: 920, Loss= 0.6922, Training Accuracy= 0.541\n",
      "Epoch: 930, Loss= 0.6896, Training Accuracy= 0.548\n",
      "Epoch: 940, Loss= 0.6981, Training Accuracy= 0.523\n",
      "Epoch: 950, Loss= 0.6969, Training Accuracy= 0.529\n",
      "Epoch: 960, Loss= 0.6956, Training Accuracy= 0.535\n",
      "Epoch: 970, Loss= 0.6878, Training Accuracy= 0.543\n",
      "Epoch: 980, Loss= 0.7100, Training Accuracy= 0.512\n",
      "Epoch: 990, Loss= 0.6959, Training Accuracy= 0.518\n",
      "Epoch: 1000, Loss= 0.6935, Training Accuracy= 0.527\n",
      "Epoch: 1010, Loss= 0.6970, Training Accuracy= 0.529\n",
      "Epoch: 1020, Loss= 0.6945, Training Accuracy= 0.523\n",
      "Epoch: 1030, Loss= 0.6921, Training Accuracy= 0.540\n",
      "Epoch: 1040, Loss= 0.6972, Training Accuracy= 0.528\n",
      "Epoch: 1050, Loss= 0.6919, Training Accuracy= 0.544\n",
      "Epoch: 1060, Loss= 0.6987, Training Accuracy= 0.525\n",
      "Epoch: 1070, Loss= 0.7033, Training Accuracy= 0.512\n",
      "Epoch: 1080, Loss= 0.6813, Training Accuracy= 0.557\n",
      "Epoch: 1090, Loss= 0.7063, Training Accuracy= 0.516\n",
      "Epoch: 1100, Loss= 0.6908, Training Accuracy= 0.541\n",
      "Epoch: 1110, Loss= 0.6928, Training Accuracy= 0.535\n",
      "Epoch: 1120, Loss= 0.7022, Training Accuracy= 0.521\n",
      "Epoch: 1130, Loss= 0.7005, Training Accuracy= 0.520\n",
      "Epoch: 1140, Loss= 0.6990, Training Accuracy= 0.521\n",
      "Epoch: 1150, Loss= 0.6927, Training Accuracy= 0.529\n",
      "Epoch: 1160, Loss= 0.6926, Training Accuracy= 0.535\n",
      "Epoch: 1170, Loss= 0.6964, Training Accuracy= 0.528\n",
      "Epoch: 1180, Loss= 0.6927, Training Accuracy= 0.528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1190, Loss= 0.7008, Training Accuracy= 0.523\n",
      "Epoch: 1200, Loss= 0.6857, Training Accuracy= 0.549\n",
      "Epoch: 1210, Loss= 0.7039, Training Accuracy= 0.521\n",
      "Epoch: 1220, Loss= 0.6947, Training Accuracy= 0.540\n",
      "Epoch: 1230, Loss= 0.6928, Training Accuracy= 0.540\n",
      "Epoch: 1240, Loss= 0.7025, Training Accuracy= 0.522\n",
      "Epoch: 1250, Loss= 0.6904, Training Accuracy= 0.544\n",
      "Epoch: 1260, Loss= 0.6899, Training Accuracy= 0.542\n",
      "Epoch: 1270, Loss= 0.6934, Training Accuracy= 0.530\n",
      "Epoch: 1280, Loss= 0.6919, Training Accuracy= 0.537\n",
      "Epoch: 1290, Loss= 0.6910, Training Accuracy= 0.539\n",
      "Epoch: 1300, Loss= 0.6819, Training Accuracy= 0.560\n",
      "Epoch: 1310, Loss= 0.6937, Training Accuracy= 0.541\n",
      "Epoch: 1320, Loss= 0.6926, Training Accuracy= 0.541\n",
      "Epoch: 1330, Loss= 0.7008, Training Accuracy= 0.516\n",
      "Epoch: 1340, Loss= 0.6963, Training Accuracy= 0.528\n",
      "Epoch: 1350, Loss= 0.6953, Training Accuracy= 0.533\n",
      "Epoch: 1360, Loss= 0.7075, Training Accuracy= 0.493\n",
      "Epoch: 1370, Loss= 0.6932, Training Accuracy= 0.527\n",
      "Epoch: 1380, Loss= 0.7030, Training Accuracy= 0.517\n",
      "Epoch: 1390, Loss= 0.6959, Training Accuracy= 0.519\n",
      "Epoch: 1400, Loss= 0.6988, Training Accuracy= 0.524\n",
      "Epoch: 1410, Loss= 0.6976, Training Accuracy= 0.523\n",
      "Epoch: 1420, Loss= 0.6893, Training Accuracy= 0.534\n",
      "Epoch: 1430, Loss= 0.6885, Training Accuracy= 0.537\n",
      "Epoch: 1440, Loss= 0.6850, Training Accuracy= 0.553\n",
      "Epoch: 1450, Loss= 0.6877, Training Accuracy= 0.542\n",
      "Epoch: 1460, Loss= 0.6891, Training Accuracy= 0.540\n",
      "Epoch: 1470, Loss= 0.6846, Training Accuracy= 0.548\n",
      "Epoch: 1480, Loss= 0.6826, Training Accuracy= 0.556\n",
      "Epoch: 1490, Loss= 0.6808, Training Accuracy= 0.559\n",
      "Epoch: 1500, Loss= 0.6805, Training Accuracy= 0.563\n",
      "Epoch: 1510, Loss= 0.6807, Training Accuracy= 0.560\n",
      "Epoch: 1520, Loss= 0.6782, Training Accuracy= 0.565\n",
      "Epoch: 1530, Loss= 0.6836, Training Accuracy= 0.555\n",
      "Epoch: 1540, Loss= 0.6765, Training Accuracy= 0.566\n",
      "Epoch: 1550, Loss= 0.6789, Training Accuracy= 0.563\n",
      "Epoch: 1560, Loss= 0.6732, Training Accuracy= 0.574\n",
      "Epoch: 1570, Loss= 0.6839, Training Accuracy= 0.560\n",
      "Epoch: 1580, Loss= 0.6875, Training Accuracy= 0.541\n",
      "Epoch: 1590, Loss= 0.6869, Training Accuracy= 0.553\n",
      "Epoch: 1600, Loss= 0.6857, Training Accuracy= 0.553\n",
      "Epoch: 1610, Loss= 0.6888, Training Accuracy= 0.547\n",
      "Epoch: 1620, Loss= 0.6853, Training Accuracy= 0.552\n",
      "Epoch: 1630, Loss= 0.6810, Training Accuracy= 0.558\n",
      "Epoch: 1640, Loss= 0.6903, Training Accuracy= 0.543\n",
      "Epoch: 1650, Loss= 0.6845, Training Accuracy= 0.555\n",
      "Epoch: 1660, Loss= 0.6824, Training Accuracy= 0.559\n",
      "Epoch: 1670, Loss= 0.6817, Training Accuracy= 0.560\n",
      "Epoch: 1680, Loss= 0.6802, Training Accuracy= 0.564\n",
      "Epoch: 1690, Loss= 0.6867, Training Accuracy= 0.543\n",
      "Epoch: 1700, Loss= 0.6970, Training Accuracy= 0.527\n",
      "Epoch: 1710, Loss= 0.6837, Training Accuracy= 0.558\n",
      "Epoch: 1720, Loss= 0.6844, Training Accuracy= 0.549\n",
      "Epoch: 1730, Loss= 0.6821, Training Accuracy= 0.554\n",
      "Epoch: 1740, Loss= 0.6851, Training Accuracy= 0.544\n",
      "Epoch: 1750, Loss= 0.6826, Training Accuracy= 0.565\n",
      "Epoch: 1760, Loss= 0.6795, Training Accuracy= 0.562\n",
      "Epoch: 1770, Loss= 0.6737, Training Accuracy= 0.572\n",
      "Epoch: 1780, Loss= 0.6771, Training Accuracy= 0.571\n",
      "Epoch: 1790, Loss= 0.6847, Training Accuracy= 0.553\n",
      "Epoch: 1800, Loss= 0.6915, Training Accuracy= 0.543\n",
      "Epoch: 1810, Loss= 0.6708, Training Accuracy= 0.580\n",
      "Epoch: 1820, Loss= 0.6771, Training Accuracy= 0.566\n",
      "Epoch: 1830, Loss= 0.6791, Training Accuracy= 0.567\n",
      "Epoch: 1840, Loss= 0.6703, Training Accuracy= 0.582\n",
      "Epoch: 1850, Loss= 0.6714, Training Accuracy= 0.578\n",
      "Epoch: 1860, Loss= 0.6737, Training Accuracy= 0.573\n",
      "Epoch: 1870, Loss= 0.6800, Training Accuracy= 0.557\n",
      "Epoch: 1880, Loss= 0.6741, Training Accuracy= 0.577\n",
      "Epoch: 1890, Loss= 0.6762, Training Accuracy= 0.573\n",
      "Epoch: 1900, Loss= 0.6699, Training Accuracy= 0.583\n",
      "Epoch: 1910, Loss= 0.6791, Training Accuracy= 0.565\n",
      "Epoch: 1920, Loss= 0.6838, Training Accuracy= 0.550\n",
      "Epoch: 1930, Loss= 0.6796, Training Accuracy= 0.562\n",
      "Epoch: 1940, Loss= 0.6733, Training Accuracy= 0.576\n",
      "Epoch: 1950, Loss= 0.6776, Training Accuracy= 0.562\n",
      "Epoch: 1960, Loss= 0.6713, Training Accuracy= 0.579\n",
      "Epoch: 1970, Loss= 0.6868, Training Accuracy= 0.556\n",
      "Epoch: 1980, Loss= 0.6688, Training Accuracy= 0.579\n",
      "Epoch: 1990, Loss= 0.6732, Training Accuracy= 0.574\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4972\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.7761, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.7085, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.7028, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.7002, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.6986, Training Accuracy= 0.501\n",
      "Epoch: 50, Loss= 0.6976, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6969, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6962, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6954, Training Accuracy= 0.504\n",
      "Epoch: 90, Loss= 0.6945, Training Accuracy= 0.511\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.516\n",
      "Epoch: 110, Loss= 0.6919, Training Accuracy= 0.521\n",
      "Epoch: 120, Loss= 0.6922, Training Accuracy= 0.525\n",
      "Epoch: 130, Loss= 0.6913, Training Accuracy= 0.529\n",
      "Epoch: 140, Loss= 0.6911, Training Accuracy= 0.532\n",
      "Epoch: 150, Loss= 0.6940, Training Accuracy= 0.529\n",
      "Epoch: 160, Loss= 0.6935, Training Accuracy= 0.529\n",
      "Epoch: 170, Loss= 0.7027, Training Accuracy= 0.512\n",
      "Epoch: 180, Loss= 0.7024, Training Accuracy= 0.525\n",
      "Epoch: 190, Loss= 0.6937, Training Accuracy= 0.525\n",
      "Epoch: 200, Loss= 0.7004, Training Accuracy= 0.502\n",
      "Epoch: 210, Loss= 0.6955, Training Accuracy= 0.534\n",
      "Epoch: 220, Loss= 0.6900, Training Accuracy= 0.546\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.539\n",
      "Epoch: 240, Loss= 0.6924, Training Accuracy= 0.542\n",
      "Epoch: 250, Loss= 0.6864, Training Accuracy= 0.549\n",
      "Epoch: 260, Loss= 0.6900, Training Accuracy= 0.550\n",
      "Epoch: 270, Loss= 0.6877, Training Accuracy= 0.549\n",
      "Epoch: 280, Loss= 0.6947, Training Accuracy= 0.543\n",
      "Epoch: 290, Loss= 0.6884, Training Accuracy= 0.557\n",
      "Epoch: 300, Loss= 0.6847, Training Accuracy= 0.559\n",
      "Epoch: 310, Loss= 0.6835, Training Accuracy= 0.562\n",
      "Epoch: 320, Loss= 0.6901, Training Accuracy= 0.556\n",
      "Epoch: 330, Loss= 0.6828, Training Accuracy= 0.566\n",
      "Epoch: 340, Loss= 0.6778, Training Accuracy= 0.573\n",
      "Epoch: 350, Loss= 0.7058, Training Accuracy= 0.534\n",
      "Epoch: 360, Loss= 0.6806, Training Accuracy= 0.569\n",
      "Epoch: 370, Loss= 0.6764, Training Accuracy= 0.573\n",
      "Epoch: 380, Loss= 0.6892, Training Accuracy= 0.557\n",
      "Epoch: 390, Loss= 0.6939, Training Accuracy= 0.544\n",
      "Epoch: 400, Loss= 0.6777, Training Accuracy= 0.576\n",
      "Epoch: 410, Loss= 0.6771, Training Accuracy= 0.575\n",
      "Epoch: 420, Loss= 0.6811, Training Accuracy= 0.566\n",
      "Epoch: 430, Loss= 0.6886, Training Accuracy= 0.555\n",
      "Epoch: 440, Loss= 0.6782, Training Accuracy= 0.576\n",
      "Epoch: 450, Loss= 0.6727, Training Accuracy= 0.580\n",
      "Epoch: 460, Loss= 0.6709, Training Accuracy= 0.587\n",
      "Epoch: 470, Loss= 0.6757, Training Accuracy= 0.580\n",
      "Epoch: 480, Loss= 0.6805, Training Accuracy= 0.572\n",
      "Epoch: 490, Loss= 0.6776, Training Accuracy= 0.576\n",
      "Epoch: 500, Loss= 0.6753, Training Accuracy= 0.573\n",
      "Epoch: 510, Loss= 0.7049, Training Accuracy= 0.510\n",
      "Epoch: 520, Loss= 0.6947, Training Accuracy= 0.512\n",
      "Epoch: 530, Loss= 0.6932, Training Accuracy= 0.516\n",
      "Epoch: 540, Loss= 0.6920, Training Accuracy= 0.519\n",
      "Epoch: 550, Loss= 0.6993, Training Accuracy= 0.513\n",
      "Epoch: 560, Loss= 0.6970, Training Accuracy= 0.514\n",
      "Epoch: 570, Loss= 0.6944, Training Accuracy= 0.519\n",
      "Epoch: 580, Loss= 0.6962, Training Accuracy= 0.523\n",
      "Epoch: 590, Loss= 0.6901, Training Accuracy= 0.538\n",
      "Epoch: 600, Loss= 0.6904, Training Accuracy= 0.541\n",
      "Epoch: 610, Loss= 0.6875, Training Accuracy= 0.545\n",
      "Epoch: 620, Loss= 0.6844, Training Accuracy= 0.552\n",
      "Epoch: 630, Loss= 0.6807, Training Accuracy= 0.563\n",
      "Epoch: 640, Loss= 0.6834, Training Accuracy= 0.556\n",
      "Epoch: 650, Loss= 0.6749, Training Accuracy= 0.576\n",
      "Epoch: 660, Loss= 0.6818, Training Accuracy= 0.561\n",
      "Epoch: 670, Loss= 0.6717, Training Accuracy= 0.588\n",
      "Epoch: 680, Loss= 0.6771, Training Accuracy= 0.573\n",
      "Epoch: 690, Loss= 0.6773, Training Accuracy= 0.572\n",
      "Epoch: 700, Loss= 0.6892, Training Accuracy= 0.546\n",
      "Epoch: 710, Loss= 0.6821, Training Accuracy= 0.561\n",
      "Epoch: 720, Loss= 0.6776, Training Accuracy= 0.571\n",
      "Epoch: 730, Loss= 0.6762, Training Accuracy= 0.571\n",
      "Epoch: 740, Loss= 0.6714, Training Accuracy= 0.576\n",
      "Epoch: 750, Loss= 0.6713, Training Accuracy= 0.579\n",
      "Epoch: 760, Loss= 0.6763, Training Accuracy= 0.573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 770, Loss= 0.6782, Training Accuracy= 0.573\n",
      "Epoch: 780, Loss= 0.6773, Training Accuracy= 0.574\n",
      "Epoch: 790, Loss= 0.6867, Training Accuracy= 0.560\n",
      "Epoch: 800, Loss= 0.6683, Training Accuracy= 0.588\n",
      "Epoch: 810, Loss= 0.6835, Training Accuracy= 0.562\n",
      "Epoch: 820, Loss= 0.6840, Training Accuracy= 0.564\n",
      "Epoch: 830, Loss= 0.6680, Training Accuracy= 0.593\n",
      "Epoch: 840, Loss= 0.6781, Training Accuracy= 0.574\n",
      "Epoch: 850, Loss= 0.6707, Training Accuracy= 0.589\n",
      "Epoch: 860, Loss= 0.6695, Training Accuracy= 0.587\n",
      "Epoch: 870, Loss= 0.6753, Training Accuracy= 0.582\n",
      "Epoch: 880, Loss= 0.6755, Training Accuracy= 0.573\n",
      "Epoch: 890, Loss= 0.6722, Training Accuracy= 0.583\n",
      "Epoch: 900, Loss= 0.6813, Training Accuracy= 0.564\n",
      "Epoch: 910, Loss= 0.6634, Training Accuracy= 0.602\n",
      "Epoch: 920, Loss= 0.6777, Training Accuracy= 0.574\n",
      "Epoch: 930, Loss= 0.6839, Training Accuracy= 0.561\n",
      "Epoch: 940, Loss= 0.6729, Training Accuracy= 0.580\n",
      "Epoch: 950, Loss= 0.6818, Training Accuracy= 0.567\n",
      "Epoch: 960, Loss= 0.6702, Training Accuracy= 0.585\n",
      "Epoch: 970, Loss= 0.6854, Training Accuracy= 0.556\n",
      "Epoch: 980, Loss= 0.6667, Training Accuracy= 0.588\n",
      "Epoch: 990, Loss= 0.6718, Training Accuracy= 0.582\n",
      "Epoch: 1000, Loss= 0.6777, Training Accuracy= 0.575\n",
      "Epoch: 1010, Loss= 0.6790, Training Accuracy= 0.566\n",
      "Epoch: 1020, Loss= 0.6679, Training Accuracy= 0.589\n",
      "Epoch: 1030, Loss= 0.6661, Training Accuracy= 0.593\n",
      "Epoch: 1040, Loss= 0.6771, Training Accuracy= 0.575\n",
      "Epoch: 1050, Loss= 0.6771, Training Accuracy= 0.574\n",
      "Epoch: 1060, Loss= 0.6896, Training Accuracy= 0.556\n",
      "Epoch: 1070, Loss= 0.6778, Training Accuracy= 0.573\n",
      "Epoch: 1080, Loss= 0.6746, Training Accuracy= 0.581\n",
      "Epoch: 1090, Loss= 0.6717, Training Accuracy= 0.585\n",
      "Epoch: 1100, Loss= 0.6747, Training Accuracy= 0.576\n",
      "Epoch: 1110, Loss= 0.6742, Training Accuracy= 0.580\n",
      "Epoch: 1120, Loss= 0.6735, Training Accuracy= 0.580\n",
      "Epoch: 1130, Loss= 0.6758, Training Accuracy= 0.575\n",
      "Epoch: 1140, Loss= 0.6804, Training Accuracy= 0.571\n",
      "Epoch: 1150, Loss= 0.6833, Training Accuracy= 0.564\n",
      "Epoch: 1160, Loss= 0.6749, Training Accuracy= 0.581\n",
      "Epoch: 1170, Loss= 0.6786, Training Accuracy= 0.570\n",
      "Epoch: 1180, Loss= 0.6922, Training Accuracy= 0.537\n",
      "Epoch: 1190, Loss= 0.6796, Training Accuracy= 0.567\n",
      "Epoch: 1200, Loss= 0.6853, Training Accuracy= 0.549\n",
      "Epoch: 1210, Loss= 0.6738, Training Accuracy= 0.574\n",
      "Epoch: 1220, Loss= 0.6749, Training Accuracy= 0.578\n",
      "Epoch: 1230, Loss= 0.6822, Training Accuracy= 0.563\n",
      "Epoch: 1240, Loss= 0.6836, Training Accuracy= 0.561\n",
      "Epoch: 1250, Loss= 0.6789, Training Accuracy= 0.571\n",
      "Epoch: 1260, Loss= 0.6798, Training Accuracy= 0.566\n",
      "Epoch: 1270, Loss= 0.6938, Training Accuracy= 0.525\n",
      "Epoch: 1280, Loss= 0.6903, Training Accuracy= 0.538\n",
      "Epoch: 1290, Loss= 0.6886, Training Accuracy= 0.545\n",
      "Epoch: 1300, Loss= 0.6807, Training Accuracy= 0.563\n",
      "Epoch: 1310, Loss= 0.6825, Training Accuracy= 0.562\n",
      "Epoch: 1320, Loss= 0.6799, Training Accuracy= 0.567\n",
      "Epoch: 1330, Loss= 0.6894, Training Accuracy= 0.539\n",
      "Epoch: 1340, Loss= 0.6894, Training Accuracy= 0.542\n",
      "Epoch: 1350, Loss= 0.6878, Training Accuracy= 0.545\n",
      "Epoch: 1360, Loss= 0.6814, Training Accuracy= 0.561\n",
      "Epoch: 1370, Loss= 0.6819, Training Accuracy= 0.560\n",
      "Epoch: 1380, Loss= 0.6800, Training Accuracy= 0.571\n",
      "Epoch: 1390, Loss= 0.6768, Training Accuracy= 0.577\n",
      "Epoch: 1400, Loss= 0.6801, Training Accuracy= 0.565\n",
      "Epoch: 1410, Loss= 0.6817, Training Accuracy= 0.559\n",
      "Epoch: 1420, Loss= 0.6810, Training Accuracy= 0.566\n",
      "Epoch: 1430, Loss= 0.6830, Training Accuracy= 0.558\n",
      "Epoch: 1440, Loss= 0.6778, Training Accuracy= 0.575\n",
      "Epoch: 1450, Loss= 0.6775, Training Accuracy= 0.572\n",
      "Epoch: 1460, Loss= 0.6718, Training Accuracy= 0.582\n",
      "Epoch: 1470, Loss= 0.6713, Training Accuracy= 0.589\n",
      "Epoch: 1480, Loss= 0.6775, Training Accuracy= 0.574\n",
      "Epoch: 1490, Loss= 0.6795, Training Accuracy= 0.570\n",
      "Epoch: 1500, Loss= 0.6814, Training Accuracy= 0.573\n",
      "Epoch: 1510, Loss= 0.6715, Training Accuracy= 0.585\n",
      "Epoch: 1520, Loss= 0.6793, Training Accuracy= 0.571\n",
      "Epoch: 1530, Loss= 0.6772, Training Accuracy= 0.576\n",
      "Epoch: 1540, Loss= 0.6798, Training Accuracy= 0.568\n",
      "Epoch: 1550, Loss= 0.6801, Training Accuracy= 0.571\n",
      "Epoch: 1560, Loss= 0.6835, Training Accuracy= 0.562\n",
      "Epoch: 1570, Loss= 0.6834, Training Accuracy= 0.560\n",
      "Epoch: 1580, Loss= 0.6715, Training Accuracy= 0.587\n",
      "Epoch: 1590, Loss= 0.6713, Training Accuracy= 0.582\n",
      "Epoch: 1600, Loss= 0.6794, Training Accuracy= 0.568\n",
      "Epoch: 1610, Loss= 0.6719, Training Accuracy= 0.583\n",
      "Epoch: 1620, Loss= 0.6794, Training Accuracy= 0.573\n",
      "Epoch: 1630, Loss= 0.6742, Training Accuracy= 0.578\n",
      "Epoch: 1640, Loss= 0.6707, Training Accuracy= 0.592\n",
      "Epoch: 1650, Loss= 0.6693, Training Accuracy= 0.593\n",
      "Epoch: 1660, Loss= 0.6719, Training Accuracy= 0.587\n",
      "Epoch: 1670, Loss= 0.6736, Training Accuracy= 0.586\n",
      "Epoch: 1680, Loss= 0.6707, Training Accuracy= 0.588\n",
      "Epoch: 1690, Loss= 0.6758, Training Accuracy= 0.573\n",
      "Epoch: 1700, Loss= 0.6753, Training Accuracy= 0.573\n",
      "Epoch: 1710, Loss= 0.6758, Training Accuracy= 0.577\n",
      "Epoch: 1720, Loss= 0.6745, Training Accuracy= 0.581\n",
      "Epoch: 1730, Loss= 0.6710, Training Accuracy= 0.582\n",
      "Epoch: 1740, Loss= 0.6729, Training Accuracy= 0.581\n",
      "Epoch: 1750, Loss= 0.6734, Training Accuracy= 0.582\n",
      "Epoch: 1760, Loss= 0.6759, Training Accuracy= 0.571\n",
      "Epoch: 1770, Loss= 0.6857, Training Accuracy= 0.551\n",
      "Epoch: 1780, Loss= 0.6711, Training Accuracy= 0.589\n",
      "Epoch: 1790, Loss= 0.6713, Training Accuracy= 0.580\n",
      "Epoch: 1800, Loss= 0.6673, Training Accuracy= 0.589\n",
      "Epoch: 1810, Loss= 0.6803, Training Accuracy= 0.564\n",
      "Epoch: 1820, Loss= 0.6774, Training Accuracy= 0.570\n",
      "Epoch: 1830, Loss= 0.6852, Training Accuracy= 0.558\n",
      "Epoch: 1840, Loss= 0.6891, Training Accuracy= 0.549\n",
      "Epoch: 1850, Loss= 0.6840, Training Accuracy= 0.553\n",
      "Epoch: 1860, Loss= 0.6840, Training Accuracy= 0.559\n",
      "Epoch: 1870, Loss= 0.6850, Training Accuracy= 0.555\n",
      "Epoch: 1880, Loss= 0.6850, Training Accuracy= 0.556\n",
      "Epoch: 1890, Loss= 0.6854, Training Accuracy= 0.555\n",
      "Epoch: 1900, Loss= 0.6854, Training Accuracy= 0.548\n",
      "Epoch: 1910, Loss= 0.6797, Training Accuracy= 0.563\n",
      "Epoch: 1920, Loss= 0.6816, Training Accuracy= 0.565\n",
      "Epoch: 1930, Loss= 0.6817, Training Accuracy= 0.564\n",
      "Epoch: 1940, Loss= 0.6815, Training Accuracy= 0.561\n",
      "Epoch: 1950, Loss= 0.6800, Training Accuracy= 0.566\n",
      "Epoch: 1960, Loss= 0.6873, Training Accuracy= 0.549\n",
      "Epoch: 1970, Loss= 0.6911, Training Accuracy= 0.537\n",
      "Epoch: 1980, Loss= 0.6847, Training Accuracy= 0.548\n",
      "Epoch: 1990, Loss= 0.6930, Training Accuracy= 0.537\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5006\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.8114, Training Accuracy= 0.510\n",
      "Epoch: 10, Loss= 0.7289, Training Accuracy= 0.510\n",
      "Epoch: 20, Loss= 0.7162, Training Accuracy= 0.510\n",
      "Epoch: 30, Loss= 0.7102, Training Accuracy= 0.510\n",
      "Epoch: 40, Loss= 0.7067, Training Accuracy= 0.510\n",
      "Epoch: 50, Loss= 0.7044, Training Accuracy= 0.510\n",
      "Epoch: 60, Loss= 0.7027, Training Accuracy= 0.510\n",
      "Epoch: 70, Loss= 0.7014, Training Accuracy= 0.510\n",
      "Epoch: 80, Loss= 0.7005, Training Accuracy= 0.509\n",
      "Epoch: 90, Loss= 0.6997, Training Accuracy= 0.510\n",
      "Epoch: 100, Loss= 0.6991, Training Accuracy= 0.512\n",
      "Epoch: 110, Loss= 0.6986, Training Accuracy= 0.511\n",
      "Epoch: 120, Loss= 0.6981, Training Accuracy= 0.512\n",
      "Epoch: 130, Loss= 0.6977, Training Accuracy= 0.515\n",
      "Epoch: 140, Loss= 0.6974, Training Accuracy= 0.514\n",
      "Epoch: 150, Loss= 0.6970, Training Accuracy= 0.515\n",
      "Epoch: 160, Loss= 0.6963, Training Accuracy= 0.517\n",
      "Epoch: 170, Loss= 0.6955, Training Accuracy= 0.519\n",
      "Epoch: 180, Loss= 0.6948, Training Accuracy= 0.519\n",
      "Epoch: 190, Loss= 0.6940, Training Accuracy= 0.523\n",
      "Epoch: 200, Loss= 0.6929, Training Accuracy= 0.526\n",
      "Epoch: 210, Loss= 0.6922, Training Accuracy= 0.534\n",
      "Epoch: 220, Loss= 0.6917, Training Accuracy= 0.535\n",
      "Epoch: 230, Loss= 0.6911, Training Accuracy= 0.540\n",
      "Epoch: 240, Loss= 0.6900, Training Accuracy= 0.538\n",
      "Epoch: 250, Loss= 0.6885, Training Accuracy= 0.545\n",
      "Epoch: 260, Loss= 0.6869, Training Accuracy= 0.548\n",
      "Epoch: 270, Loss= 0.6845, Training Accuracy= 0.556\n",
      "Epoch: 280, Loss= 0.6861, Training Accuracy= 0.552\n",
      "Epoch: 290, Loss= 0.6862, Training Accuracy= 0.544\n",
      "Epoch: 300, Loss= 0.6829, Training Accuracy= 0.552\n",
      "Epoch: 310, Loss= 0.6815, Training Accuracy= 0.554\n",
      "Epoch: 320, Loss= 0.6774, Training Accuracy= 0.568\n",
      "Epoch: 330, Loss= 0.6875, Training Accuracy= 0.559\n",
      "Epoch: 340, Loss= 0.6920, Training Accuracy= 0.546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 350, Loss= 0.6751, Training Accuracy= 0.573\n",
      "Epoch: 360, Loss= 0.6755, Training Accuracy= 0.571\n",
      "Epoch: 370, Loss= 0.6754, Training Accuracy= 0.572\n",
      "Epoch: 380, Loss= 0.6750, Training Accuracy= 0.573\n",
      "Epoch: 390, Loss= 0.6795, Training Accuracy= 0.570\n",
      "Epoch: 400, Loss= 0.6718, Training Accuracy= 0.578\n",
      "Epoch: 410, Loss= 0.6726, Training Accuracy= 0.578\n",
      "Epoch: 420, Loss= 0.6801, Training Accuracy= 0.557\n",
      "Epoch: 430, Loss= 0.6779, Training Accuracy= 0.577\n",
      "Epoch: 440, Loss= 0.6675, Training Accuracy= 0.590\n",
      "Epoch: 450, Loss= 0.6670, Training Accuracy= 0.589\n",
      "Epoch: 460, Loss= 0.6689, Training Accuracy= 0.583\n",
      "Epoch: 470, Loss= 0.6738, Training Accuracy= 0.578\n",
      "Epoch: 480, Loss= 0.6669, Training Accuracy= 0.584\n",
      "Epoch: 490, Loss= 0.6708, Training Accuracy= 0.581\n",
      "Epoch: 500, Loss= 0.6726, Training Accuracy= 0.582\n",
      "Epoch: 510, Loss= 0.6760, Training Accuracy= 0.572\n",
      "Epoch: 520, Loss= 0.6644, Training Accuracy= 0.590\n",
      "Epoch: 530, Loss= 0.6593, Training Accuracy= 0.603\n",
      "Epoch: 540, Loss= 0.7027, Training Accuracy= 0.551\n",
      "Epoch: 550, Loss= 0.6844, Training Accuracy= 0.570\n",
      "Epoch: 560, Loss= 0.6671, Training Accuracy= 0.586\n",
      "Epoch: 570, Loss= 0.6668, Training Accuracy= 0.589\n",
      "Epoch: 580, Loss= 0.6633, Training Accuracy= 0.593\n",
      "Epoch: 590, Loss= 0.6685, Training Accuracy= 0.584\n",
      "Epoch: 600, Loss= 0.7146, Training Accuracy= 0.521\n",
      "Epoch: 610, Loss= 0.6678, Training Accuracy= 0.590\n",
      "Epoch: 620, Loss= 0.6729, Training Accuracy= 0.576\n",
      "Epoch: 630, Loss= 0.6609, Training Accuracy= 0.603\n",
      "Epoch: 640, Loss= 0.6759, Training Accuracy= 0.583\n",
      "Epoch: 650, Loss= 0.6697, Training Accuracy= 0.586\n",
      "Epoch: 660, Loss= 0.6793, Training Accuracy= 0.582\n",
      "Epoch: 670, Loss= 0.6833, Training Accuracy= 0.574\n",
      "Epoch: 680, Loss= 0.6929, Training Accuracy= 0.563\n",
      "Epoch: 690, Loss= 0.6697, Training Accuracy= 0.582\n",
      "Epoch: 700, Loss= 0.6827, Training Accuracy= 0.573\n",
      "Epoch: 710, Loss= 0.6729, Training Accuracy= 0.581\n",
      "Epoch: 720, Loss= 0.6664, Training Accuracy= 0.592\n",
      "Epoch: 730, Loss= 0.6660, Training Accuracy= 0.594\n",
      "Epoch: 740, Loss= 0.6703, Training Accuracy= 0.589\n",
      "Epoch: 750, Loss= 0.6749, Training Accuracy= 0.574\n",
      "Epoch: 760, Loss= 0.6712, Training Accuracy= 0.580\n",
      "Epoch: 770, Loss= 0.6644, Training Accuracy= 0.589\n",
      "Epoch: 780, Loss= 0.6876, Training Accuracy= 0.555\n",
      "Epoch: 790, Loss= 0.6695, Training Accuracy= 0.583\n",
      "Epoch: 800, Loss= 0.6672, Training Accuracy= 0.586\n",
      "Epoch: 810, Loss= 0.6667, Training Accuracy= 0.584\n",
      "Epoch: 820, Loss= 0.6666, Training Accuracy= 0.591\n",
      "Epoch: 830, Loss= 0.6843, Training Accuracy= 0.566\n",
      "Epoch: 840, Loss= 0.6844, Training Accuracy= 0.559\n",
      "Epoch: 850, Loss= 0.6665, Training Accuracy= 0.584\n",
      "Epoch: 860, Loss= 0.6592, Training Accuracy= 0.607\n",
      "Epoch: 870, Loss= 0.6701, Training Accuracy= 0.586\n",
      "Epoch: 880, Loss= 0.6965, Training Accuracy= 0.545\n",
      "Epoch: 890, Loss= 0.6872, Training Accuracy= 0.550\n",
      "Epoch: 900, Loss= 0.6738, Training Accuracy= 0.579\n",
      "Epoch: 910, Loss= 0.6627, Training Accuracy= 0.592\n",
      "Epoch: 920, Loss= 0.6727, Training Accuracy= 0.578\n",
      "Epoch: 930, Loss= 0.6708, Training Accuracy= 0.583\n",
      "Epoch: 940, Loss= 0.6710, Training Accuracy= 0.587\n",
      "Epoch: 950, Loss= 0.6706, Training Accuracy= 0.585\n",
      "Epoch: 960, Loss= 0.6758, Training Accuracy= 0.578\n",
      "Epoch: 970, Loss= 0.6624, Training Accuracy= 0.593\n",
      "Epoch: 980, Loss= 0.6672, Training Accuracy= 0.587\n",
      "Epoch: 990, Loss= 0.6634, Training Accuracy= 0.597\n",
      "Epoch: 1000, Loss= 0.6624, Training Accuracy= 0.595\n",
      "Epoch: 1010, Loss= 0.6701, Training Accuracy= 0.582\n",
      "Epoch: 1020, Loss= 0.6656, Training Accuracy= 0.587\n",
      "Epoch: 1030, Loss= 0.6720, Training Accuracy= 0.578\n",
      "Epoch: 1040, Loss= 0.6643, Training Accuracy= 0.593\n",
      "Epoch: 1050, Loss= 0.6693, Training Accuracy= 0.581\n",
      "Epoch: 1060, Loss= 0.6627, Training Accuracy= 0.595\n",
      "Epoch: 1070, Loss= 0.6702, Training Accuracy= 0.583\n",
      "Epoch: 1080, Loss= 0.6860, Training Accuracy= 0.554\n",
      "Epoch: 1090, Loss= 0.6812, Training Accuracy= 0.563\n",
      "Epoch: 1100, Loss= 0.6790, Training Accuracy= 0.564\n",
      "Epoch: 1110, Loss= 0.6655, Training Accuracy= 0.590\n",
      "Epoch: 1120, Loss= 0.6728, Training Accuracy= 0.578\n",
      "Epoch: 1130, Loss= 0.6642, Training Accuracy= 0.591\n",
      "Epoch: 1140, Loss= 0.6696, Training Accuracy= 0.584\n",
      "Epoch: 1150, Loss= 0.6672, Training Accuracy= 0.593\n",
      "Epoch: 1160, Loss= 0.6636, Training Accuracy= 0.595\n",
      "Epoch: 1170, Loss= 0.6682, Training Accuracy= 0.587\n",
      "Epoch: 1180, Loss= 0.6704, Training Accuracy= 0.581\n",
      "Epoch: 1190, Loss= 0.6679, Training Accuracy= 0.582\n",
      "Epoch: 1200, Loss= 0.6717, Training Accuracy= 0.580\n",
      "Epoch: 1210, Loss= 0.6579, Training Accuracy= 0.601\n",
      "Epoch: 1220, Loss= 0.6659, Training Accuracy= 0.589\n",
      "Epoch: 1230, Loss= 0.6597, Training Accuracy= 0.601\n",
      "Epoch: 1240, Loss= 0.6854, Training Accuracy= 0.557\n",
      "Epoch: 1250, Loss= 0.6685, Training Accuracy= 0.587\n",
      "Epoch: 1260, Loss= 0.6656, Training Accuracy= 0.588\n",
      "Epoch: 1270, Loss= 0.6714, Training Accuracy= 0.573\n",
      "Epoch: 1280, Loss= 0.6995, Training Accuracy= 0.516\n",
      "Epoch: 1290, Loss= 0.6955, Training Accuracy= 0.516\n",
      "Epoch: 1300, Loss= 0.6946, Training Accuracy= 0.523\n",
      "Epoch: 1310, Loss= 0.6949, Training Accuracy= 0.519\n",
      "Epoch: 1320, Loss= 0.6919, Training Accuracy= 0.529\n",
      "Epoch: 1330, Loss= 0.6896, Training Accuracy= 0.537\n",
      "Epoch: 1340, Loss= 0.6974, Training Accuracy= 0.518\n",
      "Epoch: 1350, Loss= 0.6909, Training Accuracy= 0.539\n",
      "Epoch: 1360, Loss= 0.6919, Training Accuracy= 0.535\n",
      "Epoch: 1370, Loss= 0.6941, Training Accuracy= 0.518\n",
      "Epoch: 1380, Loss= 0.6938, Training Accuracy= 0.523\n",
      "Epoch: 1390, Loss= 0.6909, Training Accuracy= 0.534\n",
      "Epoch: 1400, Loss= 0.6875, Training Accuracy= 0.541\n",
      "Epoch: 1410, Loss= 0.6958, Training Accuracy= 0.518\n",
      "Epoch: 1420, Loss= 0.6938, Training Accuracy= 0.517\n",
      "Epoch: 1430, Loss= 0.6919, Training Accuracy= 0.529\n",
      "Epoch: 1440, Loss= 0.6929, Training Accuracy= 0.521\n",
      "Epoch: 1450, Loss= 0.6916, Training Accuracy= 0.530\n",
      "Epoch: 1460, Loss= 0.6870, Training Accuracy= 0.539\n",
      "Epoch: 1470, Loss= 0.6889, Training Accuracy= 0.541\n",
      "Epoch: 1480, Loss= 0.6879, Training Accuracy= 0.540\n",
      "Epoch: 1490, Loss= 0.6852, Training Accuracy= 0.544\n",
      "Epoch: 1500, Loss= 0.6851, Training Accuracy= 0.551\n",
      "Epoch: 1510, Loss= 0.6843, Training Accuracy= 0.543\n",
      "Epoch: 1520, Loss= 0.6858, Training Accuracy= 0.544\n",
      "Epoch: 1530, Loss= 0.6837, Training Accuracy= 0.552\n",
      "Epoch: 1540, Loss= 0.6832, Training Accuracy= 0.557\n",
      "Epoch: 1550, Loss= 0.6807, Training Accuracy= 0.560\n",
      "Epoch: 1560, Loss= 0.6872, Training Accuracy= 0.538\n",
      "Epoch: 1570, Loss= 0.6821, Training Accuracy= 0.552\n",
      "Epoch: 1580, Loss= 0.6831, Training Accuracy= 0.554\n",
      "Epoch: 1590, Loss= 0.6825, Training Accuracy= 0.556\n",
      "Epoch: 1600, Loss= 0.6833, Training Accuracy= 0.547\n",
      "Epoch: 1610, Loss= 0.6835, Training Accuracy= 0.552\n",
      "Epoch: 1620, Loss= 0.6907, Training Accuracy= 0.529\n",
      "Epoch: 1630, Loss= 0.6865, Training Accuracy= 0.543\n",
      "Epoch: 1640, Loss= 0.6849, Training Accuracy= 0.542\n",
      "Epoch: 1650, Loss= 0.6786, Training Accuracy= 0.564\n",
      "Epoch: 1660, Loss= 0.6845, Training Accuracy= 0.548\n",
      "Epoch: 1670, Loss= 0.6812, Training Accuracy= 0.550\n",
      "Epoch: 1680, Loss= 0.6879, Training Accuracy= 0.541\n",
      "Epoch: 1690, Loss= 0.6810, Training Accuracy= 0.558\n",
      "Epoch: 1700, Loss= 0.6794, Training Accuracy= 0.567\n",
      "Epoch: 1710, Loss= 0.6812, Training Accuracy= 0.557\n",
      "Epoch: 1720, Loss= 0.6837, Training Accuracy= 0.549\n",
      "Epoch: 1730, Loss= 0.6773, Training Accuracy= 0.569\n",
      "Epoch: 1740, Loss= 0.6806, Training Accuracy= 0.558\n",
      "Epoch: 1750, Loss= 0.6754, Training Accuracy= 0.569\n",
      "Epoch: 1760, Loss= 0.6730, Training Accuracy= 0.570\n",
      "Epoch: 1770, Loss= 0.6841, Training Accuracy= 0.552\n",
      "Epoch: 1780, Loss= 0.6780, Training Accuracy= 0.564\n",
      "Epoch: 1790, Loss= 0.6887, Training Accuracy= 0.537\n",
      "Epoch: 1800, Loss= 0.6813, Training Accuracy= 0.558\n",
      "Epoch: 1810, Loss= 0.6771, Training Accuracy= 0.563\n",
      "Epoch: 1820, Loss= 0.6721, Training Accuracy= 0.572\n",
      "Epoch: 1830, Loss= 0.6811, Training Accuracy= 0.555\n",
      "Epoch: 1840, Loss= 0.6756, Training Accuracy= 0.570\n",
      "Epoch: 1850, Loss= 0.6810, Training Accuracy= 0.556\n",
      "Epoch: 1860, Loss= 0.6788, Training Accuracy= 0.560\n",
      "Epoch: 1870, Loss= 0.6807, Training Accuracy= 0.557\n",
      "Epoch: 1880, Loss= 0.6735, Training Accuracy= 0.574\n",
      "Epoch: 1890, Loss= 0.6912, Training Accuracy= 0.542\n",
      "Epoch: 1900, Loss= 0.6705, Training Accuracy= 0.579\n",
      "Epoch: 1910, Loss= 0.6713, Training Accuracy= 0.576\n",
      "Epoch: 1920, Loss= 0.6884, Training Accuracy= 0.547\n",
      "Epoch: 1930, Loss= 0.6782, Training Accuracy= 0.565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1940, Loss= 0.6865, Training Accuracy= 0.551\n",
      "Epoch: 1950, Loss= 0.6848, Training Accuracy= 0.556\n",
      "Epoch: 1960, Loss= 0.6766, Training Accuracy= 0.566\n",
      "Epoch: 1970, Loss= 0.6774, Training Accuracy= 0.562\n",
      "Epoch: 1980, Loss= 0.6784, Training Accuracy= 0.566\n",
      "Epoch: 1990, Loss= 0.6756, Training Accuracy= 0.565\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4969\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 20, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 30, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 40, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 50, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 60, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 80, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 100, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 110, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 120, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 130, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 140, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 150, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 160, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 190, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 210, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 220, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 230, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 240, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 250, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 260, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 270, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 280, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 290, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 300, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 310, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 320, Loss= 0.6929, Training Accuracy= 0.503\n",
      "Epoch: 330, Loss= 0.6929, Training Accuracy= 0.502\n",
      "Epoch: 340, Loss= 0.6929, Training Accuracy= 0.502\n",
      "Epoch: 350, Loss= 0.6929, Training Accuracy= 0.502\n",
      "Epoch: 360, Loss= 0.6929, Training Accuracy= 0.503\n",
      "Epoch: 370, Loss= 0.6928, Training Accuracy= 0.505\n",
      "Epoch: 380, Loss= 0.6928, Training Accuracy= 0.505\n",
      "Epoch: 390, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 400, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 410, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 430, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 440, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 450, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 460, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 470, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 480, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 490, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 500, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 510, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 520, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 530, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 540, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 550, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 560, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 570, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 580, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 590, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 600, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 610, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 620, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 630, Loss= 0.6921, Training Accuracy= 0.520\n",
      "Epoch: 640, Loss= 0.6920, Training Accuracy= 0.518\n",
      "Epoch: 650, Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 660, Loss= 0.6922, Training Accuracy= 0.517\n",
      "Epoch: 670, Loss= 0.6921, Training Accuracy= 0.517\n",
      "Epoch: 680, Loss= 0.6920, Training Accuracy= 0.516\n",
      "Epoch: 690, Loss= 0.6919, Training Accuracy= 0.518\n",
      "Epoch: 700, Loss= 0.6918, Training Accuracy= 0.520\n",
      "Epoch: 710, Loss= 0.6918, Training Accuracy= 0.519\n",
      "Epoch: 720, Loss= 0.6919, Training Accuracy= 0.517\n",
      "Epoch: 730, Loss= 0.6920, Training Accuracy= 0.518\n",
      "Epoch: 740, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 750, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 760, Loss= 0.6922, Training Accuracy= 0.515\n",
      "Epoch: 770, Loss= 0.6922, Training Accuracy= 0.513\n",
      "Epoch: 780, Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 790, Loss= 0.6920, Training Accuracy= 0.515\n",
      "Epoch: 800, Loss= 0.6920, Training Accuracy= 0.516\n",
      "Epoch: 810, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 820, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 830, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 840, Loss= 0.6920, Training Accuracy= 0.520\n",
      "Epoch: 850, Loss= 0.6917, Training Accuracy= 0.521\n",
      "Epoch: 860, Loss= 0.6914, Training Accuracy= 0.522\n",
      "Epoch: 870, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 880, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 890, Loss= 0.6920, Training Accuracy= 0.517\n",
      "Epoch: 900, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 910, Loss= 0.6942, Training Accuracy= 0.511\n",
      "Epoch: 920, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 930, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 940, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 950, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 960, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 970, Loss= 0.6922, Training Accuracy= 0.517\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 990, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 1000, Loss= 0.6920, Training Accuracy= 0.525\n",
      "Epoch: 1010, Loss= 0.6914, Training Accuracy= 0.525\n",
      "Epoch: 1020, Loss= 0.6906, Training Accuracy= 0.530\n",
      "Epoch: 1030, Loss= 0.6911, Training Accuracy= 0.527\n",
      "Epoch: 1040, Loss= 0.6916, Training Accuracy= 0.521\n",
      "Epoch: 1050, Loss= 0.6921, Training Accuracy= 0.522\n",
      "Epoch: 1060, Loss= 0.6914, Training Accuracy= 0.526\n",
      "Epoch: 1070, Loss= 0.6914, Training Accuracy= 0.525\n",
      "Epoch: 1080, Loss= 0.6918, Training Accuracy= 0.522\n",
      "Epoch: 1090, Loss= 0.6922, Training Accuracy= 0.522\n",
      "Epoch: 1100, Loss= 0.6915, Training Accuracy= 0.521\n",
      "Epoch: 1110, Loss= 0.6916, Training Accuracy= 0.522\n",
      "Epoch: 1120, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 1130, Loss= 0.6912, Training Accuracy= 0.521\n",
      "Epoch: 1140, Loss= 0.6917, Training Accuracy= 0.523\n",
      "Epoch: 1150, Loss= 0.6942, Training Accuracy= 0.514\n",
      "Epoch: 1160, Loss= 0.6922, Training Accuracy= 0.521\n",
      "Epoch: 1170, Loss= 0.6962, Training Accuracy= 0.509\n",
      "Epoch: 1180, Loss= 0.6930, Training Accuracy= 0.522\n",
      "Epoch: 1190, Loss= 0.6932, Training Accuracy= 0.519\n",
      "Epoch: 1200, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 1210, Loss= 0.6940, Training Accuracy= 0.514\n",
      "Epoch: 1220, Loss= 0.6909, Training Accuracy= 0.528\n",
      "Epoch: 1230, Loss= 0.6925, Training Accuracy= 0.519\n",
      "Epoch: 1240, Loss= 0.6891, Training Accuracy= 0.533\n",
      "Epoch: 1250, Loss= 0.6910, Training Accuracy= 0.526\n",
      "Epoch: 1260, Loss= 0.6887, Training Accuracy= 0.535\n",
      "Epoch: 1270, Loss= 0.6875, Training Accuracy= 0.535\n",
      "Epoch: 1280, Loss= 0.6887, Training Accuracy= 0.538\n",
      "Epoch: 1290, Loss= 0.6878, Training Accuracy= 0.540\n",
      "Epoch: 1300, Loss= 0.6902, Training Accuracy= 0.530\n",
      "Epoch: 1310, Loss= 0.6883, Training Accuracy= 0.534\n",
      "Epoch: 1320, Loss= 0.6890, Training Accuracy= 0.536\n",
      "Epoch: 1330, Loss= 0.6888, Training Accuracy= 0.535\n",
      "Epoch: 1340, Loss= 0.6915, Training Accuracy= 0.523\n",
      "Epoch: 1350, Loss= 0.6856, Training Accuracy= 0.546\n",
      "Epoch: 1360, Loss= 0.6921, Training Accuracy= 0.527\n",
      "Epoch: 1370, Loss= 0.6918, Training Accuracy= 0.529\n",
      "Epoch: 1380, Loss= 0.6869, Training Accuracy= 0.544\n",
      "Epoch: 1390, Loss= 0.6892, Training Accuracy= 0.538\n",
      "Epoch: 1400, Loss= 0.6894, Training Accuracy= 0.532\n",
      "Epoch: 1410, Loss= 0.6907, Training Accuracy= 0.526\n",
      "Epoch: 1420, Loss= 0.6943, Training Accuracy= 0.508\n",
      "Epoch: 1430, Loss= 0.6888, Training Accuracy= 0.533\n",
      "Epoch: 1440, Loss= 0.6890, Training Accuracy= 0.529\n",
      "Epoch: 1450, Loss= 0.6861, Training Accuracy= 0.542\n",
      "Epoch: 1460, Loss= 0.6864, Training Accuracy= 0.539\n",
      "Epoch: 1470, Loss= 0.6870, Training Accuracy= 0.545\n",
      "Epoch: 1480, Loss= 0.7156, Training Accuracy= 0.504\n",
      "Epoch: 1490, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 1500, Loss= 0.6922, Training Accuracy= 0.522\n",
      "Epoch: 1510, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 1520, Loss= 0.6931, Training Accuracy= 0.509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1530, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 1540, Loss= 0.6924, Training Accuracy= 0.520\n",
      "Epoch: 1550, Loss= 0.6921, Training Accuracy= 0.523\n",
      "Epoch: 1560, Loss= 0.6920, Training Accuracy= 0.520\n",
      "Epoch: 1570, Loss= 0.6917, Training Accuracy= 0.522\n",
      "Epoch: 1580, Loss= 0.6913, Training Accuracy= 0.527\n",
      "Epoch: 1590, Loss= 0.6908, Training Accuracy= 0.530\n",
      "Epoch: 1600, Loss= 0.6904, Training Accuracy= 0.530\n",
      "Epoch: 1610, Loss= 0.6902, Training Accuracy= 0.534\n",
      "Epoch: 1620, Loss= 0.6900, Training Accuracy= 0.534\n",
      "Epoch: 1630, Loss= 0.6898, Training Accuracy= 0.533\n",
      "Epoch: 1640, Loss= 0.6892, Training Accuracy= 0.536\n",
      "Epoch: 1650, Loss= 0.6954, Training Accuracy= 0.509\n",
      "Epoch: 1660, Loss= 0.6884, Training Accuracy= 0.537\n",
      "Epoch: 1670, Loss= 0.6882, Training Accuracy= 0.539\n",
      "Epoch: 1680, Loss= 0.6892, Training Accuracy= 0.537\n",
      "Epoch: 1690, Loss= 0.6920, Training Accuracy= 0.522\n",
      "Epoch: 1700, Loss= 0.6878, Training Accuracy= 0.540\n",
      "Epoch: 1710, Loss= 0.6880, Training Accuracy= 0.537\n",
      "Epoch: 1720, Loss= 0.6884, Training Accuracy= 0.533\n",
      "Epoch: 1730, Loss= 0.6877, Training Accuracy= 0.539\n",
      "Epoch: 1740, Loss= 0.6869, Training Accuracy= 0.538\n",
      "Epoch: 1750, Loss= 0.6880, Training Accuracy= 0.544\n",
      "Epoch: 1760, Loss= 0.6859, Training Accuracy= 0.546\n",
      "Epoch: 1770, Loss= 0.6905, Training Accuracy= 0.529\n",
      "Epoch: 1780, Loss= 0.6928, Training Accuracy= 0.519\n",
      "Epoch: 1790, Loss= 0.6913, Training Accuracy= 0.523\n",
      "Epoch: 1800, Loss= 0.6892, Training Accuracy= 0.532\n",
      "Epoch: 1810, Loss= 0.6888, Training Accuracy= 0.535\n",
      "Epoch: 1820, Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 1830, Loss= 0.6919, Training Accuracy= 0.517\n",
      "Epoch: 1840, Loss= 0.6925, Training Accuracy= 0.509\n",
      "Epoch: 1850, Loss= 0.6920, Training Accuracy= 0.520\n",
      "Epoch: 1860, Loss= 0.6913, Training Accuracy= 0.525\n",
      "Epoch: 1870, Loss= 0.6910, Training Accuracy= 0.524\n",
      "Epoch: 1880, Loss= 0.6910, Training Accuracy= 0.526\n",
      "Epoch: 1890, Loss= 0.6902, Training Accuracy= 0.529\n",
      "Epoch: 1900, Loss= 0.6903, Training Accuracy= 0.530\n",
      "Epoch: 1910, Loss= 0.6895, Training Accuracy= 0.534\n",
      "Epoch: 1920, Loss= 0.6908, Training Accuracy= 0.522\n",
      "Epoch: 1930, Loss= 0.6902, Training Accuracy= 0.525\n",
      "Epoch: 1940, Loss= 0.6904, Training Accuracy= 0.531\n",
      "Epoch: 1950, Loss= 0.6939, Training Accuracy= 0.517\n",
      "Epoch: 1960, Loss= 0.6914, Training Accuracy= 0.517\n",
      "Epoch: 1970, Loss= 0.6898, Training Accuracy= 0.528\n",
      "Epoch: 1980, Loss= 0.6893, Training Accuracy= 0.529\n",
      "Epoch: 1990, Loss= 0.6908, Training Accuracy= 0.529\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4906\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.7051, Training Accuracy= 0.506\n",
      "Epoch: 10, Loss= 0.6992, Training Accuracy= 0.508\n",
      "Epoch: 20, Loss= 0.6970, Training Accuracy= 0.509\n",
      "Epoch: 30, Loss= 0.6958, Training Accuracy= 0.517\n",
      "Epoch: 40, Loss= 0.6947, Training Accuracy= 0.520\n",
      "Epoch: 50, Loss= 0.6927, Training Accuracy= 0.525\n",
      "Epoch: 60, Loss= 0.6913, Training Accuracy= 0.531\n",
      "Epoch: 70, Loss= 0.6905, Training Accuracy= 0.531\n",
      "Epoch: 80, Loss= 0.6894, Training Accuracy= 0.536\n",
      "Epoch: 90, Loss= 0.6875, Training Accuracy= 0.544\n",
      "Epoch: 100, Loss= 0.6862, Training Accuracy= 0.546\n",
      "Epoch: 110, Loss= 0.6857, Training Accuracy= 0.548\n",
      "Epoch: 120, Loss= 0.6874, Training Accuracy= 0.549\n",
      "Epoch: 130, Loss= 0.6946, Training Accuracy= 0.537\n",
      "Epoch: 140, Loss= 0.6913, Training Accuracy= 0.542\n",
      "Epoch: 150, Loss= 0.6851, Training Accuracy= 0.558\n",
      "Epoch: 160, Loss= 0.6881, Training Accuracy= 0.554\n",
      "Epoch: 170, Loss= 0.6857, Training Accuracy= 0.554\n",
      "Epoch: 180, Loss= 0.6853, Training Accuracy= 0.558\n",
      "Epoch: 190, Loss= 0.6987, Training Accuracy= 0.531\n",
      "Epoch: 200, Loss= 0.6979, Training Accuracy= 0.545\n",
      "Epoch: 210, Loss= 0.6885, Training Accuracy= 0.561\n",
      "Epoch: 220, Loss= 0.6816, Training Accuracy= 0.560\n",
      "Epoch: 230, Loss= 0.6841, Training Accuracy= 0.562\n",
      "Epoch: 240, Loss= 0.6874, Training Accuracy= 0.557\n",
      "Epoch: 250, Loss= 0.6831, Training Accuracy= 0.565\n",
      "Epoch: 260, Loss= 0.6832, Training Accuracy= 0.559\n",
      "Epoch: 270, Loss= 0.6797, Training Accuracy= 0.575\n",
      "Epoch: 280, Loss= 0.6809, Training Accuracy= 0.568\n",
      "Epoch: 290, Loss= 0.6754, Training Accuracy= 0.577\n",
      "Epoch: 300, Loss= 0.7071, Training Accuracy= 0.537\n",
      "Epoch: 310, Loss= 0.6790, Training Accuracy= 0.576\n",
      "Epoch: 320, Loss= 0.6731, Training Accuracy= 0.582\n",
      "Epoch: 330, Loss= 0.6725, Training Accuracy= 0.584\n",
      "Epoch: 340, Loss= 0.6758, Training Accuracy= 0.576\n",
      "Epoch: 350, Loss= 0.6779, Training Accuracy= 0.575\n",
      "Epoch: 360, Loss= 0.6688, Training Accuracy= 0.588\n",
      "Epoch: 370, Loss= 0.6676, Training Accuracy= 0.591\n",
      "Epoch: 380, Loss= 0.6618, Training Accuracy= 0.594\n",
      "Epoch: 390, Loss= 0.6886, Training Accuracy= 0.556\n",
      "Epoch: 400, Loss= 0.6635, Training Accuracy= 0.598\n",
      "Epoch: 410, Loss= 0.6698, Training Accuracy= 0.587\n",
      "Epoch: 420, Loss= 0.6645, Training Accuracy= 0.594\n",
      "Epoch: 430, Loss= 0.7018, Training Accuracy= 0.544\n",
      "Epoch: 440, Loss= 0.6619, Training Accuracy= 0.595\n",
      "Epoch: 450, Loss= 0.6561, Training Accuracy= 0.605\n",
      "Epoch: 460, Loss= 0.6683, Training Accuracy= 0.584\n",
      "Epoch: 470, Loss= 0.6679, Training Accuracy= 0.590\n",
      "Epoch: 480, Loss= 0.6637, Training Accuracy= 0.595\n",
      "Epoch: 490, Loss= 0.6711, Training Accuracy= 0.590\n",
      "Epoch: 500, Loss= 0.6601, Training Accuracy= 0.599\n",
      "Epoch: 510, Loss= 0.6566, Training Accuracy= 0.609\n",
      "Epoch: 520, Loss= 0.6629, Training Accuracy= 0.590\n",
      "Epoch: 530, Loss= 0.6705, Training Accuracy= 0.588\n",
      "Epoch: 540, Loss= 0.6717, Training Accuracy= 0.584\n",
      "Epoch: 550, Loss= 0.6996, Training Accuracy= 0.530\n",
      "Epoch: 560, Loss= 0.7223, Training Accuracy= 0.496\n",
      "Epoch: 570, Loss= 0.7193, Training Accuracy= 0.496\n",
      "Epoch: 580, Loss= 0.7184, Training Accuracy= 0.496\n",
      "Epoch: 590, Loss= 0.6965, Training Accuracy= 0.507\n",
      "Epoch: 600, Loss= 0.6947, Training Accuracy= 0.515\n",
      "Epoch: 610, Loss= 0.6937, Training Accuracy= 0.521\n",
      "Epoch: 620, Loss= 0.6908, Training Accuracy= 0.529\n",
      "Epoch: 630, Loss= 0.6895, Training Accuracy= 0.537\n",
      "Epoch: 640, Loss= 0.6907, Training Accuracy= 0.530\n",
      "Epoch: 650, Loss= 0.6936, Training Accuracy= 0.523\n",
      "Epoch: 660, Loss= 0.6902, Training Accuracy= 0.530\n",
      "Epoch: 670, Loss= 0.6897, Training Accuracy= 0.539\n",
      "Epoch: 680, Loss= 0.6890, Training Accuracy= 0.541\n",
      "Epoch: 690, Loss= 0.6882, Training Accuracy= 0.541\n",
      "Epoch: 700, Loss= 0.6852, Training Accuracy= 0.554\n",
      "Epoch: 710, Loss= 0.6853, Training Accuracy= 0.547\n",
      "Epoch: 720, Loss= 0.6903, Training Accuracy= 0.535\n",
      "Epoch: 730, Loss= 0.6945, Training Accuracy= 0.521\n",
      "Epoch: 740, Loss= 0.6873, Training Accuracy= 0.544\n",
      "Epoch: 750, Loss= 0.6872, Training Accuracy= 0.541\n",
      "Epoch: 760, Loss= 0.6865, Training Accuracy= 0.545\n",
      "Epoch: 770, Loss= 0.6896, Training Accuracy= 0.537\n",
      "Epoch: 780, Loss= 0.6880, Training Accuracy= 0.544\n",
      "Epoch: 790, Loss= 0.6889, Training Accuracy= 0.545\n",
      "Epoch: 800, Loss= 0.6947, Training Accuracy= 0.527\n",
      "Epoch: 810, Loss= 0.6863, Training Accuracy= 0.549\n",
      "Epoch: 820, Loss= 0.6876, Training Accuracy= 0.544\n",
      "Epoch: 830, Loss= 0.6966, Training Accuracy= 0.529\n",
      "Epoch: 840, Loss= 0.6845, Training Accuracy= 0.553\n",
      "Epoch: 850, Loss= 0.6905, Training Accuracy= 0.533\n",
      "Epoch: 860, Loss= 0.6911, Training Accuracy= 0.526\n",
      "Epoch: 870, Loss= 0.6901, Training Accuracy= 0.532\n",
      "Epoch: 880, Loss= 0.6901, Training Accuracy= 0.534\n",
      "Epoch: 890, Loss= 0.6870, Training Accuracy= 0.545\n",
      "Epoch: 900, Loss= 0.6852, Training Accuracy= 0.549\n",
      "Epoch: 910, Loss= 0.6861, Training Accuracy= 0.550\n",
      "Epoch: 920, Loss= 0.6853, Training Accuracy= 0.553\n",
      "Epoch: 930, Loss= 0.6849, Training Accuracy= 0.557\n",
      "Epoch: 940, Loss= 0.6849, Training Accuracy= 0.549\n",
      "Epoch: 950, Loss= 0.6828, Training Accuracy= 0.558\n",
      "Epoch: 960, Loss= 0.6862, Training Accuracy= 0.547\n",
      "Epoch: 970, Loss= 0.6885, Training Accuracy= 0.543\n",
      "Epoch: 980, Loss= 0.6844, Training Accuracy= 0.558\n",
      "Epoch: 990, Loss= 0.6812, Training Accuracy= 0.564\n",
      "Epoch: 1000, Loss= 0.6900, Training Accuracy= 0.546\n",
      "Epoch: 1010, Loss= 0.6836, Training Accuracy= 0.557\n",
      "Epoch: 1020, Loss= 0.6814, Training Accuracy= 0.560\n",
      "Epoch: 1030, Loss= 0.6888, Training Accuracy= 0.550\n",
      "Epoch: 1040, Loss= 0.6790, Training Accuracy= 0.567\n",
      "Epoch: 1050, Loss= 0.6895, Training Accuracy= 0.541\n",
      "Epoch: 1060, Loss= 0.6847, Training Accuracy= 0.555\n",
      "Epoch: 1070, Loss= 0.6882, Training Accuracy= 0.554\n",
      "Epoch: 1080, Loss= 0.6862, Training Accuracy= 0.549\n",
      "Epoch: 1090, Loss= 0.6893, Training Accuracy= 0.542\n",
      "Epoch: 1100, Loss= 0.6882, Training Accuracy= 0.543\n",
      "Epoch: 1110, Loss= 0.6810, Training Accuracy= 0.561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1120, Loss= 0.6920, Training Accuracy= 0.538\n",
      "Epoch: 1130, Loss= 0.6916, Training Accuracy= 0.526\n",
      "Epoch: 1140, Loss= 0.6900, Training Accuracy= 0.539\n",
      "Epoch: 1150, Loss= 0.7025, Training Accuracy= 0.510\n",
      "Epoch: 1160, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 1170, Loss= 0.6953, Training Accuracy= 0.514\n",
      "Epoch: 1180, Loss= 0.6949, Training Accuracy= 0.513\n",
      "Epoch: 1190, Loss= 0.6903, Training Accuracy= 0.523\n",
      "Epoch: 1200, Loss= 0.6931, Training Accuracy= 0.519\n",
      "Epoch: 1210, Loss= 0.6933, Training Accuracy= 0.523\n",
      "Epoch: 1220, Loss= 0.6926, Training Accuracy= 0.525\n",
      "Epoch: 1230, Loss= 0.6914, Training Accuracy= 0.529\n",
      "Epoch: 1240, Loss= 0.6913, Training Accuracy= 0.530\n",
      "Epoch: 1250, Loss= 0.6902, Training Accuracy= 0.539\n",
      "Epoch: 1260, Loss= 0.6933, Training Accuracy= 0.520\n",
      "Epoch: 1270, Loss= 0.6915, Training Accuracy= 0.528\n",
      "Epoch: 1280, Loss= 0.6894, Training Accuracy= 0.532\n",
      "Epoch: 1290, Loss= 0.6888, Training Accuracy= 0.535\n",
      "Epoch: 1300, Loss= 0.6911, Training Accuracy= 0.528\n",
      "Epoch: 1310, Loss= 0.6884, Training Accuracy= 0.538\n",
      "Epoch: 1320, Loss= 0.6887, Training Accuracy= 0.539\n",
      "Epoch: 1330, Loss= 0.6895, Training Accuracy= 0.533\n",
      "Epoch: 1340, Loss= 0.6911, Training Accuracy= 0.535\n",
      "Epoch: 1350, Loss= 0.6913, Training Accuracy= 0.527\n",
      "Epoch: 1360, Loss= 0.6913, Training Accuracy= 0.528\n",
      "Epoch: 1370, Loss= 0.6892, Training Accuracy= 0.544\n",
      "Epoch: 1380, Loss= 0.6896, Training Accuracy= 0.540\n",
      "Epoch: 1390, Loss= 0.6874, Training Accuracy= 0.547\n",
      "Epoch: 1400, Loss= 0.6914, Training Accuracy= 0.534\n",
      "Epoch: 1410, Loss= 0.6895, Training Accuracy= 0.540\n",
      "Epoch: 1420, Loss= 0.6895, Training Accuracy= 0.540\n",
      "Epoch: 1430, Loss= 0.6891, Training Accuracy= 0.539\n",
      "Epoch: 1440, Loss= 0.6877, Training Accuracy= 0.545\n",
      "Epoch: 1450, Loss= 0.6881, Training Accuracy= 0.542\n",
      "Epoch: 1460, Loss= 0.6864, Training Accuracy= 0.551\n",
      "Epoch: 1470, Loss= 0.6871, Training Accuracy= 0.554\n",
      "Epoch: 1480, Loss= 0.6843, Training Accuracy= 0.553\n",
      "Epoch: 1490, Loss= 0.6858, Training Accuracy= 0.550\n",
      "Epoch: 1500, Loss= 0.6846, Training Accuracy= 0.551\n",
      "Epoch: 1510, Loss= 0.6849, Training Accuracy= 0.555\n",
      "Epoch: 1520, Loss= 0.6891, Training Accuracy= 0.540\n",
      "Epoch: 1530, Loss= 0.6935, Training Accuracy= 0.523\n",
      "Epoch: 1540, Loss= 0.6918, Training Accuracy= 0.530\n",
      "Epoch: 1550, Loss= 0.6903, Training Accuracy= 0.532\n",
      "Epoch: 1560, Loss= 0.6937, Training Accuracy= 0.518\n",
      "Epoch: 1570, Loss= 0.6949, Training Accuracy= 0.526\n",
      "Epoch: 1580, Loss= 0.6918, Training Accuracy= 0.527\n",
      "Epoch: 1590, Loss= 0.6932, Training Accuracy= 0.529\n",
      "Epoch: 1600, Loss= 0.6959, Training Accuracy= 0.510\n",
      "Epoch: 1610, Loss= 0.6947, Training Accuracy= 0.508\n",
      "Epoch: 1620, Loss= 0.6949, Training Accuracy= 0.506\n",
      "Epoch: 1630, Loss= 0.6949, Training Accuracy= 0.513\n",
      "Epoch: 1640, Loss= 0.6951, Training Accuracy= 0.507\n",
      "Epoch: 1650, Loss= 0.6922, Training Accuracy= 0.526\n",
      "Epoch: 1660, Loss= 0.6947, Training Accuracy= 0.512\n",
      "Epoch: 1670, Loss= 0.6917, Training Accuracy= 0.524\n",
      "Epoch: 1680, Loss= 0.6942, Training Accuracy= 0.518\n",
      "Epoch: 1690, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 1700, Loss= 0.6977, Training Accuracy= 0.501\n",
      "Epoch: 1710, Loss= 0.6941, Training Accuracy= 0.512\n",
      "Epoch: 1720, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 1730, Loss= 0.6937, Training Accuracy= 0.519\n",
      "Epoch: 1740, Loss= 0.6938, Training Accuracy= 0.512\n",
      "Epoch: 1750, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 1760, Loss= 0.6941, Training Accuracy= 0.507\n",
      "Epoch: 1770, Loss= 0.6980, Training Accuracy= 0.510\n",
      "Epoch: 1780, Loss= 0.6961, Training Accuracy= 0.506\n",
      "Epoch: 1790, Loss= 0.6958, Training Accuracy= 0.505\n",
      "Epoch: 1800, Loss= 0.6965, Training Accuracy= 0.509\n",
      "Epoch: 1810, Loss= 0.6950, Training Accuracy= 0.508\n",
      "Epoch: 1820, Loss= 0.6987, Training Accuracy= 0.509\n",
      "Epoch: 1830, Loss= 0.6938, Training Accuracy= 0.512\n",
      "Epoch: 1840, Loss= 0.6936, Training Accuracy= 0.517\n",
      "Epoch: 1850, Loss= 0.6975, Training Accuracy= 0.509\n",
      "Epoch: 1860, Loss= 0.6954, Training Accuracy= 0.505\n",
      "Epoch: 1870, Loss= 0.7058, Training Accuracy= 0.509\n",
      "Epoch: 1880, Loss= 0.6956, Training Accuracy= 0.507\n",
      "Epoch: 1890, Loss= 0.6977, Training Accuracy= 0.512\n",
      "Epoch: 1900, Loss= 0.6957, Training Accuracy= 0.512\n",
      "Epoch: 1910, Loss= 0.6965, Training Accuracy= 0.494\n",
      "Epoch: 1920, Loss= 0.6975, Training Accuracy= 0.503\n",
      "Epoch: 1930, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 1940, Loss= 0.7023, Training Accuracy= 0.496\n",
      "Epoch: 1950, Loss= 0.7044, Training Accuracy= 0.496\n",
      "Epoch: 1960, Loss= 0.7026, Training Accuracy= 0.496\n",
      "Epoch: 1970, Loss= 0.7029, Training Accuracy= 0.496\n",
      "Epoch: 1980, Loss= 0.6990, Training Accuracy= 0.493\n",
      "Epoch: 1990, Loss= 0.6951, Training Accuracy= 0.508\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.505\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.7169, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.7068, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.7045, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.7032, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.7020, Training Accuracy= 0.506\n",
      "Epoch: 50, Loss= 0.7007, Training Accuracy= 0.508\n",
      "Epoch: 60, Loss= 0.6996, Training Accuracy= 0.515\n",
      "Epoch: 70, Loss= 0.6988, Training Accuracy= 0.516\n",
      "Epoch: 80, Loss= 0.6972, Training Accuracy= 0.521\n",
      "Epoch: 90, Loss= 0.6945, Training Accuracy= 0.527\n",
      "Epoch: 100, Loss= 0.6928, Training Accuracy= 0.530\n",
      "Epoch: 110, Loss= 0.6926, Training Accuracy= 0.533\n",
      "Epoch: 120, Loss= 0.6937, Training Accuracy= 0.530\n",
      "Epoch: 130, Loss= 0.6925, Training Accuracy= 0.533\n",
      "Epoch: 140, Loss= 0.6902, Training Accuracy= 0.538\n",
      "Epoch: 150, Loss= 0.6869, Training Accuracy= 0.546\n",
      "Epoch: 160, Loss= 0.6847, Training Accuracy= 0.552\n",
      "Epoch: 170, Loss= 0.6842, Training Accuracy= 0.551\n",
      "Epoch: 180, Loss= 0.7007, Training Accuracy= 0.507\n",
      "Epoch: 190, Loss= 0.6961, Training Accuracy= 0.509\n",
      "Epoch: 200, Loss= 0.6931, Training Accuracy= 0.520\n",
      "Epoch: 210, Loss= 0.6898, Training Accuracy= 0.537\n",
      "Epoch: 220, Loss= 0.6915, Training Accuracy= 0.537\n",
      "Epoch: 230, Loss= 0.6934, Training Accuracy= 0.543\n",
      "Epoch: 240, Loss= 0.7007, Training Accuracy= 0.542\n",
      "Epoch: 250, Loss= 0.6969, Training Accuracy= 0.544\n",
      "Epoch: 260, Loss= 0.7010, Training Accuracy= 0.533\n",
      "Epoch: 270, Loss= 0.6882, Training Accuracy= 0.555\n",
      "Epoch: 280, Loss= 0.6804, Training Accuracy= 0.571\n",
      "Epoch: 290, Loss= 0.6825, Training Accuracy= 0.558\n",
      "Epoch: 300, Loss= 0.6897, Training Accuracy= 0.550\n",
      "Epoch: 310, Loss= 0.6984, Training Accuracy= 0.534\n",
      "Epoch: 320, Loss= 0.6847, Training Accuracy= 0.559\n",
      "Epoch: 330, Loss= 0.6833, Training Accuracy= 0.561\n",
      "Epoch: 340, Loss= 0.6785, Training Accuracy= 0.569\n",
      "Epoch: 350, Loss= 0.6723, Training Accuracy= 0.584\n",
      "Epoch: 360, Loss= 0.6732, Training Accuracy= 0.583\n",
      "Epoch: 370, Loss= 0.6785, Training Accuracy= 0.577\n",
      "Epoch: 380, Loss= 0.6728, Training Accuracy= 0.580\n",
      "Epoch: 390, Loss= 0.6755, Training Accuracy= 0.580\n",
      "Epoch: 400, Loss= 0.6776, Training Accuracy= 0.570\n",
      "Epoch: 410, Loss= 0.6738, Training Accuracy= 0.583\n",
      "Epoch: 420, Loss= 0.6938, Training Accuracy= 0.550\n",
      "Epoch: 430, Loss= 0.6787, Training Accuracy= 0.573\n",
      "Epoch: 440, Loss= 0.6750, Training Accuracy= 0.576\n",
      "Epoch: 450, Loss= 0.6890, Training Accuracy= 0.557\n",
      "Epoch: 460, Loss= 0.6705, Training Accuracy= 0.583\n",
      "Epoch: 470, Loss= 0.6810, Training Accuracy= 0.571\n",
      "Epoch: 480, Loss= 0.6817, Training Accuracy= 0.572\n",
      "Epoch: 490, Loss= 0.6811, Training Accuracy= 0.566\n",
      "Epoch: 500, Loss= 0.6759, Training Accuracy= 0.580\n",
      "Epoch: 510, Loss= 0.6790, Training Accuracy= 0.577\n",
      "Epoch: 520, Loss= 0.6921, Training Accuracy= 0.576\n",
      "Epoch: 530, Loss= 0.6668, Training Accuracy= 0.596\n",
      "Epoch: 540, Loss= 0.6683, Training Accuracy= 0.592\n",
      "Epoch: 550, Loss= 0.6760, Training Accuracy= 0.580\n",
      "Epoch: 560, Loss= 0.6636, Training Accuracy= 0.595\n",
      "Epoch: 570, Loss= 0.6879, Training Accuracy= 0.557\n",
      "Epoch: 580, Loss= 0.6666, Training Accuracy= 0.591\n",
      "Epoch: 590, Loss= 0.6711, Training Accuracy= 0.583\n",
      "Epoch: 600, Loss= 0.6704, Training Accuracy= 0.586\n",
      "Epoch: 610, Loss= 0.6739, Training Accuracy= 0.585\n",
      "Epoch: 620, Loss= 0.6773, Training Accuracy= 0.572\n",
      "Epoch: 630, Loss= 0.6859, Training Accuracy= 0.562\n",
      "Epoch: 640, Loss= 0.6798, Training Accuracy= 0.573\n",
      "Epoch: 650, Loss= 0.6956, Training Accuracy= 0.534\n",
      "Epoch: 660, Loss= 0.6736, Training Accuracy= 0.573\n",
      "Epoch: 670, Loss= 0.6786, Training Accuracy= 0.568\n",
      "Epoch: 680, Loss= 0.6642, Training Accuracy= 0.596\n",
      "Epoch: 690, Loss= 0.6782, Training Accuracy= 0.572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 700, Loss= 0.6656, Training Accuracy= 0.598\n",
      "Epoch: 710, Loss= 0.6677, Training Accuracy= 0.585\n",
      "Epoch: 720, Loss= 0.6716, Training Accuracy= 0.580\n",
      "Epoch: 730, Loss= 0.6893, Training Accuracy= 0.550\n",
      "Epoch: 740, Loss= 0.6927, Training Accuracy= 0.547\n",
      "Epoch: 750, Loss= 0.6679, Training Accuracy= 0.591\n",
      "Epoch: 760, Loss= 0.6656, Training Accuracy= 0.593\n",
      "Epoch: 770, Loss= 0.6607, Training Accuracy= 0.604\n",
      "Epoch: 780, Loss= 0.6624, Training Accuracy= 0.597\n",
      "Epoch: 790, Loss= 0.6786, Training Accuracy= 0.570\n",
      "Epoch: 800, Loss= 0.6602, Training Accuracy= 0.604\n",
      "Epoch: 810, Loss= 0.6690, Training Accuracy= 0.588\n",
      "Epoch: 820, Loss= 0.6607, Training Accuracy= 0.601\n",
      "Epoch: 830, Loss= 0.6768, Training Accuracy= 0.572\n",
      "Epoch: 840, Loss= 0.6675, Training Accuracy= 0.595\n",
      "Epoch: 850, Loss= 0.6685, Training Accuracy= 0.588\n",
      "Epoch: 860, Loss= 0.6773, Training Accuracy= 0.573\n",
      "Epoch: 870, Loss= 0.6670, Training Accuracy= 0.584\n",
      "Epoch: 880, Loss= 0.6786, Training Accuracy= 0.569\n",
      "Epoch: 890, Loss= 0.6910, Training Accuracy= 0.546\n",
      "Epoch: 900, Loss= 0.6809, Training Accuracy= 0.569\n",
      "Epoch: 910, Loss= 0.6672, Training Accuracy= 0.592\n",
      "Epoch: 920, Loss= 0.6649, Training Accuracy= 0.593\n",
      "Epoch: 930, Loss= 0.6824, Training Accuracy= 0.563\n",
      "Epoch: 940, Loss= 0.6812, Training Accuracy= 0.565\n",
      "Epoch: 950, Loss= 0.6615, Training Accuracy= 0.600\n",
      "Epoch: 960, Loss= 0.6650, Training Accuracy= 0.597\n",
      "Epoch: 970, Loss= 0.6646, Training Accuracy= 0.594\n",
      "Epoch: 980, Loss= 0.6911, Training Accuracy= 0.548\n",
      "Epoch: 990, Loss= 0.6801, Training Accuracy= 0.568\n",
      "Epoch: 1000, Loss= 0.6634, Training Accuracy= 0.600\n",
      "Epoch: 1010, Loss= 0.6635, Training Accuracy= 0.598\n",
      "Epoch: 1020, Loss= 0.6645, Training Accuracy= 0.599\n",
      "Epoch: 1030, Loss= 0.6661, Training Accuracy= 0.590\n",
      "Epoch: 1040, Loss= 0.6752, Training Accuracy= 0.577\n",
      "Epoch: 1050, Loss= 0.6629, Training Accuracy= 0.598\n",
      "Epoch: 1060, Loss= 0.6672, Training Accuracy= 0.592\n",
      "Epoch: 1070, Loss= 0.6610, Training Accuracy= 0.599\n",
      "Epoch: 1080, Loss= 0.6795, Training Accuracy= 0.563\n",
      "Epoch: 1090, Loss= 0.6749, Training Accuracy= 0.581\n",
      "Epoch: 1100, Loss= 0.6714, Training Accuracy= 0.590\n",
      "Epoch: 1110, Loss= 0.6671, Training Accuracy= 0.595\n",
      "Epoch: 1120, Loss= 0.6754, Training Accuracy= 0.579\n",
      "Epoch: 1130, Loss= 0.6710, Training Accuracy= 0.583\n",
      "Epoch: 1140, Loss= 0.6746, Training Accuracy= 0.574\n",
      "Epoch: 1150, Loss= 0.6711, Training Accuracy= 0.582\n",
      "Epoch: 1160, Loss= 0.6699, Training Accuracy= 0.586\n",
      "Epoch: 1170, Loss= 0.6722, Training Accuracy= 0.578\n",
      "Epoch: 1180, Loss= 0.6642, Training Accuracy= 0.597\n",
      "Epoch: 1190, Loss= 0.6687, Training Accuracy= 0.586\n",
      "Epoch: 1200, Loss= 0.6648, Training Accuracy= 0.596\n",
      "Epoch: 1210, Loss= 0.6809, Training Accuracy= 0.562\n",
      "Epoch: 1220, Loss= 0.6902, Training Accuracy= 0.556\n",
      "Epoch: 1230, Loss= 0.6707, Training Accuracy= 0.582\n",
      "Epoch: 1240, Loss= 0.6803, Training Accuracy= 0.567\n",
      "Epoch: 1250, Loss= 0.6736, Training Accuracy= 0.578\n",
      "Epoch: 1260, Loss= 0.6765, Training Accuracy= 0.569\n",
      "Epoch: 1270, Loss= 0.6720, Training Accuracy= 0.583\n",
      "Epoch: 1280, Loss= 0.6717, Training Accuracy= 0.583\n",
      "Epoch: 1290, Loss= 0.6674, Training Accuracy= 0.591\n",
      "Epoch: 1300, Loss= 0.6697, Training Accuracy= 0.587\n",
      "Epoch: 1310, Loss= 0.6809, Training Accuracy= 0.562\n",
      "Epoch: 1320, Loss= 0.6744, Training Accuracy= 0.578\n",
      "Epoch: 1330, Loss= 0.6664, Training Accuracy= 0.590\n",
      "Epoch: 1340, Loss= 0.6712, Training Accuracy= 0.586\n",
      "Epoch: 1350, Loss= 0.6717, Training Accuracy= 0.577\n",
      "Epoch: 1360, Loss= 0.6676, Training Accuracy= 0.593\n",
      "Epoch: 1370, Loss= 0.6780, Training Accuracy= 0.572\n",
      "Epoch: 1380, Loss= 0.6776, Training Accuracy= 0.575\n",
      "Epoch: 1390, Loss= 0.6688, Training Accuracy= 0.587\n",
      "Epoch: 1400, Loss= 0.6649, Training Accuracy= 0.592\n",
      "Epoch: 1410, Loss= 0.6800, Training Accuracy= 0.564\n",
      "Epoch: 1420, Loss= 0.6705, Training Accuracy= 0.589\n",
      "Epoch: 1430, Loss= 0.6637, Training Accuracy= 0.596\n",
      "Epoch: 1440, Loss= 0.6631, Training Accuracy= 0.597\n",
      "Epoch: 1450, Loss= 0.6654, Training Accuracy= 0.588\n",
      "Epoch: 1460, Loss= 0.6805, Training Accuracy= 0.567\n",
      "Epoch: 1470, Loss= 0.6826, Training Accuracy= 0.559\n",
      "Epoch: 1480, Loss= 0.6747, Training Accuracy= 0.577\n",
      "Epoch: 1490, Loss= 0.6776, Training Accuracy= 0.572\n",
      "Epoch: 1500, Loss= 0.6695, Training Accuracy= 0.592\n",
      "Epoch: 1510, Loss= 0.6756, Training Accuracy= 0.569\n",
      "Epoch: 1520, Loss= 0.6611, Training Accuracy= 0.606\n",
      "Epoch: 1530, Loss= 0.7027, Training Accuracy= 0.517\n",
      "Epoch: 1540, Loss= 0.6765, Training Accuracy= 0.567\n",
      "Epoch: 1550, Loss= 0.6763, Training Accuracy= 0.576\n",
      "Epoch: 1560, Loss= 0.6752, Training Accuracy= 0.568\n",
      "Epoch: 1570, Loss= 0.6682, Training Accuracy= 0.594\n",
      "Epoch: 1580, Loss= 0.6800, Training Accuracy= 0.567\n",
      "Epoch: 1590, Loss= 0.6709, Training Accuracy= 0.586\n",
      "Epoch: 1600, Loss= 0.6805, Training Accuracy= 0.561\n",
      "Epoch: 1610, Loss= 0.6734, Training Accuracy= 0.576\n",
      "Epoch: 1620, Loss= 0.6824, Training Accuracy= 0.561\n",
      "Epoch: 1630, Loss= 0.6747, Training Accuracy= 0.571\n",
      "Epoch: 1640, Loss= 0.6752, Training Accuracy= 0.575\n",
      "Epoch: 1650, Loss= 0.6652, Training Accuracy= 0.592\n",
      "Epoch: 1660, Loss= 0.6717, Training Accuracy= 0.579\n",
      "Epoch: 1670, Loss= 0.6693, Training Accuracy= 0.590\n",
      "Epoch: 1680, Loss= 0.6701, Training Accuracy= 0.583\n",
      "Epoch: 1690, Loss= 0.6651, Training Accuracy= 0.592\n",
      "Epoch: 1700, Loss= 0.6748, Training Accuracy= 0.580\n",
      "Epoch: 1710, Loss= 0.6589, Training Accuracy= 0.605\n",
      "Epoch: 1720, Loss= 0.6676, Training Accuracy= 0.591\n",
      "Epoch: 1730, Loss= 0.6696, Training Accuracy= 0.584\n",
      "Epoch: 1740, Loss= 0.6607, Training Accuracy= 0.602\n",
      "Epoch: 1750, Loss= 0.6678, Training Accuracy= 0.588\n",
      "Epoch: 1760, Loss= 0.6621, Training Accuracy= 0.596\n",
      "Epoch: 1770, Loss= 0.6948, Training Accuracy= 0.541\n",
      "Epoch: 1780, Loss= 0.6595, Training Accuracy= 0.604\n",
      "Epoch: 1790, Loss= 0.6776, Training Accuracy= 0.568\n",
      "Epoch: 1800, Loss= 0.6809, Training Accuracy= 0.572\n",
      "Epoch: 1810, Loss= 0.6602, Training Accuracy= 0.602\n",
      "Epoch: 1820, Loss= 0.6957, Training Accuracy= 0.550\n",
      "Epoch: 1830, Loss= 0.6806, Training Accuracy= 0.564\n",
      "Epoch: 1840, Loss= 0.6673, Training Accuracy= 0.592\n",
      "Epoch: 1850, Loss= 0.6792, Training Accuracy= 0.569\n",
      "Epoch: 1860, Loss= 0.6720, Training Accuracy= 0.579\n",
      "Epoch: 1870, Loss= 0.6624, Training Accuracy= 0.598\n",
      "Epoch: 1880, Loss= 0.6667, Training Accuracy= 0.593\n",
      "Epoch: 1890, Loss= 0.6728, Training Accuracy= 0.582\n",
      "Epoch: 1900, Loss= 0.6672, Training Accuracy= 0.593\n",
      "Epoch: 1910, Loss= 0.6884, Training Accuracy= 0.542\n",
      "Epoch: 1920, Loss= 0.6691, Training Accuracy= 0.587\n",
      "Epoch: 1930, Loss= 0.6674, Training Accuracy= 0.591\n",
      "Epoch: 1940, Loss= 0.6821, Training Accuracy= 0.566\n",
      "Epoch: 1950, Loss= 0.6690, Training Accuracy= 0.587\n",
      "Epoch: 1960, Loss= 0.6707, Training Accuracy= 0.579\n",
      "Epoch: 1970, Loss= 0.6779, Training Accuracy= 0.571\n",
      "Epoch: 1980, Loss= 0.6728, Training Accuracy= 0.580\n",
      "Epoch: 1990, Loss= 0.6796, Training Accuracy= 0.566\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4974\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.9245, Training Accuracy= 0.499\n",
      "Epoch: 10, Loss= 0.7547, Training Accuracy= 0.499\n",
      "Epoch: 20, Loss= 0.7369, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.7290, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.7234, Training Accuracy= 0.504\n",
      "Epoch: 50, Loss= 0.7206, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.7171, Training Accuracy= 0.506\n",
      "Epoch: 70, Loss= 0.7145, Training Accuracy= 0.506\n",
      "Epoch: 80, Loss= 0.7148, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.7124, Training Accuracy= 0.511\n",
      "Epoch: 100, Loss= 0.7090, Training Accuracy= 0.519\n",
      "Epoch: 110, Loss= 0.7067, Training Accuracy= 0.522\n",
      "Epoch: 120, Loss= 0.7089, Training Accuracy= 0.524\n",
      "Epoch: 130, Loss= 0.7139, Training Accuracy= 0.522\n",
      "Epoch: 140, Loss= 0.7029, Training Accuracy= 0.533\n",
      "Epoch: 150, Loss= 0.7002, Training Accuracy= 0.536\n",
      "Epoch: 160, Loss= 0.6898, Training Accuracy= 0.555\n",
      "Epoch: 170, Loss= 0.6862, Training Accuracy= 0.558\n",
      "Epoch: 180, Loss= 0.7009, Training Accuracy= 0.540\n",
      "Epoch: 190, Loss= 0.6950, Training Accuracy= 0.543\n",
      "Epoch: 200, Loss= 0.6893, Training Accuracy= 0.554\n",
      "Epoch: 210, Loss= 0.6894, Training Accuracy= 0.548\n",
      "Epoch: 220, Loss= 0.6984, Training Accuracy= 0.533\n",
      "Epoch: 230, Loss= 0.6929, Training Accuracy= 0.548\n",
      "Epoch: 240, Loss= 0.6945, Training Accuracy= 0.543\n",
      "Epoch: 250, Loss= 0.6856, Training Accuracy= 0.566\n",
      "Epoch: 260, Loss= 0.6801, Training Accuracy= 0.570\n",
      "Epoch: 270, Loss= 0.6775, Training Accuracy= 0.577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 280, Loss= 0.6725, Training Accuracy= 0.582\n",
      "Epoch: 290, Loss= 0.6730, Training Accuracy= 0.588\n",
      "Epoch: 300, Loss= 0.6768, Training Accuracy= 0.573\n",
      "Epoch: 310, Loss= 0.6848, Training Accuracy= 0.564\n",
      "Epoch: 320, Loss= 0.6887, Training Accuracy= 0.557\n",
      "Epoch: 330, Loss= 0.6809, Training Accuracy= 0.567\n",
      "Epoch: 340, Loss= 0.6806, Training Accuracy= 0.572\n",
      "Epoch: 350, Loss= 0.7151, Training Accuracy= 0.505\n",
      "Epoch: 360, Loss= 0.6726, Training Accuracy= 0.582\n",
      "Epoch: 370, Loss= 0.6829, Training Accuracy= 0.570\n",
      "Epoch: 380, Loss= 0.6667, Training Accuracy= 0.593\n",
      "Epoch: 390, Loss= 0.6748, Training Accuracy= 0.580\n",
      "Epoch: 400, Loss= 0.6687, Training Accuracy= 0.584\n",
      "Epoch: 410, Loss= 0.6682, Training Accuracy= 0.590\n",
      "Epoch: 420, Loss= 0.6674, Training Accuracy= 0.591\n",
      "Epoch: 430, Loss= 0.6972, Training Accuracy= 0.544\n",
      "Epoch: 440, Loss= 0.6888, Training Accuracy= 0.558\n",
      "Epoch: 450, Loss= 0.6815, Training Accuracy= 0.569\n",
      "Epoch: 460, Loss= 0.6743, Training Accuracy= 0.580\n",
      "Epoch: 470, Loss= 0.6706, Training Accuracy= 0.581\n",
      "Epoch: 480, Loss= 0.6792, Training Accuracy= 0.575\n",
      "Epoch: 490, Loss= 0.6828, Training Accuracy= 0.567\n",
      "Epoch: 500, Loss= 0.6706, Training Accuracy= 0.586\n",
      "Epoch: 510, Loss= 0.6806, Training Accuracy= 0.567\n",
      "Epoch: 520, Loss= 0.6658, Training Accuracy= 0.599\n",
      "Epoch: 530, Loss= 0.6658, Training Accuracy= 0.589\n",
      "Epoch: 540, Loss= 0.6680, Training Accuracy= 0.595\n",
      "Epoch: 550, Loss= 0.6673, Training Accuracy= 0.592\n",
      "Epoch: 560, Loss= 0.6627, Training Accuracy= 0.604\n",
      "Epoch: 570, Loss= 0.6709, Training Accuracy= 0.588\n",
      "Epoch: 580, Loss= 0.6762, Training Accuracy= 0.577\n",
      "Epoch: 590, Loss= 0.6691, Training Accuracy= 0.590\n",
      "Epoch: 600, Loss= 0.6696, Training Accuracy= 0.587\n",
      "Epoch: 610, Loss= 0.6965, Training Accuracy= 0.521\n",
      "Epoch: 620, Loss= 0.6950, Training Accuracy= 0.521\n",
      "Epoch: 630, Loss= 0.6767, Training Accuracy= 0.575\n",
      "Epoch: 640, Loss= 0.6739, Training Accuracy= 0.578\n",
      "Epoch: 650, Loss= 0.6686, Training Accuracy= 0.590\n",
      "Epoch: 660, Loss= 0.6700, Training Accuracy= 0.588\n",
      "Epoch: 670, Loss= 0.6800, Training Accuracy= 0.571\n",
      "Epoch: 680, Loss= 0.6682, Training Accuracy= 0.590\n",
      "Epoch: 690, Loss= 0.6800, Training Accuracy= 0.574\n",
      "Epoch: 700, Loss= 0.6615, Training Accuracy= 0.593\n",
      "Epoch: 710, Loss= 0.6790, Training Accuracy= 0.577\n",
      "Epoch: 720, Loss= 0.6698, Training Accuracy= 0.589\n",
      "Epoch: 730, Loss= 0.6733, Training Accuracy= 0.582\n",
      "Epoch: 740, Loss= 0.6810, Training Accuracy= 0.557\n",
      "Epoch: 750, Loss= 0.6758, Training Accuracy= 0.576\n",
      "Epoch: 760, Loss= 0.7039, Training Accuracy= 0.535\n",
      "Epoch: 770, Loss= 0.6647, Training Accuracy= 0.593\n",
      "Epoch: 780, Loss= 0.6661, Training Accuracy= 0.593\n",
      "Epoch: 790, Loss= 0.6663, Training Accuracy= 0.590\n",
      "Epoch: 800, Loss= 0.6943, Training Accuracy= 0.548\n",
      "Epoch: 810, Loss= 0.7001, Training Accuracy= 0.538\n",
      "Epoch: 820, Loss= 0.6667, Training Accuracy= 0.594\n",
      "Epoch: 830, Loss= 0.6710, Training Accuracy= 0.583\n",
      "Epoch: 840, Loss= 0.6680, Training Accuracy= 0.587\n",
      "Epoch: 850, Loss= 0.6999, Training Accuracy= 0.510\n",
      "Epoch: 860, Loss= 0.6919, Training Accuracy= 0.531\n",
      "Epoch: 870, Loss= 0.6875, Training Accuracy= 0.543\n",
      "Epoch: 880, Loss= 0.6897, Training Accuracy= 0.541\n",
      "Epoch: 890, Loss= 0.6889, Training Accuracy= 0.540\n",
      "Epoch: 900, Loss= 0.6808, Training Accuracy= 0.564\n",
      "Epoch: 910, Loss= 0.6780, Training Accuracy= 0.569\n",
      "Epoch: 920, Loss= 0.6785, Training Accuracy= 0.569\n",
      "Epoch: 930, Loss= 0.6715, Training Accuracy= 0.578\n",
      "Epoch: 940, Loss= 0.6715, Training Accuracy= 0.588\n",
      "Epoch: 950, Loss= 0.6727, Training Accuracy= 0.580\n",
      "Epoch: 960, Loss= 0.6698, Training Accuracy= 0.589\n",
      "Epoch: 970, Loss= 0.6803, Training Accuracy= 0.567\n",
      "Epoch: 980, Loss= 0.6944, Training Accuracy= 0.522\n",
      "Epoch: 990, Loss= 0.6933, Training Accuracy= 0.525\n",
      "Epoch: 1000, Loss= 0.6910, Training Accuracy= 0.534\n",
      "Epoch: 1010, Loss= 0.6879, Training Accuracy= 0.544\n",
      "Epoch: 1020, Loss= 0.6901, Training Accuracy= 0.537\n",
      "Epoch: 1030, Loss= 0.6872, Training Accuracy= 0.550\n",
      "Epoch: 1040, Loss= 0.6865, Training Accuracy= 0.550\n",
      "Epoch: 1050, Loss= 0.6817, Training Accuracy= 0.561\n",
      "Epoch: 1060, Loss= 0.6869, Training Accuracy= 0.550\n",
      "Epoch: 1070, Loss= 0.6878, Training Accuracy= 0.544\n",
      "Epoch: 1080, Loss= 0.6842, Training Accuracy= 0.553\n",
      "Epoch: 1090, Loss= 0.6849, Training Accuracy= 0.552\n",
      "Epoch: 1100, Loss= 0.6849, Training Accuracy= 0.555\n",
      "Epoch: 1110, Loss= 0.6852, Training Accuracy= 0.557\n",
      "Epoch: 1120, Loss= 0.6857, Training Accuracy= 0.548\n",
      "Epoch: 1130, Loss= 0.6828, Training Accuracy= 0.560\n",
      "Epoch: 1140, Loss= 0.6846, Training Accuracy= 0.554\n",
      "Epoch: 1150, Loss= 0.6967, Training Accuracy= 0.522\n",
      "Epoch: 1160, Loss= 0.6795, Training Accuracy= 0.570\n",
      "Epoch: 1170, Loss= 0.6807, Training Accuracy= 0.562\n",
      "Epoch: 1180, Loss= 0.6808, Training Accuracy= 0.564\n",
      "Epoch: 1190, Loss= 0.6887, Training Accuracy= 0.543\n",
      "Epoch: 1200, Loss= 0.6786, Training Accuracy= 0.572\n",
      "Epoch: 1210, Loss= 0.6826, Training Accuracy= 0.561\n",
      "Epoch: 1220, Loss= 0.6844, Training Accuracy= 0.561\n",
      "Epoch: 1230, Loss= 0.6836, Training Accuracy= 0.559\n",
      "Epoch: 1240, Loss= 0.6804, Training Accuracy= 0.562\n",
      "Epoch: 1250, Loss= 0.6813, Training Accuracy= 0.564\n",
      "Epoch: 1260, Loss= 0.6772, Training Accuracy= 0.571\n",
      "Epoch: 1270, Loss= 0.6708, Training Accuracy= 0.584\n",
      "Epoch: 1280, Loss= 0.6790, Training Accuracy= 0.568\n",
      "Epoch: 1290, Loss= 0.6698, Training Accuracy= 0.582\n",
      "Epoch: 1300, Loss= 0.6699, Training Accuracy= 0.588\n",
      "Epoch: 1310, Loss= 0.6858, Training Accuracy= 0.554\n",
      "Epoch: 1320, Loss= 0.6706, Training Accuracy= 0.589\n",
      "Epoch: 1330, Loss= 0.6712, Training Accuracy= 0.581\n",
      "Epoch: 1340, Loss= 0.6852, Training Accuracy= 0.558\n",
      "Epoch: 1350, Loss= 0.6690, Training Accuracy= 0.584\n",
      "Epoch: 1360, Loss= 0.6702, Training Accuracy= 0.582\n",
      "Epoch: 1370, Loss= 0.6680, Training Accuracy= 0.592\n",
      "Epoch: 1380, Loss= 0.6810, Training Accuracy= 0.567\n",
      "Epoch: 1390, Loss= 0.6693, Training Accuracy= 0.585\n",
      "Epoch: 1400, Loss= 0.6753, Training Accuracy= 0.579\n",
      "Epoch: 1410, Loss= 0.6870, Training Accuracy= 0.550\n",
      "Epoch: 1420, Loss= 0.6865, Training Accuracy= 0.548\n",
      "Epoch: 1430, Loss= 0.6788, Training Accuracy= 0.571\n",
      "Epoch: 1440, Loss= 0.6830, Training Accuracy= 0.562\n",
      "Epoch: 1450, Loss= 0.6753, Training Accuracy= 0.570\n",
      "Epoch: 1460, Loss= 0.6770, Training Accuracy= 0.574\n",
      "Epoch: 1470, Loss= 0.6813, Training Accuracy= 0.563\n",
      "Epoch: 1480, Loss= 0.6725, Training Accuracy= 0.580\n",
      "Epoch: 1490, Loss= 0.6885, Training Accuracy= 0.548\n",
      "Epoch: 1500, Loss= 0.6871, Training Accuracy= 0.548\n",
      "Epoch: 1510, Loss= 0.6813, Training Accuracy= 0.566\n",
      "Epoch: 1520, Loss= 0.6835, Training Accuracy= 0.558\n",
      "Epoch: 1530, Loss= 0.6752, Training Accuracy= 0.576\n",
      "Epoch: 1540, Loss= 0.6821, Training Accuracy= 0.557\n",
      "Epoch: 1550, Loss= 0.6833, Training Accuracy= 0.560\n",
      "Epoch: 1560, Loss= 0.6795, Training Accuracy= 0.564\n",
      "Epoch: 1570, Loss= 0.6813, Training Accuracy= 0.567\n",
      "Epoch: 1580, Loss= 0.6792, Training Accuracy= 0.567\n",
      "Epoch: 1590, Loss= 0.6846, Training Accuracy= 0.557\n",
      "Epoch: 1600, Loss= 0.6804, Training Accuracy= 0.560\n",
      "Epoch: 1610, Loss= 0.6830, Training Accuracy= 0.560\n",
      "Epoch: 1620, Loss= 0.6839, Training Accuracy= 0.555\n",
      "Epoch: 1630, Loss= 0.6774, Training Accuracy= 0.568\n",
      "Epoch: 1640, Loss= 0.6823, Training Accuracy= 0.557\n",
      "Epoch: 1650, Loss= 0.6830, Training Accuracy= 0.562\n",
      "Epoch: 1660, Loss= 0.6864, Training Accuracy= 0.554\n",
      "Epoch: 1670, Loss= 0.6865, Training Accuracy= 0.549\n",
      "Epoch: 1680, Loss= 0.6847, Training Accuracy= 0.551\n",
      "Epoch: 1690, Loss= 0.6928, Training Accuracy= 0.529\n",
      "Epoch: 1700, Loss= 0.6956, Training Accuracy= 0.521\n",
      "Epoch: 1710, Loss= 0.6989, Training Accuracy= 0.511\n",
      "Epoch: 1720, Loss= 0.6922, Training Accuracy= 0.527\n",
      "Epoch: 1730, Loss= 0.6882, Training Accuracy= 0.545\n",
      "Epoch: 1740, Loss= 0.6908, Training Accuracy= 0.533\n",
      "Epoch: 1750, Loss= 0.6873, Training Accuracy= 0.551\n",
      "Epoch: 1760, Loss= 0.6927, Training Accuracy= 0.525\n",
      "Epoch: 1770, Loss= 0.6924, Training Accuracy= 0.525\n",
      "Epoch: 1780, Loss= 0.6933, Training Accuracy= 0.521\n",
      "Epoch: 1790, Loss= 0.6991, Training Accuracy= 0.508\n",
      "Epoch: 1800, Loss= 0.7227, Training Accuracy= 0.499\n",
      "Epoch: 1810, Loss= 0.7218, Training Accuracy= 0.502\n",
      "Epoch: 1820, Loss= 0.7244, Training Accuracy= 0.501\n",
      "Epoch: 1830, Loss= 0.7286, Training Accuracy= 0.500\n",
      "Epoch: 1840, Loss= 0.7464, Training Accuracy= 0.500\n",
      "Epoch: 1850, Loss= 0.7348, Training Accuracy= 0.501\n",
      "Epoch: 1860, Loss= 0.7527, Training Accuracy= 0.499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1870, Loss= 0.7479, Training Accuracy= 0.498\n",
      "Epoch: 1880, Loss= 0.7497, Training Accuracy= 0.499\n",
      "Epoch: 1890, Loss= 0.7472, Training Accuracy= 0.499\n",
      "Epoch: 1900, Loss= 0.7432, Training Accuracy= 0.499\n",
      "Epoch: 1910, Loss= 0.7666, Training Accuracy= 0.499\n",
      "Epoch: 1920, Loss= 0.7416, Training Accuracy= 0.498\n",
      "Epoch: 1930, Loss= 0.7675, Training Accuracy= 0.499\n",
      "Epoch: 1940, Loss= 0.7794, Training Accuracy= 0.499\n",
      "Epoch: 1950, Loss= 0.7806, Training Accuracy= 0.499\n",
      "Epoch: 1960, Loss= 0.7818, Training Accuracy= 0.499\n",
      "Epoch: 1970, Loss= 0.7816, Training Accuracy= 0.499\n",
      "Epoch: 1980, Loss= 0.7856, Training Accuracy= 0.499\n",
      "Epoch: 1990, Loss= 0.7674, Training Accuracy= 0.499\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4996\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.8284, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.7235, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.7113, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.7063, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.7032, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.7013, Training Accuracy= 0.505\n",
      "Epoch: 60, Loss= 0.7002, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.6994, Training Accuracy= 0.509\n",
      "Epoch: 80, Loss= 0.6983, Training Accuracy= 0.512\n",
      "Epoch: 90, Loss= 0.6971, Training Accuracy= 0.518\n",
      "Epoch: 100, Loss= 0.6971, Training Accuracy= 0.518\n",
      "Epoch: 110, Loss= 0.6977, Training Accuracy= 0.523\n",
      "Epoch: 120, Loss= 0.6988, Training Accuracy= 0.520\n",
      "Epoch: 130, Loss= 0.7014, Training Accuracy= 0.519\n",
      "Epoch: 140, Loss= 0.7018, Training Accuracy= 0.521\n",
      "Epoch: 150, Loss= 0.6963, Training Accuracy= 0.536\n",
      "Epoch: 160, Loss= 0.7034, Training Accuracy= 0.510\n",
      "Epoch: 170, Loss= 0.6942, Training Accuracy= 0.529\n",
      "Epoch: 180, Loss= 0.6917, Training Accuracy= 0.537\n",
      "Epoch: 190, Loss= 0.6960, Training Accuracy= 0.537\n",
      "Epoch: 200, Loss= 0.7006, Training Accuracy= 0.532\n",
      "Epoch: 210, Loss= 0.6831, Training Accuracy= 0.554\n",
      "Epoch: 220, Loss= 0.6860, Training Accuracy= 0.559\n",
      "Epoch: 230, Loss= 0.6900, Training Accuracy= 0.550\n",
      "Epoch: 240, Loss= 0.7026, Training Accuracy= 0.532\n",
      "Epoch: 250, Loss= 0.6819, Training Accuracy= 0.565\n",
      "Epoch: 260, Loss= 0.6845, Training Accuracy= 0.562\n",
      "Epoch: 270, Loss= 0.6872, Training Accuracy= 0.548\n",
      "Epoch: 280, Loss= 0.6841, Training Accuracy= 0.556\n",
      "Epoch: 290, Loss= 0.6952, Training Accuracy= 0.547\n",
      "Epoch: 300, Loss= 0.6813, Training Accuracy= 0.567\n",
      "Epoch: 310, Loss= 0.6773, Training Accuracy= 0.571\n",
      "Epoch: 320, Loss= 0.7183, Training Accuracy= 0.505\n",
      "Epoch: 330, Loss= 0.7144, Training Accuracy= 0.505\n",
      "Epoch: 340, Loss= 0.6818, Training Accuracy= 0.564\n",
      "Epoch: 350, Loss= 0.6805, Training Accuracy= 0.568\n",
      "Epoch: 360, Loss= 0.6828, Training Accuracy= 0.563\n",
      "Epoch: 370, Loss= 0.6861, Training Accuracy= 0.557\n",
      "Epoch: 380, Loss= 0.6880, Training Accuracy= 0.554\n",
      "Epoch: 390, Loss= 0.6810, Training Accuracy= 0.564\n",
      "Epoch: 400, Loss= 0.6781, Training Accuracy= 0.569\n",
      "Epoch: 410, Loss= 0.6837, Training Accuracy= 0.551\n",
      "Epoch: 420, Loss= 0.6734, Training Accuracy= 0.577\n",
      "Epoch: 430, Loss= 0.6688, Training Accuracy= 0.585\n",
      "Epoch: 440, Loss= 0.6788, Training Accuracy= 0.572\n",
      "Epoch: 450, Loss= 0.6717, Training Accuracy= 0.581\n",
      "Epoch: 460, Loss= 0.6908, Training Accuracy= 0.556\n",
      "Epoch: 470, Loss= 0.6766, Training Accuracy= 0.571\n",
      "Epoch: 480, Loss= 0.6831, Training Accuracy= 0.566\n",
      "Epoch: 490, Loss= 0.6850, Training Accuracy= 0.559\n",
      "Epoch: 500, Loss= 0.6688, Training Accuracy= 0.590\n",
      "Epoch: 510, Loss= 0.6692, Training Accuracy= 0.591\n",
      "Epoch: 520, Loss= 0.6787, Training Accuracy= 0.569\n",
      "Epoch: 530, Loss= 0.6803, Training Accuracy= 0.570\n",
      "Epoch: 540, Loss= 0.7230, Training Accuracy= 0.501\n",
      "Epoch: 550, Loss= 0.7079, Training Accuracy= 0.502\n",
      "Epoch: 560, Loss= 0.7415, Training Accuracy= 0.501\n",
      "Epoch: 570, Loss= 0.6985, Training Accuracy= 0.500\n",
      "Epoch: 580, Loss= 0.7233, Training Accuracy= 0.500\n",
      "Epoch: 590, Loss= 0.7179, Training Accuracy= 0.501\n",
      "Epoch: 600, Loss= 0.7071, Training Accuracy= 0.508\n",
      "Epoch: 610, Loss= 0.7026, Training Accuracy= 0.512\n",
      "Epoch: 620, Loss= 0.7019, Training Accuracy= 0.521\n",
      "Epoch: 630, Loss= 0.7125, Training Accuracy= 0.504\n",
      "Epoch: 640, Loss= 0.7150, Training Accuracy= 0.506\n",
      "Epoch: 650, Loss= 0.7054, Training Accuracy= 0.513\n",
      "Epoch: 660, Loss= 0.7099, Training Accuracy= 0.513\n",
      "Epoch: 670, Loss= 0.7075, Training Accuracy= 0.511\n",
      "Epoch: 680, Loss= 0.7039, Training Accuracy= 0.516\n",
      "Epoch: 690, Loss= 0.7043, Training Accuracy= 0.521\n",
      "Epoch: 700, Loss= 0.6950, Training Accuracy= 0.533\n",
      "Epoch: 710, Loss= 0.7023, Training Accuracy= 0.521\n",
      "Epoch: 720, Loss= 0.7344, Training Accuracy= 0.501\n",
      "Epoch: 730, Loss= 0.7314, Training Accuracy= 0.501\n",
      "Epoch: 740, Loss= 0.7281, Training Accuracy= 0.501\n",
      "Epoch: 750, Loss= 0.7257, Training Accuracy= 0.501\n",
      "Epoch: 760, Loss= 0.7243, Training Accuracy= 0.501\n",
      "Epoch: 770, Loss= 0.7234, Training Accuracy= 0.500\n",
      "Epoch: 780, Loss= 0.7170, Training Accuracy= 0.501\n",
      "Epoch: 790, Loss= 0.7135, Training Accuracy= 0.501\n",
      "Epoch: 800, Loss= 0.7261, Training Accuracy= 0.501\n",
      "Epoch: 810, Loss= 0.7200, Training Accuracy= 0.501\n",
      "Epoch: 820, Loss= 0.7035, Training Accuracy= 0.502\n",
      "Epoch: 830, Loss= 0.7224, Training Accuracy= 0.501\n",
      "Epoch: 840, Loss= 0.7188, Training Accuracy= 0.501\n",
      "Epoch: 850, Loss= 0.7137, Training Accuracy= 0.501\n",
      "Epoch: 860, Loss= 0.7185, Training Accuracy= 0.501\n",
      "Epoch: 870, Loss= 0.7117, Training Accuracy= 0.504\n",
      "Epoch: 880, Loss= 0.7265, Training Accuracy= 0.501\n",
      "Epoch: 890, Loss= 0.7097, Training Accuracy= 0.501\n",
      "Epoch: 900, Loss= 0.7195, Training Accuracy= 0.500\n",
      "Epoch: 910, Loss= 0.7067, Training Accuracy= 0.500\n",
      "Epoch: 920, Loss= 0.7127, Training Accuracy= 0.501\n",
      "Epoch: 930, Loss= 0.7000, Training Accuracy= 0.502\n",
      "Epoch: 940, Loss= 0.7052, Training Accuracy= 0.502\n",
      "Epoch: 950, Loss= 0.7103, Training Accuracy= 0.502\n",
      "Epoch: 960, Loss= 0.7016, Training Accuracy= 0.500\n",
      "Epoch: 970, Loss= 0.6958, Training Accuracy= 0.512\n",
      "Epoch: 980, Loss= 0.7052, Training Accuracy= 0.502\n",
      "Epoch: 990, Loss= 0.7063, Training Accuracy= 0.500\n",
      "Epoch: 1000, Loss= 0.6996, Training Accuracy= 0.506\n",
      "Epoch: 1010, Loss= 0.7087, Training Accuracy= 0.499\n",
      "Epoch: 1020, Loss= 0.7063, Training Accuracy= 0.501\n",
      "Epoch: 1030, Loss= 0.7010, Training Accuracy= 0.504\n",
      "Epoch: 1040, Loss= 0.7031, Training Accuracy= 0.500\n",
      "Epoch: 1050, Loss= 0.7003, Training Accuracy= 0.499\n",
      "Epoch: 1060, Loss= 0.7048, Training Accuracy= 0.503\n",
      "Epoch: 1070, Loss= 0.6987, Training Accuracy= 0.504\n",
      "Epoch: 1080, Loss= 0.7012, Training Accuracy= 0.504\n",
      "Epoch: 1090, Loss= 0.7052, Training Accuracy= 0.500\n",
      "Epoch: 1100, Loss= 0.6996, Training Accuracy= 0.507\n",
      "Epoch: 1110, Loss= 0.6958, Training Accuracy= 0.501\n",
      "Epoch: 1120, Loss= 0.7040, Training Accuracy= 0.501\n",
      "Epoch: 1130, Loss= 0.7020, Training Accuracy= 0.505\n",
      "Epoch: 1140, Loss= 0.7048, Training Accuracy= 0.501\n",
      "Epoch: 1150, Loss= 0.7031, Training Accuracy= 0.501\n",
      "Epoch: 1160, Loss= 0.7000, Training Accuracy= 0.505\n",
      "Epoch: 1170, Loss= 0.7025, Training Accuracy= 0.496\n",
      "Epoch: 1180, Loss= 0.6958, Training Accuracy= 0.510\n",
      "Epoch: 1190, Loss= 0.7239, Training Accuracy= 0.501\n",
      "Epoch: 1200, Loss= 0.7201, Training Accuracy= 0.501\n",
      "Epoch: 1210, Loss= 0.7149, Training Accuracy= 0.500\n",
      "Epoch: 1220, Loss= 0.7117, Training Accuracy= 0.499\n",
      "Epoch: 1230, Loss= 0.7123, Training Accuracy= 0.500\n",
      "Epoch: 1240, Loss= 0.7135, Training Accuracy= 0.501\n",
      "Epoch: 1250, Loss= 0.7040, Training Accuracy= 0.501\n",
      "Epoch: 1260, Loss= 0.7086, Training Accuracy= 0.501\n",
      "Epoch: 1270, Loss= 0.7211, Training Accuracy= 0.501\n",
      "Epoch: 1280, Loss= 0.7190, Training Accuracy= 0.501\n",
      "Epoch: 1290, Loss= 0.7095, Training Accuracy= 0.501\n",
      "Epoch: 1300, Loss= 0.7150, Training Accuracy= 0.501\n",
      "Epoch: 1310, Loss= 0.7136, Training Accuracy= 0.502\n",
      "Epoch: 1320, Loss= 0.7045, Training Accuracy= 0.503\n",
      "Epoch: 1330, Loss= 0.7191, Training Accuracy= 0.501\n",
      "Epoch: 1340, Loss= 0.7348, Training Accuracy= 0.501\n",
      "Epoch: 1350, Loss= 0.7203, Training Accuracy= 0.501\n",
      "Epoch: 1360, Loss= 0.7046, Training Accuracy= 0.501\n",
      "Epoch: 1370, Loss= 0.7058, Training Accuracy= 0.501\n",
      "Epoch: 1380, Loss= 0.7209, Training Accuracy= 0.502\n",
      "Epoch: 1390, Loss= 0.7186, Training Accuracy= 0.500\n",
      "Epoch: 1400, Loss= 0.7143, Training Accuracy= 0.505\n",
      "Epoch: 1410, Loss= 0.7033, Training Accuracy= 0.510\n",
      "Epoch: 1420, Loss= 0.7052, Training Accuracy= 0.515\n",
      "Epoch: 1430, Loss= 0.7059, Training Accuracy= 0.502\n",
      "Epoch: 1440, Loss= 0.7144, Training Accuracy= 0.499\n",
      "Epoch: 1450, Loss= 0.7115, Training Accuracy= 0.501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1460, Loss= 0.7069, Training Accuracy= 0.501\n",
      "Epoch: 1470, Loss= 0.7064, Training Accuracy= 0.500\n",
      "Epoch: 1480, Loss= 0.7047, Training Accuracy= 0.500\n",
      "Epoch: 1490, Loss= 0.7050, Training Accuracy= 0.501\n",
      "Epoch: 1500, Loss= 0.7026, Training Accuracy= 0.500\n",
      "Epoch: 1510, Loss= 0.7120, Training Accuracy= 0.501\n",
      "Epoch: 1520, Loss= 0.7072, Training Accuracy= 0.502\n",
      "Epoch: 1530, Loss= 0.7042, Training Accuracy= 0.501\n",
      "Epoch: 1540, Loss= 0.7175, Training Accuracy= 0.500\n",
      "Epoch: 1550, Loss= 0.7146, Training Accuracy= 0.501\n",
      "Epoch: 1560, Loss= 0.6999, Training Accuracy= 0.510\n",
      "Epoch: 1570, Loss= 0.7161, Training Accuracy= 0.499\n",
      "Epoch: 1580, Loss= 0.7103, Training Accuracy= 0.500\n",
      "Epoch: 1590, Loss= 0.7072, Training Accuracy= 0.505\n",
      "Epoch: 1600, Loss= 0.6981, Training Accuracy= 0.507\n",
      "Epoch: 1610, Loss= 0.7158, Training Accuracy= 0.502\n",
      "Epoch: 1620, Loss= 0.7147, Training Accuracy= 0.501\n",
      "Epoch: 1630, Loss= 0.7146, Training Accuracy= 0.500\n",
      "Epoch: 1640, Loss= 0.7044, Training Accuracy= 0.502\n",
      "Epoch: 1650, Loss= 0.7186, Training Accuracy= 0.501\n",
      "Epoch: 1660, Loss= 0.7205, Training Accuracy= 0.501\n",
      "Epoch: 1670, Loss= 0.7151, Training Accuracy= 0.504\n",
      "Epoch: 1680, Loss= 0.7066, Training Accuracy= 0.504\n",
      "Epoch: 1690, Loss= 0.7091, Training Accuracy= 0.503\n",
      "Epoch: 1700, Loss= 0.7279, Training Accuracy= 0.501\n",
      "Epoch: 1710, Loss= 0.7261, Training Accuracy= 0.501\n",
      "Epoch: 1720, Loss= 0.7253, Training Accuracy= 0.500\n",
      "Epoch: 1730, Loss= 0.7223, Training Accuracy= 0.500\n",
      "Epoch: 1740, Loss= 0.7229, Training Accuracy= 0.501\n",
      "Epoch: 1750, Loss= 0.7035, Training Accuracy= 0.506\n",
      "Epoch: 1760, Loss= 0.7146, Training Accuracy= 0.502\n",
      "Epoch: 1770, Loss= 0.7072, Training Accuracy= 0.504\n",
      "Epoch: 1780, Loss= 0.7173, Training Accuracy= 0.501\n",
      "Epoch: 1790, Loss= 0.7114, Training Accuracy= 0.506\n",
      "Epoch: 1800, Loss= 0.7155, Training Accuracy= 0.501\n",
      "Epoch: 1810, Loss= 0.7105, Training Accuracy= 0.501\n",
      "Epoch: 1820, Loss= 0.7098, Training Accuracy= 0.506\n",
      "Epoch: 1830, Loss= 0.7090, Training Accuracy= 0.497\n",
      "Epoch: 1840, Loss= 0.7083, Training Accuracy= 0.506\n",
      "Epoch: 1850, Loss= 0.7183, Training Accuracy= 0.502\n",
      "Epoch: 1860, Loss= 0.7077, Training Accuracy= 0.508\n",
      "Epoch: 1870, Loss= 0.7123, Training Accuracy= 0.503\n",
      "Epoch: 1880, Loss= 0.7030, Training Accuracy= 0.505\n",
      "Epoch: 1890, Loss= 0.6986, Training Accuracy= 0.501\n",
      "Epoch: 1900, Loss= 0.7021, Training Accuracy= 0.506\n",
      "Epoch: 1910, Loss= 0.7041, Training Accuracy= 0.507\n",
      "Epoch: 1920, Loss= 0.6985, Training Accuracy= 0.503\n",
      "Epoch: 1930, Loss= 0.6968, Training Accuracy= 0.502\n",
      "Epoch: 1940, Loss= 0.7127, Training Accuracy= 0.504\n",
      "Epoch: 1950, Loss= 0.7092, Training Accuracy= 0.502\n",
      "Epoch: 1960, Loss= 0.7123, Training Accuracy= 0.501\n",
      "Epoch: 1970, Loss= 0.7099, Training Accuracy= 0.505\n",
      "Epoch: 1980, Loss= 0.7120, Training Accuracy= 0.502\n",
      "Epoch: 1990, Loss= 0.7074, Training Accuracy= 0.509\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4967\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7029, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.7015, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.7012, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.7010, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.7008, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.7007, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.7007, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.7006, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.7006, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.7005, Training Accuracy= 0.497\n",
      "Epoch: 100, Loss= 0.7004, Training Accuracy= 0.497\n",
      "Epoch: 110, Loss= 0.7003, Training Accuracy= 0.497\n",
      "Epoch: 120, Loss= 0.7002, Training Accuracy= 0.497\n",
      "Epoch: 130, Loss= 0.7002, Training Accuracy= 0.497\n",
      "Epoch: 140, Loss= 0.7001, Training Accuracy= 0.497\n",
      "Epoch: 150, Loss= 0.7000, Training Accuracy= 0.497\n",
      "Epoch: 160, Loss= 0.7000, Training Accuracy= 0.497\n",
      "Epoch: 170, Loss= 0.7000, Training Accuracy= 0.497\n",
      "Epoch: 180, Loss= 0.6999, Training Accuracy= 0.497\n",
      "Epoch: 190, Loss= 0.6999, Training Accuracy= 0.497\n",
      "Epoch: 200, Loss= 0.6998, Training Accuracy= 0.497\n",
      "Epoch: 210, Loss= 0.6998, Training Accuracy= 0.497\n",
      "Epoch: 220, Loss= 0.6998, Training Accuracy= 0.497\n",
      "Epoch: 230, Loss= 0.6997, Training Accuracy= 0.497\n",
      "Epoch: 240, Loss= 0.6997, Training Accuracy= 0.497\n",
      "Epoch: 250, Loss= 0.6996, Training Accuracy= 0.497\n",
      "Epoch: 260, Loss= 0.6996, Training Accuracy= 0.497\n",
      "Epoch: 270, Loss= 0.6995, Training Accuracy= 0.497\n",
      "Epoch: 280, Loss= 0.6995, Training Accuracy= 0.497\n",
      "Epoch: 290, Loss= 0.6995, Training Accuracy= 0.497\n",
      "Epoch: 300, Loss= 0.6994, Training Accuracy= 0.497\n",
      "Epoch: 310, Loss= 0.6994, Training Accuracy= 0.497\n",
      "Epoch: 320, Loss= 0.6994, Training Accuracy= 0.497\n",
      "Epoch: 330, Loss= 0.6994, Training Accuracy= 0.497\n",
      "Epoch: 340, Loss= 0.6994, Training Accuracy= 0.497\n",
      "Epoch: 350, Loss= 0.6994, Training Accuracy= 0.497\n",
      "Epoch: 360, Loss= 0.6993, Training Accuracy= 0.497\n",
      "Epoch: 370, Loss= 0.6993, Training Accuracy= 0.497\n",
      "Epoch: 380, Loss= 0.6993, Training Accuracy= 0.497\n",
      "Epoch: 390, Loss= 0.6993, Training Accuracy= 0.497\n",
      "Epoch: 400, Loss= 0.6993, Training Accuracy= 0.497\n",
      "Epoch: 410, Loss= 0.6992, Training Accuracy= 0.497\n",
      "Epoch: 420, Loss= 0.6992, Training Accuracy= 0.497\n",
      "Epoch: 430, Loss= 0.6992, Training Accuracy= 0.497\n",
      "Epoch: 440, Loss= 0.6991, Training Accuracy= 0.497\n",
      "Epoch: 450, Loss= 0.6991, Training Accuracy= 0.497\n",
      "Epoch: 460, Loss= 0.6990, Training Accuracy= 0.497\n",
      "Epoch: 470, Loss= 0.6990, Training Accuracy= 0.497\n",
      "Epoch: 480, Loss= 0.6990, Training Accuracy= 0.497\n",
      "Epoch: 490, Loss= 0.6989, Training Accuracy= 0.497\n",
      "Epoch: 500, Loss= 0.6989, Training Accuracy= 0.497\n",
      "Epoch: 510, Loss= 0.6989, Training Accuracy= 0.497\n",
      "Epoch: 520, Loss= 0.6989, Training Accuracy= 0.497\n",
      "Epoch: 530, Loss= 0.6989, Training Accuracy= 0.497\n",
      "Epoch: 540, Loss= 0.6989, Training Accuracy= 0.497\n",
      "Epoch: 550, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 560, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 570, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 580, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 590, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 600, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 610, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 620, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 630, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 640, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 650, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 660, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 670, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 680, Loss= 0.6985, Training Accuracy= 0.497\n",
      "Epoch: 690, Loss= 0.6985, Training Accuracy= 0.497\n",
      "Epoch: 700, Loss= 0.6985, Training Accuracy= 0.497\n",
      "Epoch: 710, Loss= 0.6985, Training Accuracy= 0.497\n",
      "Epoch: 720, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 730, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 740, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 750, Loss= 0.6983, Training Accuracy= 0.497\n",
      "Epoch: 760, Loss= 0.6983, Training Accuracy= 0.497\n",
      "Epoch: 770, Loss= 0.6983, Training Accuracy= 0.497\n",
      "Epoch: 780, Loss= 0.6982, Training Accuracy= 0.497\n",
      "Epoch: 790, Loss= 0.6982, Training Accuracy= 0.497\n",
      "Epoch: 800, Loss= 0.6981, Training Accuracy= 0.497\n",
      "Epoch: 810, Loss= 0.6981, Training Accuracy= 0.497\n",
      "Epoch: 820, Loss= 0.6981, Training Accuracy= 0.497\n",
      "Epoch: 830, Loss= 0.6980, Training Accuracy= 0.497\n",
      "Epoch: 840, Loss= 0.6980, Training Accuracy= 0.497\n",
      "Epoch: 850, Loss= 0.6979, Training Accuracy= 0.497\n",
      "Epoch: 860, Loss= 0.6979, Training Accuracy= 0.497\n",
      "Epoch: 870, Loss= 0.6978, Training Accuracy= 0.497\n",
      "Epoch: 880, Loss= 0.6977, Training Accuracy= 0.497\n",
      "Epoch: 890, Loss= 0.6977, Training Accuracy= 0.497\n",
      "Epoch: 900, Loss= 0.6976, Training Accuracy= 0.497\n",
      "Epoch: 910, Loss= 0.6976, Training Accuracy= 0.497\n",
      "Epoch: 920, Loss= 0.6975, Training Accuracy= 0.497\n",
      "Epoch: 930, Loss= 0.6975, Training Accuracy= 0.497\n",
      "Epoch: 940, Loss= 0.6974, Training Accuracy= 0.497\n",
      "Epoch: 950, Loss= 0.6974, Training Accuracy= 0.497\n",
      "Epoch: 960, Loss= 0.6973, Training Accuracy= 0.497\n",
      "Epoch: 970, Loss= 0.6973, Training Accuracy= 0.497\n",
      "Epoch: 980, Loss= 0.6973, Training Accuracy= 0.497\n",
      "Epoch: 990, Loss= 0.6972, Training Accuracy= 0.497\n",
      "Epoch: 1000, Loss= 0.6972, Training Accuracy= 0.497\n",
      "Epoch: 1010, Loss= 0.6971, Training Accuracy= 0.497\n",
      "Epoch: 1020, Loss= 0.6970, Training Accuracy= 0.496\n",
      "Epoch: 1030, Loss= 0.6969, Training Accuracy= 0.497\n",
      "Epoch: 1040, Loss= 0.6969, Training Accuracy= 0.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1050, Loss= 0.7011, Training Accuracy= 0.497\n",
      "Epoch: 1060, Loss= 0.7010, Training Accuracy= 0.497\n",
      "Epoch: 1070, Loss= 0.7009, Training Accuracy= 0.497\n",
      "Epoch: 1080, Loss= 0.7008, Training Accuracy= 0.497\n",
      "Epoch: 1090, Loss= 0.7008, Training Accuracy= 0.497\n",
      "Epoch: 1100, Loss= 0.7008, Training Accuracy= 0.497\n",
      "Epoch: 1110, Loss= 0.7007, Training Accuracy= 0.497\n",
      "Epoch: 1120, Loss= 0.7007, Training Accuracy= 0.497\n",
      "Epoch: 1130, Loss= 0.7007, Training Accuracy= 0.497\n",
      "Epoch: 1140, Loss= 0.7006, Training Accuracy= 0.497\n",
      "Epoch: 1150, Loss= 0.7006, Training Accuracy= 0.497\n",
      "Epoch: 1160, Loss= 0.7006, Training Accuracy= 0.497\n",
      "Epoch: 1170, Loss= 0.7006, Training Accuracy= 0.497\n",
      "Epoch: 1180, Loss= 0.7006, Training Accuracy= 0.497\n",
      "Epoch: 1190, Loss= 0.7005, Training Accuracy= 0.497\n",
      "Epoch: 1200, Loss= 0.7005, Training Accuracy= 0.497\n",
      "Epoch: 1210, Loss= 0.7004, Training Accuracy= 0.497\n",
      "Epoch: 1220, Loss= 0.7004, Training Accuracy= 0.497\n",
      "Epoch: 1230, Loss= 0.7003, Training Accuracy= 0.497\n",
      "Epoch: 1240, Loss= 0.7003, Training Accuracy= 0.497\n",
      "Epoch: 1250, Loss= 0.7003, Training Accuracy= 0.497\n",
      "Epoch: 1260, Loss= 0.7002, Training Accuracy= 0.497\n",
      "Epoch: 1270, Loss= 0.7002, Training Accuracy= 0.497\n",
      "Epoch: 1280, Loss= 0.7002, Training Accuracy= 0.497\n",
      "Epoch: 1290, Loss= 0.7001, Training Accuracy= 0.497\n",
      "Epoch: 1300, Loss= 0.7001, Training Accuracy= 0.497\n",
      "Epoch: 1310, Loss= 0.7000, Training Accuracy= 0.497\n",
      "Epoch: 1320, Loss= 0.7000, Training Accuracy= 0.497\n",
      "Epoch: 1330, Loss= 0.6999, Training Accuracy= 0.497\n",
      "Epoch: 1340, Loss= 0.6999, Training Accuracy= 0.497\n",
      "Epoch: 1350, Loss= 0.6998, Training Accuracy= 0.497\n",
      "Epoch: 1360, Loss= 0.6997, Training Accuracy= 0.497\n",
      "Epoch: 1370, Loss= 0.6996, Training Accuracy= 0.497\n",
      "Epoch: 1380, Loss= 0.6995, Training Accuracy= 0.497\n",
      "Epoch: 1390, Loss= 0.6994, Training Accuracy= 0.497\n",
      "Epoch: 1400, Loss= 0.6993, Training Accuracy= 0.497\n",
      "Epoch: 1410, Loss= 0.6992, Training Accuracy= 0.497\n",
      "Epoch: 1420, Loss= 0.6991, Training Accuracy= 0.497\n",
      "Epoch: 1430, Loss= 0.6991, Training Accuracy= 0.497\n",
      "Epoch: 1440, Loss= 0.6990, Training Accuracy= 0.497\n",
      "Epoch: 1450, Loss= 0.6990, Training Accuracy= 0.497\n",
      "Epoch: 1460, Loss= 0.6989, Training Accuracy= 0.497\n",
      "Epoch: 1470, Loss= 0.6989, Training Accuracy= 0.497\n",
      "Epoch: 1480, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 1490, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 1500, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 1510, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 1520, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 1530, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 1540, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 1550, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 1560, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 1570, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 1580, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 1590, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 1600, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 1610, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 1620, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 1630, Loss= 0.6985, Training Accuracy= 0.497\n",
      "Epoch: 1640, Loss= 0.6985, Training Accuracy= 0.497\n",
      "Epoch: 1650, Loss= 0.6985, Training Accuracy= 0.497\n",
      "Epoch: 1660, Loss= 0.6985, Training Accuracy= 0.497\n",
      "Epoch: 1670, Loss= 0.6985, Training Accuracy= 0.497\n",
      "Epoch: 1680, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 1690, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 1700, Loss= 0.6983, Training Accuracy= 0.497\n",
      "Epoch: 1710, Loss= 0.6983, Training Accuracy= 0.497\n",
      "Epoch: 1720, Loss= 0.6982, Training Accuracy= 0.497\n",
      "Epoch: 1730, Loss= 0.6982, Training Accuracy= 0.497\n",
      "Epoch: 1740, Loss= 0.6981, Training Accuracy= 0.497\n",
      "Epoch: 1750, Loss= 0.6980, Training Accuracy= 0.497\n",
      "Epoch: 1760, Loss= 0.6980, Training Accuracy= 0.497\n",
      "Epoch: 1770, Loss= 0.6979, Training Accuracy= 0.497\n",
      "Epoch: 1780, Loss= 0.6977, Training Accuracy= 0.497\n",
      "Epoch: 1790, Loss= 0.6976, Training Accuracy= 0.497\n",
      "Epoch: 1800, Loss= 0.6973, Training Accuracy= 0.497\n",
      "Epoch: 1810, Loss= 0.6968, Training Accuracy= 0.497\n",
      "Epoch: 1820, Loss= 0.6964, Training Accuracy= 0.500\n",
      "Epoch: 1830, Loss= 0.6958, Training Accuracy= 0.502\n",
      "Epoch: 1840, Loss= 0.6952, Training Accuracy= 0.504\n",
      "Epoch: 1850, Loss= 0.6948, Training Accuracy= 0.508\n",
      "Epoch: 1860, Loss= 0.6945, Training Accuracy= 0.513\n",
      "Epoch: 1870, Loss= 0.6945, Training Accuracy= 0.515\n",
      "Epoch: 1880, Loss= 0.6940, Training Accuracy= 0.516\n",
      "Epoch: 1890, Loss= 0.6955, Training Accuracy= 0.507\n",
      "Epoch: 1900, Loss= 0.6937, Training Accuracy= 0.516\n",
      "Epoch: 1910, Loss= 0.6925, Training Accuracy= 0.527\n",
      "Epoch: 1920, Loss= 0.6921, Training Accuracy= 0.532\n",
      "Epoch: 1930, Loss= 0.6924, Training Accuracy= 0.530\n",
      "Epoch: 1940, Loss= 0.6949, Training Accuracy= 0.520\n",
      "Epoch: 1950, Loss= 0.6973, Training Accuracy= 0.514\n",
      "Epoch: 1960, Loss= 0.6970, Training Accuracy= 0.516\n",
      "Epoch: 1970, Loss= 0.6963, Training Accuracy= 0.516\n",
      "Epoch: 1980, Loss= 0.6960, Training Accuracy= 0.521\n",
      "Epoch: 1990, Loss= 0.6961, Training Accuracy= 0.521\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.491\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.08\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 2000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.4878, 0.49720001, 0.50059998, 0.49689999, 0.49059999, 0.505, 0.49739999, 0.49959999, 0.49669999, 0.491]\n",
      "mean of test_accuracies_10replications:  0.49628\n",
      "standard deviation of test_accuracies_10replications_std_mean:  4.90342546254e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xd4FNX6wPHvS+gdpEtHFFFABBUb\nKDZEr72LFa+9Y736U0Sv13Lteu1dUUHQiygiXhERQelIk14ivYVASEh5f3+c2WST7G5mw7bI+3me\nebI7e2bmncnsnJ1z5pwjqooxxhjjV6VkB2CMMaZisYzDGGNMVCzjMMYYExXLOIwxxkTFMg5jjDFR\nsYzDGGNMVCzjMCZGROQ4EUkPej9PRI6Lw3bGiMgVsV6vMX5ZxmFSnojcLCLTRCRHRN6LYrkVInJi\nHEOLSFUPUtUf92QdIjJYRD4qsd5TVfX9PQrOmD1QOdkBGOPDGuAx4BSgRrw2IiKVVTUvXus35q/C\n7jhMylPVkar6JbC55Gci0khERovINhHZIiITRaSSiHwItAa+EpEdInJPiGWPE5F0EblXRNYB73rz\nTxeRWd46fxGRrkHLrBCR+0VkvohsFZF3RaR6qLiD73hEJE1E/iEiS0UkU0Smi0gr77MXRGS1iGz3\n5h/rze8H/AO40NuH2d78H0XkGu91JRF5UERWisgGEflAROp5n7UVERWRK0RklYhsEpEHyv+fMMax\njMNUdIOAdKAx0BR3oVVVvQxYBfxNVWur6lNhlm8GNATaANeKyKHAO8B1wD7A68AoEakWtMyluLuf\nDsD+wIM+4rwTuBjoD9QFrgayvM+mAod4cQwFhotIdVX9Fngc+Mzbh24h1nulNx0PtAdqAy+XSHMM\ncABwAvCQiBzoI15jwrKMw1R0uUBzoI2q5qrqRI2uA7YC4GFVzVHVXcDfgddV9VdVzffqEnKAXkHL\nvKyqq1V1C/BPXIZQlmuAB1X1D3Vmq+pmAFX9SFU3q2qeqj4DVMNd6P24FHhWVZep6g7gfuAiEQku\nhn5EVXep6mxgNhAqAzLGN8s4TEX3NLAE+E5ElonIfVEuv1FVs4PetwEGecVU20RkG9AKaBGUZnXQ\n65UlPgunFbA01AciMkhEFohIhre9ekAjn/G38GIIjqcy7u4rYF3Q6yzcXYkx5WYZh6nQVDVTVQep\nanvgb8CdInJC4GM/qyjxfjXwT1WtHzTVVNVPgtK0CnrdGld5X5bVuKKtYrz6jHuBC4AGqlofyADE\n5z6swWV2wfHkAet9xGRMuVjGYVKeiFT2KqDTgDQRqR4oivEqsvcTEQG2A/neBO7i2T7Kzb0JXC8i\nR4hTS0ROE5E6QWluEpGWItIQV6fymY/1vgU8KiIdvfV2FZF9gDq4C/1GoLKIPISrAwlYD7QVkXDf\n1U+AO0SknYjUpqhOxJ4OM3FjGYepCB4EdgH3AQO814EK6Y7A98AOYDLwn6C2E/8CHvSKnO7ysyFV\nnYar53gZ2IorBruyRLKhwHfAMm96zMeqnwWGecttB97GPVo8FhgDLMIVM2VTvChsuPd3s4jMCLHe\nd4APgZ+A5d7yt/iIx5hyExvIyRj/RGQFcI2qfp/sWIxJFrvjMMYYE5UyMw4ROVpExonIIu+pleUi\nsszHcu94DZLmhvn8UhGZ402/iIg9ImiMMRVAmUVVIrIQuAOYTlGlI4Fn0CMs1xtX7vyBqh4c4vOj\ngAWqulVETgUGq+oR0e+CMcaYRPLTV1WGqo6JdsWq+pOItI3w+S9Bb6cALaPdhjHGmMTzk3GMF5Gn\ngZG4FrQAqGqoJzzKayDuyZKQRORa4FqAWrVq9ejUqVMMN22MMX9906dP36SqjWOxLj8ZR6D4qGfQ\nPAX6xiIAETkel3EcEy6Nqr4BvAHQs2dPnTZtWiw2bYwxew0RWVl2Kn/KzDhU9fhYbawkr9fRt4BT\ny6ozMcYYkxr8PFXVVETeFpEx3vvOIjJwTzcsIq1xxV+XqeqiPV2fMcaYxPDTjuM9XOvWQEdui4Db\ny1pIRD7BteQ9wBvzYKCIXC8i13tJHsJ1W/0fb+wDK38yxpgKwE8dRyNVHSYi9wOoap6I5Je1kKpG\n7GpaVa/BdTVtjDGmAvFzx7HT64xNAUSkF673TmOMMXshP3ccdwKjgA4iMgk30tp5cY3KGGNMyvLz\nVNUMEemDG5FMgD9UNTfukRljjElJfp6qqonrzvp2VZ2LGxvg9LhHZowxJiX5qeN4F9gNHOm9T8ff\n+APGGGP+gvxkHB1U9SkgF0BVd1E0rKUxxpi9jJ+MY7eI1KDoqaoOBPVZZYwxZu/i56mqh4FvgVYi\n8jFwNKWH0jTGGLOXiJhxiIgAC4FzgF64IqrbVHVTAmIzxhiTgiJmHKqqIvKlqvYAvk5QTMYYY1KY\nnzqOKSJyWNwjMcYYUyH4qeM4HrjO68t9J664SlW1a1wjM8YYk5L8ZBynxj0KY4wxFYafjCPT5zxj\njDF7AT91HDOAjbhxOBZ7r5eLyAwR6RHP4IwxxqQePxnHt0B/VW2kqvvgiq6GATcC/4lncMYYY1KP\nn4yjp6qODbxR1e+A3qo6BagWt8iMMcakJD91HFtE5F7gU+/9hcBWEUkDCuIWmTHGmJTk547jEqAl\n8KU3tfLmpQEXxC80Y4wxqcjPQE6bgFvCfLwktuEYY4xJdX7uOIwxxphClnEYY4yJimUcxhhjolJm\nHYeINAb+DrQNTq+qV8cvLGOMManKz+O4/wUmAt8D+fENxxhjTKrzk3HUVNV74x6JMcaYCsFPHcdo\nEekf90iMMcZUCH4yjttwmccuEdkuIpkisr2shUTkHRHZICJzw3wuIvKiiCwRkTkicmi0wRtjjEm8\nMjMOVa2jqpVUtYaq1vXe1/Wx7veAfhE+PxXo6E3XAq/6CdgYY0xyha3jEJFOqrow3J2Aqs6ItGJV\n/UlE2kZIcibwgaoqbnja+iLSXFXX+ojbGGNMkkSqHL8TdyfwTIjPFOi7h9veF1gd9D7dm2cZhzHG\npLCwGYeqXuv9PT5O25ZQmw2ZUORaXCZG69at4xSOMcYYP5LZcjwd19NuQEtgTaiEqvqGqvZU1Z6N\nGzdOSHDGGGNCS2bGMQq43Hu6qheQYfUbxhiT+vw0ACwXEfkEOA5oJCLpwMNAFQBVfQ34BuiP65o9\nC7gqXrEYY4yJHT99VR0NzFLVnSIyADgUeEFVV0ZaTlUvLuNzBW6KJlhjjDHJ56eo6lUgS0S6AfcA\nK4EP4hqVMcaYlOUn48jz7g7OxN1pvADUiW9YxhhjUpWfOo5MEbkfGAD0FpE0vLoKY4wxex8/dxwX\nAjnAQFVdh2uk93RcozLGGJOyfN1x4Iqo8kVkf6AT8El8wzLGGJOq/Nxx/ARUE5F9gf/hHpt9L55B\nGWOMSV1+Mg5R1SzgHOAlVT0bOCi+YRljjElVvjIOETkSuBT42puXFr+QjDHGpDI/GcftwP3AF6o6\nT0TaA+PjG5YxxphUVWbluKpOACaISB0Rqa2qy4Bb4x+aMcaYVFTmHYeIdBGRmcBcYL6ITBcRq+Mw\nxpi9lJ+iqteBO1W1jaq2BgYBb8Y3LGOMManKT8ZRS1UL6zRU9UegVtwiMsYYk9L8NABcJiL/B3zo\nvR8ALI9fSMYYY1KZnzuOq4HGwEjgC++1jZ1hjDF7KT9PVW3FnqIyxhjjCZtxiMhXgIb7XFXPiEtE\nxhhjUlqkO45/JywKY4wxFUbYjMNr+GeMMcYU46dy3BhjjClkGYcxxpioWMZhjDEmKn4aABYjIo8D\nGcBbqro59iEZY4xJZeW54/gNyAOei3EsxhhjKoCo7zhU9ct4BGKMMaZiiNQA8CUiNwC01uTGGLMX\nilRUNQ2YDlQHDgUWe9MhQH78Qwttc9Zm3pz+JuOWjktWCMYYs1cT1bA3FS6ByHjgZFXN9d5XAb5T\n1eMTEF/peFqIch2c1/k8hp8/PBkhGGNMhSMi01W1ZyzW5adyvAVQJ+h9bW9emUSkn4j8ISJLROS+\nEJ+3FpHxIjJTROaISH9/YUNufq7fpMYYY2LIT+X4E8BM784DoA8wuKyFRCQNeAU4CUgHporIKFWd\nH5TsQWCYqr4qIp2Bb4C2fgLPLbCMwxhjksFPt+rvisgY4Ahv1n2qus7Hug8HlqjqMgAR+RQ4EwjO\nOBSo672uB6zxG3heQZ7fpMYYY2KozKIqERHgRKCbqv4XqCoih/tY977A6qD36d68YIOBASKSjrvb\nuCVMDNeKyDQRmRaYZ0VVxhiTHH7qOP4DHAlc7L3PxBVBlUVCzCtZE38x8J6qtgT6Ax+KSKmYVPUN\nVe0ZXLFjRVXGGJMcfjKOI1T1JiAbCkcErOpjuXSgVdD7lpQuihoIDPPWOxn36G8jH+u2Ow5jjEkS\nPxlHrlfRrQAi0hgo8LHcVKCjiLQTkarARcCoEmlWASd46z0Ql3Fs9BO43XEYY0xy+Mk4XgS+AJqI\nyD+Bn4HHy1pIVfOAm4GxwALc01PzRGSIiASGnR0E/F1EZgOfAFdqWQ1LPFY5bowxyeHnqaqPRWQ6\n7s5AgLNUdYGflavqN7hK7+B5DwW9ng8cHVXEHiuqMsaY5IiYcXgV1XNU9WBgYWJC8seKqowxJjki\nFlWpagEwW0RaJyge37Jys5IdgjHG7JX8tBxvDswTkd+AnYGZqnpG+EXib3PWZlQV18zEGGNMovjJ\nOB6JexTlkJOfw87cndSuWjvZoRhjzF7FT+X4hEQEUh6bszZbxmGMMQlWnqFjU8aKbSuSHULURi4Y\nyfnDz+fGr29kc5YN2W6MqXiiHjo2lbw3+z0ObnIwU9Kn8MvqX/hj8x9UrlSZRjUb0aZeG9rWb0ub\n+u5v45qNY14fsmzrMgq0gP0a7ucr/Q2jb+C16a8Vvv9p5U/8fsPvVk9jjKlQyhzIKdUEBnKK1r51\n9qVP2z70bt2bU/Y7hbb124ZMl52XzUdzPmL51uWc2vFUjml9TKk0BVrA30f9nXdmvQNAhwYdOKrV\nUcxcN5MWdVrw0qkvsf8++wOwPWc7grAzdyfNn2leal0Tr5pIlyZdWLtjLRNWTEBRLjjoAhrWaBj9\nTpbTjt07mLZmGo1qNuLgJgcnbLvG7AlV5aM5H7Fi2wr67dePw/Y9LNkhJUROXg5rMtfQpn4bKpXu\n2i+sWA7k5GcEwKNxvdi2wd2hCKCq2j4WAUSrvBlHsXUgnH/Q+fRo3oM+bfpwREvXY3xeQR4nfnAi\nE1a6ap1KUolh5w3jnVnv8M1i146xWlo19t9nf37f8HvEbWT9I4ubvrmJd2e9S5VKVcK2Ozmy5ZH8\nsfkPtuzaUmz+Vxd/xen7n75nO+rDki1L6PdRP5ZuXQrA4D6Defi4h+O+XWP21DWjruHtmW8D7rs6\n4oIRnNXprCRHFV8TVkzgkpGXsCZzDS3rtuT7y77ngEYH+Fo20RnHQuAO3PjjhWONq2pSCuhjkXGE\n07pea1ZlrIrPysvhmZOf4c4j74zrNu749g6e//X5wveVK1Vm3aB17FNzH3bu3smr017l+SnPIyJ0\na9qN5055jo77dIxrTJNWTeK60dexMWsjl3a5lMdPeJzqlavHdZumYtmWvY0mTzcp9oPsyJZH8svA\nX5IYVexs2bWFn1f9TJNaTejVshfgSjoOfOVAFm1eVCr9+rvW06RWk4jrTHTG8auqHhExUQI17dhU\nNwzYkOwwEqJetXqsv2s91SpXi5iuQF2fk7vzd7M7fzd1q9WNmD6YPFK6fuWZk5/h1iNupcurXVi4\nqXiHAc1rN2f1Hav5aeVPXPbFZfyZ+SfNazfnjl530LddX3q06MHcDXNJ355Oj+Y9aFyrse9YAvvQ\n7N/N2Jq9tXDeS6e+xM2H3xzVesxf25jFY+g/tPRI0/rwnhW9r8pYxcBRA5m7YS692/TmpVNfKvOC\nHDB++XhWZqzk+LbH06Z+m3LHsGDjAk768CT+zPwTgDt73ckzpzzDgo0L6PyfzmGXm3/jfFrUacFH\ncz6iklTi8m6XU6tqrcLPY5lx+KkcHy8iTwMjgZzATFWdEYsAotWqXiteu+A1hvw0hFnrZgFwYKMD\nOarVUfRq2YvqlauzJnMNK7etZEXGClZuW8kfm/+okJ0iZuRkMH7FePrt14/Fmxdz85ibWbltJUe2\nOpIBXQZQNa0qt317GzPXzSy23IUHXchbZ7wV9lHlAi3ghSkvcOd3oe9mMnMyGbtkbKlMA2DtjrUM\nmzeM2769jY1ZGwvn3fP9PaRJGgc2PpB5G+ahKE1qNeGOXnewT4196NO2D23qtWHS6knUqVonbHn0\n8HnDi2UaALeMuYUbD7uRxyc+ztO/PE3VtKr864R/cVnXy1iwaQH1qtWjXYN2ZR7PaGTlZvHN4m/I\nK8ij3379qF+9fsT0gR9gWblZ1KhSo7DsOSM7gzvG3sHYpWNpXLMxFx98Mb1a9qJP2z4xjTfYtuxt\n3P3d3czfNJ8+bfpwfufzmbdxHg1rNKTffv2oJJVYnbGaSasncXCTg2NWrxU4rz5f8DnNazfnmZOf\noU39NuQX5LNo8yKGzx/OvnX25YpDrqBypcqs3LaSW8bcQv3q9bnm0GsYt3QcVdKqcNNhN7FPzX0A\nWJO5hqzcLNo3aF+sPL9mlZohY8jNz6VKWpVy78PfPvkbc9bPAWDYvGHUqVqHt854C4Ap6VNYnbGa\nvu36FsYXcN/39/HkpCcL3399ydf071g6YwtYlbGKpyc9TW5BLrf3up1OjToVfvbK1FcKMw2A5399\nnnuOvof1O9dHjL3zfzpTv3p9tmVvA+C5Kc8x87qZxTKPWPFzxzE+xGxV1b4xj8aHnj176rRpbiDA\n7LxsgDKLMXbu3snk9MmMXz6eV6e9WurC9Ff03CnPUTWtKos3L+bKQ66kW7NuAOQX5HPa0NMYu3Rs\n2GVP63gaU9KnsHlX/EojL+92Oe+f9X6p+fd/fz9PTHqi1PwvLvyCsz87u9i82lVrs2P3jsL3vVr2\nYnf+bmpUrsGazDX0atmLe4++l8ETBjNz7UwObX4oFxx0AY1rNub4dscXuxCNmD+C84afB7hj9+nc\nT/n1z18j7kPTWk35dsC3vPzby4Vl7QBdm3bluh7XkVeQx8gFIwvrzIId0/oYBnQZQLsG7Ti5w8ml\nPl+4aSErt62kXYN25OTl0LJuSxrUaFD4+c7dO1mVsYq5G+ZSuVJljmt7HPWq10MQ2jzfhtXbV5da\nZ0Cz2s1Yt6No9OcBXQfw1t/einhnm5ufy8u/vczzvz5P//368+KpL5a6QL869VVu/ObGsOsIqF21\nNi/2e5GrR10dNs2O+3fwxM9P8NjExwrnXdLlEm467Ca6Nu3K9DXTOe7940IuO+bSMZzY/kQqSaXC\n//G27G1kZGfQul7rwqcYM3MyuXPsnUxcNZEDGh3Asa2P5e5xd5daX4/mPWhRpwVfLfoKgIY1GvLD\n5T/QvE5zHvvpMRZvWcy3S74ttdz2+7ZTu2ptPp37KVPXTKVb025cdPBF7MrbRcMnG6JB49otvXUp\n7Ru4auNQpQBPnvgkXZp0CXmXFUmbem24vNvlHNLsEM7tfG7iiqpSTXDGUR4Z2RkMnz+cob8PZfyK\nUHmiSbTDWhzG8POHs3bHWo58+8iEbLNmlZpsvXcrVdOqMvT3oVw68tKEbDeUe466hydPepKJKycy\nf+N8rv/6+pDpTmp/El2bdqVprabc97/7CosoA9rVb0f69vSYdQD67pnvsmjzIpZsWcLw+cNDpvnx\nih9pW78t//jhHwz9fWhMtgtQt1pdtudsj9n6Aro06cLL/V/moMYH8fCPD/PKVD+DmZZ2Vqez+HLh\nl2WmG9h9YLEfFZGsvH0lbZ4PX8R1xgFnMOqPkkMaRWEw8c84RGSAqn4kIiHLM1T12VgEEK09zTiC\nLdy0kLu+u4uvF38dk/WZimdA1wF8NOejZIdhEiy4SGevEcOMI9JDwIGCsTphpgqvU6NOjL5kNAUP\nFdC5cfhKJ/PXZZnG3mmvyzRiLGzluKq+7v1NyU4OY0lEmHfjPMDVAYgIN359I69Pfz3JkRljTOrZ\n6+o4opFXkMeXC7/k+2Xfc/dRdzNr3SzyCvLo37E/darVYdHmRUxePZljWh9Dh4YdmJI+haPePqpY\npZcxxqSERNRxpKpEZhzltXP3Tm7/9nbemvlWyM/b1W/HbUfcxtKtS1m6dWlhq3RjjInGukHr+L/x\n/8ebM94sO7FlHKmdcQTbnLWZhZsW0rZ+W/atu2/INJk5mezYvYMxS8YwadUklmxdwk8rf0pwpMbE\nxyVdLuHjcz4GXMegnV/pTE5+ThlLRe/LC7/kzE5nsjpjNUMmDAn7w+2vYsKVE+jdpjcA8zbM44xP\nz2DZ1mXhF0hkxiEijwNPqeo2730DYJCqPhiLAKJV0TKOeFi2dRkdXuwQ03W+fOrLfDbvMyaumhjT\n9e6NPjz7Qy7tcilbs7fya/qvDJ07NKmV8Kd1PI3xK8bTvHZzBh05qMy2Fn3b9eWH5T/EZNvnHHgO\nIy4YUWr+7HWz6fV2r8K2WOVxQrsTmLluJt2aduPybpdz5SFXFvu8QAu449s7ePG3F8u1/od6P8SQ\nn4aUO754qVetHhvu3kDVtKqlPpu/cT4H/eegUvNHXDAise04RGSmqnYvMW+Gqh4aiwCiZRlHkfTt\n6Qz+cTDLty1n/4b7M/DQgXRv1p20SmmFaXbl7iIrN4sGNRoUNobKzc9l3sZ53D3ubvZvuD83HX5T\n4VNlgfNh3Y51PP3L0+zYvYMbet5A9+bd+XP7n7w+/XUWb1nMz6t+Jjsvm01Zm+jbri9T0qcUGwf+\npVNf4qpDruKzeZ8Vpnv4x9KdJ57V6Sx+Wf0LG3b660bmhp43MHPdTKakTwFc48/d+bsp0ALqVK3D\nC/1eoEpaFU7pcAqNazUmMyeT92a9x63f3gpA7za9effMdzn7s7MLWwgDXHTwRXx49odUrlT0vMgH\nsz/gii+vKHzfrWk3Zl43kx9X/MiuvF2c1P4k0iqlkZ2XTdW0quQX5IdtRPfjih+Zs34OvVr24vB9\nDy+cv3LbSobNG0ZWbhaLtyzm49/dL/PD9z2c3/78zdcxiSTQWWaBFhRr8Dh2yVjWZK7hxPYnUqda\nHS4ZcQnT1kzjtdNf45wDz6FAC7hm1DW8O+vdwmW6NOlC/er1w/646NuuL99f9j3g+lqqUaVG2Bbe\nwQq0gPHLx7Mrbxc9W/RkwcYF9GzRkzrVij+8uTlrM+NXjKdDgw50b949zNpCW7BxAf2H9vc9hs/4\nK8ZzXNvj2LBzA+cNOy+qH1RNajVh2a3LGL1oNBeNuIjKlSoz8oKR/O2Av/Hd0u8Y+vtQGlRvwHU9\nrytsMR74LpfV5uOk9ifx3CnPcVCT0plDwKMTHmXwhMEUaAGHNDuEH6/40TUOTXBfVXOAw1Q1x3tf\nA5imquEjjyPLOFJTRnYGz095nq3ZW7nm0GtCdmOhqizYtIBh84ZRt1pdru95feGFZXvOdpZuWcq6\nHevo1qwbgjBx1URmrJ3BeZ3Po2eL8Of7ll1byM3PpWntpmHTFGgBmTmZ1Kter3BeXkEeW3ZtCdsX\nUaDb7s8XfE6zWs345wn/pFHNRn4PScypKiKCqrJ061KqV65Oy7otyc7LplpaNRZsWsCyrcvo0bwH\nzWo3i+s4L/kF+cV+oFQUmTmZfD7/c2pXrc1Znc6iSloVsnKzeHvG2yzYtIBjWx/L8e2Op1ntZmWu\na8W2FWzYuYEDGx3IlPQpvDnjTepXr8/DfR4OWyztR05eDt8t/Y4aVWrQt11ftudsp3KlylGNdro2\ncy278nbRrn67wvMg0RnHPcAZwLuAAlcDo1T1qVgEEC3LOIwxJnoJ7eRQVZ/y7jpOxI3F8aiqhu/o\nyJgwCgrgo4/ghx+gfn045xzo3RsWLHDz6tSBk06C5kHjXeXnQ1rF+2FbbtnZsHVr0TFYtw5++w1U\n4dhjoWHixvcqRhX+qgNV5uXBlCnu2PfqBbV9/LDfsgW+/x6WLoU+faB6dXcON2kCJ54ILVrEPs5t\n26BWLahS/j4cY6bMjENE2gE/quq33vsaItJWVVfEOziTevLzYeFCaNcOagYVX0+dCp99BllZcMwx\ncPHFxS80eXlwxRUwNKg7oxdfhM6dYd48f9u+8Ua49lqXkezYAd27w/btLuPJyHCZUL16Za8HIDfX\nffEnT4Z994WrroKqJeoa//wTHn0UfvoJ2raFK6+ECy7wt/5oLVsGBx4Iu3dHTnfyye7iNHUq9Ojh\nLibfhHma+7HH4B//KP8Ff/Vq+PhjuP9+9759e3dRrFvXXSQPO8xdMN9+221jyBC47bY9z2AyM92P\niFjKzYVZs2DcOBg/HiZOdBf8jh3dPm4Lakh+1VVF+xRs7lx46CH44ovI26pfH0aPdhnQl1/Chg1w\n/PHuh5KIy2xWr3bbrxTUd8eOHe77cZ033lDr1tCypfuefe+qjqhbFy66CF56qfT5GrBpk0u/davb\nbqdOodPtEVWNOAHTgKpB76sCU8taLl5Tjx491Dg5OapTpqh+9JHqypXRLbt4seoDD6i+9JLqsmXF\nP9uxQ3XQINXGjVUbNlS99FLVOXNUX39dtV49Vff7s+zp6KNVK1Xynz4WU9u2qvfco/rJJ24/1q5V\nrV276PMePVTffDP0sl9+6fa/oED1999V27UrnebFF4sfq7w81d9+c8fx7rtVhw1T3bmzeJoVK1Qf\ne0z1/PNV//lP1XXrVPPzXXyffBL/Y3L66aqdOqkeeqjqf/6jOnmy6g03qPbr5+K5+eaitIcd5uK6\n5Zbybat2bbd/I0a47ezeHfk83LFD9YgjSq9nv/3ceb19u0uXne2O/cCBqrfdprpokf9z/bvvyrcv\n2dmq//1v7P4P3buXnnf55aqbN4f+LNJ06aWh93Xy5NLf0ebN3TmJq5smFlPZCWBWiHmzfa0c+gF/\nAEuA+8KkuQCYD8wDhpa1Tss43An95JOlT6a+fVVzc12adetUhw5V7dNHtVkz1XPPVZ0+3WU2oU7E\nxx5THTJEtVev2H1R/spT/fqqTZqE/qxHD9WtW1XXrFH9+9+TH2syp8MOK/3DRNUdm2OP3fP19+rl\ntnHvvaobNxbfxtKlyd//eE2esV+xAAAfPElEQVRpaS7DCZabq1q9eqTlEptxjAPOCHp/JvA/H8ul\nAUuB9t5dymygc4k0HYGZQAPvfZOy1vtXzzi++cb9orrrLtUJE4rm5+e7i/7y5ck/aW2yKZqpY8fi\n5/jo0fHZTrduRXd7kyYlf7/jPV12WfHrw8UXl7VMYjOODsAUYBWwGvgF2M/HckcCY4Pe3w/cXyLN\nU8A10QRcETKOzExX7PH446r/+1/RXUAoW7a4tAMGJP9EtMmmeE2ffebO9+efj+92Xn7ZFR0me38T\nNb34oisOPOccP+ljl3H47nJERGrjHt/N9Jn+PKCfql7jvb8MOEJVbw5K8yWwCDjau0MZrF4lfIl1\nXQtcC9C6deseK1eu9BVzMqxZA337wh9/FJ9/wAFuXpUqcPPN8OCD7v1RRyUnTmPM3iaxY44jIqcB\nBwHVA41JVLWstvihnq0omUtVxhVXHQe0BCaKyMHqdW9SuJDqG8Ab4Npx+Ik5GTIy3BM6oQQyktxc\neO45NxljTEXk53Hc14CawPHAW8B5gJ++ENKBVkHvWwJrQqSZoqq5wHIR+QOXkUz1sf6kys93j+Ut\nX+4eTd2+HQYOTHZUxhgTf37uOI5S1a4iMkdVHxGRZ4CRPpabCnT02oH8CVwEXFIizZfAxcB7ItII\n2B+I0L1j4hQUwFdfwVNPubYBN93kngF/4QX3XPTChcmO0BhjksNPxrHL+5slIi2AzUC7shZS1TwR\nuRkYi6u/eEdV54nIEFwlzSjvs5NFZD6QD9ytqpvLsyOxlJdXunXmmDFFry3TMMbszfxkHKNFpD7w\nNDADV0/hY9QQUNVvgG9KzHso6LUCd3pT0hQUuBbBI0r3/myMMaYEP31VPeq9HCEio4HqqpoR37AS\na2/qC8kYY/ZUpbKTFFHVnL9SprFtG5x6arKjMMZ54IFkR2CMP1FlHH8VGzbALbdAgwbwbalWIyaR\nqlWDrl3dgwiDBiV22998A4sXJ3ab4Vx8seuU8IUXQn8+ciTs2uU655s5E5qVPVxEQjz5JDRK3hAl\n5fLdd3D00cmOooKLVUvCRE170nI8MzP5LT0TOZ1zjuqHH6reeqvqO++obtrkuiZIdlyBqVcv16Fg\nwH33JWa7b71V/LyIZUd25Zn69XOd/QWMGKF6yimuw8bLLlOdNy/8Ob1iheqCBe447ml3NFlZxdf9\n55/uO1NQoHr77cXTpqWpbtjg0m3cqCqSuOPVpo2/dK1aqb77ruunrXdv1UcecZ1Sqqo+99yex3He\nea6zxUhp5s1Tvf56f+vr1Cnexy6xXY6U6pcq1LxETeXJOPLzXc+giTqxYz394x/lW27UqNDH43//\nK6sztNhP48apHnCAe52W5i6WW7cWj+v992OzrQEDii4Qzz6r2rSpm9+0aenebQN273a95t5xR3Qd\nPaalFX/fpYvrkVfVZdZlLb9iRdSnc1h5eaoNGviL+7LLXA+5F17oOhz0Iz9fdds212daSYno5RdU\nDzlE9ddf/aXNzw+/LwUF7gfVnsQyYoRbT/v2oT9/4IHS2127Nvz65s51PdnG+phVqhT4wZiAjAOo\nDjTEdU7YwHvdEGgLLIhVANFO0WYc27ap1q0b+39GoqZTTnH78e67qscco1q5spt/7LGqr7yiOmNG\n8W6xA9MNN0T+4pS3Px8R1eOOi26Zp54q2u7q1aoZGaFj2rFDtWrV6NY9Zozq+vXuzmrwYNXx44sy\njYCCAtX09OJ3N2UZN879Urz8cteX2IQJRRkQqB50UNFFf/Nm1znlzJnFt1FQoPrgg6Hj7tgxtplG\nwLXXhj9WjRq5vpyiOQ5+FRS4buWj+d8FXySbNHEZ/sCB7nh//XXp9FWquJ6hp03zd174sXatu+Oc\nPTvy+rp1K/7+n/8sOo4jRhR9LwNThw7uXA/lww9LDzfwzDPus2eeif015OOP3boTlXHcBiwHcnCN\n8pZ702zg5lgFEO3kJ+PIy3Mn1yGHxP6fkOgpO7vM3VVVd8v87LOuuGfsWH8Xh0jbfeghd8EP/vX9\n8MOqf/zh1v3MM6oHH6xas6Zq166uW/YxY9z4HcHr6du3aEwFP6IZB6JVq8gdSMZafr7rmn7p0ugu\nvuvXq371lerUqe4ucOrU+MWdlaXav3/x43TzzZF/RMRKQYHqwoVu3JaxY1Xnz1etUyf0/+6448pe\n35IlbpyTRx5R/fnnovkzZ5Z9bpQnczzqqNDrGjDAHb9p01SHD3dFeCX98IPqFVe4MWjuussNaxDJ\nnDmqTzzhvje//VY0f9Mm930qGcN115Xv+lG1alF384kuqrolVhuLxRQq48jNdReyLl3cOAnJvtiD\n6uGHu95x92QdJcucYy3StgcP9reOkhekFStc+fGNN7pfVrt2RRdTfr7LtDp0UK1Rw91xzZ/vBqxq\n0aIovgMPdBdwU1pBgfshMXJk+F+9ifTWW+5uIfi7sWlT+ddX1t1y69blW++4carVqhVfV4MGLqNK\npE2bVO+/X/XII11R4qhRbmCxaK8f1aq5O+GAWGYcZfaOKyLnA9+qaqaIPAgcCjymqjNiUz0fnebN\ne+oll0xj9243TOnSpW5ozzJ2I6GWLIEOHYreT5vmhtkMOICFXM07jOIMJnEM06fDhx/CnDlun044\nwQ35WTg0a2DnYjzoc6TVDRsG558f082Flp7ueoYMEUzJ8cYLCtwxqlvX9Q/2Vx0Du9DGjW4nK1d2\n45CuWhXbHc/Lc+P2dulSfAzTcHbvduOb+h34fONGN2D6CSewM786kye74VA7dtyzXVi0yPU2Hc4n\nn7jhVaOycyfUqsXEifDaa/D77+47O2iQG9442cra52AffOBOmb59oWnTovkisesdt8ycBZjj/T0G\nmIgbyOnXWOVc0U7QI+Z3ByLFhxdNI1dbsTLsrd9DD6keVGu5tmeJgmrN6vn6wSvbQw+TuWyZ6po1\n+vzzqk1Ypw/zcLEV5lFJC1q1cvfiQ4a4n9GvvebKhbZvV/3iC5e2Th1XK7h5syv7UHWPvCxb5io6\ntm5148Eed5zbmaOPduVcO3eqfv65+6xE4f8jjwQdA/ILX9eqpZq9fptbNi/PVZhUqqT6r38V1fyW\n9MMPrlxhxgwX/2mnuce6tm51P4HPPtut/Nxz3b3z55+rnnSSm3fOOa4S4qmnXCH36tWutjojw93C\nPP540c++4DKISOUvOTmuXGH69PBpwpVnLF/uygYmTXLvFy1y5S+rV6u+8IIbV3fuXDe26ejR7tGZ\nFStchcigQe4n4+efq558stu3Bx90aYcOdWV/J5/sbqHy8lwl3Ntvu1r7m292ZRiqrmwr1Al4/fWq\n773nyiUDjzUFmz/fnSczZ7pKsN69VX/5xf1k/eYbF8ebb7rb9BNPLFrvoEHu9nDoUDdk5JVXuvNs\n4UK335s3F31Zhg4t2l5enupPP7lKoIICl/auu9w+BdZdpYp79GvePNVVq1yap55yFQsB333nRiJ6\n9tnSY+8uXuzOc6+MMC9P9Yh6C/QZ7tBTGKM12FnsEGVna1HFX8OG7tzMzAx/Hjz6qDu/zz47crrA\n/gaXVZU8h/Lzw5dTLVjgzqPA7daOHa4CYt48dx4EKygoVsa7YoVqDXZqJfK0KtmF+9qMNfojvfV7\n+mpH/tBrec3tbwgkuKhqpvf3X8AlwfOSMR1IDZ1KD53C4foLvfRnjtK5dNavOVUnc4R+Qz9V0J3U\nCPnFm8Cx2poVCu78fvlldx4/8YSXMZCtm3AF9Y9zn1ZmtzbnT33k/3L1ted36Yrpm9w/JlwuFK5Q\nN1WmI45QbdnSFUKH+DyTWv7XdeaZqVOR1KxZ5M8vvdTVcD7yiBtPt1aI/bziir1j6LhYTd9+u+fr\n+Ogj96MheN4pp7gMeeLEqNY1jwM1c79ukdP9+9+qP/7oni4Jftqh5NS2rXsa5bzz3JCc8+a5jLpL\nF/f5SSepdu4ceVuBstXevV2GEfxZuGenr746+idEQk0nnOAypt273QVu+nSNZcbhp6hqNK532xOB\nHrhOD39T1W4xueWJUk8RnRarlQ0ZApdeCpMmweWXx2qtxhiTcgRiVlTlJ+OoCfQDflfVxSLSHOii\nqt/FIoBoxTTjMMaYvUQsMw4/nRxmicgGXB3HYiDP+5sU8zmQw3ifShSQRj5V2U0rVlOZPFqwhixq\n0oQN7KYqfZjAcUxgadVOtK25gbRtW5IVtjHG/HX4qON4GPgKWOS9bwFMSlYdR1mV4127urrAsM9x\n72m/DKE2+Mor/ptiH3mk6vffu0rkc891Lbauuip8+i5dilptvfyyK3P94oui1lPdu7tK6wcfdBU1\nl12methhRcsfemjkstxop2iaVUc7PfWUq1SdPLn4/Hr1il6XbDkVPCWy34vg6bnnXJl4NMv06eMa\nwgTen3JK+LRnneUqZCdMcK3sAs8hDxlSOm2rVq4pc8OG7vwqKFDdssWdI4E0deq4SvjfflO95BLX\njL/kelq0cOfRuHFF82691bV8LGvf7rrLPShwzDGqn37qHvQomSa48ry8U/fuqp995h7Y+PLLPVtX\npPMqEdO//qV6+ulRLVODndqWZb7Tk+DK8Vm48cNnBs2bk6yMo0WLHvrvf7uHUF57zXXrMH58Odo8\nrFrl+t64807XbHXkSPe0z5w57qmO2bOLnu5YutR9eW+91fVJAS4DCJc75ea6Zd55x20n8KRUenr4\neAKPZK1b577oBQV71sR3+/biT1Ht3u2anz/2mHt6Ztcutx1VV9n/xRcu/a5d7gmWwNMlwZ0oBSso\ncE+75OQUxTl2rHsAfdWq0ulzcty+vfWW6quvumMUaf/y8zX0Y2pBfv/dPREzd27RvIkTXWberp27\nMObmqq5c6bY1a5aLcf16t34/xzc/3z2hlJVV9D/Jz3f7OXCgO1bBtmxxn69fX9TKb/dul7mccYZr\ncPT+++7Rn/x8lxmEa5CyapW78PppQZmbW9TSK5zs7NLxBsvKKt38PZQdO9y6Nmxw/ZXk56s++aSr\nLJ48uexYg61Z4zLAM890T68tW+aeCDzkEPdk21dfuf/v9u0uvrlz3XYjnRsLFxYdi5wc9zRTXp6L\nefZst+zy5e5iHfxkVziLFqlecIGL89dfXSvbY491v1C3bnUxLlrk/ufr1xc9GZeX5xpiBF/A33vP\nPfHVqZN7gizcE1i7drn//e+/F51HGRla8N9RuvCJL/Td57bqf//rfT3feMOt+7DD3BNu4JqyB85b\nr2VjLDMOP3Ucv6nq4SIyQ1UPFZFawGRV7Rqfe6DIevbsqdOmWS2HMaYCmTnTtYM5/PD4NEAqKAjf\nFic3FypXRipVSlwdBzBMRF4H6ovI34GrgbdisXFjjNkrdO8e3/VHasBZchzsGPBTOf5vETkJ2A4c\nADykquNiHokxxpgKocyMQ0SeVNV7gXEh5hljjNnL+BkB8KQQ82zAVWOM2UuFveMQkRuAG4H2IjIn\n6KM6wKR4B2aMMSY1RSqqGgqMwfVRdV/Q/ExVtZZ0xhizlwqbcahqBpABXJy4cIwxxqQ6P3Ucxhhj\nTCHLOIwxxkTFMg5jjDFRiTrjEJHvRWSMiJzuI20/EflDRJaIyH0R0p0nIioisRnW0BhjTNz46XKk\npMuB5kCvSIlEJA14BdcOJB2YKiKjVHV+iXR1gFuBX8sRizHGmATzdcchIjVE5AAAVV2jqtNV9ZUy\nFjscWKKqy1R1N/Apbrzykh4FngKyo4jbGGNMkpSZcYjI33Bdq3/rvT9EREb5WPe+wOqg9+nevOB1\ndwdaqeroMmK4VkSmici0jRs3+ti0McaYePFzxzEYd/ewDUBVZwFtfSwXqu/gwj7cRaQS8BwwqKwV\nqeobqtpTVXs2btzYx6aNMcbEi5+MI89rDBitdKBV0PuWwJqg93WAg4EfRWQFrs5klFWQG2NMavOT\nccwVkUuANBHpKCIvAb/4WG4q0FFE2olIVeAioLCIS1UzVLWRqrZV1bbAFOAMVbVRmowxJoX5yThu\nAQ4CcoBPcONy3F7WQqqaB9wMjAUWAMNUdZ6IDBGRM8ofsjHGmGQqc+jYVGNDxxpjTPREJHFDx4rI\neIIqtQNUtW8sAjDGGFOx+GkAeFfQ6+rAuUBefMIxxhiT6vyMOT69xKxJIjIhTvEYY4xJcX6KqhoG\nva0E9ACaxS0iY4wxKc1PUdV0XB2H4IqolgMD4xmUMcaY1OWnqKpdIgIxxhhTMYTNOETknEgLqurI\n2IdjjDEm1UW64/hbhM8UsIzDGGP2QmEzDlW9KpGBGGOMqRj8dKu+j4i8KCIzRGS6iLwgIvskIjhj\njDGpx09fVZ8CG3EN/87zXn8Wz6CMMcakLj+P4zZU1UeD3j8mImfFKyBjjDGpzc8dx3gRuUhEKnnT\nBcDX8Q7MGGNMaor0OG4mRQ3/7gQ+9D5KA3YAD8c9OmOMMSkn0lNVdRIZiDHGmIrBT1GVMcYYU8gy\nDmOMMVGxjMMYY0xU/DyOi4ikAU2D06vqqngFZYwxJnX5GY/jFtwTVOuBAm+2Al3jGJcxxpgU5eeO\n4zbgAFXdHO9gjDHGpD4/dRyrgYx4B2KMMaZi8HPHsQz4UUS+BnICM1X12bhFZYwxJmX5yThWeVNV\nbzLGGLMX8zN07COJCMQYY0zFEKmvqudV9XYR+Qr3FFUxqnpGXCMzxhiTkiLdcQQ6Nfx3IgIxxhhT\nMUTq5HC693dCeVcuIv2AF3A96r6lqk+U+PxO4BogDzdA1NWqurK82zPGGBN/cetyxGtt/gpwKtAZ\nuFhEOpdINhPoqapdgc+Bp+IVjzHGmNiIZ19VhwNLVHWZqu7GDUF7ZnACVR2vqlne2ylAyzjGY4wx\nJgbimXHsi2s8GJDuzQtnIDAmjvEYY4yJgTIzDhEZJyL1g943EJGxPtYtIeaVejrLW+cAoCfwdJjP\nrxWRaSIybePGjT42bYwxJl783HE0UtVtgTequhVo4mO5dKBV0PuWwJqSiUTkROAB4AxVzSn5ubfN\nN1S1p6r2bNy4sY9NG2OMiRc/GUeBiLQOvBGRNoS5cyhhKtBRRNqJSFXgImBUcAIR6Q68jss0NvgP\n2xhjTLL46XLkAeBnEQk8ltsbuLashVQ1T0RuBsbiHsd9R1XnicgQYJqqjsIVTdUGhosIwCprWGiM\nMalNVMu+eRCRRkAvXL3FZFXdFO/AwunZs6dOmzYtWZs3xpgKSUSmq2rPWKzLT+X42UCuqo5W1a+A\nPBE5KxYbN8YYU/H4qeN4WFULx+PwKsofjl9IxhhjUpmfjCNUGl9jlRtjjPnr8ZNxTBORZ0Wkg4i0\nF5HngOnxDswYY0xq8pNx3ALsBj4DhgPZwE3xDMoYY0zq8jOQ007gvgTEYowxpgIoM+MQkcbAPcBB\nQPXAfFXtG8e4jDHGpCg/RVUfAwuBdsAjwApcq3BjjDF7IT8Zxz6q+jauLccEVb0a1xjQGGPMXsjP\nY7W53t+1InIarqNCGzfDGGP2Un4yjsdEpB4wCHgJqAvcEdeojDHGpCw/T1WN9l5mAMfHNxxjjDGp\nLp4jABpjjPkLsozDGGNMVCzjMMYYExU/DQCrAecCbYPTq+qQ+IVljDEmVfl5quq/uIrx6UDIMcGN\nMcbsPfxkHC1VtV/cIzHGGFMh+Knj+EVEusQ9EmOMMRWCnzuOY4ArRWQ5rqhKAFXVrnGNzBhjTEry\nk3GcGvcojDHGVBhhMw4Rqauq24HMBMZjjDEmxUW64xgKnI57mkpxRVQBCrSPY1zGGGNSVNiMQ1VP\n9/62S1w4xhhjUp2fOg5EpAHQkeIjAP4Ur6CMMcakLj8tx68BbsONwTELN4jTZMCGjjXGmL2Qn3Yc\ntwGHAStV9XigO7AxrlEZY4xJWX4yjmxVzQbXb5WqLgQOiG9YxhhjUpWfjCNdROoDXwLjROS/uOFj\nyyQi/UTkDxFZIiL3hfi8moh85n3+q4i0jSZ4Y4wxiednBMCzvZeDRWQ8UA/4tqzlRCQNeAU4CUgH\nporIKFWdH5RsILBVVfcTkYuAJ4ELo9wHY4wxCRTxjkNEKonI3MB7VZ2gqqNUdbePdR8OLFHVZV76\nT4EzS6Q5E3jfe/05cIKICMYYY1JWxDsOVS0Qkdki0lpVV0W57n2B1UHv04EjwqVR1TwRyQD2ATYF\nJxKRa4Frvbc5wZlZCmtEif1IURZnbFWEOCtCjGBxxlrM6qb9tONoDswTkd+AnYGZqnpGGcuFunPQ\ncqRBVd8A3gAQkWmq2rOMbSedxRlbFmfsVIQYweKMNRGZFqt1+ck4HinnutOBVkHvW1K6Uj2QJl1E\nKuPqT7aUc3vGGGMSwM9TVf29uo3CCejvY7mpQEcRaSciVYGLgFEl0owCrvBenwf8oKql7jiMMcak\nDj8Zx0kh5pXZ1bqq5gE3A2OBBcAwVZ0nIkNEJFDM9Tawj4gsAe4ESj2yG8IbPtKkAosztizO2KkI\nMYLFGWsxi1PC/cAXkRuAG3G94C4N+qgOMElVB8QqCGOMMRVHpIyjHtAA+BfF7wQyVdXqIYwxZi8V\nNuMwxhhjQvFTx5EyyurCJIFxtBKR8SKyQETmicht3vzBIvKniMzypv5By9zvxf2HiJySwFhXiMjv\nXjzTvHkNRWSciCz2/jbw5ouIvOjFOUdEDk1QjAcEHbNZIrJdRG5PheMpIu+IyIbgtkPlOX4icoWX\nfrGIXBFqW3GI82kRWejF8oXXdRAi0lZEdgUd19eClunhnS9LvH2JaYPcMHFG/X+O97UgTJyfBcW4\nQkRmefOTcjwjXIfif36qaoWYgDRcXUt7oCowG+icpFiaA4d6r+sAi4DOwGDgrhDpO3vxVgPaefuR\nlqBYVwCNSsx7CrjPe30f8KT3uj8wBte+phfwa5L+z+uANqlwPIHewKHA3PIeP6AhsMz728B73SAB\ncZ4MVPZePxkUZ9vgdCXW8xtwpLcPY4BTExBnVP/nRFwLQsVZ4vNngIeSeTwjXIfifn5WpDsOP12Y\nJISqrlXVGd7rTNxTY/tGWORM4FNVzVHV5cAS3P4kS3BXL+8DZwXN/0CdKUB9EWme4NhOAJaq6soI\naRJ2PNUNWFayTi/a43cKME5Vt6jqVmAc0C/ecarqd+qebgSYgmtLFZYXa11VnazuivIBRfsWtzgj\nCPd/jvu1IFKc3l3DBcAnkdYR7+MZ4ToU9/OzImUcobowiXSxTghxPfp2B371Zt3s3Qa+E7hFJLmx\nK/CdiEwX13ULQFNVXQvu5AOapECcARdR/AuZascToj9+yY4X4Grcr82AdiIyU0QmiMix3rx9vdgC\nEhlnNP/nZB/PY4H1qro4aF5Sj2eJ61Dcz8+KlHH46p4kkUSkNjACuF1VtwOvAh2AQ4C1uNtZSG7s\nR6vqobi2NzeJSO8IaZN6jMU1FD0DGO7NSsXjGUm4uJJ9XB8A8oCPvVlrgdaq2h3XfmqoiNQleXFG\n+39O9v//Yor/uEnq8QxxHQqbNEw8UcdZkTIOP12YJIyIVMH9sz5W1ZEAqrpeVfNVtQB4k6Lik6TF\nrqprvL8bgC+8mNYHiqC8vxuSHafnVGCGqq6H1DyenmiPX9Li9So6Twcu9YpL8Ip+Nnuvp+PqC/b3\n4gwuzkpInOX4PyfzeFYGzgE+C8xL5vEMdR0iAednRco4/HRhkhBeGefbwAJVfTZofnB9wNlA4ImM\nUcBF4gauagd0xFWaxTvOWiJSJ/AaV1k6l+JdvVwB/Dcozsu9py96ARmBW94EKfZLLtWOZ5Boj99Y\n4GQRaeAVw5zszYsrEekH3AucoapZQfMbixsvBxFpjzt+y7xYM0Wkl3eOXx60b/GMM9r/czKvBScC\nC1W1sAgqWccz3HWIRJyfsarhT8SEeypgES5HfyCJcRyDu5WbA8zypv7Ah8Dv3vxRQPOgZR7w4v6D\nGD+pEiHO9rgnTmYD8wLHDNd1/f+Axd7fht58wQ2+tdTbj54JPKY1gc1AvaB5ST+euIxsLZCL+2U2\nsDzHD1fHsMSbrkpQnEtwZdeBc/Q1L+253vkwG5gB/C1oPT1xF+6lwMt4bb3iHGfU/+d4XwtCxenN\nfw+4vkTapBxPwl+H4n5+WgNAY4wxUalIRVXGGGNSgGUcxhhjomIZhzHGmKhYxmGMMSYqlnEYY4yJ\nimUcxiSQiBwnIqOTHYcxe8IyDmOMMVGxjMOYEERkgIj8Jm58hddFJE1EdojIMyIyQ0T+JyKNvbSH\niMgUKRr3IjD+wX4i8r2IzPaW6eCtvraIfC5urIyPvRbAxlQYlnEYU4KIHAhciOsg8hAgH7gUqIXr\nS+tQYALwsLfIB8C9qtoV1yI3MP9j4BVV7QYchWuJDK4X09txYye0B46O+04ZE0OVkx2AMSnoBKAH\nMNW7GaiB6yiugKLO7T4CRopIPaC+qk7w5r8PDPf6CNtXVb8AUNVsAG99v6nX15G4UeTaAj/Hf7eM\niQ3LOIwpTYD3VfX+YjNF/q9Eukj99UQqfsoJep2PfQ9NBWNFVcaU9j/gPBFpAoVjOLfBfV/O89Jc\nAvysqhnA1qDBey4DJqgbFyFdRM7y1lFNRGomdC+MiRP7pWNMCao6X0QexI2cWAnXQ+pNwE7gIBGZ\nDmTg6kHAdV39mpcxLAOu8uZfBrwuIkO8dZyfwN0wJm6sd1xjfBKRHapaO9lxGJNsVlRljDEmKnbH\nYYwxJip2x2GMMSYqlnEYY4yJimUcxhhjomIZhzHGmKhYxmGMMSYq/w8lBvsvdtaiswAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ebc5a7590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
