{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 10\n",
    "N = 25\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.7002, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.6966, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.6957, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.6952, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6948, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6945, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6939, Training Accuracy= 0.504\n",
      "Epoch: 90, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 120, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 140, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 150, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 160, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 200, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 210, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 220, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 230, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 240, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 250, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 260, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 270, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 280, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 290, Loss= 0.6921, Training Accuracy= 0.520\n",
      "Epoch: 300, Loss= 0.6921, Training Accuracy= 0.522\n",
      "Epoch: 310, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 320, Loss= 0.6919, Training Accuracy= 0.524\n",
      "Epoch: 330, Loss= 0.6918, Training Accuracy= 0.524\n",
      "Epoch: 340, Loss= 0.6918, Training Accuracy= 0.523\n",
      "Epoch: 350, Loss= 0.6917, Training Accuracy= 0.523\n",
      "Epoch: 360, Loss= 0.6916, Training Accuracy= 0.524\n",
      "Epoch: 370, Loss= 0.6915, Training Accuracy= 0.524\n",
      "Epoch: 380, Loss= 0.6914, Training Accuracy= 0.523\n",
      "Epoch: 390, Loss= 0.6914, Training Accuracy= 0.523\n",
      "Epoch: 400, Loss= 0.6913, Training Accuracy= 0.524\n",
      "Epoch: 410, Loss= 0.6912, Training Accuracy= 0.523\n",
      "Epoch: 420, Loss= 0.6911, Training Accuracy= 0.524\n",
      "Epoch: 430, Loss= 0.6911, Training Accuracy= 0.525\n",
      "Epoch: 440, Loss= 0.6910, Training Accuracy= 0.525\n",
      "Epoch: 450, Loss= 0.6909, Training Accuracy= 0.526\n",
      "Epoch: 460, Loss= 0.6909, Training Accuracy= 0.527\n",
      "Epoch: 470, Loss= 0.6908, Training Accuracy= 0.528\n",
      "Epoch: 480, Loss= 0.6908, Training Accuracy= 0.528\n",
      "Epoch: 490, Loss= 0.6907, Training Accuracy= 0.527\n",
      "Epoch: 500, Loss= 0.6907, Training Accuracy= 0.527\n",
      "Epoch: 510, Loss= 0.6906, Training Accuracy= 0.524\n",
      "Epoch: 520, Loss= 0.6905, Training Accuracy= 0.526\n",
      "Epoch: 530, Loss= 0.6905, Training Accuracy= 0.526\n",
      "Epoch: 540, Loss= 0.6904, Training Accuracy= 0.526\n",
      "Epoch: 550, Loss= 0.6903, Training Accuracy= 0.527\n",
      "Epoch: 560, Loss= 0.6903, Training Accuracy= 0.528\n",
      "Epoch: 570, Loss= 0.6902, Training Accuracy= 0.527\n",
      "Epoch: 580, Loss= 0.6902, Training Accuracy= 0.527\n",
      "Epoch: 590, Loss= 0.6901, Training Accuracy= 0.528\n",
      "Epoch: 600, Loss= 0.6901, Training Accuracy= 0.528\n",
      "Epoch: 610, Loss= 0.6900, Training Accuracy= 0.529\n",
      "Epoch: 620, Loss= 0.6900, Training Accuracy= 0.529\n",
      "Epoch: 630, Loss= 0.6899, Training Accuracy= 0.530\n",
      "Epoch: 640, Loss= 0.6899, Training Accuracy= 0.529\n",
      "Epoch: 650, Loss= 0.6898, Training Accuracy= 0.531\n",
      "Epoch: 660, Loss= 0.6898, Training Accuracy= 0.530\n",
      "Epoch: 670, Loss= 0.6897, Training Accuracy= 0.530\n",
      "Epoch: 680, Loss= 0.6897, Training Accuracy= 0.531\n",
      "Epoch: 690, Loss= 0.6897, Training Accuracy= 0.530\n",
      "Epoch: 700, Loss= 0.6896, Training Accuracy= 0.531\n",
      "Epoch: 710, Loss= 0.6896, Training Accuracy= 0.531\n",
      "Epoch: 720, Loss= 0.6895, Training Accuracy= 0.531\n",
      "Epoch: 730, Loss= 0.6895, Training Accuracy= 0.533\n",
      "Epoch: 740, Loss= 0.6894, Training Accuracy= 0.533\n",
      "Epoch: 750, Loss= 0.6893, Training Accuracy= 0.533\n",
      "Epoch: 760, Loss= 0.6893, Training Accuracy= 0.532\n",
      "Epoch: 770, Loss= 0.6892, Training Accuracy= 0.534\n",
      "Epoch: 780, Loss= 0.6891, Training Accuracy= 0.535\n",
      "Epoch: 790, Loss= 0.6890, Training Accuracy= 0.535\n",
      "Epoch: 800, Loss= 0.6889, Training Accuracy= 0.535\n",
      "Epoch: 810, Loss= 0.6887, Training Accuracy= 0.536\n",
      "Epoch: 820, Loss= 0.6886, Training Accuracy= 0.537\n",
      "Epoch: 830, Loss= 0.6884, Training Accuracy= 0.538\n",
      "Epoch: 840, Loss= 0.6883, Training Accuracy= 0.538\n",
      "Epoch: 850, Loss= 0.6881, Training Accuracy= 0.540\n",
      "Epoch: 860, Loss= 0.6880, Training Accuracy= 0.541\n",
      "Epoch: 870, Loss= 0.6879, Training Accuracy= 0.542\n",
      "Epoch: 880, Loss= 0.6878, Training Accuracy= 0.541\n",
      "Epoch: 890, Loss= 0.6879, Training Accuracy= 0.540\n",
      "Epoch: 900, Loss= 0.6880, Training Accuracy= 0.539\n",
      "Epoch: 910, Loss= 0.6881, Training Accuracy= 0.537\n",
      "Epoch: 920, Loss= 0.6881, Training Accuracy= 0.538\n",
      "Epoch: 930, Loss= 0.6881, Training Accuracy= 0.540\n",
      "Epoch: 940, Loss= 0.6881, Training Accuracy= 0.541\n",
      "Epoch: 950, Loss= 0.6879, Training Accuracy= 0.541\n",
      "Epoch: 960, Loss= 0.6877, Training Accuracy= 0.541\n",
      "Epoch: 970, Loss= 0.6874, Training Accuracy= 0.542\n",
      "Epoch: 980, Loss= 0.6874, Training Accuracy= 0.540\n",
      "Epoch: 990, Loss= 0.6872, Training Accuracy= 0.542\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5024\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.6980, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 30, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 40, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 70, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 80, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 100, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 110, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 120, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 130, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 160, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 190, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 200, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 210, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 220, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 240, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 250, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 260, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 270, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 290, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 320, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 330, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 340, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 350, Loss= 0.6934, Training Accuracy= 0.496\n",
      "Epoch: 360, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 370, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 380, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 390, Loss= 0.6933, Training Accuracy= 0.501\n",
      "Epoch: 400, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 410, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 420, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 430, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 450, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 460, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 470, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 480, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 490, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 500, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 510, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 520, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 530, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 540, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 550, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 560, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 570, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 580, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 590, Loss= 0.6932, Training Accuracy= 0.501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 610, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 620, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 630, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 640, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 650, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 660, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 670, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 680, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 690, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 700, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 710, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 720, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 730, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 740, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 750, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 760, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 770, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 780, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 790, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 800, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 810, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 820, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 830, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 840, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 850, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 860, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 870, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 880, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 890, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 900, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 910, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 920, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 930, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 940, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 950, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 960, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 970, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 980, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 990, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5007\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.7232, Training Accuracy= 0.494\n",
      "Epoch: 10, Loss= 0.7003, Training Accuracy= 0.494\n",
      "Epoch: 20, Loss= 0.6988, Training Accuracy= 0.494\n",
      "Epoch: 30, Loss= 0.6980, Training Accuracy= 0.494\n",
      "Epoch: 40, Loss= 0.6975, Training Accuracy= 0.494\n",
      "Epoch: 50, Loss= 0.6971, Training Accuracy= 0.494\n",
      "Epoch: 60, Loss= 0.6969, Training Accuracy= 0.494\n",
      "Epoch: 70, Loss= 0.6966, Training Accuracy= 0.494\n",
      "Epoch: 80, Loss= 0.6964, Training Accuracy= 0.494\n",
      "Epoch: 90, Loss= 0.6962, Training Accuracy= 0.494\n",
      "Epoch: 100, Loss= 0.6961, Training Accuracy= 0.494\n",
      "Epoch: 110, Loss= 0.6959, Training Accuracy= 0.494\n",
      "Epoch: 120, Loss= 0.6958, Training Accuracy= 0.494\n",
      "Epoch: 130, Loss= 0.6957, Training Accuracy= 0.494\n",
      "Epoch: 140, Loss= 0.6956, Training Accuracy= 0.494\n",
      "Epoch: 150, Loss= 0.6955, Training Accuracy= 0.494\n",
      "Epoch: 160, Loss= 0.6954, Training Accuracy= 0.494\n",
      "Epoch: 170, Loss= 0.6953, Training Accuracy= 0.494\n",
      "Epoch: 180, Loss= 0.6952, Training Accuracy= 0.494\n",
      "Epoch: 190, Loss= 0.6952, Training Accuracy= 0.494\n",
      "Epoch: 200, Loss= 0.6951, Training Accuracy= 0.494\n",
      "Epoch: 210, Loss= 0.6950, Training Accuracy= 0.494\n",
      "Epoch: 220, Loss= 0.6950, Training Accuracy= 0.494\n",
      "Epoch: 230, Loss= 0.6949, Training Accuracy= 0.494\n",
      "Epoch: 240, Loss= 0.6949, Training Accuracy= 0.494\n",
      "Epoch: 250, Loss= 0.6948, Training Accuracy= 0.494\n",
      "Epoch: 260, Loss= 0.6948, Training Accuracy= 0.494\n",
      "Epoch: 270, Loss= 0.6947, Training Accuracy= 0.494\n",
      "Epoch: 280, Loss= 0.6947, Training Accuracy= 0.494\n",
      "Epoch: 290, Loss= 0.6946, Training Accuracy= 0.494\n",
      "Epoch: 300, Loss= 0.6946, Training Accuracy= 0.494\n",
      "Epoch: 310, Loss= 0.6946, Training Accuracy= 0.494\n",
      "Epoch: 320, Loss= 0.6945, Training Accuracy= 0.494\n",
      "Epoch: 330, Loss= 0.6945, Training Accuracy= 0.494\n",
      "Epoch: 340, Loss= 0.6945, Training Accuracy= 0.494\n",
      "Epoch: 350, Loss= 0.6944, Training Accuracy= 0.494\n",
      "Epoch: 360, Loss= 0.6944, Training Accuracy= 0.494\n",
      "Epoch: 370, Loss= 0.6944, Training Accuracy= 0.494\n",
      "Epoch: 380, Loss= 0.6943, Training Accuracy= 0.494\n",
      "Epoch: 390, Loss= 0.6943, Training Accuracy= 0.494\n",
      "Epoch: 400, Loss= 0.6943, Training Accuracy= 0.494\n",
      "Epoch: 410, Loss= 0.6942, Training Accuracy= 0.494\n",
      "Epoch: 420, Loss= 0.6942, Training Accuracy= 0.494\n",
      "Epoch: 430, Loss= 0.6942, Training Accuracy= 0.494\n",
      "Epoch: 440, Loss= 0.6941, Training Accuracy= 0.494\n",
      "Epoch: 450, Loss= 0.6941, Training Accuracy= 0.495\n",
      "Epoch: 460, Loss= 0.6941, Training Accuracy= 0.495\n",
      "Epoch: 470, Loss= 0.6940, Training Accuracy= 0.495\n",
      "Epoch: 480, Loss= 0.6940, Training Accuracy= 0.495\n",
      "Epoch: 490, Loss= 0.6940, Training Accuracy= 0.495\n",
      "Epoch: 500, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 510, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 520, Loss= 0.6939, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 550, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 560, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 570, Loss= 0.6937, Training Accuracy= 0.498\n",
      "Epoch: 580, Loss= 0.6937, Training Accuracy= 0.498\n",
      "Epoch: 590, Loss= 0.6937, Training Accuracy= 0.499\n",
      "Epoch: 600, Loss= 0.6936, Training Accuracy= 0.499\n",
      "Epoch: 610, Loss= 0.6936, Training Accuracy= 0.499\n",
      "Epoch: 620, Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 630, Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 640, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 650, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 660, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 670, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 680, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 690, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 700, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 710, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 720, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 730, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 740, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 750, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 760, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 770, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 780, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 790, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 800, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 810, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 820, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 830, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 840, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 850, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 860, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 870, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 880, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 890, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 900, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 910, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 920, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 930, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 940, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 950, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 960, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 970, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 980, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 990, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5027\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.7129, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.7026, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.7005, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6993, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.6977, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.6972, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.6968, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.6965, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.6963, Training Accuracy= 0.497\n",
      "Epoch: 100, Loss= 0.6960, Training Accuracy= 0.497\n",
      "Epoch: 110, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 120, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 130, Loss= 0.6956, Training Accuracy= 0.497\n",
      "Epoch: 140, Loss= 0.6955, Training Accuracy= 0.497\n",
      "Epoch: 150, Loss= 0.6954, Training Accuracy= 0.497\n",
      "Epoch: 160, Loss= 0.6953, Training Accuracy= 0.497\n",
      "Epoch: 170, Loss= 0.6952, Training Accuracy= 0.497\n",
      "Epoch: 180, Loss= 0.6951, Training Accuracy= 0.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.6951, Training Accuracy= 0.497\n",
      "Epoch: 200, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 210, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 220, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 230, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 240, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 250, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 260, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 270, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 280, Loss= 0.6946, Training Accuracy= 0.497\n",
      "Epoch: 290, Loss= 0.6946, Training Accuracy= 0.498\n",
      "Epoch: 300, Loss= 0.6946, Training Accuracy= 0.498\n",
      "Epoch: 310, Loss= 0.6945, Training Accuracy= 0.498\n",
      "Epoch: 320, Loss= 0.6945, Training Accuracy= 0.498\n",
      "Epoch: 330, Loss= 0.6944, Training Accuracy= 0.499\n",
      "Epoch: 340, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 350, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 360, Loss= 0.6942, Training Accuracy= 0.501\n",
      "Epoch: 370, Loss= 0.6942, Training Accuracy= 0.505\n",
      "Epoch: 380, Loss= 0.6941, Training Accuracy= 0.507\n",
      "Epoch: 390, Loss= 0.6940, Training Accuracy= 0.508\n",
      "Epoch: 400, Loss= 0.6939, Training Accuracy= 0.511\n",
      "Epoch: 410, Loss= 0.6939, Training Accuracy= 0.511\n",
      "Epoch: 420, Loss= 0.6938, Training Accuracy= 0.511\n",
      "Epoch: 430, Loss= 0.6938, Training Accuracy= 0.511\n",
      "Epoch: 440, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 450, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 460, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 470, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 480, Loss= 0.6936, Training Accuracy= 0.511\n",
      "Epoch: 490, Loss= 0.6935, Training Accuracy= 0.511\n",
      "Epoch: 500, Loss= 0.6935, Training Accuracy= 0.510\n",
      "Epoch: 510, Loss= 0.6935, Training Accuracy= 0.510\n",
      "Epoch: 520, Loss= 0.6935, Training Accuracy= 0.510\n",
      "Epoch: 530, Loss= 0.6934, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 550, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 560, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 570, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 580, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 590, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 600, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 610, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 620, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 630, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 640, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 650, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 660, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 670, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 680, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 690, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 700, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 710, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 720, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 730, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 740, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 750, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 770, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 780, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 790, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 800, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 810, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 820, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 830, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 840, Loss= 0.6935, Training Accuracy= 0.516\n",
      "Epoch: 850, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 860, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 870, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 880, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 890, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 900, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 910, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 920, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 930, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 940, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 950, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 960, Loss= 0.6928, Training Accuracy= 0.517\n",
      "Epoch: 970, Loss= 0.6922, Training Accuracy= 0.515\n",
      "Epoch: 980, Loss= 0.6920, Training Accuracy= 0.517\n",
      "Epoch: 990, Loss= 0.6919, Training Accuracy= 0.517\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5028\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.6955, Training Accuracy= 0.510\n",
      "Epoch: 10, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 20, Loss= 0.6919, Training Accuracy= 0.517\n",
      "Epoch: 30, Loss= 0.6916, Training Accuracy= 0.521\n",
      "Epoch: 40, Loss= 0.6913, Training Accuracy= 0.523\n",
      "Epoch: 50, Loss= 0.6910, Training Accuracy= 0.524\n",
      "Epoch: 60, Loss= 0.6909, Training Accuracy= 0.524\n",
      "Epoch: 70, Loss= 0.6907, Training Accuracy= 0.526\n",
      "Epoch: 80, Loss= 0.6905, Training Accuracy= 0.528\n",
      "Epoch: 90, Loss= 0.6903, Training Accuracy= 0.528\n",
      "Epoch: 100, Loss= 0.6901, Training Accuracy= 0.528\n",
      "Epoch: 110, Loss= 0.6900, Training Accuracy= 0.527\n",
      "Epoch: 120, Loss= 0.6898, Training Accuracy= 0.527\n",
      "Epoch: 130, Loss= 0.6896, Training Accuracy= 0.530\n",
      "Epoch: 140, Loss= 0.6895, Training Accuracy= 0.531\n",
      "Epoch: 150, Loss= 0.6893, Training Accuracy= 0.531\n",
      "Epoch: 160, Loss= 0.6892, Training Accuracy= 0.532\n",
      "Epoch: 170, Loss= 0.6891, Training Accuracy= 0.532\n",
      "Epoch: 180, Loss= 0.6889, Training Accuracy= 0.532\n",
      "Epoch: 190, Loss= 0.6888, Training Accuracy= 0.533\n",
      "Epoch: 200, Loss= 0.6887, Training Accuracy= 0.533\n",
      "Epoch: 210, Loss= 0.6886, Training Accuracy= 0.534\n",
      "Epoch: 220, Loss= 0.6885, Training Accuracy= 0.533\n",
      "Epoch: 230, Loss= 0.6884, Training Accuracy= 0.534\n",
      "Epoch: 240, Loss= 0.6883, Training Accuracy= 0.535\n",
      "Epoch: 250, Loss= 0.6883, Training Accuracy= 0.537\n",
      "Epoch: 260, Loss= 0.6883, Training Accuracy= 0.537\n",
      "Epoch: 270, Loss= 0.6882, Training Accuracy= 0.536\n",
      "Epoch: 280, Loss= 0.6882, Training Accuracy= 0.537\n",
      "Epoch: 290, Loss= 0.6881, Training Accuracy= 0.538\n",
      "Epoch: 300, Loss= 0.6881, Training Accuracy= 0.538\n",
      "Epoch: 310, Loss= 0.6882, Training Accuracy= 0.538\n",
      "Epoch: 320, Loss= 0.6882, Training Accuracy= 0.538\n",
      "Epoch: 330, Loss= 0.6885, Training Accuracy= 0.534\n",
      "Epoch: 340, Loss= 0.6890, Training Accuracy= 0.532\n",
      "Epoch: 350, Loss= 0.6899, Training Accuracy= 0.529\n",
      "Epoch: 360, Loss= 0.6900, Training Accuracy= 0.530\n",
      "Epoch: 370, Loss= 0.6901, Training Accuracy= 0.529\n",
      "Epoch: 380, Loss= 0.6904, Training Accuracy= 0.530\n",
      "Epoch: 390, Loss= 0.6906, Training Accuracy= 0.530\n",
      "Epoch: 400, Loss= 0.6907, Training Accuracy= 0.531\n",
      "Epoch: 410, Loss= 0.6907, Training Accuracy= 0.530\n",
      "Epoch: 420, Loss= 0.6905, Training Accuracy= 0.531\n",
      "Epoch: 430, Loss= 0.6900, Training Accuracy= 0.533\n",
      "Epoch: 440, Loss= 0.6906, Training Accuracy= 0.532\n",
      "Epoch: 450, Loss= 0.6890, Training Accuracy= 0.536\n",
      "Epoch: 460, Loss= 0.6877, Training Accuracy= 0.539\n",
      "Epoch: 470, Loss= 0.6879, Training Accuracy= 0.539\n",
      "Epoch: 480, Loss= 0.6875, Training Accuracy= 0.540\n",
      "Epoch: 490, Loss= 0.6871, Training Accuracy= 0.543\n",
      "Epoch: 500, Loss= 0.6869, Training Accuracy= 0.543\n",
      "Epoch: 510, Loss= 0.6867, Training Accuracy= 0.543\n",
      "Epoch: 520, Loss= 0.6864, Training Accuracy= 0.541\n",
      "Epoch: 530, Loss= 0.6865, Training Accuracy= 0.542\n",
      "Epoch: 540, Loss= 0.6868, Training Accuracy= 0.542\n",
      "Epoch: 550, Loss= 0.6864, Training Accuracy= 0.544\n",
      "Epoch: 560, Loss= 0.6867, Training Accuracy= 0.541\n",
      "Epoch: 570, Loss= 0.6859, Training Accuracy= 0.544\n",
      "Epoch: 580, Loss= 0.6857, Training Accuracy= 0.546\n",
      "Epoch: 590, Loss= 0.6861, Training Accuracy= 0.547\n",
      "Epoch: 600, Loss= 0.6861, Training Accuracy= 0.545\n",
      "Epoch: 610, Loss= 0.6869, Training Accuracy= 0.541\n",
      "Epoch: 620, Loss= 0.6877, Training Accuracy= 0.537\n",
      "Epoch: 630, Loss= 0.6865, Training Accuracy= 0.544\n",
      "Epoch: 640, Loss= 0.6880, Training Accuracy= 0.538\n",
      "Epoch: 650, Loss= 0.6882, Training Accuracy= 0.540\n",
      "Epoch: 660, Loss= 0.6879, Training Accuracy= 0.538\n",
      "Epoch: 670, Loss= 0.6868, Training Accuracy= 0.543\n",
      "Epoch: 680, Loss= 0.6868, Training Accuracy= 0.543\n",
      "Epoch: 690, Loss= 0.6867, Training Accuracy= 0.544\n",
      "Epoch: 700, Loss= 0.6853, Training Accuracy= 0.545\n",
      "Epoch: 710, Loss= 0.6846, Training Accuracy= 0.549\n",
      "Epoch: 720, Loss= 0.6867, Training Accuracy= 0.542\n",
      "Epoch: 730, Loss= 0.6855, Training Accuracy= 0.551\n",
      "Epoch: 740, Loss= 0.6859, Training Accuracy= 0.547\n",
      "Epoch: 750, Loss= 0.6848, Training Accuracy= 0.547\n",
      "Epoch: 760, Loss= 0.6844, Training Accuracy= 0.550\n",
      "Epoch: 770, Loss= 0.6847, Training Accuracy= 0.549\n",
      "Epoch: 780, Loss= 0.6884, Training Accuracy= 0.539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.6883, Training Accuracy= 0.542\n",
      "Epoch: 800, Loss= 0.6874, Training Accuracy= 0.541\n",
      "Epoch: 810, Loss= 0.6902, Training Accuracy= 0.535\n",
      "Epoch: 820, Loss= 0.6851, Training Accuracy= 0.545\n",
      "Epoch: 830, Loss= 0.6877, Training Accuracy= 0.538\n",
      "Epoch: 840, Loss= 0.6849, Training Accuracy= 0.546\n",
      "Epoch: 850, Loss= 0.6854, Training Accuracy= 0.544\n",
      "Epoch: 860, Loss= 0.6855, Training Accuracy= 0.546\n",
      "Epoch: 870, Loss= 0.6872, Training Accuracy= 0.539\n",
      "Epoch: 880, Loss= 0.6849, Training Accuracy= 0.548\n",
      "Epoch: 890, Loss= 0.6853, Training Accuracy= 0.548\n",
      "Epoch: 900, Loss= 0.6851, Training Accuracy= 0.549\n",
      "Epoch: 910, Loss= 0.6842, Training Accuracy= 0.547\n",
      "Epoch: 920, Loss= 0.6867, Training Accuracy= 0.544\n",
      "Epoch: 930, Loss= 0.6859, Training Accuracy= 0.544\n",
      "Epoch: 940, Loss= 0.6857, Training Accuracy= 0.547\n",
      "Epoch: 950, Loss= 0.6871, Training Accuracy= 0.542\n",
      "Epoch: 960, Loss= 0.6835, Training Accuracy= 0.555\n",
      "Epoch: 970, Loss= 0.6864, Training Accuracy= 0.545\n",
      "Epoch: 980, Loss= 0.6842, Training Accuracy= 0.550\n",
      "Epoch: 990, Loss= 0.6838, Training Accuracy= 0.553\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5029\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.6964, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.6935, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.6933, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6931, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 70, Loss= 0.6930, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 100, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 110, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 120, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 130, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 140, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 150, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 160, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 170, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 180, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 190, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 200, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 220, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 230, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 240, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 250, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 260, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 270, Loss= 0.6928, Training Accuracy= 0.506\n",
      "Epoch: 280, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 290, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 300, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 310, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 320, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 330, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 340, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 350, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 360, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 370, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 380, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 390, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 400, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 420, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 430, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 440, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 450, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 460, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 470, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 480, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 490, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 500, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 510, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 520, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 530, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 540, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 550, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 560, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 570, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 580, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 590, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 600, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 610, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 620, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 630, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 640, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 650, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 660, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 670, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 680, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 690, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 700, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 710, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 720, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 730, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 740, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 750, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 760, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 770, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 780, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 790, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 800, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 810, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 820, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 830, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 840, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 850, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 860, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 870, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 880, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 890, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 900, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 910, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 920, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 930, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 940, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 950, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 960, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 970, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 980, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 990, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4924\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.6966, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 20, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 30, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 40, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 50, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 60, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 70, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 80, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 90, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 100, Loss= 0.6928, Training Accuracy= 0.516\n",
      "Epoch: 110, Loss= 0.6928, Training Accuracy= 0.516\n",
      "Epoch: 120, Loss= 0.6928, Training Accuracy= 0.517\n",
      "Epoch: 130, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 140, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 150, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 160, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 170, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 180, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 190, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 200, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 210, Loss= 0.6925, Training Accuracy= 0.513\n",
      "Epoch: 220, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 230, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 240, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 250, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 260, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 270, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 280, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 290, Loss= 0.6922, Training Accuracy= 0.517\n",
      "Epoch: 300, Loss= 0.6922, Training Accuracy= 0.517\n",
      "Epoch: 310, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 320, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 330, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 340, Loss= 0.6921, Training Accuracy= 0.519\n",
      "Epoch: 350, Loss= 0.6921, Training Accuracy= 0.521\n",
      "Epoch: 360, Loss= 0.6921, Training Accuracy= 0.521\n",
      "Epoch: 370, Loss= 0.6921, Training Accuracy= 0.522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6921, Training Accuracy= 0.523\n",
      "Epoch: 390, Loss= 0.6920, Training Accuracy= 0.524\n",
      "Epoch: 400, Loss= 0.6920, Training Accuracy= 0.524\n",
      "Epoch: 410, Loss= 0.6920, Training Accuracy= 0.524\n",
      "Epoch: 420, Loss= 0.6920, Training Accuracy= 0.524\n",
      "Epoch: 430, Loss= 0.6919, Training Accuracy= 0.523\n",
      "Epoch: 440, Loss= 0.6919, Training Accuracy= 0.524\n",
      "Epoch: 450, Loss= 0.6919, Training Accuracy= 0.525\n",
      "Epoch: 460, Loss= 0.6919, Training Accuracy= 0.525\n",
      "Epoch: 470, Loss= 0.6918, Training Accuracy= 0.526\n",
      "Epoch: 480, Loss= 0.6918, Training Accuracy= 0.528\n",
      "Epoch: 490, Loss= 0.6918, Training Accuracy= 0.526\n",
      "Epoch: 500, Loss= 0.6917, Training Accuracy= 0.526\n",
      "Epoch: 510, Loss= 0.6917, Training Accuracy= 0.525\n",
      "Epoch: 520, Loss= 0.6916, Training Accuracy= 0.525\n",
      "Epoch: 530, Loss= 0.6916, Training Accuracy= 0.525\n",
      "Epoch: 540, Loss= 0.6915, Training Accuracy= 0.524\n",
      "Epoch: 550, Loss= 0.6914, Training Accuracy= 0.524\n",
      "Epoch: 560, Loss= 0.6914, Training Accuracy= 0.526\n",
      "Epoch: 570, Loss= 0.6913, Training Accuracy= 0.527\n",
      "Epoch: 580, Loss= 0.6912, Training Accuracy= 0.526\n",
      "Epoch: 590, Loss= 0.6911, Training Accuracy= 0.527\n",
      "Epoch: 600, Loss= 0.6910, Training Accuracy= 0.526\n",
      "Epoch: 610, Loss= 0.6910, Training Accuracy= 0.525\n",
      "Epoch: 620, Loss= 0.6909, Training Accuracy= 0.527\n",
      "Epoch: 630, Loss= 0.6908, Training Accuracy= 0.526\n",
      "Epoch: 640, Loss= 0.6907, Training Accuracy= 0.526\n",
      "Epoch: 650, Loss= 0.6907, Training Accuracy= 0.523\n",
      "Epoch: 660, Loss= 0.6906, Training Accuracy= 0.525\n",
      "Epoch: 670, Loss= 0.6906, Training Accuracy= 0.525\n",
      "Epoch: 680, Loss= 0.6906, Training Accuracy= 0.526\n",
      "Epoch: 690, Loss= 0.6905, Training Accuracy= 0.527\n",
      "Epoch: 700, Loss= 0.6905, Training Accuracy= 0.527\n",
      "Epoch: 710, Loss= 0.6904, Training Accuracy= 0.528\n",
      "Epoch: 720, Loss= 0.6904, Training Accuracy= 0.528\n",
      "Epoch: 730, Loss= 0.6904, Training Accuracy= 0.529\n",
      "Epoch: 740, Loss= 0.6903, Training Accuracy= 0.528\n",
      "Epoch: 750, Loss= 0.6903, Training Accuracy= 0.527\n",
      "Epoch: 760, Loss= 0.6902, Training Accuracy= 0.527\n",
      "Epoch: 770, Loss= 0.6902, Training Accuracy= 0.528\n",
      "Epoch: 780, Loss= 0.6902, Training Accuracy= 0.528\n",
      "Epoch: 790, Loss= 0.6901, Training Accuracy= 0.528\n",
      "Epoch: 800, Loss= 0.6901, Training Accuracy= 0.529\n",
      "Epoch: 810, Loss= 0.6900, Training Accuracy= 0.530\n",
      "Epoch: 820, Loss= 0.6900, Training Accuracy= 0.530\n",
      "Epoch: 830, Loss= 0.6899, Training Accuracy= 0.529\n",
      "Epoch: 840, Loss= 0.6898, Training Accuracy= 0.530\n",
      "Epoch: 850, Loss= 0.6897, Training Accuracy= 0.528\n",
      "Epoch: 860, Loss= 0.6897, Training Accuracy= 0.528\n",
      "Epoch: 870, Loss= 0.6896, Training Accuracy= 0.529\n",
      "Epoch: 880, Loss= 0.6895, Training Accuracy= 0.529\n",
      "Epoch: 890, Loss= 0.6893, Training Accuracy= 0.530\n",
      "Epoch: 900, Loss= 0.6892, Training Accuracy= 0.528\n",
      "Epoch: 910, Loss= 0.6891, Training Accuracy= 0.530\n",
      "Epoch: 920, Loss= 0.6889, Training Accuracy= 0.533\n",
      "Epoch: 930, Loss= 0.6887, Training Accuracy= 0.533\n",
      "Epoch: 940, Loss= 0.6886, Training Accuracy= 0.533\n",
      "Epoch: 950, Loss= 0.6884, Training Accuracy= 0.532\n",
      "Epoch: 960, Loss= 0.6882, Training Accuracy= 0.535\n",
      "Epoch: 970, Loss= 0.6880, Training Accuracy= 0.535\n",
      "Epoch: 980, Loss= 0.6878, Training Accuracy= 0.538\n",
      "Epoch: 990, Loss= 0.6876, Training Accuracy= 0.540\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5021\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.6973, Training Accuracy= 0.505\n",
      "Epoch: 10, Loss= 0.6945, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.6943, Training Accuracy= 0.504\n",
      "Epoch: 30, Loss= 0.6942, Training Accuracy= 0.505\n",
      "Epoch: 40, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 50, Loss= 0.6939, Training Accuracy= 0.507\n",
      "Epoch: 60, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Epoch: 80, Loss= 0.6936, Training Accuracy= 0.511\n",
      "Epoch: 90, Loss= 0.6935, Training Accuracy= 0.510\n",
      "Epoch: 100, Loss= 0.6934, Training Accuracy= 0.511\n",
      "Epoch: 110, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 130, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 140, Loss= 0.6928, Training Accuracy= 0.519\n",
      "Epoch: 150, Loss= 0.6925, Training Accuracy= 0.521\n",
      "Epoch: 160, Loss= 0.6921, Training Accuracy= 0.522\n",
      "Epoch: 170, Loss= 0.6918, Training Accuracy= 0.526\n",
      "Epoch: 180, Loss= 0.6915, Training Accuracy= 0.533\n",
      "Epoch: 190, Loss= 0.6912, Training Accuracy= 0.531\n",
      "Epoch: 200, Loss= 0.6911, Training Accuracy= 0.530\n",
      "Epoch: 210, Loss= 0.6910, Training Accuracy= 0.531\n",
      "Epoch: 220, Loss= 0.6910, Training Accuracy= 0.534\n",
      "Epoch: 230, Loss= 0.6909, Training Accuracy= 0.534\n",
      "Epoch: 240, Loss= 0.6909, Training Accuracy= 0.533\n",
      "Epoch: 250, Loss= 0.6909, Training Accuracy= 0.535\n",
      "Epoch: 260, Loss= 0.6908, Training Accuracy= 0.535\n",
      "Epoch: 270, Loss= 0.6908, Training Accuracy= 0.535\n",
      "Epoch: 280, Loss= 0.6907, Training Accuracy= 0.536\n",
      "Epoch: 290, Loss= 0.6906, Training Accuracy= 0.535\n",
      "Epoch: 300, Loss= 0.6905, Training Accuracy= 0.534\n",
      "Epoch: 310, Loss= 0.6904, Training Accuracy= 0.534\n",
      "Epoch: 320, Loss= 0.6902, Training Accuracy= 0.534\n",
      "Epoch: 330, Loss= 0.6901, Training Accuracy= 0.534\n",
      "Epoch: 340, Loss= 0.6900, Training Accuracy= 0.534\n",
      "Epoch: 350, Loss= 0.6899, Training Accuracy= 0.535\n",
      "Epoch: 360, Loss= 0.6898, Training Accuracy= 0.535\n",
      "Epoch: 370, Loss= 0.6897, Training Accuracy= 0.536\n",
      "Epoch: 380, Loss= 0.6897, Training Accuracy= 0.537\n",
      "Epoch: 390, Loss= 0.6896, Training Accuracy= 0.539\n",
      "Epoch: 400, Loss= 0.6896, Training Accuracy= 0.541\n",
      "Epoch: 410, Loss= 0.6895, Training Accuracy= 0.541\n",
      "Epoch: 420, Loss= 0.6895, Training Accuracy= 0.540\n",
      "Epoch: 430, Loss= 0.6895, Training Accuracy= 0.539\n",
      "Epoch: 440, Loss= 0.6895, Training Accuracy= 0.540\n",
      "Epoch: 450, Loss= 0.6895, Training Accuracy= 0.539\n",
      "Epoch: 460, Loss= 0.6896, Training Accuracy= 0.539\n",
      "Epoch: 470, Loss= 0.6897, Training Accuracy= 0.541\n",
      "Epoch: 480, Loss= 0.6897, Training Accuracy= 0.542\n",
      "Epoch: 490, Loss= 0.6896, Training Accuracy= 0.540\n",
      "Epoch: 500, Loss= 0.6895, Training Accuracy= 0.540\n",
      "Epoch: 510, Loss= 0.6895, Training Accuracy= 0.540\n",
      "Epoch: 520, Loss= 0.6894, Training Accuracy= 0.539\n",
      "Epoch: 530, Loss= 0.6894, Training Accuracy= 0.538\n",
      "Epoch: 540, Loss= 0.6893, Training Accuracy= 0.539\n",
      "Epoch: 550, Loss= 0.6893, Training Accuracy= 0.541\n",
      "Epoch: 560, Loss= 0.6892, Training Accuracy= 0.541\n",
      "Epoch: 570, Loss= 0.6890, Training Accuracy= 0.542\n",
      "Epoch: 580, Loss= 0.6889, Training Accuracy= 0.542\n",
      "Epoch: 590, Loss= 0.6888, Training Accuracy= 0.544\n",
      "Epoch: 600, Loss= 0.6887, Training Accuracy= 0.545\n",
      "Epoch: 610, Loss= 0.6887, Training Accuracy= 0.545\n",
      "Epoch: 620, Loss= 0.6886, Training Accuracy= 0.545\n",
      "Epoch: 630, Loss= 0.6886, Training Accuracy= 0.545\n",
      "Epoch: 640, Loss= 0.6886, Training Accuracy= 0.544\n",
      "Epoch: 650, Loss= 0.6885, Training Accuracy= 0.546\n",
      "Epoch: 660, Loss= 0.6882, Training Accuracy= 0.549\n",
      "Epoch: 670, Loss= 0.6881, Training Accuracy= 0.551\n",
      "Epoch: 680, Loss= 0.6881, Training Accuracy= 0.551\n",
      "Epoch: 690, Loss= 0.6898, Training Accuracy= 0.547\n",
      "Epoch: 700, Loss= 0.6905, Training Accuracy= 0.543\n",
      "Epoch: 710, Loss= 0.6915, Training Accuracy= 0.538\n",
      "Epoch: 720, Loss= 0.6906, Training Accuracy= 0.539\n",
      "Epoch: 730, Loss= 0.6908, Training Accuracy= 0.539\n",
      "Epoch: 740, Loss= 0.6907, Training Accuracy= 0.539\n",
      "Epoch: 750, Loss= 0.6923, Training Accuracy= 0.537\n",
      "Epoch: 760, Loss= 0.6932, Training Accuracy= 0.535\n",
      "Epoch: 770, Loss= 0.6933, Training Accuracy= 0.535\n",
      "Epoch: 780, Loss= 0.6929, Training Accuracy= 0.534\n",
      "Epoch: 790, Loss= 0.6920, Training Accuracy= 0.539\n",
      "Epoch: 800, Loss= 0.6923, Training Accuracy= 0.535\n",
      "Epoch: 810, Loss= 0.6922, Training Accuracy= 0.538\n",
      "Epoch: 820, Loss= 0.6910, Training Accuracy= 0.539\n",
      "Epoch: 830, Loss= 0.6921, Training Accuracy= 0.533\n",
      "Epoch: 840, Loss= 0.6916, Training Accuracy= 0.538\n",
      "Epoch: 850, Loss= 0.6934, Training Accuracy= 0.538\n",
      "Epoch: 860, Loss= 0.6952, Training Accuracy= 0.532\n",
      "Epoch: 870, Loss= 0.6891, Training Accuracy= 0.543\n",
      "Epoch: 880, Loss= 0.6899, Training Accuracy= 0.543\n",
      "Epoch: 890, Loss= 0.6899, Training Accuracy= 0.542\n",
      "Epoch: 900, Loss= 0.6901, Training Accuracy= 0.543\n",
      "Epoch: 910, Loss= 0.6908, Training Accuracy= 0.541\n",
      "Epoch: 920, Loss= 0.6919, Training Accuracy= 0.536\n",
      "Epoch: 930, Loss= 0.6915, Training Accuracy= 0.536\n",
      "Epoch: 940, Loss= 0.6939, Training Accuracy= 0.531\n",
      "Epoch: 950, Loss= 0.6978, Training Accuracy= 0.523\n",
      "Epoch: 960, Loss= 0.6929, Training Accuracy= 0.537\n",
      "Epoch: 970, Loss= 0.6937, Training Accuracy= 0.534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.6925, Training Accuracy= 0.537\n",
      "Epoch: 990, Loss= 0.6943, Training Accuracy= 0.535\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4984\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.7084, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.7020, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.6999, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6989, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6981, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.6975, Training Accuracy= 0.498\n",
      "Epoch: 60, Loss= 0.6970, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.6965, Training Accuracy= 0.499\n",
      "Epoch: 80, Loss= 0.6960, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 100, Loss= 0.6953, Training Accuracy= 0.501\n",
      "Epoch: 110, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6947, Training Accuracy= 0.504\n",
      "Epoch: 130, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6944, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.6943, Training Accuracy= 0.505\n",
      "Epoch: 160, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6941, Training Accuracy= 0.507\n",
      "Epoch: 180, Loss= 0.6940, Training Accuracy= 0.508\n",
      "Epoch: 190, Loss= 0.6939, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 210, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 220, Loss= 0.6937, Training Accuracy= 0.511\n",
      "Epoch: 230, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Epoch: 240, Loss= 0.6936, Training Accuracy= 0.511\n",
      "Epoch: 250, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 260, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 270, Loss= 0.6932, Training Accuracy= 0.516\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.518\n",
      "Epoch: 290, Loss= 0.6929, Training Accuracy= 0.519\n",
      "Epoch: 300, Loss= 0.6928, Training Accuracy= 0.517\n",
      "Epoch: 310, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 320, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 330, Loss= 0.6926, Training Accuracy= 0.517\n",
      "Epoch: 340, Loss= 0.6926, Training Accuracy= 0.518\n",
      "Epoch: 350, Loss= 0.6926, Training Accuracy= 0.519\n",
      "Epoch: 360, Loss= 0.6926, Training Accuracy= 0.517\n",
      "Epoch: 370, Loss= 0.6925, Training Accuracy= 0.519\n",
      "Epoch: 380, Loss= 0.6924, Training Accuracy= 0.522\n",
      "Epoch: 390, Loss= 0.6924, Training Accuracy= 0.523\n",
      "Epoch: 400, Loss= 0.6923, Training Accuracy= 0.522\n",
      "Epoch: 410, Loss= 0.6922, Training Accuracy= 0.524\n",
      "Epoch: 420, Loss= 0.6920, Training Accuracy= 0.523\n",
      "Epoch: 430, Loss= 0.6921, Training Accuracy= 0.523\n",
      "Epoch: 440, Loss= 0.6923, Training Accuracy= 0.522\n",
      "Epoch: 450, Loss= 0.6923, Training Accuracy= 0.523\n",
      "Epoch: 460, Loss= 0.6921, Training Accuracy= 0.525\n",
      "Epoch: 470, Loss= 0.6921, Training Accuracy= 0.525\n",
      "Epoch: 480, Loss= 0.6920, Training Accuracy= 0.525\n",
      "Epoch: 490, Loss= 0.6919, Training Accuracy= 0.526\n",
      "Epoch: 500, Loss= 0.6916, Training Accuracy= 0.527\n",
      "Epoch: 510, Loss= 0.6916, Training Accuracy= 0.526\n",
      "Epoch: 520, Loss= 0.6917, Training Accuracy= 0.526\n",
      "Epoch: 530, Loss= 0.6917, Training Accuracy= 0.526\n",
      "Epoch: 540, Loss= 0.6918, Training Accuracy= 0.527\n",
      "Epoch: 550, Loss= 0.6916, Training Accuracy= 0.526\n",
      "Epoch: 560, Loss= 0.6908, Training Accuracy= 0.525\n",
      "Epoch: 570, Loss= 0.6907, Training Accuracy= 0.524\n",
      "Epoch: 580, Loss= 0.6912, Training Accuracy= 0.525\n",
      "Epoch: 590, Loss= 0.6914, Training Accuracy= 0.524\n",
      "Epoch: 600, Loss= 0.6907, Training Accuracy= 0.526\n",
      "Epoch: 610, Loss= 0.6905, Training Accuracy= 0.529\n",
      "Epoch: 620, Loss= 0.6913, Training Accuracy= 0.523\n",
      "Epoch: 630, Loss= 0.6910, Training Accuracy= 0.521\n",
      "Epoch: 640, Loss= 0.6923, Training Accuracy= 0.521\n",
      "Epoch: 650, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 660, Loss= 0.6933, Training Accuracy= 0.518\n",
      "Epoch: 670, Loss= 0.6929, Training Accuracy= 0.518\n",
      "Epoch: 680, Loss= 0.6918, Training Accuracy= 0.522\n",
      "Epoch: 690, Loss= 0.6936, Training Accuracy= 0.520\n",
      "Epoch: 700, Loss= 0.6921, Training Accuracy= 0.523\n",
      "Epoch: 710, Loss= 0.6898, Training Accuracy= 0.535\n",
      "Epoch: 720, Loss= 0.6907, Training Accuracy= 0.525\n",
      "Epoch: 730, Loss= 0.6905, Training Accuracy= 0.525\n",
      "Epoch: 740, Loss= 0.6887, Training Accuracy= 0.535\n",
      "Epoch: 750, Loss= 0.6905, Training Accuracy= 0.522\n",
      "Epoch: 760, Loss= 0.6894, Training Accuracy= 0.536\n",
      "Epoch: 770, Loss= 0.6897, Training Accuracy= 0.537\n",
      "Epoch: 780, Loss= 0.6890, Training Accuracy= 0.537\n",
      "Epoch: 790, Loss= 0.6940, Training Accuracy= 0.513\n",
      "Epoch: 800, Loss= 0.6896, Training Accuracy= 0.538\n",
      "Epoch: 810, Loss= 0.6913, Training Accuracy= 0.527\n",
      "Epoch: 820, Loss= 0.6892, Training Accuracy= 0.537\n",
      "Epoch: 830, Loss= 0.6895, Training Accuracy= 0.536\n",
      "Epoch: 840, Loss= 0.6890, Training Accuracy= 0.537\n",
      "Epoch: 850, Loss= 0.6891, Training Accuracy= 0.535\n",
      "Epoch: 860, Loss= 0.6943, Training Accuracy= 0.510\n",
      "Epoch: 870, Loss= 0.6898, Training Accuracy= 0.532\n",
      "Epoch: 880, Loss= 0.6905, Training Accuracy= 0.529\n",
      "Epoch: 890, Loss= 0.6906, Training Accuracy= 0.529\n",
      "Epoch: 900, Loss= 0.6906, Training Accuracy= 0.533\n",
      "Epoch: 910, Loss= 0.6912, Training Accuracy= 0.529\n",
      "Epoch: 920, Loss= 0.6915, Training Accuracy= 0.525\n",
      "Epoch: 930, Loss= 0.6910, Training Accuracy= 0.530\n",
      "Epoch: 940, Loss= 0.6898, Training Accuracy= 0.535\n",
      "Epoch: 950, Loss= 0.6897, Training Accuracy= 0.533\n",
      "Epoch: 960, Loss= 0.6890, Training Accuracy= 0.535\n",
      "Epoch: 970, Loss= 0.6897, Training Accuracy= 0.529\n",
      "Epoch: 980, Loss= 0.6884, Training Accuracy= 0.539\n",
      "Epoch: 990, Loss= 0.6885, Training Accuracy= 0.540\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5005\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.6983, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.6955, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6942, Training Accuracy= 0.507\n",
      "Epoch: 60, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6939, Training Accuracy= 0.506\n",
      "Epoch: 90, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Epoch: 100, Loss= 0.6935, Training Accuracy= 0.511\n",
      "Epoch: 110, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 120, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 130, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 140, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 150, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 160, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 170, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 180, Loss= 0.6921, Training Accuracy= 0.520\n",
      "Epoch: 190, Loss= 0.6921, Training Accuracy= 0.520\n",
      "Epoch: 200, Loss= 0.6920, Training Accuracy= 0.518\n",
      "Epoch: 210, Loss= 0.6919, Training Accuracy= 0.519\n",
      "Epoch: 220, Loss= 0.6919, Training Accuracy= 0.520\n",
      "Epoch: 230, Loss= 0.6918, Training Accuracy= 0.519\n",
      "Epoch: 240, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 250, Loss= 0.6917, Training Accuracy= 0.522\n",
      "Epoch: 260, Loss= 0.6917, Training Accuracy= 0.522\n",
      "Epoch: 270, Loss= 0.6916, Training Accuracy= 0.522\n",
      "Epoch: 280, Loss= 0.6916, Training Accuracy= 0.523\n",
      "Epoch: 290, Loss= 0.6915, Training Accuracy= 0.524\n",
      "Epoch: 300, Loss= 0.6915, Training Accuracy= 0.525\n",
      "Epoch: 310, Loss= 0.6914, Training Accuracy= 0.523\n",
      "Epoch: 320, Loss= 0.6914, Training Accuracy= 0.523\n",
      "Epoch: 330, Loss= 0.6913, Training Accuracy= 0.521\n",
      "Epoch: 340, Loss= 0.6913, Training Accuracy= 0.521\n",
      "Epoch: 350, Loss= 0.6912, Training Accuracy= 0.522\n",
      "Epoch: 360, Loss= 0.6912, Training Accuracy= 0.522\n",
      "Epoch: 370, Loss= 0.6911, Training Accuracy= 0.522\n",
      "Epoch: 380, Loss= 0.6910, Training Accuracy= 0.522\n",
      "Epoch: 390, Loss= 0.6910, Training Accuracy= 0.522\n",
      "Epoch: 400, Loss= 0.6909, Training Accuracy= 0.522\n",
      "Epoch: 410, Loss= 0.6908, Training Accuracy= 0.522\n",
      "Epoch: 420, Loss= 0.6907, Training Accuracy= 0.524\n",
      "Epoch: 430, Loss= 0.6906, Training Accuracy= 0.525\n",
      "Epoch: 440, Loss= 0.6905, Training Accuracy= 0.525\n",
      "Epoch: 450, Loss= 0.6904, Training Accuracy= 0.526\n",
      "Epoch: 460, Loss= 0.6902, Training Accuracy= 0.525\n",
      "Epoch: 470, Loss= 0.6901, Training Accuracy= 0.526\n",
      "Epoch: 480, Loss= 0.6899, Training Accuracy= 0.526\n",
      "Epoch: 490, Loss= 0.6897, Training Accuracy= 0.528\n",
      "Epoch: 500, Loss= 0.6895, Training Accuracy= 0.531\n",
      "Epoch: 510, Loss= 0.6893, Training Accuracy= 0.532\n",
      "Epoch: 520, Loss= 0.6890, Training Accuracy= 0.533\n",
      "Epoch: 530, Loss= 0.6887, Training Accuracy= 0.538\n",
      "Epoch: 540, Loss= 0.6885, Training Accuracy= 0.539\n",
      "Epoch: 550, Loss= 0.6883, Training Accuracy= 0.538\n",
      "Epoch: 560, Loss= 0.6881, Training Accuracy= 0.536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.6880, Training Accuracy= 0.539\n",
      "Epoch: 580, Loss= 0.6879, Training Accuracy= 0.538\n",
      "Epoch: 590, Loss= 0.6878, Training Accuracy= 0.538\n",
      "Epoch: 600, Loss= 0.6877, Training Accuracy= 0.540\n",
      "Epoch: 610, Loss= 0.6876, Training Accuracy= 0.539\n",
      "Epoch: 620, Loss= 0.6874, Training Accuracy= 0.540\n",
      "Epoch: 630, Loss= 0.6872, Training Accuracy= 0.542\n",
      "Epoch: 640, Loss= 0.6870, Training Accuracy= 0.541\n",
      "Epoch: 650, Loss= 0.6868, Training Accuracy= 0.540\n",
      "Epoch: 660, Loss= 0.6866, Training Accuracy= 0.540\n",
      "Epoch: 670, Loss= 0.6864, Training Accuracy= 0.541\n",
      "Epoch: 680, Loss= 0.6862, Training Accuracy= 0.542\n",
      "Epoch: 690, Loss= 0.6861, Training Accuracy= 0.545\n",
      "Epoch: 700, Loss= 0.6859, Training Accuracy= 0.546\n",
      "Epoch: 710, Loss= 0.6861, Training Accuracy= 0.547\n",
      "Epoch: 720, Loss= 0.6863, Training Accuracy= 0.546\n",
      "Epoch: 730, Loss= 0.6868, Training Accuracy= 0.544\n",
      "Epoch: 740, Loss= 0.6872, Training Accuracy= 0.544\n",
      "Epoch: 750, Loss= 0.6880, Training Accuracy= 0.541\n",
      "Epoch: 760, Loss= 0.6883, Training Accuracy= 0.541\n",
      "Epoch: 770, Loss= 0.6882, Training Accuracy= 0.546\n",
      "Epoch: 780, Loss= 0.6891, Training Accuracy= 0.544\n",
      "Epoch: 790, Loss= 0.6890, Training Accuracy= 0.543\n",
      "Epoch: 800, Loss= 0.6884, Training Accuracy= 0.542\n",
      "Epoch: 810, Loss= 0.6879, Training Accuracy= 0.545\n",
      "Epoch: 820, Loss= 0.6881, Training Accuracy= 0.547\n",
      "Epoch: 830, Loss= 0.6872, Training Accuracy= 0.549\n",
      "Epoch: 840, Loss= 0.6876, Training Accuracy= 0.550\n",
      "Epoch: 850, Loss= 0.6865, Training Accuracy= 0.551\n",
      "Epoch: 860, Loss= 0.6883, Training Accuracy= 0.547\n",
      "Epoch: 870, Loss= 0.6869, Training Accuracy= 0.553\n",
      "Epoch: 880, Loss= 0.6871, Training Accuracy= 0.550\n",
      "Epoch: 890, Loss= 0.6863, Training Accuracy= 0.552\n",
      "Epoch: 900, Loss= 0.6867, Training Accuracy= 0.551\n",
      "Epoch: 910, Loss= 0.6860, Training Accuracy= 0.553\n",
      "Epoch: 920, Loss= 0.6875, Training Accuracy= 0.550\n",
      "Epoch: 930, Loss= 0.6875, Training Accuracy= 0.548\n",
      "Epoch: 940, Loss= 0.6877, Training Accuracy= 0.551\n",
      "Epoch: 950, Loss= 0.6883, Training Accuracy= 0.550\n",
      "Epoch: 960, Loss= 0.6896, Training Accuracy= 0.546\n",
      "Epoch: 970, Loss= 0.6895, Training Accuracy= 0.543\n",
      "Epoch: 980, Loss= 0.6870, Training Accuracy= 0.550\n",
      "Epoch: 990, Loss= 0.6864, Training Accuracy= 0.555\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.506\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.03\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.50239998, 0.5007, 0.50269997, 0.50279999, 0.5029, 0.49239999, 0.50209999, 0.4984, 0.50050002, 0.50599998]\n",
      "mean of test_accuracies_10replications:  0.50109\n",
      "standard deviation of test_accuracies_10replications_std_mean:  3.44802509062e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncZiZWUmcOMzMrCT/PwaIpcTK/rAUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1be51c5610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
