{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 10\n",
    "N = 25\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.8214, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 0.7155, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.7026, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6965, Training Accuracy= 0.495\n",
      "Epoch: 50, Loss= 0.6954, Training Accuracy= 0.496\n",
      "Epoch: 60, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 110, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 130, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 150, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 160, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 170, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 180, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 190, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 200, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 210, Loss= 0.6945, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6949, Training Accuracy= 0.509\n",
      "Epoch: 230, Loss= 0.6969, Training Accuracy= 0.512\n",
      "Epoch: 240, Loss= 0.6962, Training Accuracy= 0.513\n",
      "Epoch: 250, Loss= 0.6948, Training Accuracy= 0.513\n",
      "Epoch: 260, Loss= 0.6947, Training Accuracy= 0.517\n",
      "Epoch: 270, Loss= 0.6944, Training Accuracy= 0.524\n",
      "Epoch: 280, Loss= 0.6977, Training Accuracy= 0.514\n",
      "Epoch: 290, Loss= 0.6973, Training Accuracy= 0.525\n",
      "Epoch: 300, Loss= 0.6931, Training Accuracy= 0.520\n",
      "Epoch: 310, Loss= 0.7080, Training Accuracy= 0.505\n",
      "Epoch: 320, Loss= 0.6948, Training Accuracy= 0.513\n",
      "Epoch: 330, Loss= 0.7308, Training Accuracy= 0.495\n",
      "Epoch: 340, Loss= 0.7260, Training Accuracy= 0.495\n",
      "Epoch: 350, Loss= 0.7186, Training Accuracy= 0.495\n",
      "Epoch: 360, Loss= 0.7213, Training Accuracy= 0.495\n",
      "Epoch: 370, Loss= 0.8237, Training Accuracy= 0.501\n",
      "Epoch: 380, Loss= 0.8051, Training Accuracy= 0.501\n",
      "Epoch: 390, Loss= 0.7784, Training Accuracy= 0.501\n",
      "Epoch: 400, Loss= 0.7651, Training Accuracy= 0.501\n",
      "Epoch: 410, Loss= 0.7606, Training Accuracy= 0.501\n",
      "Epoch: 420, Loss= 0.7567, Training Accuracy= 0.501\n",
      "Epoch: 430, Loss= 0.7525, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.7475, Training Accuracy= 0.501\n",
      "Epoch: 450, Loss= 0.7453, Training Accuracy= 0.501\n",
      "Epoch: 460, Loss= 0.7749, Training Accuracy= 0.495\n",
      "Epoch: 470, Loss= 0.7237, Training Accuracy= 0.501\n",
      "Epoch: 480, Loss= 0.7351, Training Accuracy= 0.501\n",
      "Epoch: 490, Loss= 0.7192, Training Accuracy= 0.501\n",
      "Epoch: 500, Loss= 0.7186, Training Accuracy= 0.501\n",
      "Epoch: 510, Loss= 0.7243, Training Accuracy= 0.501\n",
      "Epoch: 520, Loss= 0.7244, Training Accuracy= 0.501\n",
      "Epoch: 530, Loss= 0.7204, Training Accuracy= 0.501\n",
      "Epoch: 540, Loss= 0.7207, Training Accuracy= 0.501\n",
      "Epoch: 550, Loss= 0.7182, Training Accuracy= 0.501\n",
      "Epoch: 560, Loss= 0.7514, Training Accuracy= 0.495\n",
      "Epoch: 570, Loss= 0.7691, Training Accuracy= 0.495\n",
      "Epoch: 580, Loss= 0.7632, Training Accuracy= 0.495\n",
      "Epoch: 590, Loss= 0.7547, Training Accuracy= 0.495\n",
      "Epoch: 600, Loss= 0.7597, Training Accuracy= 0.499\n",
      "Epoch: 610, Loss= 0.8565, Training Accuracy= 0.495\n",
      "Epoch: 620, Loss= 0.8550, Training Accuracy= 0.495\n",
      "Epoch: 630, Loss= 0.8599, Training Accuracy= 0.495\n",
      "Epoch: 640, Loss= 0.8572, Training Accuracy= 0.495\n",
      "Epoch: 650, Loss= 0.8233, Training Accuracy= 0.495\n",
      "Epoch: 660, Loss= 0.8454, Training Accuracy= 0.495\n",
      "Epoch: 670, Loss= 0.8498, Training Accuracy= 0.495\n",
      "Epoch: 680, Loss= 0.8479, Training Accuracy= 0.495\n",
      "Epoch: 690, Loss= 0.8173, Training Accuracy= 0.495\n",
      "Epoch: 700, Loss= 0.8091, Training Accuracy= 0.495\n",
      "Epoch: 710, Loss= 0.8022, Training Accuracy= 0.495\n",
      "Epoch: 720, Loss= 0.7987, Training Accuracy= 0.495\n",
      "Epoch: 730, Loss= 0.8203, Training Accuracy= 0.495\n",
      "Epoch: 740, Loss= 0.8082, Training Accuracy= 0.495\n",
      "Epoch: 750, Loss= 0.7981, Training Accuracy= 0.495\n",
      "Epoch: 760, Loss= 0.7960, Training Accuracy= 0.495\n",
      "Epoch: 770, Loss= 0.7945, Training Accuracy= 0.495\n",
      "Epoch: 780, Loss= 0.7935, Training Accuracy= 0.495\n",
      "Epoch: 790, Loss= 0.7926, Training Accuracy= 0.495\n",
      "Epoch: 800, Loss= 0.7920, Training Accuracy= 0.495\n",
      "Epoch: 810, Loss= 0.7914, Training Accuracy= 0.495\n",
      "Epoch: 820, Loss= 0.7910, Training Accuracy= 0.495\n",
      "Epoch: 830, Loss= 0.7906, Training Accuracy= 0.495\n",
      "Epoch: 840, Loss= 0.7903, Training Accuracy= 0.495\n",
      "Epoch: 850, Loss= 0.7900, Training Accuracy= 0.495\n",
      "Epoch: 860, Loss= 0.7898, Training Accuracy= 0.495\n",
      "Epoch: 870, Loss= 0.7897, Training Accuracy= 0.495\n",
      "Epoch: 880, Loss= 0.7895, Training Accuracy= 0.495\n",
      "Epoch: 890, Loss= 0.7894, Training Accuracy= 0.495\n",
      "Epoch: 900, Loss= 0.7894, Training Accuracy= 0.495\n",
      "Epoch: 910, Loss= 0.7893, Training Accuracy= 0.495\n",
      "Epoch: 920, Loss= 0.7893, Training Accuracy= 0.495\n",
      "Epoch: 930, Loss= 0.7892, Training Accuracy= 0.495\n",
      "Epoch: 940, Loss= 0.7892, Training Accuracy= 0.495\n",
      "Epoch: 950, Loss= 0.7892, Training Accuracy= 0.495\n",
      "Epoch: 960, Loss= 0.7892, Training Accuracy= 0.495\n",
      "Epoch: 970, Loss= 0.7892, Training Accuracy= 0.495\n",
      "Epoch: 980, Loss= 0.7892, Training Accuracy= 0.495\n",
      "Epoch: 990, Loss= 0.7892, Training Accuracy= 0.495\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4905\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7171, Training Accuracy= 0.506\n",
      "Epoch: 10, Loss= 0.6972, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6951, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6944, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.6942, Training Accuracy= 0.499\n",
      "Epoch: 60, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6941, Training Accuracy= 0.501\n",
      "Epoch: 80, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 90, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 100, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 120, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 130, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 140, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 160, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 170, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 180, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 190, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 230, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 250, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 260, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 270, Loss= 0.6921, Training Accuracy= 0.523\n",
      "Epoch: 280, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 290, Loss= 0.6934, Training Accuracy= 0.515\n",
      "Epoch: 300, Loss= 0.6931, Training Accuracy= 0.517\n",
      "Epoch: 310, Loss= 0.6942, Training Accuracy= 0.512\n",
      "Epoch: 320, Loss= 0.6928, Training Accuracy= 0.520\n",
      "Epoch: 330, Loss= 0.6921, Training Accuracy= 0.523\n",
      "Epoch: 340, Loss= 0.6940, Training Accuracy= 0.519\n",
      "Epoch: 350, Loss= 0.6954, Training Accuracy= 0.519\n",
      "Epoch: 360, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 370, Loss= 0.7000, Training Accuracy= 0.501\n",
      "Epoch: 380, Loss= 0.6965, Training Accuracy= 0.504\n",
      "Epoch: 390, Loss= 0.6952, Training Accuracy= 0.502\n",
      "Epoch: 400, Loss= 0.6953, Training Accuracy= 0.501\n",
      "Epoch: 410, Loss= 0.7037, Training Accuracy= 0.501\n",
      "Epoch: 420, Loss= 0.7018, Training Accuracy= 0.501\n",
      "Epoch: 430, Loss= 0.6987, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.7146, Training Accuracy= 0.502\n",
      "Epoch: 450, Loss= 0.6959, Training Accuracy= 0.504\n",
      "Epoch: 460, Loss= 0.7079, Training Accuracy= 0.500\n",
      "Epoch: 470, Loss= 0.6999, Training Accuracy= 0.498\n",
      "Epoch: 480, Loss= 0.7319, Training Accuracy= 0.498\n",
      "Epoch: 490, Loss= 0.7412, Training Accuracy= 0.500\n",
      "Epoch: 500, Loss= 0.7409, Training Accuracy= 0.500\n",
      "Epoch: 510, Loss= 0.7403, Training Accuracy= 0.500\n",
      "Epoch: 520, Loss= 0.7335, Training Accuracy= 0.500\n",
      "Epoch: 530, Loss= 0.7207, Training Accuracy= 0.500\n",
      "Epoch: 540, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 550, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 560, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 570, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 580, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 590, Loss= 0.7205, Training Accuracy= 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 610, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 620, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 630, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 640, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 650, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 660, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 670, Loss= 0.7206, Training Accuracy= 0.500\n",
      "Epoch: 680, Loss= 0.7206, Training Accuracy= 0.500\n",
      "Epoch: 690, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 700, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 710, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 720, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 730, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 740, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 750, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 760, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 770, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 780, Loss= 0.7205, Training Accuracy= 0.500\n",
      "Epoch: 790, Loss= 0.7204, Training Accuracy= 0.500\n",
      "Epoch: 800, Loss= 0.7204, Training Accuracy= 0.500\n",
      "Epoch: 810, Loss= 0.7203, Training Accuracy= 0.500\n",
      "Epoch: 820, Loss= 0.7201, Training Accuracy= 0.500\n",
      "Epoch: 830, Loss= 0.7195, Training Accuracy= 0.500\n",
      "Epoch: 840, Loss= 0.7136, Training Accuracy= 0.500\n",
      "Epoch: 850, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 860, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 870, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 880, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 890, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 900, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 910, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 920, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 930, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 940, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 950, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 960, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 970, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 980, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 990, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4942\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.7257, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.7148, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.7083, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.7043, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.7018, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.7005, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6996, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.6990, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6986, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6984, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.6982, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.6980, Training Accuracy= 0.502\n",
      "Epoch: 120, Loss= 0.6977, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.6972, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.6956, Training Accuracy= 0.511\n",
      "Epoch: 150, Loss= 0.6954, Training Accuracy= 0.515\n",
      "Epoch: 160, Loss= 0.6958, Training Accuracy= 0.508\n",
      "Epoch: 170, Loss= 0.6950, Training Accuracy= 0.511\n",
      "Epoch: 180, Loss= 0.6960, Training Accuracy= 0.512\n",
      "Epoch: 190, Loss= 0.6979, Training Accuracy= 0.514\n",
      "Epoch: 200, Loss= 0.7002, Training Accuracy= 0.503\n",
      "Epoch: 210, Loss= 0.6959, Training Accuracy= 0.514\n",
      "Epoch: 220, Loss= 0.6999, Training Accuracy= 0.508\n",
      "Epoch: 230, Loss= 0.7006, Training Accuracy= 0.508\n",
      "Epoch: 240, Loss= 0.6974, Training Accuracy= 0.509\n",
      "Epoch: 250, Loss= 0.7003, Training Accuracy= 0.506\n",
      "Epoch: 260, Loss= 0.6953, Training Accuracy= 0.517\n",
      "Epoch: 270, Loss= 0.7166, Training Accuracy= 0.502\n",
      "Epoch: 280, Loss= 0.7008, Training Accuracy= 0.506\n",
      "Epoch: 290, Loss= 0.6956, Training Accuracy= 0.515\n",
      "Epoch: 300, Loss= 0.7004, Training Accuracy= 0.515\n",
      "Epoch: 310, Loss= 0.6938, Training Accuracy= 0.520\n",
      "Epoch: 320, Loss= 0.6922, Training Accuracy= 0.521\n",
      "Epoch: 330, Loss= 0.7012, Training Accuracy= 0.503\n",
      "Epoch: 340, Loss= 0.7023, Training Accuracy= 0.504\n",
      "Epoch: 350, Loss= 0.7038, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.6997, Training Accuracy= 0.501\n",
      "Epoch: 370, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 380, Loss= 0.7011, Training Accuracy= 0.506\n",
      "Epoch: 390, Loss= 0.6992, Training Accuracy= 0.503\n",
      "Epoch: 400, Loss= 0.6941, Training Accuracy= 0.521\n",
      "Epoch: 410, Loss= 0.7018, Training Accuracy= 0.503\n",
      "Epoch: 420, Loss= 0.7008, Training Accuracy= 0.499\n",
      "Epoch: 430, Loss= 0.7002, Training Accuracy= 0.502\n",
      "Epoch: 440, Loss= 0.7006, Training Accuracy= 0.505\n",
      "Epoch: 450, Loss= 0.6989, Training Accuracy= 0.508\n",
      "Epoch: 460, Loss= 0.7123, Training Accuracy= 0.502\n",
      "Epoch: 470, Loss= 0.7008, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.7005, Training Accuracy= 0.502\n",
      "Epoch: 490, Loss= 0.7058, Training Accuracy= 0.505\n",
      "Epoch: 500, Loss= 0.7084, Training Accuracy= 0.503\n",
      "Epoch: 510, Loss= 0.7049, Training Accuracy= 0.504\n",
      "Epoch: 520, Loss= 0.6975, Training Accuracy= 0.504\n",
      "Epoch: 530, Loss= 0.6987, Training Accuracy= 0.502\n",
      "Epoch: 540, Loss= 0.7193, Training Accuracy= 0.502\n",
      "Epoch: 550, Loss= 0.7156, Training Accuracy= 0.499\n",
      "Epoch: 560, Loss= 0.7147, Training Accuracy= 0.505\n",
      "Epoch: 570, Loss= 0.7213, Training Accuracy= 0.502\n",
      "Epoch: 580, Loss= 0.7110, Training Accuracy= 0.502\n",
      "Epoch: 590, Loss= 0.7182, Training Accuracy= 0.502\n",
      "Epoch: 600, Loss= 0.7144, Training Accuracy= 0.502\n",
      "Epoch: 610, Loss= 0.7102, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 0.7155, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 0.7131, Training Accuracy= 0.502\n",
      "Epoch: 640, Loss= 0.7009, Training Accuracy= 0.502\n",
      "Epoch: 650, Loss= 0.7320, Training Accuracy= 0.502\n",
      "Epoch: 660, Loss= 0.7305, Training Accuracy= 0.502\n",
      "Epoch: 670, Loss= 0.7298, Training Accuracy= 0.502\n",
      "Epoch: 680, Loss= 0.7354, Training Accuracy= 0.496\n",
      "Epoch: 690, Loss= 0.7345, Training Accuracy= 0.496\n",
      "Epoch: 700, Loss= 0.7695, Training Accuracy= 0.496\n",
      "Epoch: 710, Loss= 0.7600, Training Accuracy= 0.496\n",
      "Epoch: 720, Loss= 0.7649, Training Accuracy= 0.496\n",
      "Epoch: 730, Loss= 0.7728, Training Accuracy= 0.497\n",
      "Epoch: 740, Loss= 0.7710, Training Accuracy= 0.505\n",
      "Epoch: 750, Loss= 0.7709, Training Accuracy= 0.496\n",
      "Epoch: 760, Loss= 0.7071, Training Accuracy= 0.504\n",
      "Epoch: 770, Loss= 0.7771, Training Accuracy= 0.496\n",
      "Epoch: 780, Loss= 0.7746, Training Accuracy= 0.500\n",
      "Epoch: 790, Loss= 0.7670, Training Accuracy= 0.501\n",
      "Epoch: 800, Loss= 0.7432, Training Accuracy= 0.500\n",
      "Epoch: 810, Loss= 0.7346, Training Accuracy= 0.499\n",
      "Epoch: 820, Loss= 0.6991, Training Accuracy= 0.502\n",
      "Epoch: 830, Loss= 0.7361, Training Accuracy= 0.503\n",
      "Epoch: 840, Loss= 0.7284, Training Accuracy= 0.502\n",
      "Epoch: 850, Loss= 0.7287, Training Accuracy= 0.502\n",
      "Epoch: 860, Loss= 0.7289, Training Accuracy= 0.502\n",
      "Epoch: 870, Loss= 0.7291, Training Accuracy= 0.502\n",
      "Epoch: 880, Loss= 0.7292, Training Accuracy= 0.502\n",
      "Epoch: 890, Loss= 0.7292, Training Accuracy= 0.502\n",
      "Epoch: 900, Loss= 0.7292, Training Accuracy= 0.502\n",
      "Epoch: 910, Loss= 0.7292, Training Accuracy= 0.502\n",
      "Epoch: 920, Loss= 0.7292, Training Accuracy= 0.502\n",
      "Epoch: 930, Loss= 0.7292, Training Accuracy= 0.502\n",
      "Epoch: 940, Loss= 0.7292, Training Accuracy= 0.502\n",
      "Epoch: 950, Loss= 0.7292, Training Accuracy= 0.502\n",
      "Epoch: 960, Loss= 0.7292, Training Accuracy= 0.502\n",
      "Epoch: 970, Loss= 0.7292, Training Accuracy= 0.502\n",
      "Epoch: 980, Loss= 0.7292, Training Accuracy= 0.502\n",
      "Epoch: 990, Loss= 0.7293, Training Accuracy= 0.502\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.505\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.8294, Training Accuracy= 0.508\n",
      "Epoch: 10, Loss= 0.7683, Training Accuracy= 0.508\n",
      "Epoch: 20, Loss= 0.7514, Training Accuracy= 0.508\n",
      "Epoch: 30, Loss= 0.7425, Training Accuracy= 0.508\n",
      "Epoch: 40, Loss= 0.7381, Training Accuracy= 0.508\n",
      "Epoch: 50, Loss= 0.7356, Training Accuracy= 0.508\n",
      "Epoch: 60, Loss= 0.7337, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.7322, Training Accuracy= 0.508\n",
      "Epoch: 80, Loss= 0.7308, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.7293, Training Accuracy= 0.509\n",
      "Epoch: 100, Loss= 0.7280, Training Accuracy= 0.508\n",
      "Epoch: 110, Loss= 0.7269, Training Accuracy= 0.508\n",
      "Epoch: 120, Loss= 0.7258, Training Accuracy= 0.509\n",
      "Epoch: 130, Loss= 0.7248, Training Accuracy= 0.509\n",
      "Epoch: 140, Loss= 0.7238, Training Accuracy= 0.510\n",
      "Epoch: 150, Loss= 0.7229, Training Accuracy= 0.510\n",
      "Epoch: 160, Loss= 0.7225, Training Accuracy= 0.510\n",
      "Epoch: 170, Loss= 0.7225, Training Accuracy= 0.510\n",
      "Epoch: 180, Loss= 0.7223, Training Accuracy= 0.511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.7251, Training Accuracy= 0.513\n",
      "Epoch: 200, Loss= 0.7228, Training Accuracy= 0.511\n",
      "Epoch: 210, Loss= 0.7191, Training Accuracy= 0.510\n",
      "Epoch: 220, Loss= 0.7207, Training Accuracy= 0.515\n",
      "Epoch: 230, Loss= 0.7220, Training Accuracy= 0.515\n",
      "Epoch: 240, Loss= 0.7231, Training Accuracy= 0.513\n",
      "Epoch: 250, Loss= 0.7252, Training Accuracy= 0.511\n",
      "Epoch: 260, Loss= 0.7249, Training Accuracy= 0.511\n",
      "Epoch: 270, Loss= 0.7473, Training Accuracy= 0.508\n",
      "Epoch: 280, Loss= 0.7288, Training Accuracy= 0.508\n",
      "Epoch: 290, Loss= 0.7279, Training Accuracy= 0.508\n",
      "Epoch: 300, Loss= 0.7237, Training Accuracy= 0.511\n",
      "Epoch: 310, Loss= 0.7252, Training Accuracy= 0.511\n",
      "Epoch: 320, Loss= 0.7192, Training Accuracy= 0.512\n",
      "Epoch: 330, Loss= 0.7206, Training Accuracy= 0.512\n",
      "Epoch: 340, Loss= 0.7349, Training Accuracy= 0.508\n",
      "Epoch: 350, Loss= 0.7262, Training Accuracy= 0.513\n",
      "Epoch: 360, Loss= 0.7262, Training Accuracy= 0.514\n",
      "Epoch: 370, Loss= 0.7132, Training Accuracy= 0.512\n",
      "Epoch: 380, Loss= 0.7133, Training Accuracy= 0.508\n",
      "Epoch: 390, Loss= 0.7197, Training Accuracy= 0.511\n",
      "Epoch: 400, Loss= 0.7182, Training Accuracy= 0.511\n",
      "Epoch: 410, Loss= 0.7231, Training Accuracy= 0.515\n",
      "Epoch: 420, Loss= 0.7267, Training Accuracy= 0.515\n",
      "Epoch: 430, Loss= 0.7226, Training Accuracy= 0.517\n",
      "Epoch: 440, Loss= 0.7232, Training Accuracy= 0.509\n",
      "Epoch: 450, Loss= 0.7204, Training Accuracy= 0.514\n",
      "Epoch: 460, Loss= 0.7175, Training Accuracy= 0.517\n",
      "Epoch: 470, Loss= 0.7155, Training Accuracy= 0.509\n",
      "Epoch: 480, Loss= 0.7125, Training Accuracy= 0.508\n",
      "Epoch: 490, Loss= 0.7188, Training Accuracy= 0.508\n",
      "Epoch: 500, Loss= 0.7192, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.7150, Training Accuracy= 0.509\n",
      "Epoch: 520, Loss= 0.7180, Training Accuracy= 0.510\n",
      "Epoch: 530, Loss= 0.7211, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.7275, Training Accuracy= 0.509\n",
      "Epoch: 550, Loss= 0.7517, Training Accuracy= 0.508\n",
      "Epoch: 560, Loss= 0.7381, Training Accuracy= 0.508\n",
      "Epoch: 570, Loss= 0.7335, Training Accuracy= 0.508\n",
      "Epoch: 580, Loss= 0.7515, Training Accuracy= 0.508\n",
      "Epoch: 590, Loss= 0.7497, Training Accuracy= 0.508\n",
      "Epoch: 600, Loss= 0.7715, Training Accuracy= 0.508\n",
      "Epoch: 610, Loss= 0.7417, Training Accuracy= 0.508\n",
      "Epoch: 620, Loss= 0.7308, Training Accuracy= 0.510\n",
      "Epoch: 630, Loss= 0.7276, Training Accuracy= 0.507\n",
      "Epoch: 640, Loss= 0.7337, Training Accuracy= 0.507\n",
      "Epoch: 650, Loss= 0.7363, Training Accuracy= 0.508\n",
      "Epoch: 660, Loss= 0.7186, Training Accuracy= 0.508\n",
      "Epoch: 670, Loss= 0.7321, Training Accuracy= 0.508\n",
      "Epoch: 680, Loss= 0.7264, Training Accuracy= 0.510\n",
      "Epoch: 690, Loss= 0.7268, Training Accuracy= 0.510\n",
      "Epoch: 700, Loss= 0.7260, Training Accuracy= 0.507\n",
      "Epoch: 710, Loss= 0.7233, Training Accuracy= 0.509\n",
      "Epoch: 720, Loss= 0.7795, Training Accuracy= 0.508\n",
      "Epoch: 730, Loss= 0.7920, Training Accuracy= 0.508\n",
      "Epoch: 740, Loss= 0.7768, Training Accuracy= 0.508\n",
      "Epoch: 750, Loss= 0.7720, Training Accuracy= 0.508\n",
      "Epoch: 760, Loss= 0.7629, Training Accuracy= 0.508\n",
      "Epoch: 770, Loss= 0.8008, Training Accuracy= 0.508\n",
      "Epoch: 780, Loss= 0.7856, Training Accuracy= 0.508\n",
      "Epoch: 790, Loss= 0.7596, Training Accuracy= 0.502\n",
      "Epoch: 800, Loss= 0.7554, Training Accuracy= 0.508\n",
      "Epoch: 810, Loss= 0.7665, Training Accuracy= 0.502\n",
      "Epoch: 820, Loss= 0.7614, Training Accuracy= 0.508\n",
      "Epoch: 830, Loss= 0.7559, Training Accuracy= 0.508\n",
      "Epoch: 840, Loss= 0.7524, Training Accuracy= 0.508\n",
      "Epoch: 850, Loss= 0.7522, Training Accuracy= 0.508\n",
      "Epoch: 860, Loss= 0.7521, Training Accuracy= 0.508\n",
      "Epoch: 870, Loss= 0.7520, Training Accuracy= 0.508\n",
      "Epoch: 880, Loss= 0.7518, Training Accuracy= 0.508\n",
      "Epoch: 890, Loss= 0.7516, Training Accuracy= 0.508\n",
      "Epoch: 900, Loss= 0.7514, Training Accuracy= 0.508\n",
      "Epoch: 910, Loss= 0.7510, Training Accuracy= 0.508\n",
      "Epoch: 920, Loss= 0.7505, Training Accuracy= 0.508\n",
      "Epoch: 930, Loss= 0.7499, Training Accuracy= 0.508\n",
      "Epoch: 940, Loss= 0.7493, Training Accuracy= 0.508\n",
      "Epoch: 950, Loss= 0.7488, Training Accuracy= 0.508\n",
      "Epoch: 960, Loss= 0.7484, Training Accuracy= 0.508\n",
      "Epoch: 970, Loss= 0.7480, Training Accuracy= 0.508\n",
      "Epoch: 980, Loss= 0.7473, Training Accuracy= 0.508\n",
      "Epoch: 990, Loss= 0.7467, Training Accuracy= 0.508\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4909\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.7334, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.7220, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.7129, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.7061, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6995, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.6968, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.7126, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.7036, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.6952, Training Accuracy= 0.495\n",
      "Epoch: 90, Loss= 0.7054, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.7056, Training Accuracy= 0.497\n",
      "Epoch: 110, Loss= 0.6997, Training Accuracy= 0.497\n",
      "Epoch: 120, Loss= 0.7314, Training Accuracy= 0.497\n",
      "Epoch: 130, Loss= 0.7294, Training Accuracy= 0.497\n",
      "Epoch: 140, Loss= 0.7258, Training Accuracy= 0.497\n",
      "Epoch: 150, Loss= 0.7167, Training Accuracy= 0.497\n",
      "Epoch: 160, Loss= 0.7165, Training Accuracy= 0.497\n",
      "Epoch: 170, Loss= 0.7161, Training Accuracy= 0.497\n",
      "Epoch: 180, Loss= 0.7148, Training Accuracy= 0.497\n",
      "Epoch: 190, Loss= 0.7091, Training Accuracy= 0.497\n",
      "Epoch: 200, Loss= 0.7086, Training Accuracy= 0.497\n",
      "Epoch: 210, Loss= 0.7086, Training Accuracy= 0.497\n",
      "Epoch: 220, Loss= 0.7086, Training Accuracy= 0.497\n",
      "Epoch: 230, Loss= 0.7086, Training Accuracy= 0.497\n",
      "Epoch: 240, Loss= 0.7086, Training Accuracy= 0.497\n",
      "Epoch: 250, Loss= 0.7086, Training Accuracy= 0.497\n",
      "Epoch: 260, Loss= 0.7086, Training Accuracy= 0.497\n",
      "Epoch: 270, Loss= 0.7085, Training Accuracy= 0.497\n",
      "Epoch: 280, Loss= 0.7085, Training Accuracy= 0.497\n",
      "Epoch: 290, Loss= 0.7085, Training Accuracy= 0.497\n",
      "Epoch: 300, Loss= 0.7085, Training Accuracy= 0.497\n",
      "Epoch: 310, Loss= 0.7085, Training Accuracy= 0.497\n",
      "Epoch: 320, Loss= 0.7085, Training Accuracy= 0.497\n",
      "Epoch: 330, Loss= 0.7085, Training Accuracy= 0.497\n",
      "Epoch: 340, Loss= 0.7085, Training Accuracy= 0.497\n",
      "Epoch: 350, Loss= 0.7086, Training Accuracy= 0.497\n",
      "Epoch: 360, Loss= 0.7086, Training Accuracy= 0.497\n",
      "Epoch: 370, Loss= 0.7083, Training Accuracy= 0.497\n",
      "Epoch: 380, Loss= 0.7080, Training Accuracy= 0.497\n",
      "Epoch: 390, Loss= 0.7079, Training Accuracy= 0.497\n",
      "Epoch: 400, Loss= 0.7078, Training Accuracy= 0.497\n",
      "Epoch: 410, Loss= 0.7077, Training Accuracy= 0.497\n",
      "Epoch: 420, Loss= 0.7076, Training Accuracy= 0.497\n",
      "Epoch: 430, Loss= 0.7075, Training Accuracy= 0.497\n",
      "Epoch: 440, Loss= 0.7073, Training Accuracy= 0.497\n",
      "Epoch: 450, Loss= 0.7071, Training Accuracy= 0.497\n",
      "Epoch: 460, Loss= 0.7070, Training Accuracy= 0.497\n",
      "Epoch: 470, Loss= 0.7070, Training Accuracy= 0.497\n",
      "Epoch: 480, Loss= 0.7069, Training Accuracy= 0.497\n",
      "Epoch: 490, Loss= 0.7069, Training Accuracy= 0.497\n",
      "Epoch: 500, Loss= 0.7068, Training Accuracy= 0.497\n",
      "Epoch: 510, Loss= 0.7067, Training Accuracy= 0.497\n",
      "Epoch: 520, Loss= 0.7067, Training Accuracy= 0.497\n",
      "Epoch: 530, Loss= 0.7066, Training Accuracy= 0.497\n",
      "Epoch: 540, Loss= 0.7064, Training Accuracy= 0.497\n",
      "Epoch: 550, Loss= 0.7063, Training Accuracy= 0.497\n",
      "Epoch: 560, Loss= 0.7061, Training Accuracy= 0.497\n",
      "Epoch: 570, Loss= 0.7061, Training Accuracy= 0.497\n",
      "Epoch: 580, Loss= 0.7060, Training Accuracy= 0.497\n",
      "Epoch: 590, Loss= 0.7060, Training Accuracy= 0.497\n",
      "Epoch: 600, Loss= 0.7060, Training Accuracy= 0.497\n",
      "Epoch: 610, Loss= 0.7060, Training Accuracy= 0.497\n",
      "Epoch: 620, Loss= 0.7060, Training Accuracy= 0.497\n",
      "Epoch: 630, Loss= 0.7059, Training Accuracy= 0.497\n",
      "Epoch: 640, Loss= 0.7059, Training Accuracy= 0.497\n",
      "Epoch: 650, Loss= 0.7059, Training Accuracy= 0.497\n",
      "Epoch: 660, Loss= 0.7059, Training Accuracy= 0.497\n",
      "Epoch: 670, Loss= 0.7059, Training Accuracy= 0.497\n",
      "Epoch: 680, Loss= 0.7059, Training Accuracy= 0.497\n",
      "Epoch: 690, Loss= 0.7059, Training Accuracy= 0.497\n",
      "Epoch: 700, Loss= 0.7060, Training Accuracy= 0.497\n",
      "Epoch: 710, Loss= 0.7060, Training Accuracy= 0.497\n",
      "Epoch: 720, Loss= 0.7061, Training Accuracy= 0.497\n",
      "Epoch: 730, Loss= 0.7062, Training Accuracy= 0.497\n",
      "Epoch: 740, Loss= 0.7062, Training Accuracy= 0.497\n",
      "Epoch: 750, Loss= 0.7063, Training Accuracy= 0.497\n",
      "Epoch: 760, Loss= 0.7062, Training Accuracy= 0.497\n",
      "Epoch: 770, Loss= 0.7065, Training Accuracy= 0.497\n",
      "Epoch: 780, Loss= 0.7064, Training Accuracy= 0.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.7076, Training Accuracy= 0.497\n",
      "Epoch: 800, Loss= 0.7065, Training Accuracy= 0.497\n",
      "Epoch: 810, Loss= 0.7066, Training Accuracy= 0.497\n",
      "Epoch: 820, Loss= 0.7057, Training Accuracy= 0.497\n",
      "Epoch: 830, Loss= 0.7049, Training Accuracy= 0.497\n",
      "Epoch: 840, Loss= 0.7049, Training Accuracy= 0.497\n",
      "Epoch: 850, Loss= 0.7069, Training Accuracy= 0.497\n",
      "Epoch: 860, Loss= 0.7065, Training Accuracy= 0.497\n",
      "Epoch: 870, Loss= 0.7068, Training Accuracy= 0.497\n",
      "Epoch: 880, Loss= 0.7073, Training Accuracy= 0.497\n",
      "Epoch: 890, Loss= 0.7090, Training Accuracy= 0.497\n",
      "Epoch: 900, Loss= 0.7065, Training Accuracy= 0.497\n",
      "Epoch: 910, Loss= 0.7065, Training Accuracy= 0.497\n",
      "Epoch: 920, Loss= 0.7064, Training Accuracy= 0.497\n",
      "Epoch: 930, Loss= 0.7063, Training Accuracy= 0.497\n",
      "Epoch: 940, Loss= 0.7056, Training Accuracy= 0.497\n",
      "Epoch: 950, Loss= 0.7071, Training Accuracy= 0.497\n",
      "Epoch: 960, Loss= 0.7053, Training Accuracy= 0.497\n",
      "Epoch: 970, Loss= 0.7049, Training Accuracy= 0.497\n",
      "Epoch: 980, Loss= 0.7049, Training Accuracy= 0.497\n",
      "Epoch: 990, Loss= 0.7049, Training Accuracy= 0.497\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5023\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.8110, Training Accuracy= 0.493\n",
      "Epoch: 10, Loss= 0.7172, Training Accuracy= 0.493\n",
      "Epoch: 20, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 30, Loss= 0.7047, Training Accuracy= 0.493\n",
      "Epoch: 40, Loss= 0.7032, Training Accuracy= 0.493\n",
      "Epoch: 50, Loss= 0.7023, Training Accuracy= 0.493\n",
      "Epoch: 60, Loss= 0.7007, Training Accuracy= 0.495\n",
      "Epoch: 70, Loss= 0.6999, Training Accuracy= 0.499\n",
      "Epoch: 80, Loss= 0.6992, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6988, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.6983, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.6976, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.6972, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.6970, Training Accuracy= 0.504\n",
      "Epoch: 140, Loss= 0.6968, Training Accuracy= 0.505\n",
      "Epoch: 150, Loss= 0.6969, Training Accuracy= 0.507\n",
      "Epoch: 160, Loss= 0.6972, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6985, Training Accuracy= 0.507\n",
      "Epoch: 180, Loss= 0.6986, Training Accuracy= 0.507\n",
      "Epoch: 190, Loss= 0.7021, Training Accuracy= 0.507\n",
      "Epoch: 200, Loss= 0.6988, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.7008, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.7050, Training Accuracy= 0.507\n",
      "Epoch: 230, Loss= 0.6998, Training Accuracy= 0.509\n",
      "Epoch: 240, Loss= 0.6997, Training Accuracy= 0.511\n",
      "Epoch: 250, Loss= 0.6977, Training Accuracy= 0.512\n",
      "Epoch: 260, Loss= 0.6969, Training Accuracy= 0.510\n",
      "Epoch: 270, Loss= 0.6978, Training Accuracy= 0.513\n",
      "Epoch: 280, Loss= 0.7010, Training Accuracy= 0.508\n",
      "Epoch: 290, Loss= 0.7001, Training Accuracy= 0.502\n",
      "Epoch: 300, Loss= 0.6969, Training Accuracy= 0.511\n",
      "Epoch: 310, Loss= 0.6988, Training Accuracy= 0.510\n",
      "Epoch: 320, Loss= 0.7285, Training Accuracy= 0.493\n",
      "Epoch: 330, Loss= 0.7312, Training Accuracy= 0.493\n",
      "Epoch: 340, Loss= 0.7214, Training Accuracy= 0.494\n",
      "Epoch: 350, Loss= 0.7033, Training Accuracy= 0.499\n",
      "Epoch: 360, Loss= 0.6974, Training Accuracy= 0.510\n",
      "Epoch: 370, Loss= 0.6998, Training Accuracy= 0.504\n",
      "Epoch: 380, Loss= 0.6978, Training Accuracy= 0.512\n",
      "Epoch: 390, Loss= 0.6979, Training Accuracy= 0.510\n",
      "Epoch: 400, Loss= 0.6982, Training Accuracy= 0.514\n",
      "Epoch: 410, Loss= 0.7074, Training Accuracy= 0.497\n",
      "Epoch: 420, Loss= 0.7148, Training Accuracy= 0.500\n",
      "Epoch: 430, Loss= 0.7066, Training Accuracy= 0.502\n",
      "Epoch: 440, Loss= 0.7107, Training Accuracy= 0.499\n",
      "Epoch: 450, Loss= 0.7171, Training Accuracy= 0.494\n",
      "Epoch: 460, Loss= 0.7395, Training Accuracy= 0.493\n",
      "Epoch: 470, Loss= 0.7326, Training Accuracy= 0.493\n",
      "Epoch: 480, Loss= 0.7136, Training Accuracy= 0.494\n",
      "Epoch: 490, Loss= 0.7780, Training Accuracy= 0.493\n",
      "Epoch: 500, Loss= 0.7553, Training Accuracy= 0.493\n",
      "Epoch: 510, Loss= 0.7474, Training Accuracy= 0.493\n",
      "Epoch: 520, Loss= 0.7431, Training Accuracy= 0.493\n",
      "Epoch: 530, Loss= 0.7396, Training Accuracy= 0.493\n",
      "Epoch: 540, Loss= 0.8051, Training Accuracy= 0.493\n",
      "Epoch: 550, Loss= 0.7673, Training Accuracy= 0.493\n",
      "Epoch: 560, Loss= 0.7564, Training Accuracy= 0.493\n",
      "Epoch: 570, Loss= 0.7659, Training Accuracy= 0.495\n",
      "Epoch: 580, Loss= 0.7594, Training Accuracy= 0.493\n",
      "Epoch: 590, Loss= 0.7551, Training Accuracy= 0.493\n",
      "Epoch: 600, Loss= 0.7606, Training Accuracy= 0.493\n",
      "Epoch: 610, Loss= 0.7596, Training Accuracy= 0.493\n",
      "Epoch: 620, Loss= 0.7548, Training Accuracy= 0.493\n",
      "Epoch: 630, Loss= 0.7545, Training Accuracy= 0.493\n",
      "Epoch: 640, Loss= 0.7661, Training Accuracy= 0.493\n",
      "Epoch: 650, Loss= 0.7477, Training Accuracy= 0.493\n",
      "Epoch: 660, Loss= 0.7533, Training Accuracy= 0.493\n",
      "Epoch: 670, Loss= 0.7576, Training Accuracy= 0.493\n",
      "Epoch: 680, Loss= 0.7557, Training Accuracy= 0.493\n",
      "Epoch: 690, Loss= 0.7430, Training Accuracy= 0.493\n",
      "Epoch: 700, Loss= 0.7449, Training Accuracy= 0.493\n",
      "Epoch: 710, Loss= 0.7389, Training Accuracy= 0.493\n",
      "Epoch: 720, Loss= 0.8000, Training Accuracy= 0.493\n",
      "Epoch: 730, Loss= 0.7630, Training Accuracy= 0.493\n",
      "Epoch: 740, Loss= 0.7471, Training Accuracy= 0.493\n",
      "Epoch: 750, Loss= 0.7291, Training Accuracy= 0.493\n",
      "Epoch: 760, Loss= 0.7173, Training Accuracy= 0.500\n",
      "Epoch: 770, Loss= 0.7855, Training Accuracy= 0.493\n",
      "Epoch: 780, Loss= 0.7766, Training Accuracy= 0.493\n",
      "Epoch: 790, Loss= 0.7620, Training Accuracy= 0.495\n",
      "Epoch: 800, Loss= 0.7540, Training Accuracy= 0.496\n",
      "Epoch: 810, Loss= 0.7563, Training Accuracy= 0.495\n",
      "Epoch: 820, Loss= 0.7554, Training Accuracy= 0.496\n",
      "Epoch: 830, Loss= 0.7568, Training Accuracy= 0.496\n",
      "Epoch: 840, Loss= 0.7843, Training Accuracy= 0.493\n",
      "Epoch: 850, Loss= 0.7735, Training Accuracy= 0.493\n",
      "Epoch: 860, Loss= 0.7702, Training Accuracy= 0.493\n",
      "Epoch: 870, Loss= 0.7693, Training Accuracy= 0.493\n",
      "Epoch: 880, Loss= 0.7689, Training Accuracy= 0.493\n",
      "Epoch: 890, Loss= 0.7687, Training Accuracy= 0.493\n",
      "Epoch: 900, Loss= 0.7686, Training Accuracy= 0.493\n",
      "Epoch: 910, Loss= 0.7681, Training Accuracy= 0.493\n",
      "Epoch: 920, Loss= 0.7630, Training Accuracy= 0.493\n",
      "Epoch: 930, Loss= 0.7596, Training Accuracy= 0.493\n",
      "Epoch: 940, Loss= 0.7584, Training Accuracy= 0.493\n",
      "Epoch: 950, Loss= 0.7573, Training Accuracy= 0.493\n",
      "Epoch: 960, Loss= 0.7557, Training Accuracy= 0.493\n",
      "Epoch: 970, Loss= 0.7538, Training Accuracy= 0.493\n",
      "Epoch: 980, Loss= 0.7529, Training Accuracy= 0.493\n",
      "Epoch: 990, Loss= 0.7524, Training Accuracy= 0.493\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5069\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 50, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 60, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 80, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 90, Loss= 0.6929, Training Accuracy= 0.521\n",
      "Epoch: 100, Loss= 0.6925, Training Accuracy= 0.527\n",
      "Epoch: 110, Loss= 0.6924, Training Accuracy= 0.527\n",
      "Epoch: 120, Loss= 0.6923, Training Accuracy= 0.526\n",
      "Epoch: 130, Loss= 0.6919, Training Accuracy= 0.529\n",
      "Epoch: 140, Loss= 0.6916, Training Accuracy= 0.532\n",
      "Epoch: 150, Loss= 0.6924, Training Accuracy= 0.524\n",
      "Epoch: 160, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 180, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 190, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 200, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 210, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 220, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 230, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 240, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 250, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 260, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 270, Loss= 0.6925, Training Accuracy= 0.522\n",
      "Epoch: 280, Loss= 0.6933, Training Accuracy= 0.517\n",
      "Epoch: 290, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 300, Loss= 0.6933, Training Accuracy= 0.498\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 320, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 330, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 340, Loss= 0.6960, Training Accuracy= 0.516\n",
      "Epoch: 350, Loss= 0.6997, Training Accuracy= 0.518\n",
      "Epoch: 360, Loss= 0.6938, Training Accuracy= 0.515\n",
      "Epoch: 370, Loss= 0.6945, Training Accuracy= 0.517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6946, Training Accuracy= 0.518\n",
      "Epoch: 390, Loss= 0.6936, Training Accuracy= 0.521\n",
      "Epoch: 400, Loss= 0.6919, Training Accuracy= 0.522\n",
      "Epoch: 410, Loss= 0.6936, Training Accuracy= 0.529\n",
      "Epoch: 420, Loss= 0.6911, Training Accuracy= 0.519\n",
      "Epoch: 430, Loss= 0.6917, Training Accuracy= 0.528\n",
      "Epoch: 440, Loss= 0.6951, Training Accuracy= 0.519\n",
      "Epoch: 450, Loss= 0.6916, Training Accuracy= 0.526\n",
      "Epoch: 460, Loss= 0.6953, Training Accuracy= 0.523\n",
      "Epoch: 470, Loss= 0.7195, Training Accuracy= 0.515\n",
      "Epoch: 480, Loss= 0.6912, Training Accuracy= 0.528\n",
      "Epoch: 490, Loss= 0.6943, Training Accuracy= 0.523\n",
      "Epoch: 500, Loss= 0.6924, Training Accuracy= 0.522\n",
      "Epoch: 510, Loss= 0.6937, Training Accuracy= 0.514\n",
      "Epoch: 520, Loss= 0.6924, Training Accuracy= 0.521\n",
      "Epoch: 530, Loss= 0.6909, Training Accuracy= 0.527\n",
      "Epoch: 540, Loss= 0.6941, Training Accuracy= 0.516\n",
      "Epoch: 550, Loss= 0.6935, Training Accuracy= 0.518\n",
      "Epoch: 560, Loss= 0.6925, Training Accuracy= 0.521\n",
      "Epoch: 570, Loss= 0.6929, Training Accuracy= 0.518\n",
      "Epoch: 580, Loss= 0.6918, Training Accuracy= 0.526\n",
      "Epoch: 590, Loss= 0.6946, Training Accuracy= 0.506\n",
      "Epoch: 600, Loss= 0.7014, Training Accuracy= 0.500\n",
      "Epoch: 610, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 620, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 630, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 650, Loss= 0.6933, Training Accuracy= 0.519\n",
      "Epoch: 660, Loss= 0.6935, Training Accuracy= 0.514\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 680, Loss= 0.6929, Training Accuracy= 0.517\n",
      "Epoch: 690, Loss= 0.6928, Training Accuracy= 0.520\n",
      "Epoch: 700, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 710, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 720, Loss= 0.6939, Training Accuracy= 0.514\n",
      "Epoch: 730, Loss= 0.6944, Training Accuracy= 0.511\n",
      "Epoch: 740, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 750, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 760, Loss= 0.6919, Training Accuracy= 0.518\n",
      "Epoch: 770, Loss= 0.6912, Training Accuracy= 0.524\n",
      "Epoch: 780, Loss= 0.6945, Training Accuracy= 0.508\n",
      "Epoch: 790, Loss= 0.6931, Training Accuracy= 0.522\n",
      "Epoch: 800, Loss= 0.6943, Training Accuracy= 0.514\n",
      "Epoch: 810, Loss= 0.6940, Training Accuracy= 0.516\n",
      "Epoch: 820, Loss= 0.6932, Training Accuracy= 0.515\n",
      "Epoch: 830, Loss= 0.6933, Training Accuracy= 0.520\n",
      "Epoch: 840, Loss= 0.6974, Training Accuracy= 0.506\n",
      "Epoch: 850, Loss= 0.7009, Training Accuracy= 0.512\n",
      "Epoch: 860, Loss= 0.6979, Training Accuracy= 0.502\n",
      "Epoch: 870, Loss= 0.6946, Training Accuracy= 0.499\n",
      "Epoch: 880, Loss= 0.7133, Training Accuracy= 0.495\n",
      "Epoch: 890, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 900, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 910, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 920, Loss= 0.6983, Training Accuracy= 0.496\n",
      "Epoch: 930, Loss= 0.7002, Training Accuracy= 0.497\n",
      "Epoch: 940, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 950, Loss= 0.6937, Training Accuracy= 0.497\n",
      "Epoch: 960, Loss= 0.6935, Training Accuracy= 0.498\n",
      "Epoch: 970, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 980, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 990, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4888\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.7528, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.6999, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.6978, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6968, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6963, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.6960, Training Accuracy= 0.498\n",
      "Epoch: 70, Loss= 0.6960, Training Accuracy= 0.498\n",
      "Epoch: 80, Loss= 0.6959, Training Accuracy= 0.498\n",
      "Epoch: 90, Loss= 0.6959, Training Accuracy= 0.501\n",
      "Epoch: 100, Loss= 0.6960, Training Accuracy= 0.504\n",
      "Epoch: 110, Loss= 0.6959, Training Accuracy= 0.506\n",
      "Epoch: 120, Loss= 0.6955, Training Accuracy= 0.508\n",
      "Epoch: 130, Loss= 0.6951, Training Accuracy= 0.511\n",
      "Epoch: 140, Loss= 0.6946, Training Accuracy= 0.509\n",
      "Epoch: 150, Loss= 0.6943, Training Accuracy= 0.509\n",
      "Epoch: 160, Loss= 0.6946, Training Accuracy= 0.510\n",
      "Epoch: 170, Loss= 0.6947, Training Accuracy= 0.509\n",
      "Epoch: 180, Loss= 0.6950, Training Accuracy= 0.510\n",
      "Epoch: 190, Loss= 0.6964, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6959, Training Accuracy= 0.511\n",
      "Epoch: 210, Loss= 0.6963, Training Accuracy= 0.504\n",
      "Epoch: 220, Loss= 0.6962, Training Accuracy= 0.504\n",
      "Epoch: 230, Loss= 0.6957, Training Accuracy= 0.509\n",
      "Epoch: 240, Loss= 0.6952, Training Accuracy= 0.512\n",
      "Epoch: 250, Loss= 0.6969, Training Accuracy= 0.517\n",
      "Epoch: 260, Loss= 0.6945, Training Accuracy= 0.524\n",
      "Epoch: 270, Loss= 0.6909, Training Accuracy= 0.521\n",
      "Epoch: 280, Loss= 0.6901, Training Accuracy= 0.526\n",
      "Epoch: 290, Loss= 0.7164, Training Accuracy= 0.497\n",
      "Epoch: 300, Loss= 0.7157, Training Accuracy= 0.497\n",
      "Epoch: 310, Loss= 0.7143, Training Accuracy= 0.497\n",
      "Epoch: 320, Loss= 0.7152, Training Accuracy= 0.497\n",
      "Epoch: 330, Loss= 0.7135, Training Accuracy= 0.497\n",
      "Epoch: 340, Loss= 0.7135, Training Accuracy= 0.497\n",
      "Epoch: 350, Loss= 0.7135, Training Accuracy= 0.497\n",
      "Epoch: 360, Loss= 0.7135, Training Accuracy= 0.497\n",
      "Epoch: 370, Loss= 0.6940, Training Accuracy= 0.497\n",
      "Epoch: 380, Loss= 0.6936, Training Accuracy= 0.590\n",
      "Epoch: 390, Loss= 0.6933, Training Accuracy= 0.589\n",
      "Epoch: 400, Loss= 0.6931, Training Accuracy= 0.587\n",
      "Epoch: 410, Loss= 0.6933, Training Accuracy= 0.590\n",
      "Epoch: 420, Loss= 0.6924, Training Accuracy= 0.588\n",
      "Epoch: 430, Loss= 0.6929, Training Accuracy= 0.604\n",
      "Epoch: 440, Loss= 0.6932, Training Accuracy= 0.619\n",
      "Epoch: 450, Loss= 0.6933, Training Accuracy= 0.618\n",
      "Epoch: 460, Loss= 0.6932, Training Accuracy= 0.617\n",
      "Epoch: 470, Loss= 0.6932, Training Accuracy= 0.618\n",
      "Epoch: 480, Loss= 0.6933, Training Accuracy= 0.618\n",
      "Epoch: 490, Loss= 0.6931, Training Accuracy= 0.611\n",
      "Epoch: 500, Loss= 0.6933, Training Accuracy= 0.618\n",
      "Epoch: 510, Loss= 0.6931, Training Accuracy= 0.618\n",
      "Epoch: 520, Loss= 0.6932, Training Accuracy= 0.617\n",
      "Epoch: 530, Loss= 0.7192, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.7189, Training Accuracy= 0.498\n",
      "Epoch: 550, Loss= 0.7189, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.7152, Training Accuracy= 0.497\n",
      "Epoch: 570, Loss= 0.7153, Training Accuracy= 0.497\n",
      "Epoch: 580, Loss= 0.7153, Training Accuracy= 0.497\n",
      "Epoch: 590, Loss= 0.7152, Training Accuracy= 0.497\n",
      "Epoch: 600, Loss= 0.7153, Training Accuracy= 0.497\n",
      "Epoch: 610, Loss= 0.7153, Training Accuracy= 0.497\n",
      "Epoch: 620, Loss= 0.7153, Training Accuracy= 0.497\n",
      "Epoch: 630, Loss= 0.7153, Training Accuracy= 0.497\n",
      "Epoch: 640, Loss= 0.7152, Training Accuracy= 0.497\n",
      "Epoch: 650, Loss= 0.7152, Training Accuracy= 0.497\n",
      "Epoch: 660, Loss= 0.7145, Training Accuracy= 0.497\n",
      "Epoch: 670, Loss= 0.7139, Training Accuracy= 0.497\n",
      "Epoch: 680, Loss= 0.7140, Training Accuracy= 0.497\n",
      "Epoch: 690, Loss= 0.7139, Training Accuracy= 0.497\n",
      "Epoch: 700, Loss= 0.7130, Training Accuracy= 0.501\n",
      "Epoch: 710, Loss= 0.7134, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.7121, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.7122, Training Accuracy= 0.502\n",
      "Epoch: 740, Loss= 0.7114, Training Accuracy= 0.501\n",
      "Epoch: 750, Loss= 0.7111, Training Accuracy= 0.500\n",
      "Epoch: 760, Loss= 0.7052, Training Accuracy= 0.502\n",
      "Epoch: 770, Loss= 0.7051, Training Accuracy= 0.502\n",
      "Epoch: 780, Loss= 0.7070, Training Accuracy= 0.501\n",
      "Epoch: 790, Loss= 0.7114, Training Accuracy= 0.502\n",
      "Epoch: 800, Loss= 0.7115, Training Accuracy= 0.502\n",
      "Epoch: 810, Loss= 0.7114, Training Accuracy= 0.503\n",
      "Epoch: 820, Loss= 0.7111, Training Accuracy= 0.504\n",
      "Epoch: 830, Loss= 0.7107, Training Accuracy= 0.504\n",
      "Epoch: 840, Loss= 0.7137, Training Accuracy= 0.500\n",
      "Epoch: 850, Loss= 0.7138, Training Accuracy= 0.500\n",
      "Epoch: 860, Loss= 0.7139, Training Accuracy= 0.501\n",
      "Epoch: 870, Loss= 0.7137, Training Accuracy= 0.501\n",
      "Epoch: 880, Loss= 0.7133, Training Accuracy= 0.501\n",
      "Epoch: 890, Loss= 0.7127, Training Accuracy= 0.501\n",
      "Epoch: 900, Loss= 0.7121, Training Accuracy= 0.501\n",
      "Epoch: 910, Loss= 0.7112, Training Accuracy= 0.500\n",
      "Epoch: 920, Loss= 0.7107, Training Accuracy= 0.500\n",
      "Epoch: 930, Loss= 0.7105, Training Accuracy= 0.500\n",
      "Epoch: 940, Loss= 0.7104, Training Accuracy= 0.500\n",
      "Epoch: 950, Loss= 0.7103, Training Accuracy= 0.500\n",
      "Epoch: 960, Loss= 0.7102, Training Accuracy= 0.500\n",
      "Epoch: 970, Loss= 0.7102, Training Accuracy= 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.7101, Training Accuracy= 0.500\n",
      "Epoch: 990, Loss= 0.7101, Training Accuracy= 0.500\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4874\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.7660, Training Accuracy= 0.491\n",
      "Epoch: 10, Loss= 0.7139, Training Accuracy= 0.492\n",
      "Epoch: 20, Loss= 0.7052, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.7056, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.7056, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.7099, Training Accuracy= 0.501\n",
      "Epoch: 60, Loss= 0.7135, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.7144, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.7133, Training Accuracy= 0.496\n",
      "Epoch: 90, Loss= 0.7007, Training Accuracy= 0.512\n",
      "Epoch: 100, Loss= 0.6941, Training Accuracy= 0.516\n",
      "Epoch: 110, Loss= 0.6939, Training Accuracy= 0.516\n",
      "Epoch: 120, Loss= 0.6944, Training Accuracy= 0.515\n",
      "Epoch: 130, Loss= 0.6940, Training Accuracy= 0.521\n",
      "Epoch: 140, Loss= 0.6963, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 160, Loss= 0.6952, Training Accuracy= 0.516\n",
      "Epoch: 170, Loss= 0.6942, Training Accuracy= 0.517\n",
      "Epoch: 180, Loss= 0.6942, Training Accuracy= 0.512\n",
      "Epoch: 190, Loss= 0.6932, Training Accuracy= 0.518\n",
      "Epoch: 200, Loss= 0.6928, Training Accuracy= 0.519\n",
      "Epoch: 210, Loss= 0.6922, Training Accuracy= 0.524\n",
      "Epoch: 220, Loss= 0.6936, Training Accuracy= 0.515\n",
      "Epoch: 230, Loss= 0.6975, Training Accuracy= 0.521\n",
      "Epoch: 240, Loss= 0.6954, Training Accuracy= 0.512\n",
      "Epoch: 250, Loss= 0.6938, Training Accuracy= 0.525\n",
      "Epoch: 260, Loss= 0.7222, Training Accuracy= 0.494\n",
      "Epoch: 270, Loss= 0.6919, Training Accuracy= 0.525\n",
      "Epoch: 280, Loss= 0.6922, Training Accuracy= 0.531\n",
      "Epoch: 290, Loss= 0.7302, Training Accuracy= 0.494\n",
      "Epoch: 300, Loss= 0.6941, Training Accuracy= 0.519\n",
      "Epoch: 310, Loss= 0.6914, Training Accuracy= 0.527\n",
      "Epoch: 320, Loss= 0.7167, Training Accuracy= 0.495\n",
      "Epoch: 330, Loss= 0.7236, Training Accuracy= 0.494\n",
      "Epoch: 340, Loss= 0.7183, Training Accuracy= 0.494\n",
      "Epoch: 350, Loss= 0.7139, Training Accuracy= 0.494\n",
      "Epoch: 360, Loss= 0.7134, Training Accuracy= 0.495\n",
      "Epoch: 370, Loss= 0.6998, Training Accuracy= 0.502\n",
      "Epoch: 380, Loss= 0.7038, Training Accuracy= 0.496\n",
      "Epoch: 390, Loss= 0.7072, Training Accuracy= 0.494\n",
      "Epoch: 400, Loss= 0.7055, Training Accuracy= 0.498\n",
      "Epoch: 410, Loss= 0.7047, Training Accuracy= 0.498\n",
      "Epoch: 420, Loss= 0.7078, Training Accuracy= 0.496\n",
      "Epoch: 430, Loss= 0.7259, Training Accuracy= 0.494\n",
      "Epoch: 440, Loss= 0.7227, Training Accuracy= 0.494\n",
      "Epoch: 450, Loss= 0.7201, Training Accuracy= 0.494\n",
      "Epoch: 460, Loss= 0.7185, Training Accuracy= 0.493\n",
      "Epoch: 470, Loss= 0.7166, Training Accuracy= 0.494\n",
      "Epoch: 480, Loss= 0.7190, Training Accuracy= 0.494\n",
      "Epoch: 490, Loss= 0.7150, Training Accuracy= 0.495\n",
      "Epoch: 500, Loss= 0.7195, Training Accuracy= 0.494\n",
      "Epoch: 510, Loss= 0.7257, Training Accuracy= 0.494\n",
      "Epoch: 520, Loss= 0.7268, Training Accuracy= 0.494\n",
      "Epoch: 530, Loss= 0.7209, Training Accuracy= 0.493\n",
      "Epoch: 540, Loss= 0.7120, Training Accuracy= 0.499\n",
      "Epoch: 550, Loss= 0.7113, Training Accuracy= 0.500\n",
      "Epoch: 560, Loss= 0.7180, Training Accuracy= 0.500\n",
      "Epoch: 570, Loss= 0.7126, Training Accuracy= 0.504\n",
      "Epoch: 580, Loss= 0.7072, Training Accuracy= 0.498\n",
      "Epoch: 590, Loss= 0.7645, Training Accuracy= 0.494\n",
      "Epoch: 600, Loss= 0.7377, Training Accuracy= 0.494\n",
      "Epoch: 610, Loss= 0.7327, Training Accuracy= 0.494\n",
      "Epoch: 620, Loss= 0.7149, Training Accuracy= 0.494\n",
      "Epoch: 630, Loss= 0.7105, Training Accuracy= 0.494\n",
      "Epoch: 640, Loss= 0.7084, Training Accuracy= 0.494\n",
      "Epoch: 650, Loss= 0.7066, Training Accuracy= 0.494\n",
      "Epoch: 660, Loss= 0.7064, Training Accuracy= 0.494\n",
      "Epoch: 670, Loss= 0.7370, Training Accuracy= 0.502\n",
      "Epoch: 680, Loss= 0.7363, Training Accuracy= 0.502\n",
      "Epoch: 690, Loss= 0.7366, Training Accuracy= 0.502\n",
      "Epoch: 700, Loss= 0.7359, Training Accuracy= 0.502\n",
      "Epoch: 710, Loss= 0.7354, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.7352, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.7365, Training Accuracy= 0.502\n",
      "Epoch: 740, Loss= 0.7368, Training Accuracy= 0.502\n",
      "Epoch: 750, Loss= 0.7369, Training Accuracy= 0.501\n",
      "Epoch: 760, Loss= 0.7351, Training Accuracy= 0.501\n",
      "Epoch: 770, Loss= 0.7176, Training Accuracy= 0.499\n",
      "Epoch: 780, Loss= 0.7367, Training Accuracy= 0.501\n",
      "Epoch: 790, Loss= 0.7366, Training Accuracy= 0.501\n",
      "Epoch: 800, Loss= 0.7338, Training Accuracy= 0.504\n",
      "Epoch: 810, Loss= 0.7198, Training Accuracy= 0.495\n",
      "Epoch: 820, Loss= 0.7176, Training Accuracy= 0.496\n",
      "Epoch: 830, Loss= 0.7158, Training Accuracy= 0.498\n",
      "Epoch: 840, Loss= 0.7551, Training Accuracy= 0.494\n",
      "Epoch: 850, Loss= 0.7583, Training Accuracy= 0.494\n",
      "Epoch: 860, Loss= 0.7499, Training Accuracy= 0.494\n",
      "Epoch: 870, Loss= 0.7590, Training Accuracy= 0.494\n",
      "Epoch: 880, Loss= 0.7549, Training Accuracy= 0.494\n",
      "Epoch: 890, Loss= 0.7530, Training Accuracy= 0.494\n",
      "Epoch: 900, Loss= 0.7519, Training Accuracy= 0.494\n",
      "Epoch: 910, Loss= 0.7524, Training Accuracy= 0.494\n",
      "Epoch: 920, Loss= 0.7515, Training Accuracy= 0.494\n",
      "Epoch: 930, Loss= 0.7517, Training Accuracy= 0.494\n",
      "Epoch: 940, Loss= 0.7084, Training Accuracy= 0.504\n",
      "Epoch: 950, Loss= 0.7650, Training Accuracy= 0.494\n",
      "Epoch: 960, Loss= 0.7637, Training Accuracy= 0.494\n",
      "Epoch: 970, Loss= 0.7647, Training Accuracy= 0.494\n",
      "Epoch: 980, Loss= 0.7661, Training Accuracy= 0.494\n",
      "Epoch: 990, Loss= 0.7562, Training Accuracy= 0.494\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4931\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.8169, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.7271, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.7103, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.7041, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.7017, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.7018, Training Accuracy= 0.498\n",
      "Epoch: 60, Loss= 0.7006, Training Accuracy= 0.499\n",
      "Epoch: 70, Loss= 0.6996, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6988, Training Accuracy= 0.501\n",
      "Epoch: 90, Loss= 0.6976, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.6973, Training Accuracy= 0.507\n",
      "Epoch: 110, Loss= 0.6970, Training Accuracy= 0.509\n",
      "Epoch: 120, Loss= 0.6965, Training Accuracy= 0.508\n",
      "Epoch: 130, Loss= 0.6973, Training Accuracy= 0.497\n",
      "Epoch: 140, Loss= 0.6956, Training Accuracy= 0.514\n",
      "Epoch: 150, Loss= 0.6962, Training Accuracy= 0.505\n",
      "Epoch: 160, Loss= 0.6950, Training Accuracy= 0.519\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 180, Loss= 0.6934, Training Accuracy= 0.513\n",
      "Epoch: 190, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 200, Loss= 0.6924, Training Accuracy= 0.519\n",
      "Epoch: 210, Loss= 0.6982, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6970, Training Accuracy= 0.498\n",
      "Epoch: 230, Loss= 0.6915, Training Accuracy= 0.522\n",
      "Epoch: 240, Loss= 0.7508, Training Accuracy= 0.498\n",
      "Epoch: 250, Loss= 0.7429, Training Accuracy= 0.498\n",
      "Epoch: 260, Loss= 0.7269, Training Accuracy= 0.498\n",
      "Epoch: 270, Loss= 0.7197, Training Accuracy= 0.498\n",
      "Epoch: 280, Loss= 0.7092, Training Accuracy= 0.498\n",
      "Epoch: 290, Loss= 0.7086, Training Accuracy= 0.497\n",
      "Epoch: 300, Loss= 0.7051, Training Accuracy= 0.498\n",
      "Epoch: 310, Loss= 0.7046, Training Accuracy= 0.503\n",
      "Epoch: 320, Loss= 0.7035, Training Accuracy= 0.499\n",
      "Epoch: 330, Loss= 0.6995, Training Accuracy= 0.497\n",
      "Epoch: 340, Loss= 0.6991, Training Accuracy= 0.501\n",
      "Epoch: 350, Loss= 0.7000, Training Accuracy= 0.501\n",
      "Epoch: 360, Loss= 0.7000, Training Accuracy= 0.502\n",
      "Epoch: 370, Loss= 0.6998, Training Accuracy= 0.502\n",
      "Epoch: 380, Loss= 0.6993, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.6982, Training Accuracy= 0.498\n",
      "Epoch: 400, Loss= 0.6987, Training Accuracy= 0.503\n",
      "Epoch: 410, Loss= 0.6987, Training Accuracy= 0.503\n",
      "Epoch: 420, Loss= 0.6974, Training Accuracy= 0.502\n",
      "Epoch: 430, Loss= 0.6953, Training Accuracy= 0.507\n",
      "Epoch: 440, Loss= 0.7118, Training Accuracy= 0.494\n",
      "Epoch: 450, Loss= 0.7141, Training Accuracy= 0.495\n",
      "Epoch: 460, Loss= 0.7025, Training Accuracy= 0.498\n",
      "Epoch: 470, Loss= 0.7027, Training Accuracy= 0.498\n",
      "Epoch: 480, Loss= 0.7025, Training Accuracy= 0.498\n",
      "Epoch: 490, Loss= 0.7039, Training Accuracy= 0.498\n",
      "Epoch: 500, Loss= 0.7001, Training Accuracy= 0.498\n",
      "Epoch: 510, Loss= 0.6995, Training Accuracy= 0.498\n",
      "Epoch: 520, Loss= 0.6984, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.7056, Training Accuracy= 0.497\n",
      "Epoch: 540, Loss= 0.7019, Training Accuracy= 0.497\n",
      "Epoch: 550, Loss= 0.8866, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.8629, Training Accuracy= 0.498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.8421, Training Accuracy= 0.498\n",
      "Epoch: 580, Loss= 0.8392, Training Accuracy= 0.498\n",
      "Epoch: 590, Loss= 0.8392, Training Accuracy= 0.498\n",
      "Epoch: 600, Loss= 0.8394, Training Accuracy= 0.498\n",
      "Epoch: 610, Loss= 0.8394, Training Accuracy= 0.498\n",
      "Epoch: 620, Loss= 0.8394, Training Accuracy= 0.498\n",
      "Epoch: 630, Loss= 0.8394, Training Accuracy= 0.498\n",
      "Epoch: 640, Loss= 0.8394, Training Accuracy= 0.498\n",
      "Epoch: 650, Loss= 0.8394, Training Accuracy= 0.498\n",
      "Epoch: 660, Loss= 0.8394, Training Accuracy= 0.498\n",
      "Epoch: 670, Loss= 0.8394, Training Accuracy= 0.498\n",
      "Epoch: 680, Loss= 0.8393, Training Accuracy= 0.498\n",
      "Epoch: 690, Loss= 0.8393, Training Accuracy= 0.498\n",
      "Epoch: 700, Loss= 0.8392, Training Accuracy= 0.498\n",
      "Epoch: 710, Loss= 0.8391, Training Accuracy= 0.498\n",
      "Epoch: 720, Loss= 0.8390, Training Accuracy= 0.498\n",
      "Epoch: 730, Loss= 0.8388, Training Accuracy= 0.498\n",
      "Epoch: 740, Loss= 0.8385, Training Accuracy= 0.498\n",
      "Epoch: 750, Loss= 0.8379, Training Accuracy= 0.498\n",
      "Epoch: 760, Loss= 0.8365, Training Accuracy= 0.498\n",
      "Epoch: 770, Loss= 0.8292, Training Accuracy= 0.498\n",
      "Epoch: 780, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 790, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 800, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 810, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 820, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 830, Loss= 0.8057, Training Accuracy= 0.498\n",
      "Epoch: 840, Loss= 0.8057, Training Accuracy= 0.498\n",
      "Epoch: 850, Loss= 0.8057, Training Accuracy= 0.498\n",
      "Epoch: 860, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 870, Loss= 0.8057, Training Accuracy= 0.498\n",
      "Epoch: 880, Loss= 0.8057, Training Accuracy= 0.498\n",
      "Epoch: 890, Loss= 0.8057, Training Accuracy= 0.498\n",
      "Epoch: 900, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 910, Loss= 0.8057, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 930, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 940, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 950, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 960, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 970, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 980, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Epoch: 990, Loss= 0.8056, Training Accuracy= 0.498\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5057\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.3\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.4905, 0.49419999, 0.505, 0.49090001, 0.50230002, 0.50690001, 0.48879999, 0.4874, 0.49309999, 0.50569999]\n",
      "mean of test_accuracies_10replications:  0.49648\n",
      "standard deviation of test_accuracies_10replications_std_mean:  7.2456644848e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nziXEE4dzzrmE/H/Qn6PKUZCYUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9a5d3f2690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
