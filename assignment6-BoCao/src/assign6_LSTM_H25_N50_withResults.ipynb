{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 25\n",
    "N = 50\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.7021, Training Accuracy= 0.499\n",
      "Epoch: 10, Loss= 0.6992, Training Accuracy= 0.499\n",
      "Epoch: 20, Loss= 0.6978, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.6970, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.6964, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.6959, Training Accuracy= 0.499\n",
      "Epoch: 60, Loss= 0.6956, Training Accuracy= 0.499\n",
      "Epoch: 70, Loss= 0.6953, Training Accuracy= 0.499\n",
      "Epoch: 80, Loss= 0.6950, Training Accuracy= 0.499\n",
      "Epoch: 90, Loss= 0.6948, Training Accuracy= 0.499\n",
      "Epoch: 100, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 140, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 150, Loss= 0.6941, Training Accuracy= 0.501\n",
      "Epoch: 160, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 170, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 180, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 190, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 200, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 210, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 220, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 230, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 270, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 280, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 290, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 300, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 310, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 320, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 330, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 340, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 350, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 360, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 370, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 380, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 400, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 410, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 430, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 440, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 450, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 460, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 470, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 490, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 500, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 510, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 520, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 530, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 540, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 550, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 560, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 570, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 580, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 590, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 600, Loss= 0.6916, Training Accuracy= 0.516\n",
      "Epoch: 610, Loss= 0.6917, Training Accuracy= 0.516\n",
      "Epoch: 620, Loss= 0.6916, Training Accuracy= 0.517\n",
      "Epoch: 630, Loss= 0.7097, Training Accuracy= 0.505\n",
      "Epoch: 640, Loss= 0.6986, Training Accuracy= 0.500\n",
      "Epoch: 650, Loss= 0.6962, Training Accuracy= 0.503\n",
      "Epoch: 660, Loss= 0.6946, Training Accuracy= 0.506\n",
      "Epoch: 670, Loss= 0.6937, Training Accuracy= 0.511\n",
      "Epoch: 680, Loss= 0.6936, Training Accuracy= 0.511\n",
      "Epoch: 690, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 700, Loss= 0.6919, Training Accuracy= 0.518\n",
      "Epoch: 710, Loss= 0.6910, Training Accuracy= 0.527\n",
      "Epoch: 720, Loss= 0.6905, Training Accuracy= 0.530\n",
      "Epoch: 730, Loss= 0.6896, Training Accuracy= 0.532\n",
      "Epoch: 740, Loss= 0.6908, Training Accuracy= 0.522\n",
      "Epoch: 750, Loss= 0.6898, Training Accuracy= 0.531\n",
      "Epoch: 760, Loss= 0.6888, Training Accuracy= 0.533\n",
      "Epoch: 770, Loss= 0.6915, Training Accuracy= 0.529\n",
      "Epoch: 780, Loss= 0.6878, Training Accuracy= 0.538\n",
      "Epoch: 790, Loss= 0.6879, Training Accuracy= 0.537\n",
      "Epoch: 800, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 810, Loss= 0.6885, Training Accuracy= 0.537\n",
      "Epoch: 820, Loss= 0.6887, Training Accuracy= 0.540\n",
      "Epoch: 830, Loss= 0.6832, Training Accuracy= 0.547\n",
      "Epoch: 840, Loss= 0.6947, Training Accuracy= 0.511\n",
      "Epoch: 850, Loss= 0.6811, Training Accuracy= 0.552\n",
      "Epoch: 860, Loss= 0.6797, Training Accuracy= 0.559\n",
      "Epoch: 870, Loss= 0.6899, Training Accuracy= 0.533\n",
      "Epoch: 880, Loss= 0.6960, Training Accuracy= 0.503\n",
      "Epoch: 890, Loss= 0.6945, Training Accuracy= 0.508\n",
      "Epoch: 900, Loss= 0.6916, Training Accuracy= 0.514\n",
      "Epoch: 910, Loss= 0.6890, Training Accuracy= 0.521\n",
      "Epoch: 920, Loss= 0.6931, Training Accuracy= 0.519\n",
      "Epoch: 930, Loss= 0.6885, Training Accuracy= 0.527\n",
      "Epoch: 940, Loss= 0.6896, Training Accuracy= 0.529\n",
      "Epoch: 950, Loss= 0.6897, Training Accuracy= 0.521\n",
      "Epoch: 960, Loss= 0.6887, Training Accuracy= 0.526\n",
      "Epoch: 970, Loss= 0.6870, Training Accuracy= 0.535\n",
      "Epoch: 980, Loss= 0.6870, Training Accuracy= 0.532\n",
      "Epoch: 990, Loss= 0.6817, Training Accuracy= 0.554\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5021\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7053, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.6965, Training Accuracy= 0.495\n",
      "Epoch: 20, Loss= 0.6953, Training Accuracy= 0.495\n",
      "Epoch: 30, Loss= 0.6948, Training Accuracy= 0.495\n",
      "Epoch: 40, Loss= 0.6945, Training Accuracy= 0.495\n",
      "Epoch: 50, Loss= 0.6942, Training Accuracy= 0.495\n",
      "Epoch: 60, Loss= 0.6940, Training Accuracy= 0.495\n",
      "Epoch: 70, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.6937, Training Accuracy= 0.499\n",
      "Epoch: 90, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.6933, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 120, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 140, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 150, Loss= 0.6925, Training Accuracy= 0.513\n",
      "Epoch: 160, Loss= 0.6923, Training Accuracy= 0.512\n",
      "Epoch: 170, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 180, Loss= 0.6920, Training Accuracy= 0.519\n",
      "Epoch: 190, Loss= 0.6918, Training Accuracy= 0.518\n",
      "Epoch: 200, Loss= 0.6917, Training Accuracy= 0.523\n",
      "Epoch: 210, Loss= 0.6915, Training Accuracy= 0.524\n",
      "Epoch: 220, Loss= 0.6913, Training Accuracy= 0.524\n",
      "Epoch: 230, Loss= 0.6911, Training Accuracy= 0.522\n",
      "Epoch: 240, Loss= 0.6910, Training Accuracy= 0.522\n",
      "Epoch: 250, Loss= 0.6908, Training Accuracy= 0.526\n",
      "Epoch: 260, Loss= 0.6907, Training Accuracy= 0.526\n",
      "Epoch: 270, Loss= 0.6905, Training Accuracy= 0.528\n",
      "Epoch: 280, Loss= 0.6903, Training Accuracy= 0.529\n",
      "Epoch: 290, Loss= 0.6901, Training Accuracy= 0.531\n",
      "Epoch: 300, Loss= 0.6896, Training Accuracy= 0.536\n",
      "Epoch: 310, Loss= 0.6891, Training Accuracy= 0.538\n",
      "Epoch: 320, Loss= 0.6887, Training Accuracy= 0.541\n",
      "Epoch: 330, Loss= 0.6894, Training Accuracy= 0.537\n",
      "Epoch: 340, Loss= 0.6887, Training Accuracy= 0.541\n",
      "Epoch: 350, Loss= 0.6870, Training Accuracy= 0.547\n",
      "Epoch: 360, Loss= 0.6871, Training Accuracy= 0.545\n",
      "Epoch: 370, Loss= 0.6871, Training Accuracy= 0.543\n",
      "Epoch: 380, Loss= 0.6863, Training Accuracy= 0.546\n",
      "Epoch: 390, Loss= 0.6867, Training Accuracy= 0.544\n",
      "Epoch: 400, Loss= 0.6864, Training Accuracy= 0.545\n",
      "Epoch: 410, Loss= 0.6856, Training Accuracy= 0.547\n",
      "Epoch: 420, Loss= 0.6826, Training Accuracy= 0.554\n",
      "Epoch: 430, Loss= 0.6809, Training Accuracy= 0.558\n",
      "Epoch: 440, Loss= 0.6805, Training Accuracy= 0.558\n",
      "Epoch: 450, Loss= 0.6815, Training Accuracy= 0.559\n",
      "Epoch: 460, Loss= 0.6891, Training Accuracy= 0.536\n",
      "Epoch: 470, Loss= 0.6874, Training Accuracy= 0.536\n",
      "Epoch: 480, Loss= 0.6857, Training Accuracy= 0.545\n",
      "Epoch: 490, Loss= 0.6838, Training Accuracy= 0.553\n",
      "Epoch: 500, Loss= 0.6815, Training Accuracy= 0.557\n",
      "Epoch: 510, Loss= 0.6814, Training Accuracy= 0.556\n",
      "Epoch: 520, Loss= 0.6807, Training Accuracy= 0.562\n",
      "Epoch: 530, Loss= 0.6782, Training Accuracy= 0.567\n",
      "Epoch: 540, Loss= 0.6817, Training Accuracy= 0.559\n",
      "Epoch: 550, Loss= 0.6821, Training Accuracy= 0.561\n",
      "Epoch: 560, Loss= 0.6745, Training Accuracy= 0.568\n",
      "Epoch: 570, Loss= 0.6767, Training Accuracy= 0.565\n",
      "Epoch: 580, Loss= 0.6768, Training Accuracy= 0.568\n",
      "Epoch: 590, Loss= 0.6802, Training Accuracy= 0.564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6867, Training Accuracy= 0.556\n",
      "Epoch: 610, Loss= 0.6725, Training Accuracy= 0.579\n",
      "Epoch: 620, Loss= 0.6777, Training Accuracy= 0.569\n",
      "Epoch: 630, Loss= 0.6691, Training Accuracy= 0.588\n",
      "Epoch: 640, Loss= 0.6695, Training Accuracy= 0.583\n",
      "Epoch: 650, Loss= 0.7018, Training Accuracy= 0.495\n",
      "Epoch: 660, Loss= 0.6935, Training Accuracy= 0.523\n",
      "Epoch: 670, Loss= 0.6889, Training Accuracy= 0.544\n",
      "Epoch: 680, Loss= 0.6871, Training Accuracy= 0.547\n",
      "Epoch: 690, Loss= 0.6836, Training Accuracy= 0.557\n",
      "Epoch: 700, Loss= 0.6894, Training Accuracy= 0.546\n",
      "Epoch: 710, Loss= 0.6834, Training Accuracy= 0.557\n",
      "Epoch: 720, Loss= 0.6875, Training Accuracy= 0.543\n",
      "Epoch: 730, Loss= 0.6812, Training Accuracy= 0.563\n",
      "Epoch: 740, Loss= 0.6732, Training Accuracy= 0.578\n",
      "Epoch: 750, Loss= 0.6705, Training Accuracy= 0.578\n",
      "Epoch: 760, Loss= 0.6802, Training Accuracy= 0.564\n",
      "Epoch: 770, Loss= 0.6738, Training Accuracy= 0.577\n",
      "Epoch: 780, Loss= 0.6695, Training Accuracy= 0.587\n",
      "Epoch: 790, Loss= 0.6605, Training Accuracy= 0.597\n",
      "Epoch: 800, Loss= 0.6739, Training Accuracy= 0.576\n",
      "Epoch: 810, Loss= 0.6797, Training Accuracy= 0.572\n",
      "Epoch: 820, Loss= 0.6951, Training Accuracy= 0.538\n",
      "Epoch: 830, Loss= 0.6934, Training Accuracy= 0.538\n",
      "Epoch: 840, Loss= 0.6804, Training Accuracy= 0.567\n",
      "Epoch: 850, Loss= 0.6647, Training Accuracy= 0.593\n",
      "Epoch: 860, Loss= 0.6934, Training Accuracy= 0.548\n",
      "Epoch: 870, Loss= 0.6647, Training Accuracy= 0.591\n",
      "Epoch: 880, Loss= 0.6633, Training Accuracy= 0.599\n",
      "Epoch: 890, Loss= 0.6501, Training Accuracy= 0.617\n",
      "Epoch: 900, Loss= 0.6416, Training Accuracy= 0.626\n",
      "Epoch: 910, Loss= 0.6932, Training Accuracy= 0.548\n",
      "Epoch: 920, Loss= 0.6543, Training Accuracy= 0.614\n",
      "Epoch: 930, Loss= 0.6358, Training Accuracy= 0.634\n",
      "Epoch: 940, Loss= 0.6436, Training Accuracy= 0.627\n",
      "Epoch: 950, Loss= 0.6498, Training Accuracy= 0.617\n",
      "Epoch: 960, Loss= 0.6381, Training Accuracy= 0.629\n",
      "Epoch: 970, Loss= 0.6480, Training Accuracy= 0.626\n",
      "Epoch: 980, Loss= 0.6130, Training Accuracy= 0.657\n",
      "Epoch: 990, Loss= 0.6801, Training Accuracy= 0.568\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5012\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.7370, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 0.7165, Training Accuracy= 0.496\n",
      "Epoch: 20, Loss= 0.7114, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.7087, Training Accuracy= 0.496\n",
      "Epoch: 40, Loss= 0.7070, Training Accuracy= 0.496\n",
      "Epoch: 50, Loss= 0.7058, Training Accuracy= 0.496\n",
      "Epoch: 60, Loss= 0.7049, Training Accuracy= 0.496\n",
      "Epoch: 70, Loss= 0.7041, Training Accuracy= 0.496\n",
      "Epoch: 80, Loss= 0.7035, Training Accuracy= 0.496\n",
      "Epoch: 90, Loss= 0.7030, Training Accuracy= 0.496\n",
      "Epoch: 100, Loss= 0.7026, Training Accuracy= 0.496\n",
      "Epoch: 110, Loss= 0.7022, Training Accuracy= 0.496\n",
      "Epoch: 120, Loss= 0.7019, Training Accuracy= 0.496\n",
      "Epoch: 130, Loss= 0.7016, Training Accuracy= 0.496\n",
      "Epoch: 140, Loss= 0.7013, Training Accuracy= 0.496\n",
      "Epoch: 150, Loss= 0.7011, Training Accuracy= 0.496\n",
      "Epoch: 160, Loss= 0.7009, Training Accuracy= 0.496\n",
      "Epoch: 170, Loss= 0.7007, Training Accuracy= 0.496\n",
      "Epoch: 180, Loss= 0.7005, Training Accuracy= 0.496\n",
      "Epoch: 190, Loss= 0.7004, Training Accuracy= 0.496\n",
      "Epoch: 200, Loss= 0.7003, Training Accuracy= 0.496\n",
      "Epoch: 210, Loss= 0.7001, Training Accuracy= 0.496\n",
      "Epoch: 220, Loss= 0.7000, Training Accuracy= 0.496\n",
      "Epoch: 230, Loss= 0.6999, Training Accuracy= 0.496\n",
      "Epoch: 240, Loss= 0.6998, Training Accuracy= 0.496\n",
      "Epoch: 250, Loss= 0.6997, Training Accuracy= 0.496\n",
      "Epoch: 260, Loss= 0.6996, Training Accuracy= 0.496\n",
      "Epoch: 270, Loss= 0.6995, Training Accuracy= 0.496\n",
      "Epoch: 280, Loss= 0.6994, Training Accuracy= 0.496\n",
      "Epoch: 290, Loss= 0.6994, Training Accuracy= 0.496\n",
      "Epoch: 300, Loss= 0.6993, Training Accuracy= 0.496\n",
      "Epoch: 310, Loss= 0.6992, Training Accuracy= 0.496\n",
      "Epoch: 320, Loss= 0.6992, Training Accuracy= 0.496\n",
      "Epoch: 330, Loss= 0.6991, Training Accuracy= 0.496\n",
      "Epoch: 340, Loss= 0.6991, Training Accuracy= 0.496\n",
      "Epoch: 350, Loss= 0.6990, Training Accuracy= 0.496\n",
      "Epoch: 360, Loss= 0.6990, Training Accuracy= 0.496\n",
      "Epoch: 370, Loss= 0.6989, Training Accuracy= 0.496\n",
      "Epoch: 380, Loss= 0.6989, Training Accuracy= 0.496\n",
      "Epoch: 390, Loss= 0.6988, Training Accuracy= 0.496\n",
      "Epoch: 400, Loss= 0.6988, Training Accuracy= 0.496\n",
      "Epoch: 410, Loss= 0.6987, Training Accuracy= 0.496\n",
      "Epoch: 420, Loss= 0.6987, Training Accuracy= 0.496\n",
      "Epoch: 430, Loss= 0.6986, Training Accuracy= 0.496\n",
      "Epoch: 440, Loss= 0.6986, Training Accuracy= 0.496\n",
      "Epoch: 450, Loss= 0.6985, Training Accuracy= 0.496\n",
      "Epoch: 460, Loss= 0.6985, Training Accuracy= 0.496\n",
      "Epoch: 470, Loss= 0.6985, Training Accuracy= 0.496\n",
      "Epoch: 480, Loss= 0.6984, Training Accuracy= 0.496\n",
      "Epoch: 490, Loss= 0.6984, Training Accuracy= 0.496\n",
      "Epoch: 500, Loss= 0.6984, Training Accuracy= 0.496\n",
      "Epoch: 510, Loss= 0.6983, Training Accuracy= 0.496\n",
      "Epoch: 520, Loss= 0.6983, Training Accuracy= 0.496\n",
      "Epoch: 530, Loss= 0.6983, Training Accuracy= 0.496\n",
      "Epoch: 540, Loss= 0.6982, Training Accuracy= 0.496\n",
      "Epoch: 550, Loss= 0.6982, Training Accuracy= 0.496\n",
      "Epoch: 560, Loss= 0.6982, Training Accuracy= 0.496\n",
      "Epoch: 570, Loss= 0.6982, Training Accuracy= 0.496\n",
      "Epoch: 580, Loss= 0.6981, Training Accuracy= 0.496\n",
      "Epoch: 590, Loss= 0.6981, Training Accuracy= 0.496\n",
      "Epoch: 600, Loss= 0.6982, Training Accuracy= 0.496\n",
      "Epoch: 610, Loss= 0.6983, Training Accuracy= 0.496\n",
      "Epoch: 620, Loss= 0.6983, Training Accuracy= 0.497\n",
      "Epoch: 630, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 640, Loss= 0.6985, Training Accuracy= 0.499\n",
      "Epoch: 650, Loss= 0.6988, Training Accuracy= 0.500\n",
      "Epoch: 660, Loss= 0.6989, Training Accuracy= 0.501\n",
      "Epoch: 670, Loss= 0.6989, Training Accuracy= 0.501\n",
      "Epoch: 680, Loss= 0.6991, Training Accuracy= 0.502\n",
      "Epoch: 690, Loss= 0.6992, Training Accuracy= 0.501\n",
      "Epoch: 700, Loss= 0.6994, Training Accuracy= 0.501\n",
      "Epoch: 710, Loss= 0.6994, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.6994, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.6997, Training Accuracy= 0.501\n",
      "Epoch: 740, Loss= 0.6995, Training Accuracy= 0.502\n",
      "Epoch: 750, Loss= 0.6992, Training Accuracy= 0.504\n",
      "Epoch: 760, Loss= 0.6990, Training Accuracy= 0.504\n",
      "Epoch: 770, Loss= 0.6986, Training Accuracy= 0.507\n",
      "Epoch: 780, Loss= 0.6986, Training Accuracy= 0.509\n",
      "Epoch: 790, Loss= 0.6989, Training Accuracy= 0.508\n",
      "Epoch: 800, Loss= 0.6986, Training Accuracy= 0.507\n",
      "Epoch: 810, Loss= 0.6982, Training Accuracy= 0.509\n",
      "Epoch: 820, Loss= 0.6987, Training Accuracy= 0.506\n",
      "Epoch: 830, Loss= 0.6993, Training Accuracy= 0.506\n",
      "Epoch: 840, Loss= 0.6988, Training Accuracy= 0.508\n",
      "Epoch: 850, Loss= 0.6992, Training Accuracy= 0.507\n",
      "Epoch: 860, Loss= 0.6981, Training Accuracy= 0.511\n",
      "Epoch: 870, Loss= 0.6983, Training Accuracy= 0.511\n",
      "Epoch: 880, Loss= 0.6989, Training Accuracy= 0.508\n",
      "Epoch: 890, Loss= 0.6978, Training Accuracy= 0.511\n",
      "Epoch: 900, Loss= 0.6981, Training Accuracy= 0.512\n",
      "Epoch: 910, Loss= 0.6977, Training Accuracy= 0.515\n",
      "Epoch: 920, Loss= 0.6969, Training Accuracy= 0.513\n",
      "Epoch: 930, Loss= 0.6973, Training Accuracy= 0.506\n",
      "Epoch: 940, Loss= 0.6981, Training Accuracy= 0.515\n",
      "Epoch: 950, Loss= 0.6990, Training Accuracy= 0.516\n",
      "Epoch: 960, Loss= 0.6993, Training Accuracy= 0.515\n",
      "Epoch: 970, Loss= 0.6965, Training Accuracy= 0.524\n",
      "Epoch: 980, Loss= 0.6972, Training Accuracy= 0.522\n",
      "Epoch: 990, Loss= 0.6961, Training Accuracy= 0.520\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4946\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.7500, Training Accuracy= 0.507\n",
      "Epoch: 10, Loss= 0.7010, Training Accuracy= 0.507\n",
      "Epoch: 20, Loss= 0.6968, Training Accuracy= 0.507\n",
      "Epoch: 30, Loss= 0.6953, Training Accuracy= 0.507\n",
      "Epoch: 40, Loss= 0.6946, Training Accuracy= 0.507\n",
      "Epoch: 50, Loss= 0.6942, Training Accuracy= 0.507\n",
      "Epoch: 60, Loss= 0.6940, Training Accuracy= 0.507\n",
      "Epoch: 70, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 80, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 100, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 110, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 120, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 140, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 150, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 160, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 210, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 220, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 230, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 240, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 250, Loss= 0.6931, Training Accuracy= 0.513\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 270, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 280, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 290, Loss= 0.6921, Training Accuracy= 0.517\n",
      "Epoch: 300, Loss= 0.6921, Training Accuracy= 0.515\n",
      "Epoch: 310, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 320, Loss= 0.6946, Training Accuracy= 0.512\n",
      "Epoch: 330, Loss= 0.6913, Training Accuracy= 0.518\n",
      "Epoch: 340, Loss= 0.6916, Training Accuracy= 0.513\n",
      "Epoch: 350, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 360, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 370, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 380, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 390, Loss= 0.6928, Training Accuracy= 0.516\n",
      "Epoch: 400, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 410, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 420, Loss= 0.6929, Training Accuracy= 0.517\n",
      "Epoch: 430, Loss= 0.6920, Training Accuracy= 0.516\n",
      "Epoch: 440, Loss= 0.6912, Training Accuracy= 0.519\n",
      "Epoch: 450, Loss= 0.6909, Training Accuracy= 0.520\n",
      "Epoch: 460, Loss= 0.6911, Training Accuracy= 0.523\n",
      "Epoch: 470, Loss= 0.6911, Training Accuracy= 0.524\n",
      "Epoch: 480, Loss= 0.6904, Training Accuracy= 0.528\n",
      "Epoch: 490, Loss= 0.6944, Training Accuracy= 0.519\n",
      "Epoch: 500, Loss= 0.6879, Training Accuracy= 0.542\n",
      "Epoch: 510, Loss= 0.6898, Training Accuracy= 0.533\n",
      "Epoch: 520, Loss= 0.6876, Training Accuracy= 0.540\n",
      "Epoch: 530, Loss= 0.6875, Training Accuracy= 0.540\n",
      "Epoch: 540, Loss= 0.6879, Training Accuracy= 0.543\n",
      "Epoch: 550, Loss= 0.6944, Training Accuracy= 0.520\n",
      "Epoch: 560, Loss= 0.6865, Training Accuracy= 0.542\n",
      "Epoch: 570, Loss= 0.6849, Training Accuracy= 0.549\n",
      "Epoch: 580, Loss= 0.6842, Training Accuracy= 0.548\n",
      "Epoch: 590, Loss= 0.6842, Training Accuracy= 0.549\n",
      "Epoch: 600, Loss= 0.6839, Training Accuracy= 0.552\n",
      "Epoch: 610, Loss= 0.6821, Training Accuracy= 0.560\n",
      "Epoch: 620, Loss= 0.6808, Training Accuracy= 0.558\n",
      "Epoch: 630, Loss= 0.6851, Training Accuracy= 0.548\n",
      "Epoch: 640, Loss= 0.6799, Training Accuracy= 0.563\n",
      "Epoch: 650, Loss= 0.6759, Training Accuracy= 0.571\n",
      "Epoch: 660, Loss= 0.6813, Training Accuracy= 0.560\n",
      "Epoch: 670, Loss= 0.6753, Training Accuracy= 0.576\n",
      "Epoch: 680, Loss= 0.6742, Training Accuracy= 0.570\n",
      "Epoch: 690, Loss= 0.6789, Training Accuracy= 0.564\n",
      "Epoch: 700, Loss= 0.6733, Training Accuracy= 0.580\n",
      "Epoch: 710, Loss= 0.6833, Training Accuracy= 0.559\n",
      "Epoch: 720, Loss= 0.6732, Training Accuracy= 0.579\n",
      "Epoch: 730, Loss= 0.6990, Training Accuracy= 0.508\n",
      "Epoch: 740, Loss= 0.6944, Training Accuracy= 0.511\n",
      "Epoch: 750, Loss= 0.6916, Training Accuracy= 0.519\n",
      "Epoch: 760, Loss= 0.6908, Training Accuracy= 0.521\n",
      "Epoch: 770, Loss= 0.6901, Training Accuracy= 0.525\n",
      "Epoch: 780, Loss= 0.6890, Training Accuracy= 0.524\n",
      "Epoch: 790, Loss= 0.6889, Training Accuracy= 0.525\n",
      "Epoch: 800, Loss= 0.6941, Training Accuracy= 0.509\n",
      "Epoch: 810, Loss= 0.6900, Training Accuracy= 0.519\n",
      "Epoch: 820, Loss= 0.6878, Training Accuracy= 0.529\n",
      "Epoch: 830, Loss= 0.6875, Training Accuracy= 0.536\n",
      "Epoch: 840, Loss= 0.6876, Training Accuracy= 0.536\n",
      "Epoch: 850, Loss= 0.6841, Training Accuracy= 0.546\n",
      "Epoch: 860, Loss= 0.6902, Training Accuracy= 0.522\n",
      "Epoch: 870, Loss= 0.6890, Training Accuracy= 0.527\n",
      "Epoch: 880, Loss= 0.6849, Training Accuracy= 0.542\n",
      "Epoch: 890, Loss= 0.6891, Training Accuracy= 0.529\n",
      "Epoch: 900, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 910, Loss= 0.6918, Training Accuracy= 0.523\n",
      "Epoch: 920, Loss= 0.6903, Training Accuracy= 0.525\n",
      "Epoch: 930, Loss= 0.6896, Training Accuracy= 0.533\n",
      "Epoch: 940, Loss= 0.6886, Training Accuracy= 0.534\n",
      "Epoch: 950, Loss= 0.6883, Training Accuracy= 0.540\n",
      "Epoch: 960, Loss= 0.6880, Training Accuracy= 0.544\n",
      "Epoch: 970, Loss= 0.6876, Training Accuracy= 0.542\n",
      "Epoch: 980, Loss= 0.6870, Training Accuracy= 0.543\n",
      "Epoch: 990, Loss= 0.6870, Training Accuracy= 0.539\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4922\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.7784, Training Accuracy= 0.511\n",
      "Epoch: 10, Loss= 0.7244, Training Accuracy= 0.511\n",
      "Epoch: 20, Loss= 0.7151, Training Accuracy= 0.511\n",
      "Epoch: 30, Loss= 0.7107, Training Accuracy= 0.511\n",
      "Epoch: 40, Loss= 0.7079, Training Accuracy= 0.511\n",
      "Epoch: 50, Loss= 0.7060, Training Accuracy= 0.511\n",
      "Epoch: 60, Loss= 0.7046, Training Accuracy= 0.511\n",
      "Epoch: 70, Loss= 0.7035, Training Accuracy= 0.511\n",
      "Epoch: 80, Loss= 0.7026, Training Accuracy= 0.511\n",
      "Epoch: 90, Loss= 0.7018, Training Accuracy= 0.511\n",
      "Epoch: 100, Loss= 0.7011, Training Accuracy= 0.511\n",
      "Epoch: 110, Loss= 0.7006, Training Accuracy= 0.511\n",
      "Epoch: 120, Loss= 0.7001, Training Accuracy= 0.511\n",
      "Epoch: 130, Loss= 0.6996, Training Accuracy= 0.511\n",
      "Epoch: 140, Loss= 0.6992, Training Accuracy= 0.511\n",
      "Epoch: 150, Loss= 0.6989, Training Accuracy= 0.511\n",
      "Epoch: 160, Loss= 0.6986, Training Accuracy= 0.511\n",
      "Epoch: 170, Loss= 0.6983, Training Accuracy= 0.511\n",
      "Epoch: 180, Loss= 0.6980, Training Accuracy= 0.511\n",
      "Epoch: 190, Loss= 0.6978, Training Accuracy= 0.511\n",
      "Epoch: 200, Loss= 0.6975, Training Accuracy= 0.511\n",
      "Epoch: 210, Loss= 0.6973, Training Accuracy= 0.511\n",
      "Epoch: 220, Loss= 0.6971, Training Accuracy= 0.511\n",
      "Epoch: 230, Loss= 0.6969, Training Accuracy= 0.511\n",
      "Epoch: 240, Loss= 0.6967, Training Accuracy= 0.511\n",
      "Epoch: 250, Loss= 0.6966, Training Accuracy= 0.511\n",
      "Epoch: 260, Loss= 0.6966, Training Accuracy= 0.511\n",
      "Epoch: 270, Loss= 0.6965, Training Accuracy= 0.511\n",
      "Epoch: 280, Loss= 0.6964, Training Accuracy= 0.511\n",
      "Epoch: 290, Loss= 0.6963, Training Accuracy= 0.511\n",
      "Epoch: 300, Loss= 0.6961, Training Accuracy= 0.511\n",
      "Epoch: 310, Loss= 0.6960, Training Accuracy= 0.512\n",
      "Epoch: 320, Loss= 0.6960, Training Accuracy= 0.512\n",
      "Epoch: 330, Loss= 0.6960, Training Accuracy= 0.512\n",
      "Epoch: 340, Loss= 0.6960, Training Accuracy= 0.511\n",
      "Epoch: 350, Loss= 0.6960, Training Accuracy= 0.512\n",
      "Epoch: 360, Loss= 0.6959, Training Accuracy= 0.511\n",
      "Epoch: 370, Loss= 0.6958, Training Accuracy= 0.510\n",
      "Epoch: 380, Loss= 0.6955, Training Accuracy= 0.511\n",
      "Epoch: 390, Loss= 0.6951, Training Accuracy= 0.511\n",
      "Epoch: 400, Loss= 0.6949, Training Accuracy= 0.512\n",
      "Epoch: 410, Loss= 0.6947, Training Accuracy= 0.517\n",
      "Epoch: 420, Loss= 0.6946, Training Accuracy= 0.522\n",
      "Epoch: 430, Loss= 0.6963, Training Accuracy= 0.519\n",
      "Epoch: 440, Loss= 0.6946, Training Accuracy= 0.517\n",
      "Epoch: 450, Loss= 0.6961, Training Accuracy= 0.516\n",
      "Epoch: 460, Loss= 0.6956, Training Accuracy= 0.515\n",
      "Epoch: 470, Loss= 0.6974, Training Accuracy= 0.518\n",
      "Epoch: 480, Loss= 0.6972, Training Accuracy= 0.513\n",
      "Epoch: 490, Loss= 0.6972, Training Accuracy= 0.514\n",
      "Epoch: 500, Loss= 0.7157, Training Accuracy= 0.512\n",
      "Epoch: 510, Loss= 0.6941, Training Accuracy= 0.516\n",
      "Epoch: 520, Loss= 0.6934, Training Accuracy= 0.517\n",
      "Epoch: 530, Loss= 0.7099, Training Accuracy= 0.512\n",
      "Epoch: 540, Loss= 0.6999, Training Accuracy= 0.512\n",
      "Epoch: 550, Loss= 0.6973, Training Accuracy= 0.511\n",
      "Epoch: 560, Loss= 0.6967, Training Accuracy= 0.512\n",
      "Epoch: 570, Loss= 0.6966, Training Accuracy= 0.513\n",
      "Epoch: 580, Loss= 0.6969, Training Accuracy= 0.513\n",
      "Epoch: 590, Loss= 0.6967, Training Accuracy= 0.513\n",
      "Epoch: 600, Loss= 0.6959, Training Accuracy= 0.517\n",
      "Epoch: 610, Loss= 0.6947, Training Accuracy= 0.522\n",
      "Epoch: 620, Loss= 0.6947, Training Accuracy= 0.523\n",
      "Epoch: 630, Loss= 0.6953, Training Accuracy= 0.527\n",
      "Epoch: 640, Loss= 0.6941, Training Accuracy= 0.528\n",
      "Epoch: 650, Loss= 0.6941, Training Accuracy= 0.529\n",
      "Epoch: 660, Loss= 0.6929, Training Accuracy= 0.532\n",
      "Epoch: 670, Loss= 0.6910, Training Accuracy= 0.534\n",
      "Epoch: 680, Loss= 0.7208, Training Accuracy= 0.512\n",
      "Epoch: 690, Loss= 0.7127, Training Accuracy= 0.512\n",
      "Epoch: 700, Loss= 0.7094, Training Accuracy= 0.512\n",
      "Epoch: 710, Loss= 0.7077, Training Accuracy= 0.511\n",
      "Epoch: 720, Loss= 0.7061, Training Accuracy= 0.512\n",
      "Epoch: 730, Loss= 0.7054, Training Accuracy= 0.512\n",
      "Epoch: 740, Loss= 0.7044, Training Accuracy= 0.513\n",
      "Epoch: 750, Loss= 0.7037, Training Accuracy= 0.515\n",
      "Epoch: 760, Loss= 0.7029, Training Accuracy= 0.515\n",
      "Epoch: 770, Loss= 0.7023, Training Accuracy= 0.517\n",
      "Epoch: 780, Loss= 0.7010, Training Accuracy= 0.518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.7014, Training Accuracy= 0.518\n",
      "Epoch: 800, Loss= 0.7000, Training Accuracy= 0.520\n",
      "Epoch: 810, Loss= 0.6998, Training Accuracy= 0.519\n",
      "Epoch: 820, Loss= 0.6965, Training Accuracy= 0.528\n",
      "Epoch: 830, Loss= 0.6954, Training Accuracy= 0.531\n",
      "Epoch: 840, Loss= 0.6933, Training Accuracy= 0.531\n",
      "Epoch: 850, Loss= 0.6933, Training Accuracy= 0.535\n",
      "Epoch: 860, Loss= 0.6995, Training Accuracy= 0.529\n",
      "Epoch: 870, Loss= 0.6967, Training Accuracy= 0.527\n",
      "Epoch: 880, Loss= 0.6891, Training Accuracy= 0.541\n",
      "Epoch: 890, Loss= 0.6921, Training Accuracy= 0.541\n",
      "Epoch: 900, Loss= 0.6898, Training Accuracy= 0.545\n",
      "Epoch: 910, Loss= 0.6928, Training Accuracy= 0.543\n",
      "Epoch: 920, Loss= 0.6929, Training Accuracy= 0.541\n",
      "Epoch: 930, Loss= 0.7127, Training Accuracy= 0.525\n",
      "Epoch: 940, Loss= 0.6838, Training Accuracy= 0.558\n",
      "Epoch: 950, Loss= 0.6879, Training Accuracy= 0.547\n",
      "Epoch: 960, Loss= 0.6984, Training Accuracy= 0.533\n",
      "Epoch: 970, Loss= 0.6846, Training Accuracy= 0.558\n",
      "Epoch: 980, Loss= 0.6960, Training Accuracy= 0.541\n",
      "Epoch: 990, Loss= 0.6795, Training Accuracy= 0.560\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4961\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.7299, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.7116, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.7072, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.7050, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.7036, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.7026, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.7018, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.7012, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.7007, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.7003, Training Accuracy= 0.497\n",
      "Epoch: 100, Loss= 0.6999, Training Accuracy= 0.497\n",
      "Epoch: 110, Loss= 0.6996, Training Accuracy= 0.497\n",
      "Epoch: 120, Loss= 0.6994, Training Accuracy= 0.497\n",
      "Epoch: 130, Loss= 0.6991, Training Accuracy= 0.497\n",
      "Epoch: 140, Loss= 0.6989, Training Accuracy= 0.497\n",
      "Epoch: 150, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 160, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 170, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 180, Loss= 0.6983, Training Accuracy= 0.497\n",
      "Epoch: 190, Loss= 0.6982, Training Accuracy= 0.497\n",
      "Epoch: 200, Loss= 0.6981, Training Accuracy= 0.497\n",
      "Epoch: 210, Loss= 0.6979, Training Accuracy= 0.497\n",
      "Epoch: 220, Loss= 0.6978, Training Accuracy= 0.497\n",
      "Epoch: 230, Loss= 0.6978, Training Accuracy= 0.497\n",
      "Epoch: 240, Loss= 0.6977, Training Accuracy= 0.497\n",
      "Epoch: 250, Loss= 0.6976, Training Accuracy= 0.497\n",
      "Epoch: 260, Loss= 0.6975, Training Accuracy= 0.497\n",
      "Epoch: 270, Loss= 0.6974, Training Accuracy= 0.497\n",
      "Epoch: 280, Loss= 0.6974, Training Accuracy= 0.497\n",
      "Epoch: 290, Loss= 0.6973, Training Accuracy= 0.497\n",
      "Epoch: 300, Loss= 0.6972, Training Accuracy= 0.497\n",
      "Epoch: 310, Loss= 0.6972, Training Accuracy= 0.497\n",
      "Epoch: 320, Loss= 0.6971, Training Accuracy= 0.497\n",
      "Epoch: 330, Loss= 0.6971, Training Accuracy= 0.497\n",
      "Epoch: 340, Loss= 0.6970, Training Accuracy= 0.497\n",
      "Epoch: 350, Loss= 0.6970, Training Accuracy= 0.497\n",
      "Epoch: 360, Loss= 0.6969, Training Accuracy= 0.497\n",
      "Epoch: 370, Loss= 0.6969, Training Accuracy= 0.497\n",
      "Epoch: 380, Loss= 0.6969, Training Accuracy= 0.497\n",
      "Epoch: 390, Loss= 0.6968, Training Accuracy= 0.497\n",
      "Epoch: 400, Loss= 0.6968, Training Accuracy= 0.497\n",
      "Epoch: 410, Loss= 0.6967, Training Accuracy= 0.497\n",
      "Epoch: 420, Loss= 0.6967, Training Accuracy= 0.497\n",
      "Epoch: 430, Loss= 0.6967, Training Accuracy= 0.497\n",
      "Epoch: 440, Loss= 0.6966, Training Accuracy= 0.497\n",
      "Epoch: 450, Loss= 0.6966, Training Accuracy= 0.497\n",
      "Epoch: 460, Loss= 0.6966, Training Accuracy= 0.497\n",
      "Epoch: 470, Loss= 0.6966, Training Accuracy= 0.497\n",
      "Epoch: 480, Loss= 0.6965, Training Accuracy= 0.497\n",
      "Epoch: 490, Loss= 0.6965, Training Accuracy= 0.497\n",
      "Epoch: 500, Loss= 0.6965, Training Accuracy= 0.497\n",
      "Epoch: 510, Loss= 0.6964, Training Accuracy= 0.497\n",
      "Epoch: 520, Loss= 0.6964, Training Accuracy= 0.497\n",
      "Epoch: 530, Loss= 0.6964, Training Accuracy= 0.497\n",
      "Epoch: 540, Loss= 0.6964, Training Accuracy= 0.497\n",
      "Epoch: 550, Loss= 0.6963, Training Accuracy= 0.497\n",
      "Epoch: 560, Loss= 0.6963, Training Accuracy= 0.497\n",
      "Epoch: 570, Loss= 0.6963, Training Accuracy= 0.497\n",
      "Epoch: 580, Loss= 0.6963, Training Accuracy= 0.497\n",
      "Epoch: 590, Loss= 0.6963, Training Accuracy= 0.497\n",
      "Epoch: 600, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 610, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 620, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 630, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 640, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 650, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 660, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 670, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 680, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 690, Loss= 0.6960, Training Accuracy= 0.497\n",
      "Epoch: 700, Loss= 0.6960, Training Accuracy= 0.497\n",
      "Epoch: 710, Loss= 0.6960, Training Accuracy= 0.497\n",
      "Epoch: 720, Loss= 0.6960, Training Accuracy= 0.497\n",
      "Epoch: 730, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 740, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 750, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 760, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 770, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 780, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 790, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 800, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 810, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 820, Loss= 0.6957, Training Accuracy= 0.496\n",
      "Epoch: 830, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 840, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 850, Loss= 0.6956, Training Accuracy= 0.497\n",
      "Epoch: 860, Loss= 0.6956, Training Accuracy= 0.497\n",
      "Epoch: 870, Loss= 0.6956, Training Accuracy= 0.498\n",
      "Epoch: 880, Loss= 0.6955, Training Accuracy= 0.498\n",
      "Epoch: 890, Loss= 0.6955, Training Accuracy= 0.499\n",
      "Epoch: 900, Loss= 0.6954, Training Accuracy= 0.500\n",
      "Epoch: 910, Loss= 0.6954, Training Accuracy= 0.499\n",
      "Epoch: 920, Loss= 0.6953, Training Accuracy= 0.501\n",
      "Epoch: 930, Loss= 0.6952, Training Accuracy= 0.501\n",
      "Epoch: 940, Loss= 0.6951, Training Accuracy= 0.502\n",
      "Epoch: 950, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 960, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 970, Loss= 0.6948, Training Accuracy= 0.506\n",
      "Epoch: 980, Loss= 0.6947, Training Accuracy= 0.507\n",
      "Epoch: 990, Loss= 0.6945, Training Accuracy= 0.509\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4991\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.7237, Training Accuracy= 0.507\n",
      "Epoch: 10, Loss= 0.7103, Training Accuracy= 0.507\n",
      "Epoch: 20, Loss= 0.7066, Training Accuracy= 0.507\n",
      "Epoch: 30, Loss= 0.7045, Training Accuracy= 0.507\n",
      "Epoch: 40, Loss= 0.7030, Training Accuracy= 0.507\n",
      "Epoch: 50, Loss= 0.7020, Training Accuracy= 0.507\n",
      "Epoch: 60, Loss= 0.7011, Training Accuracy= 0.507\n",
      "Epoch: 70, Loss= 0.7005, Training Accuracy= 0.507\n",
      "Epoch: 80, Loss= 0.6999, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6995, Training Accuracy= 0.507\n",
      "Epoch: 100, Loss= 0.6991, Training Accuracy= 0.507\n",
      "Epoch: 110, Loss= 0.6987, Training Accuracy= 0.507\n",
      "Epoch: 120, Loss= 0.6984, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6982, Training Accuracy= 0.507\n",
      "Epoch: 140, Loss= 0.6979, Training Accuracy= 0.507\n",
      "Epoch: 150, Loss= 0.6977, Training Accuracy= 0.507\n",
      "Epoch: 160, Loss= 0.6975, Training Accuracy= 0.507\n",
      "Epoch: 170, Loss= 0.6974, Training Accuracy= 0.506\n",
      "Epoch: 180, Loss= 0.6972, Training Accuracy= 0.507\n",
      "Epoch: 190, Loss= 0.6971, Training Accuracy= 0.507\n",
      "Epoch: 200, Loss= 0.6969, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.6968, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6967, Training Accuracy= 0.507\n",
      "Epoch: 230, Loss= 0.6966, Training Accuracy= 0.507\n",
      "Epoch: 240, Loss= 0.6965, Training Accuracy= 0.507\n",
      "Epoch: 250, Loss= 0.6964, Training Accuracy= 0.506\n",
      "Epoch: 260, Loss= 0.6963, Training Accuracy= 0.507\n",
      "Epoch: 270, Loss= 0.6963, Training Accuracy= 0.506\n",
      "Epoch: 280, Loss= 0.6962, Training Accuracy= 0.507\n",
      "Epoch: 290, Loss= 0.6961, Training Accuracy= 0.506\n",
      "Epoch: 300, Loss= 0.6960, Training Accuracy= 0.507\n",
      "Epoch: 310, Loss= 0.6960, Training Accuracy= 0.507\n",
      "Epoch: 320, Loss= 0.6959, Training Accuracy= 0.507\n",
      "Epoch: 330, Loss= 0.6959, Training Accuracy= 0.507\n",
      "Epoch: 340, Loss= 0.6958, Training Accuracy= 0.507\n",
      "Epoch: 350, Loss= 0.6958, Training Accuracy= 0.507\n",
      "Epoch: 360, Loss= 0.6957, Training Accuracy= 0.506\n",
      "Epoch: 370, Loss= 0.6957, Training Accuracy= 0.506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6956, Training Accuracy= 0.507\n",
      "Epoch: 390, Loss= 0.6956, Training Accuracy= 0.507\n",
      "Epoch: 400, Loss= 0.6955, Training Accuracy= 0.507\n",
      "Epoch: 410, Loss= 0.6955, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.6955, Training Accuracy= 0.507\n",
      "Epoch: 430, Loss= 0.6954, Training Accuracy= 0.508\n",
      "Epoch: 440, Loss= 0.6954, Training Accuracy= 0.508\n",
      "Epoch: 450, Loss= 0.6953, Training Accuracy= 0.508\n",
      "Epoch: 460, Loss= 0.6953, Training Accuracy= 0.508\n",
      "Epoch: 470, Loss= 0.6953, Training Accuracy= 0.508\n",
      "Epoch: 480, Loss= 0.6952, Training Accuracy= 0.508\n",
      "Epoch: 490, Loss= 0.6952, Training Accuracy= 0.508\n",
      "Epoch: 500, Loss= 0.6952, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.6951, Training Accuracy= 0.509\n",
      "Epoch: 520, Loss= 0.6951, Training Accuracy= 0.509\n",
      "Epoch: 530, Loss= 0.6951, Training Accuracy= 0.509\n",
      "Epoch: 540, Loss= 0.6950, Training Accuracy= 0.509\n",
      "Epoch: 550, Loss= 0.6950, Training Accuracy= 0.510\n",
      "Epoch: 560, Loss= 0.6949, Training Accuracy= 0.510\n",
      "Epoch: 570, Loss= 0.6949, Training Accuracy= 0.510\n",
      "Epoch: 580, Loss= 0.6949, Training Accuracy= 0.510\n",
      "Epoch: 590, Loss= 0.6948, Training Accuracy= 0.509\n",
      "Epoch: 600, Loss= 0.6948, Training Accuracy= 0.509\n",
      "Epoch: 610, Loss= 0.6947, Training Accuracy= 0.509\n",
      "Epoch: 620, Loss= 0.6947, Training Accuracy= 0.510\n",
      "Epoch: 630, Loss= 0.6946, Training Accuracy= 0.510\n",
      "Epoch: 640, Loss= 0.6946, Training Accuracy= 0.510\n",
      "Epoch: 650, Loss= 0.6946, Training Accuracy= 0.510\n",
      "Epoch: 660, Loss= 0.6946, Training Accuracy= 0.509\n",
      "Epoch: 670, Loss= 0.6946, Training Accuracy= 0.509\n",
      "Epoch: 680, Loss= 0.6946, Training Accuracy= 0.509\n",
      "Epoch: 690, Loss= 0.6944, Training Accuracy= 0.508\n",
      "Epoch: 700, Loss= 0.6940, Training Accuracy= 0.511\n",
      "Epoch: 710, Loss= 0.6935, Training Accuracy= 0.515\n",
      "Epoch: 720, Loss= 0.6931, Training Accuracy= 0.518\n",
      "Epoch: 730, Loss= 0.6928, Training Accuracy= 0.521\n",
      "Epoch: 740, Loss= 0.6924, Training Accuracy= 0.524\n",
      "Epoch: 750, Loss= 0.6910, Training Accuracy= 0.528\n",
      "Epoch: 760, Loss= 0.6911, Training Accuracy= 0.529\n",
      "Epoch: 770, Loss= 0.6906, Training Accuracy= 0.533\n",
      "Epoch: 780, Loss= 0.6926, Training Accuracy= 0.530\n",
      "Epoch: 790, Loss= 0.6888, Training Accuracy= 0.539\n",
      "Epoch: 800, Loss= 0.6996, Training Accuracy= 0.517\n",
      "Epoch: 810, Loss= 0.6871, Training Accuracy= 0.544\n",
      "Epoch: 820, Loss= 0.6866, Training Accuracy= 0.543\n",
      "Epoch: 830, Loss= 0.6861, Training Accuracy= 0.548\n",
      "Epoch: 840, Loss= 0.6836, Training Accuracy= 0.558\n",
      "Epoch: 850, Loss= 0.6830, Training Accuracy= 0.558\n",
      "Epoch: 860, Loss= 0.7031, Training Accuracy= 0.510\n",
      "Epoch: 870, Loss= 0.6980, Training Accuracy= 0.516\n",
      "Epoch: 880, Loss= 0.6956, Training Accuracy= 0.520\n",
      "Epoch: 890, Loss= 0.6950, Training Accuracy= 0.525\n",
      "Epoch: 900, Loss= 0.6956, Training Accuracy= 0.522\n",
      "Epoch: 910, Loss= 0.6915, Training Accuracy= 0.529\n",
      "Epoch: 920, Loss= 0.6910, Training Accuracy= 0.533\n",
      "Epoch: 930, Loss= 0.6909, Training Accuracy= 0.537\n",
      "Epoch: 940, Loss= 0.6895, Training Accuracy= 0.539\n",
      "Epoch: 950, Loss= 0.6874, Training Accuracy= 0.548\n",
      "Epoch: 960, Loss= 0.6861, Training Accuracy= 0.548\n",
      "Epoch: 970, Loss= 0.6882, Training Accuracy= 0.547\n",
      "Epoch: 980, Loss= 0.6842, Training Accuracy= 0.554\n",
      "Epoch: 990, Loss= 0.6844, Training Accuracy= 0.556\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5078\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.7048, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.6984, Training Accuracy= 0.495\n",
      "Epoch: 20, Loss= 0.6971, Training Accuracy= 0.495\n",
      "Epoch: 30, Loss= 0.6965, Training Accuracy= 0.495\n",
      "Epoch: 40, Loss= 0.6961, Training Accuracy= 0.495\n",
      "Epoch: 50, Loss= 0.6959, Training Accuracy= 0.495\n",
      "Epoch: 60, Loss= 0.6957, Training Accuracy= 0.495\n",
      "Epoch: 70, Loss= 0.6955, Training Accuracy= 0.495\n",
      "Epoch: 80, Loss= 0.6954, Training Accuracy= 0.495\n",
      "Epoch: 90, Loss= 0.6953, Training Accuracy= 0.495\n",
      "Epoch: 100, Loss= 0.6952, Training Accuracy= 0.495\n",
      "Epoch: 110, Loss= 0.6951, Training Accuracy= 0.495\n",
      "Epoch: 120, Loss= 0.6951, Training Accuracy= 0.495\n",
      "Epoch: 130, Loss= 0.6950, Training Accuracy= 0.495\n",
      "Epoch: 140, Loss= 0.6950, Training Accuracy= 0.495\n",
      "Epoch: 150, Loss= 0.6949, Training Accuracy= 0.495\n",
      "Epoch: 160, Loss= 0.6948, Training Accuracy= 0.495\n",
      "Epoch: 170, Loss= 0.6948, Training Accuracy= 0.496\n",
      "Epoch: 180, Loss= 0.6947, Training Accuracy= 0.498\n",
      "Epoch: 190, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 200, Loss= 0.6946, Training Accuracy= 0.496\n",
      "Epoch: 210, Loss= 0.6946, Training Accuracy= 0.497\n",
      "Epoch: 220, Loss= 0.6945, Training Accuracy= 0.496\n",
      "Epoch: 230, Loss= 0.6944, Training Accuracy= 0.497\n",
      "Epoch: 240, Loss= 0.6944, Training Accuracy= 0.498\n",
      "Epoch: 250, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 260, Loss= 0.6943, Training Accuracy= 0.497\n",
      "Epoch: 270, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 280, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 290, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 300, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 310, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 320, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 330, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 340, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 350, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 360, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 370, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 380, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 390, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 400, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 410, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 430, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 440, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 450, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 460, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 470, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 490, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 500, Loss= 0.6936, Training Accuracy= 0.510\n",
      "Epoch: 510, Loss= 0.6944, Training Accuracy= 0.506\n",
      "Epoch: 520, Loss= 0.6960, Training Accuracy= 0.506\n",
      "Epoch: 530, Loss= 0.6967, Training Accuracy= 0.504\n",
      "Epoch: 540, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 550, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 560, Loss= 0.6925, Training Accuracy= 0.520\n",
      "Epoch: 570, Loss= 0.6918, Training Accuracy= 0.523\n",
      "Epoch: 580, Loss= 0.6918, Training Accuracy= 0.520\n",
      "Epoch: 590, Loss= 0.6927, Training Accuracy= 0.521\n",
      "Epoch: 600, Loss= 0.6912, Training Accuracy= 0.524\n",
      "Epoch: 610, Loss= 0.6921, Training Accuracy= 0.523\n",
      "Epoch: 620, Loss= 0.6904, Training Accuracy= 0.529\n",
      "Epoch: 630, Loss= 0.6946, Training Accuracy= 0.521\n",
      "Epoch: 640, Loss= 0.6913, Training Accuracy= 0.527\n",
      "Epoch: 650, Loss= 0.6946, Training Accuracy= 0.520\n",
      "Epoch: 660, Loss= 0.6871, Training Accuracy= 0.542\n",
      "Epoch: 670, Loss= 0.6914, Training Accuracy= 0.529\n",
      "Epoch: 680, Loss= 0.6879, Training Accuracy= 0.543\n",
      "Epoch: 690, Loss= 0.6887, Training Accuracy= 0.543\n",
      "Epoch: 700, Loss= 0.6914, Training Accuracy= 0.531\n",
      "Epoch: 710, Loss= 0.6946, Training Accuracy= 0.534\n",
      "Epoch: 720, Loss= 0.6972, Training Accuracy= 0.526\n",
      "Epoch: 730, Loss= 0.6913, Training Accuracy= 0.546\n",
      "Epoch: 740, Loss= 0.6862, Training Accuracy= 0.552\n",
      "Epoch: 750, Loss= 0.6947, Training Accuracy= 0.533\n",
      "Epoch: 760, Loss= 0.7058, Training Accuracy= 0.518\n",
      "Epoch: 770, Loss= 0.6740, Training Accuracy= 0.573\n",
      "Epoch: 780, Loss= 0.6934, Training Accuracy= 0.538\n",
      "Epoch: 790, Loss= 0.6762, Training Accuracy= 0.568\n",
      "Epoch: 800, Loss= 0.6883, Training Accuracy= 0.549\n",
      "Epoch: 810, Loss= 0.6725, Training Accuracy= 0.580\n",
      "Epoch: 820, Loss= 0.6942, Training Accuracy= 0.540\n",
      "Epoch: 830, Loss= 0.6866, Training Accuracy= 0.558\n",
      "Epoch: 840, Loss= 0.6955, Training Accuracy= 0.546\n",
      "Epoch: 850, Loss= 0.6815, Training Accuracy= 0.567\n",
      "Epoch: 860, Loss= 0.6619, Training Accuracy= 0.602\n",
      "Epoch: 870, Loss= 0.7016, Training Accuracy= 0.544\n",
      "Epoch: 880, Loss= 0.6617, Training Accuracy= 0.600\n",
      "Epoch: 890, Loss= 0.6718, Training Accuracy= 0.590\n",
      "Epoch: 900, Loss= 0.7047, Training Accuracy= 0.549\n",
      "Epoch: 910, Loss= 0.6609, Training Accuracy= 0.603\n",
      "Epoch: 920, Loss= 0.6667, Training Accuracy= 0.592\n",
      "Epoch: 930, Loss= 0.6547, Training Accuracy= 0.615\n",
      "Epoch: 940, Loss= 0.6857, Training Accuracy= 0.576\n",
      "Epoch: 950, Loss= 0.6677, Training Accuracy= 0.598\n",
      "Epoch: 960, Loss= 0.6450, Training Accuracy= 0.623\n",
      "Epoch: 970, Loss= 0.6516, Training Accuracy= 0.617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.6613, Training Accuracy= 0.609\n",
      "Epoch: 990, Loss= 0.6809, Training Accuracy= 0.596\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4991\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 10, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 20, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 30, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 40, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 50, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 60, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 70, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 80, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 100, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 110, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 120, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 140, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 150, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 160, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 170, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 180, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 190, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 230, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 240, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 250, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 260, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 270, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 290, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 300, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 310, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 320, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 330, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 340, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 350, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 360, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 370, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 380, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 390, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 400, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 410, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 420, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 430, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 470, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 500, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 510, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 520, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 530, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 540, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 550, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 560, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 570, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 580, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 590, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 600, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 610, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 620, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 630, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 640, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 650, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 660, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 670, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 680, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 690, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 700, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 710, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 720, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 730, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 740, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 750, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 760, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 770, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 780, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 790, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 800, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 810, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 820, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 830, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 840, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 850, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 860, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 870, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 880, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 890, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 900, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 910, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 920, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 930, Loss= 0.6922, Training Accuracy= 0.520\n",
      "Epoch: 940, Loss= 0.6922, Training Accuracy= 0.521\n",
      "Epoch: 950, Loss= 0.6921, Training Accuracy= 0.522\n",
      "Epoch: 960, Loss= 0.6921, Training Accuracy= 0.522\n",
      "Epoch: 970, Loss= 0.6921, Training Accuracy= 0.522\n",
      "Epoch: 980, Loss= 0.6921, Training Accuracy= 0.523\n",
      "Epoch: 990, Loss= 0.6920, Training Accuracy= 0.522\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4998\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7714, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.7216, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.7119, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.7073, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.7047, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.7029, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.7016, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.7007, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.6999, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.6993, Training Accuracy= 0.497\n",
      "Epoch: 100, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 110, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 120, Loss= 0.6981, Training Accuracy= 0.497\n",
      "Epoch: 130, Loss= 0.6978, Training Accuracy= 0.497\n",
      "Epoch: 140, Loss= 0.6976, Training Accuracy= 0.497\n",
      "Epoch: 150, Loss= 0.6973, Training Accuracy= 0.497\n",
      "Epoch: 160, Loss= 0.6971, Training Accuracy= 0.497\n",
      "Epoch: 170, Loss= 0.6970, Training Accuracy= 0.497\n",
      "Epoch: 180, Loss= 0.6968, Training Accuracy= 0.497\n",
      "Epoch: 190, Loss= 0.6967, Training Accuracy= 0.497\n",
      "Epoch: 200, Loss= 0.6965, Training Accuracy= 0.497\n",
      "Epoch: 210, Loss= 0.6964, Training Accuracy= 0.497\n",
      "Epoch: 220, Loss= 0.6963, Training Accuracy= 0.497\n",
      "Epoch: 230, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 240, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 250, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 260, Loss= 0.6960, Training Accuracy= 0.497\n",
      "Epoch: 270, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 280, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 290, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 300, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 310, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 320, Loss= 0.6956, Training Accuracy= 0.497\n",
      "Epoch: 330, Loss= 0.6956, Training Accuracy= 0.497\n",
      "Epoch: 340, Loss= 0.6956, Training Accuracy= 0.497\n",
      "Epoch: 350, Loss= 0.6955, Training Accuracy= 0.497\n",
      "Epoch: 360, Loss= 0.6955, Training Accuracy= 0.497\n",
      "Epoch: 370, Loss= 0.6954, Training Accuracy= 0.497\n",
      "Epoch: 380, Loss= 0.6954, Training Accuracy= 0.497\n",
      "Epoch: 390, Loss= 0.6954, Training Accuracy= 0.497\n",
      "Epoch: 400, Loss= 0.6953, Training Accuracy= 0.497\n",
      "Epoch: 410, Loss= 0.6953, Training Accuracy= 0.497\n",
      "Epoch: 420, Loss= 0.6953, Training Accuracy= 0.497\n",
      "Epoch: 430, Loss= 0.6953, Training Accuracy= 0.497\n",
      "Epoch: 440, Loss= 0.6952, Training Accuracy= 0.497\n",
      "Epoch: 450, Loss= 0.6952, Training Accuracy= 0.497\n",
      "Epoch: 460, Loss= 0.6952, Training Accuracy= 0.497\n",
      "Epoch: 470, Loss= 0.6952, Training Accuracy= 0.497\n",
      "Epoch: 480, Loss= 0.6952, Training Accuracy= 0.497\n",
      "Epoch: 490, Loss= 0.6951, Training Accuracy= 0.497\n",
      "Epoch: 500, Loss= 0.6951, Training Accuracy= 0.497\n",
      "Epoch: 510, Loss= 0.6951, Training Accuracy= 0.497\n",
      "Epoch: 520, Loss= 0.6951, Training Accuracy= 0.497\n",
      "Epoch: 530, Loss= 0.6951, Training Accuracy= 0.497\n",
      "Epoch: 540, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 550, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 560, Loss= 0.6950, Training Accuracy= 0.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 580, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 590, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 600, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 610, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 620, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 630, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 640, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 650, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 660, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 670, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 680, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 690, Loss= 0.6949, Training Accuracy= 0.497\n",
      "Epoch: 700, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 710, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 720, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 730, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 740, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 750, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 760, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 770, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 780, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 790, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 800, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 810, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 820, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 830, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 840, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 850, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 860, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 870, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 880, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 890, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 900, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 910, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 920, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 930, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 940, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 950, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 960, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 970, Loss= 0.6947, Training Accuracy= 0.497\n",
      "Epoch: 980, Loss= 0.6946, Training Accuracy= 0.497\n",
      "Epoch: 990, Loss= 0.6946, Training Accuracy= 0.497\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5121\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.25\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.50209999, 0.50120002, 0.4946, 0.49219999, 0.49610001, 0.4991, 0.50779998, 0.4991, 0.4998, 0.51209998]\n",
      "mean of test_accuracies_10replications:  0.50041\n",
      "standard deviation of test_accuracies_10replications_std_mean:  5.64170628786e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmcHFW5//HPd/YlK0kIgbAkEFll\nDYsCsiuETf15QRTZVPRecQMVUK6oqNcF8boLqKCoIHoBAYEoCAgIQsJOCCGEkMRA9mSyzD7P74+q\nmXSSWarD9ExP5vt+veo1XVWnqp6u6elnqs6pcxQRmJmZZVXS3wGYmdnA4sRhZmZ5ceIwM7O8OHGY\nmVlenDjMzCwvThxmZpYXJw6zXiLpSEkLcuZfkHRkAY5zt6Sze3u/Zlk5cVjRk3SBpGmSGiVdn8d2\ncyUdW8DQuhURe0bEA29mH5K+Ium3G+33hIj49ZsKzuxNKOvvAMwyWAh8HXgXUF2og0gqi4iWQu3f\nbEvhKw4rehFxS0TcBizbeJ2k0ZLulLRS0nJJD0kqkXQDsANwh6Q1kr7QybZHSlog6WJJbwDXpctP\nkvR0us9/Sto7Z5u5ki6VNEPSCknXSarqLO7cKx5JpZK+KOkVSaslTZe0fbruB5LmS6pLlx+eLj8e\n+CJwevoenkmXPyDpI+nrEkmXSXpN0mJJv5E0PF23k6SQdLakeZKWSvrS5v8mzBJOHDbQXQQsAMYA\nY0m+aCMiPgTMA06OiCER8Z0utt8G2ArYEThf0v7Ar4CPAaOAq4HbJVXmbPNBkqufnYG3AJdliPNC\n4AxgCjAMOA9Yl657Atg3jeP3wB8lVUXEPcA3gT+k72GfTvZ7TjodBUwEhgA/3qjMYcCuwDHAlyXt\nniFesy45cdhA1wyMA3aMiOaIeCjy64CtDbg8Ihojoh74KHB1RPwrIlrTuoRG4JCcbX4cEfMjYjnw\nDZKE0JOPAJdFxEuReCYilgFExG8jYllEtETE94BKki/6LD4IXBURcyJiDXAp8H5JubehvxoR9RHx\nDPAM0FkCMsvMicMGuu8Cs4G/Spoj6ZI8t18SEQ058zsCF6W3qVZKWglsD2ybU2Z+zuvXNlrXle2B\nVzpbIekiSS9KWpUebzgwOmP826Yx5MZTRnL11e6NnNfrSK5KzDabE4cNaBGxOiIuioiJwMnAhZKO\naV+dZRcbzc8HvhERI3Kmmoi4MafM9jmvdyCpvO/JfJJbWxtI6zMuBk4DRkbECGAVoIzvYSFJssuN\npwVYlCEms83ixGFFT1JZWgFdCpRKqmq/FZNWZO8iSUAd0JpOkHx5TszzcNcCH5d0sBK1kk6UNDSn\nzCckjZe0FUmdyh8y7PcXwBWSJqX73VvSKGAoyRf9EqBM0pdJ6kDaLQJ2ktTV3+qNwGclTZA0hPV1\nIm4dZgXjxGEDwWVAPXAJcGb6ur1CehJwL7AGeBT4ac6zE/8DXJbecvpclgNFxDSSeo4fAytIboOd\ns1Gx3wN/Beak09cz7Poq4OZ0uzrglyRNi6cCdwOzSG4zNbDhrbA/pj+XSXqyk/3+CrgB+Afwarr9\nJzPEY7bZ5IGczLKTNBf4SETc29+xmPUXX3GYmVleekwckg6V9DdJs9JWK69KmpNhu1+lDyQ938X6\nD0p6Np3+KclNBM3MBoAeb1VJmgl8FpjO+kpH2tugd7PdO0juO/8mIvbqZP3bgRcjYoWkE4CvRMTB\n+b8FMzPrS1n6qloVEXfnu+OI+IeknbpZ/8+c2ceA8fkew8zM+l6WxHG/pO8Ct5A8QQtARHTWwmNz\nfZikZUmnJJ0PnA9QW1t7wG677daLhzYz2/JNnz59aUSM6Y19ZUkc7bePJucsC+Do3ghA0lEkieOw\nrspExDXANQCTJ0+OadOm9cahzcwGDUmv9Vwqmx4TR0Qc1VsH21ja6+gvgBN6qjMxM7PikKVV1VhJ\nv5R0dzq/h6QPv9kDS9qB5PbXhyJi1pvdn5mZ9Y0sz3FcT/J0a3tHbrOAz/S0kaQbSZ7k3TUd8+DD\nkj4u6eNpkS+TdFv903TsA99/MjMbALLUcYyOiJslXQoQES2SWnvaKCK67Wo6Ij5C0tW0mZkNIFmu\nONamnbEFgKRDSHrvNDOzQSjLFceFwO3AzpIeIRlp7X0FjcrMzIpWllZVT0o6gmREMgEvRURzwSMz\nM7OilKVVVQ1Jd9afiYjnScYGOKngkZmZWVHKUsdxHdAEvC2dX0C28QfMzGwLlCVx7BwR3wGaASKi\nnvXDWpqZ2SCTJXE0Sapmfauqncnps8rMzAaXLK2qLgfuAbaX9DvgUDYdStPMzAaJbhOHJAEzgfcC\nh5Dcovp0RCztg9jMzKwIdZs4IiIk3RYRBwB/6aOYzMysiGWp43hM0oEFj8TMzAaELHUcRwEfS/ty\nX0tyuyoiYu+CRmZmZkUpS+I4oeBRmJnZgJElcazOuMzMzAaBLHUcTwJLSMbheDl9/aqkJyUdUMjg\nzMys+GRJHPcAUyJidESMIrl1dTPwX8BPCxmcmZkVnyyJY3JETG2fiYi/Au+IiMeAyoJFZmZmRSlL\nHcdySRcDN6XzpwMrJJUCbQWLzMzMilKWK44PAOOB29Jp+3RZKXBa4UIzM7NilGUgp6XAJ7tYPbt3\nwzEzs2KX5YrDzMysgxOHmZnlxYnDzMzy0mMdh6QxwEeBnXLLR8R5hQvLzMyKVZbmuH8GHgLuBVoL\nG46ZmRW7LImjJiIuLngkZmY2IGSp47hT0pSCR2JmZgNClsTxaZLkUS+pTtJqSXU9bSTpV5IWS3q+\ni/WS9ENJsyU9K2n/fIM3M7O+12PiiIihEVESEdURMSydH5Zh39cDx3ez/gRgUjqdD/wsS8BmZta/\nuqzjkLRbRMzs6kogIp7sbscR8Q9JO3VT5FTgNxERJMPTjpA0LiJezxC3mZn1k+4qxy8kuRL4Xifr\nAjj6TR57O2B+zvyCdJkTh5lZEesycUTE+enPowp0bHV22E4LSueTJDF22GGHAoVjZmZZ9OeT4wtI\netptNx5Y2FnBiLgmIiZHxOQxY8b0SXBmZta5/kwctwNnpa2rDgFWuX7DzKz4ZXkAcLNIuhE4Ehgt\naQFwOVAOEBE/B+4CppB0zb4OOLdQsZiZWe/J0lfVocDTEbFW0pnA/sAPIuK17raLiDN6WB/AJ/IJ\n1szM+l+WW1U/A9ZJ2gf4AvAa8JuCRmVmZkUrS+JoSa8OTiW50vgBMLSwYZmZWbHKUsexWtKlwJnA\nOySVktZVmJnZ4JPliuN0oBH4cES8QfKQ3ncLGpWZmRWtTFccJLeoWiW9BdgNuLGwYZmZWbHKcsXx\nD6BS0nbAfSTNZq8vZFBmZla8siQORcQ64L3AjyLiPcCehQ3LzMyKVabEIeltwAeBv6TLSgsXkpmZ\nFbMsieMzwKXArRHxgqSJwP2FDcvMzIpVj5XjEfEg8KCkoZKGRMQc4FOFD83MzIpRj1cckt4q6Sng\neWCGpOmSXMdhZjZIZblVdTVwYUTsGBE7ABcB1xY2LDMzK1ZZEkdtRHTUaUTEA0BtwSIyM7OiluUB\nwDmS/hu4IZ0/E3i1cCGZmVkxy3LFcR4wBrgFuDV97bEzzMwGqSytqlbgVlRmZpbqMnFIugOIrtZH\nxCkFicjMzIpad1ccV/ZZFGZmNmB0mTjSB//MzMw2kKVy3MzMrIMTh5mZ5cWJw8zM8pLlAcANSPom\nsAr4RUQs6/2QzMysmG3OFcfjQAvw/V6OxczMBoC8rzgi4rZCBGJmZgNDdw8A/ojuHwD00+RmZoNQ\nd7eqpgHTgSpgf+DldNoXaC18aGZmVoy6ewDw1wCSzgGOiojmdP7nwF/7JDozMys6WSrHtwWG5swP\nSZf1SNLxkl6SNFvSJZ2s30HS/ZKekvSspCnZwjYzs/6SpXL8W8BTktoHczoC+EpPG0kqBX4CHAcs\nAJ6QdHtEzMgpdhlwc0T8TNIewF3ATtnDNzOzvpalW/XrJN0NHJwuuiQi3siw74OA2RExB0DSTcCp\nQG7iCGBY+no4sDBr4GZm1j96vFUlScCxwD4R8WegQtJBGfa9HTA/Z35BuizXV4AzJS0gudr4ZBcx\nnC9pmqRpS5YsyXBoMzMrlCx1HD8F3gackc6vJrkF1RN1smzj5r1nANdHxHhgCnCDpE1iiohrImJy\nREweM2ZMhkObmVmhZEkcB0fEJ4AG6BgRsCLDdguA7XPmx7PpragPAzen+32UpOnv6Az7NjOzfpIl\ncTSnFd0BIGkM0JZhuyeASZImSKoA3g/cvlGZecAx6X53J0kcvhdlZlbEsiSOHwK3AltL+gbwMPDN\nnjaKiBbgAmAq8CJJ66kXJH1NUvuwsxcBH5X0DHAjcE5EdPm0upmZ9T9l+Z6WtBvJlYGA+yLixUIH\n1pXJkyfHtGnT+uvwZmYDkqTpETG5N/bVbXPctKL62YjYC5jZGwc0M7OBrdtbVRHRBjwjaYc+isfM\nzIpclifHxwEvSHocWNu+MCJO6XoTMzPbUmVJHF8teBRmZjZgZOly5MG+CMTMzAaGzRk61szMBjEn\nDjMzy4sTh9kgdNfLd1H9jWoqrqjgqF8fxdyVc/s7pG7VN9dzwzM3cMMzN9DQ0tDf4Qx6WXrHPVTS\n3yTNkjRH0quS5vRFcGbW+z57z2c58fcn0tDSQHNbMw/MfYDP/fVz/R3WJmYuncl9c+7j9dWvs/81\n+3PWbWdx1m1nceC1Bzp59LMenxyXNBP4LMn44x1jjUfEssKG1rny8eVx4pUnMmmrSewwfAfGDR3H\niKoRG0w15TVUlVVRsmlHu2aD2nOLnmPvn+/d6brp509nzzF7UllW2cdRbai1rZVTbjqFu16+q8sy\nv3vv7/jAWz/Qh1FtavHaxaxpWsP4YeOpKM3S72v/6s0nx7Mkjn9FxMHdFupD2lbBx7KVrSitoKqs\nqmOqLqumqqyKspKyjqm0pHTDeZV2uq5UpZSqFEkIUaISpPQnKvhrgGRoFDqd725doctm2U+hyhbq\nfeeKjUYDaG1rpbG1kRlLZvCXl//C3JVz2W30bkzZZQrbDt2W5rZmmlqbePL1Jzlou4PYbuh2tLS1\n0NLWQnNbMy1tLUQEbdFGkP6M6PJ1e7mykjKGVgyluryaytJKKssqqSytpKqsquN1ZVklt754K7fP\nup0dh+/Ilw7/EtsM2YbnFj/H66tf56N3fJR/r/433Rk3ZBxjh4xlVPUoRteMZlT1KEbVjOr4Z+zH\nj/+4Yx+jqkdxyq6ncPY+Z1NdXk11WTUVpRWMrB7J8MrhXSahiGBFwwoaWxqpa6xjTO0YtqreipUN\nKxn57ZHdxtduyeeXMLpmNGub1vLqylfZY8weHf8sRgS3zbyNj//l49SU13DcxOP47nHfZV3zOuau\nnMtbx76VIRVDOvY1e/lsZi2bxbxV81i4eiFHTziaI3Y8ouMzkauusY7T/3Q698y+Z4NzNmnUJMYN\nGUdlWSX1zfU0tzWz88ideefO72S/bfajRCVc9ehVvLz8ZQ7c9kDes/t7WNmwknFDxrFw9UL23WZf\nXlz6IruO2rXT8xYRzFw6k9nLZ3P0hKOprajdYN1TbzxFWUkZe4/dm9dWvsZFf72IV1e+yrt3fTcX\nH3YxlWWVfZo4vgWUArcAjTmBPtkbAeQrn8RhZv2rorSC2vJaasprqCmvoby0nBlLZvS8YR+44qgr\nuOSwS/jVU7/igrsuoLmteYP1x008jmtPvpZrn7yWuSvnsqBuAYvWLmLm0r7vfamitIKm1qbN3v7K\n467kc4d+rk8Tx/2dLI6IOLo3AsiXE4eZ2Wb4Cn3TySFARBzVGwcyM7MtQ5eJQ9KZEfFbSRd2tj4i\nripcWF3bbfRuXHzqxcxbNY95q+axvH45KxpWsLJhZcfU0NLgVhdmedp26LYsXL3xIJ1mm+ruiqO9\n5mVoXwSSVW1FLefse06P5SKCptYm6lvqOxJJ+9ReUdnS1kJrW+uG89Ha6frmtuYeKzEL8botksEW\n2yto228t5s53t25zyvb2/goWZ4b9bG7ZjStFcyvMS1RCZVkl1WXV1JTXMGPJDCaMnEBLWwtlJWW0\ntrUybeG0jgrkPcfsyda1W1NeWt7R2KK90UN7I4ssDS6aWptY07SGhpYGGlsak5+tjTS2NHb8bGhp\nYFl9fg0eLzjwAv73+P+ltKSUhpYGFq5eyLJ1y1i6binL6pOfK+pX0NTaxIqGFdw7515eWfHKBvvY\nc8yerGxYSV1jHSUqoUQl1DXW0RqtXRx1y3HlcVcyZdIUXlnxCnWNddQ319PU2sTclXO5deatvLz8\n5f4OsddlGsipmHggJ7PsIqLjy7uspIy2aKMt2ihVKaubVlNVVlWwpqQRQUNLA+ua13VMja2NlKqU\nitIK5tfNZ13zOt4y6i0d8axtWoskmluTlmkTR07kpudv4hsPfYP6lnrKS8o5duKxANw9++6CxJ2P\nQ7c/lHvPupeqsqouyyxeu5hH5z/KvFXzeH3N68xaNouDtzuYl5a9xG0zb2Pr2q0ZP2w8M5bM2KTF\n24mTTmSXrXbhjll3MLZ2LI8ueHTzg+3FOg4nDjPb4rRFGzOXzmRs7Vjm182nta2V/cbtx9qmtdRW\n1DJ35VwunHohs5bNYuLIiYwfNp431rxBS1sL9865l5232rnb1l9n7XMW+47dl/P2O4/hVcP78J1t\nqqm1ieX1y9m6dmteXPIijy14jBlLZvDb537L4rWL1xd04nDiMLO+1X41VFNe09+h5OX+V+/npBtP\nYt2X1vVa4vCj1WZmGdRW1A64pAFw1ISjmPOp3u0lKktfVd+UNCJnfqSkr/dqFGZmVjBjh4zt1f1l\nueI4ISJWts9ExApgSq9GYWZmA0aWxFEqqaPjFEnVQP/2gmZmZv0my5jjvwXuk3QdEMB5wK8LGpWZ\nmRWtLF2OfEfSs8CxgIArImJqwSMzM7Oi1GPikDQBeCAi7knnqyXtFBFzCx2cmZkVnyx1HH8E2nLm\nW9NlZmY2CGVJHGUR0dERfPo6Ux8Fko6X9JKk2ZIu6aLMaZJmSHpB0u+zhW1mZv0lS+X4EkmnRMTt\nAJJOBZb2tJGkUuAnwHHAAuAJSbdHxIycMpOAS4FDI2KFpK03502YmVnfyZI4Pg78TtKPSSrH5wNn\nZdjuIGB2RMwBkHQTcCqQ2wHMR4GfpM+GEBGLN9mLmZkVlSytql4BDpE0hKRvq9UZ970dSZJptwDY\neOzytwBIeoRkeNqvtFfC55J0PnA+wA477JDx8GZmVghZrjiQdCKwJ1DVPk5BRHytp806WbZxj4pl\nwCTgSGA88JCkvXKfVE+PdQ1wDSSdHGaJ2czMCiNLc9yfAzXAUcAvgPcBj2fY9wJg+5z58cDGw4st\nAB6LiGbgVUkvkSSSJzLs38zMuvH970NrK5xxRu/uN0urqrdHxFnAioj4KvA2NkwIXXkCmCRpgqQK\n4P3A7RuVuY0kISFpNMmtq97txtHMrBdEQGNjf0eRXUsLfP3r8PnPw/ZZvrHzkCVx1Kc/10naFmgG\nJvS0UUS0ABcAU4EXgZsj4gVJX5N0SlpsKrBM0gzgfuDzEZHfuJdmZr3oxRfhrrvguedg1Sp49FE4\n6yzYemsYNgyOPx7mFPm/t8uWwejRsHx5Mt/bwy71OJCTpP8GfgQcQ9K8NoBrI+LLvRtKNh7Iycze\nrNZWuO02uO8+aGpKksGUKXDEEZD16+WBB+Cww6C0tKCh5u2hh+Ad7+hsjfpnBMC0l9yqiFjVGwff\nHE4cZtadlha4/np46ilYsAC22gpGjIAnnoB//QsqKmDdut451m67wdSpUAyNPZubk1tTX+uy2VLv\nJY5MraraRUQjMIDu8pnZlqT9/9zmZrj77uRLe+rU/G4dtbT0XjwzZyZXKTNnQmU/DjZxzz1wwgl9\nd7y8EoeZWW9qa4OSTmpaV6+GWbOS/+QXLoR99+372LKaOze57XX66X1zvJkz4cYb4ec/h6VLk3PY\n1zzmuJkVzKxZ8O53g5RULO+4Y/K6fSothW22gc99LvnPvX35sGEweXJSIV3MSaPdn//85rZfswZu\nvXXDcyPByJHwm98k5/Gxx+CUU2D33ZPbUYsX90/SgGzPcdwXEcf0tMzMBrdly5IvwGHDkjqET38a\n/u//1q9fvTqZNrZoEXzve30XZyG8+mq2chHw739DfX3S6umRR+Dkk7suv3IlnH1278TYm7pMHJKq\nSB78Gy1pJOufBB8GbNsHsZlZHtauTZqSzp0L+++fLPv2t+Gaa7re5sAD4c47k//sN8ejj8IxxyRf\nhINZV4ljzRq45JIkgb7xRt/GVEjdXXF8DPgMSZKYzvrEUUfSLNdsi/Laa3DHHUkTzX/+M7kVAPCu\nd8E558B735u0yOlLDQ1JLI89Bi+9lDxb8MwzsNdecN55cNllyZfT5nriCRg7NmlaesQR3ZddtCip\nhH7++aRy+hOf2PzjbmkWL06uJpTT0dJNN/X+E9vFIstzHJ+MiB/1UTw9cnPcLVNzc/Kw1fPPJ9Oq\nVckfYXl5ch/3jTfWTw0NUFOTfGG+8kryBzt2bDING5asf/nlpKnk8ccny2bNSu4hr1sHtbVJE82K\niuQ/xWHDYMaM/FvbHHFEcv/+oIOSZPP3vydfwBsrL0/uVa9blySgAw6Ac8/dsMzw4cl7zlVS0nf3\nsGtqksQ0fnwyP3s23HBDEtOaNfDLX/ZNHMXsQB5nLIu4iykEoooG6qnpWH/vvcnnrTdbbfWuPnyO\nQ9J/APdExGpJlwH7A1+PiCd7I4B87bvv5Jg6NUkc7aFHbPi6N5Z1tW7duuRpzLa2DafW1g3nm5uT\nsuXlyRdU+9TSkty3XLcOqqqSJnxS8iVRWZlM1dXJl2BZWfJf8MqVUFeXtKBoaEj20dy8fmpqWn+8\n9rJNTZtOzc3r911Tk8TZfuzcn129LilJzkNZWdKFQUVFEk9EElNDQ9IlQ2Nj8j7a17Wft/bminV1\nydTQkNxeWbw4eW39b+ut119pbSlqWcPuvMgH+D3LGMXP+E+WM2qjUsEw6jiX69iNmZzGzTzFflzI\nVXzmzGWM/O2PeDe3bbBFKyW8i6ncx7F992belL5NHM9GxN6SDgP+B7gS+GJEbNxFep+QJgf4isNs\nc5TSwtH8nTlM5BV26bc4ymjmbH7NaoZyM6cxmWlsy0JeZhJjWcRDHE5rx530YCuWU091x3/4k5jF\nzrzCydxBGS2cz7UA/IBPsZIRHMy/mMw0FjGWPTcYAqh3zWRXdmdmwfbfW0QbQWmfJo6nImI/Sf8D\nPBcRv29f1hsB5MuJw/pWMJZF7MMzvIVZPMxhPM1+JD3vdDZyQHEax0Iu56t8jA1ryi/lm3yPi2jO\nNhr0ZtuRuXyKH/J2Pcoj8XZqWcvHubqgx+wr2mS0iL5RQisf4Reso4bfciYieDe3cSQPUEIbKxjJ\ny0xiP57iPVV3M6HhpT5NHHcC/waOBQ4g6fTw8YjYpzcCyJcThxXacFbyEX7Bu5jKoTxCDRs2GWqm\njHJaWEMtQ1gLwBVcxm85k+2ZTxUNTOBV5jCRhWzLCkZyBjdyIE8wjcmsYQj3cxTL2Yr9eAoRtFLK\nP3k7ZbTwLqZSxzCqqaeVUnbkNd7Kc9QxjBLaeIr9uJdjGcpqhrCG8SxgH56hhDYaqWQeO/Brzsn8\nfi/jCr7BZXmfp6HUcRo3M5TVLGEMV/DfTGBu3vsZ6IazkjqGb7K8nCYmM43neCsH8ThrGMI5XM8B\nTOd6zuH3fIChrKaWtezEXPbhGf7CibzAXlRRz9t3X8lObxvHA796hXXU8AbbsA/P8EM+xTt4KO84\nBX2aOGqA40muNl6WNA54a0T8tTcCyFe19ooJ3AxApP/xbfxzc9dlKbOGIZTQRh3DqKCJ8SxgJCsY\nwhqGkDRvaaGMZso7pvlszzx2ICihgkbKaKGUVtZRQzX11LKWIayhlrXUsI5mygEop7mj7GqG0kw5\nZbRQnV60t0/rqGE1Q2mllAaqaKCKRippooKRrGA0SztiqqaeIayhnGbqGMZKRlDHsI4Y2iihlVJa\nKGM1Q2mhjBLaqKeaCprYhjcooY3XGUcdw6hhHcNZRSmtBOo4L5U0UkVDx/r2qZp6qmjoWF9BU8eZ\nqqSRWtayhDGsYjgNVFFCGyNYyRDWMIplzGMHFrIti9maVkp5mUlMYzKrGUb7OGGltDKcVaxgJJHz\njOsYFtNGCe/lFg7lEd7CLN7GY/yB07iDkxlGHYfxMCdzB0N5E02VBqD3cAvP8VbmMJGRrGBfnqae\nauYwkWrq2YrljGAlI1nBWBZxGA9zBjf1d9hv3ic+AT/J2Ej029/mi0+fxl03rkyvOhNncgN/4UTG\n8TotlPFFvsmL7M4J3M2RPFigwPPXm4mDiOhxAg4Dzk1fjwEmZNmuENMB6+usB9S0mtpYwfB+j2NL\nnFpRrGJoNFEWyxkRdQzpOOcPcWi8woRYzog3dYzX2D4eZ3K/v9dCT2upLtz+a2q6X/+e97y5/X/s\nYxFz5kT8/OcRhx3WeZnRoyP22iviqquiwxtvRIwcuUG5ujETounDH4s4/PCIk06K+OEPIyJi6tSk\nyJ94b7//rvKdgGm99T2c5YrjcmAysGtEvCUdk+OPEXFor2SuPE2WwjeqrOB23x3+4z9g6FAazziH\nl5aN5v6/B9X33Er91AeZzS5U0MTHuJq38HKmXS5ia+7gZEazlHfqb9TEOuqpYqVGMi5e36Bs86ix\nlO2wHQwfhqSkGVxNTfK0Xn09bLdd0kRu6dKkre/++yc93a1du+FBDz88efhj5Ur4r/+C736Xiz/f\nxvE/PZmjeKCXTlYndtwx6aI2InlY5IAD4JBDkvfwxS8mbX3//e+k7NixcOmlyaPmAI8+Stvh76Ck\ntYV7K6bwyM5n8ZfmdzJu/uPsGjMZs994PnLjMYwc3pZ0e9vSknSDO3HihjGsWpU05xs3rud4Fy2C\nhx+GY49N9jdkSKe9Fra1wYknws73/Jgf88k3d44K4dxz4TvfgS9/Gf72N9h7b7jlFthzT/TCC316\nq+ppYD/gyfYK8faWVr0RQL72UnX8X+mEjgqp3J/tN5o2WCdQbPhzkzJsWMHV1ToRDGtdSWtpOTWt\nq2mllGVV27G6YjQNZbXUlw/bIngNAAARQ0lEQVQDibJopixaKFcz5S0NjK2bxbCmZHyqVpXSqjKi\npJTylnqaympoLKulobSW+tIhNKgGmptoaRXNlBOlZVRWlzCkrY7y0jZUVkpLeTWt5dW0VlYT5ZVU\ntKyjonkNJbRR0dZAeVsjpc0NlDQ3ErVDaBuzDaKNkpZmmlVOfUktbZU1lK1bRdnqFZTWr0HNTTSN\n2JooLYe2VtTSTNm6OtTWSqiEkqakvey6rcbT2ChqVy2kvGkNrZW1NFYPJ0rLKKWNioZVREUVzWVV\nRGUVbZXVtFQNpW3YCNZVDCeqaykbUknF0CpKayqJ8gpGjCmnZng5qihP2igvX570TdHeRreqKnnw\nYuTIpH3y668nbUYXLky+FNu/gLKaOBE+8AHYYw948snkS+ff/06Os/feSR8Qhx/eee97qddeg+uu\nS3po3WUXOPngxbxn8nwal62hbOwoag7aK/kCmjcvefjk4IM3Hbhh7tykXXNpafLFfscdyRdWe+dO\nm2vt2uQBl913T9qDd+LFF+HabyzmwDsvp7p1NduVLWJi3dOMalu6YcHy8qQzqfbzP3Jk8u15111J\n++5hw+C00+ALX0hOxJuJO9fSpUm77xEjOhY1Na1/tqc/NTTAFz+9llOvmcIR/CPbRl/8IlxwQfJ5\nuO46uPrq5Pf8zW8mCW7OHPjWt5LPc7uxY5MHf+bOhWefhQ99CI46Kik/cSL86U9w1VVJ2/bPfz7p\nU70rEaikpE8Tx+MRcZCkJyNif0m1wKP9lTiK5gHA9vOW5Q8lIunIp6Qk+UMrc6fEvar9ARIpOc+Q\nfCE//HDyJTpqVPIf+rhxye+gm4QwqEUkCXTRoiSpVlf3d0RFrbU1fcbp1TlJQth/fxg6NFnZ/uBS\nvn/rixfDmDG9l4BzSH07HsfNkq4GRkj6KHAe8IveOPiAls8vVkp6NLPCqKpKJkgSc7sjj+yXcAYs\nKXl0vP3xcetWxwXkxImb3iarrd28nW5up2F9rMfEERFXSjqOpI+qXYEvR8TfCh6ZmZkVpSzdqn87\nIi4G/tbJMjMzG2Sy3Ow9rpNlfThIoZmZFZPuxuP4T+C/gImSns1ZNRR4pNCBmZlZceruVtXvgbtJ\nOja8JGf56ohYXtCozMysaHWZOCJiFbAK2EKHIjEzs83hBu1mZpYXJw4zM8uLE4eZmeUl78Qh6V5J\nd0s6KUPZ4yW9JGm2pEu6Kfc+SSGpd7r8NTOzgtmcTpPOAsYBh3RXSFIp8BOS50AWAE9Iuj0iZmxU\nbijwKeBfmxGLmZn1sUxXHJKqJe0KEBELI2J6RPQ0+slBwOyImBMRTcBNwKmdlLsC+A7QkEfcZmbW\nT3pMHJJOBp4G7knn95V0e4Z9bwfMz5lfkC7L3fd+wPYRcWcPMZwvaZqkaUuWLMlwaDMzK5QsVxxf\nIbl6WAkQEU8DO2XYrrPuYzv6cJdUAnwfuKinHUXENRExOSImjxkzJsOhzcysULIkjpb0YcB8LQC2\nz5kfDyzMmR8K7AU8IGkuSZ3J7a4gNzMrblkSx/OSPgCUSpok6UfAPzNs9wQwSdIESRXA+4GOW1wR\nsSoiRkfEThGxE/AYcEqER4Y1MytmWRLHJ4E9gUbgRpJxOT7T00YR0QJcAEwFXgRujogXJH1N0imb\nH7KZmfWnHoeOLTZFM3SsmdkA0qdDx0q6n5xK7XYRcXRvBGBmZgNLlgcAP5fzugr4f0BLYcIxM7Ni\nl2XM8ekbLXpE0oMFisfMzIpclltVW+XMlgAHANsULCIzMytqWW5VTSep4xDJLapXgQ8XMigzMyte\nWW5VTeiLQMzMbGDoMnFIem93G0bELb0fjpmZFbvurjhO7mZdAE4cZmaDUJeJIyLO7ctAzMxsYMjS\nrfooST+U9KSk6ZJ+IGlUXwRnZmbFJ0tfVTcBS0ge/Htf+voPhQzKzMyKV5bmuFtFxBU581+X9O5C\nBWRmZsUtyxXH/ZLeL6kknU4D/lLowMzMrDh11xx3Nesf/LsQuCFdVQqsAS4veHRmZlZ0umtVNbQv\nAzEzs4Ehy60qMzOzDk4cZmaWFycOMzPLS5bmuEgqBcbmlo+IeYUKyszMileW8Tg+SdKCahHQli4O\nYO8CxmVmZkUqyxXHp4FdI2JZoYMxM7Pil6WOYz6wqtCBmJnZwJDlimMO8ICkvwCN7Qsj4qqCRWVm\nZkUrS+KYl04V6WRmZoNYlqFjv9oXgZiZ2cDQXV9V/xsRn5F0B0krqg1ExCkFjczMzIpSd1cc7Z0a\nXtkXgZiZ2cDQXSeH09OfD27uziUdD/yApEfdX0TEtzZafyHwEaCFZICo8yLitc09npmZFV7BuhxJ\nnzb/CXACsAdwhqQ9Nir2FDA5IvYG/gR8p1DxmJlZ7yhkX1UHAbMjYk5ENJEMQXtqboGIuD8i1qWz\njwHjCxiPmZn1gkImju1IHh5styBd1pUPA3cXMB4zM+sFPSYOSX+TNCJnfqSkqRn2rU6WbdI6K93n\nmcBk4LtdrD9f0jRJ05YsWZLh0GZmVihZrjhGR8TK9pmIWAFsnWG7BcD2OfPjgYUbF5J0LPAl4JSI\naNx4fXrMayJickRMHjNmTIZDm5lZoWRJHG2SdmifkbQjXVw5bOQJYJKkCZIqgPcDt+cWkLQfcDVJ\n0licPWwzM+svWboc+RLwsKT2ZrnvAM7vaaOIaJF0ATCVpDnuryLiBUlfA6ZFxO0kt6aGAH+UBDDP\nDxaamRU3RfR88SBpNHAISb3FoxGxtNCBdWXy5Mkxbdq0/jq8mdmAJGl6REzujX1lqRx/D9AcEXdG\nxB1Ai6R398bBzcxs4MlSx3F5RHSMx5FWlF9euJDMzKyYZUkcnZXJNFa5mZltebIkjmmSrpK0s6SJ\nkr4PTC90YGZmVpyyJI5PAk3AH4A/Ag3AJwoZlJmZFa8sAzmtBS7pg1jMzGwA6DFxSBoDfAHYE6hq\nXx4RRxcwLjMzK1JZblX9DpgJTAC+CswleSrczMwGoSyJY1RE/JLkWY4HI+I8kocBzcxsEMrSrLY5\n/fm6pBNJOir0uBlmZoNUlsTxdUnDgYuAHwHDgM8WNCozMytaWVpV3Zm+XAUcVdhwzMys2BVyBEAz\nM9sCOXGYmVlenDjMzCwvWR4ArAT+H7BTbvmI+FrhwjIzs2KVpVXVn0kqxqcDnY4JbmZmg0eWxDE+\nIo4veCRmZjYgZKnj+KektxY8EjMzGxCyXHEcBpwj6VWSW1UCIiL2LmhkZmZWlLIkjhMKHoWZmQ0Y\nXSYOScMiog5Y3YfxmJlZkevuiuP3wEkkramC5BZVuwAmFjAuMzMrUl0mjog4Kf05oe/CMTOzYpel\njgNJI4FJbDgC4D8KFZSZmRWvLE+OfwT4NMkYHE+TDOL0KOChY83MBqEsz3F8GjgQeC0ijgL2A5YU\nNCozMytaWRJHQ0Q0QNJvVUTMBHYtbFhmZlassiSOBZJGALcBf5P0Z5LhY3sk6XhJL0maLemSTtZX\nSvpDuv5fknbKJ3gzM+t7WUYAfE/68iuS7geGA/f0tJ2kUuAnwHHAAuAJSbdHxIycYh8GVkTELpLe\nD3wbOD3P92BmZn2o2ysOSSWSnm+fj4gHI+L2iGjKsO+DgNkRMSctfxNw6kZlTgV+nb7+E3CMJGFm\nZkWr2yuOiGiT9IykHSJiXp773g6YnzO/ADi4qzIR0SJpFTAKWJpbSNL5wPnpbGNuMhvkRrPRuRrE\nfC7W87lYz+divV6rm87yHMc44AVJjwNr2xdGxCk9bNfZlUNsRhki4hrgGgBJ0yJicg/HHhR8Ltbz\nuVjP52I9n4v1JE3rrX1lSRxf3cx9LwC2z5kfz6aV6u1lFkgqI6k/Wb6ZxzMzsz6QpVXVlLRuo2MC\npmTY7glgkqQJkiqA9wO3b1TmduDs9PX7gL9HxCZXHGZmVjyyJI7jOlnWY1frEdECXABMBV4Ebo6I\nFyR9TVL7ba5fAqMkzQYuBDZpstuJazKUGSx8LtbzuVjP52I9n4v1eu1cqKt/8CX9J/BfJL3gvpKz\naijwSESc2VtBmJnZwNFd4hgOjAT+hw2vBFZHhOshzMwGqS4Th5mZWWey1HEUjZ66MNmSSNpe0v2S\nXpT0gqRPp8u3kvQ3SS+nP0emyyXph+m5eVbS/v37DnqfpFJJT0m6M52fkHZV83LadU1FunyL7spG\n0ghJf5I0M/18vG2wfi4kfTb9+3he0o2SqgbT50LSryQtzn22bXM+C5LOTsu/LOnszo6Va8Akjpwu\nTE4A9gDOkLRH/0ZVUC3ARRGxO0lX9p9I3+8lwH0RMQm4j/W3EU8gGTNlEsnDkj/r+5AL7tMkDS3a\nfRv4fnouVpB0YQM5XdkA30/LbUl+ANwTEbsB+5Cck0H3uZC0HfApYHJE7AWUkrTeHEyfi+uB4zda\nltdnQdJWwOUkD2gfBFzenmy6FBEDYgLeBkzNmb8UuLS/4+rD9/9nkhZuLwHj0mXjgJfS11cDZ+SU\n7yi3JUwkzwHdRzIOzJ0kD48uBco2/nyQtOR7W/q6LC2n/n4PvXQehgGvbvx+BuPngvU9T2yV/p7v\nBN412D4XwE7A85v7WQDOAK7OWb5Buc6mAXPFQeddmGzXT7H0qfSSej/gX8DYiHgdIP25dVpsSz8/\n/wt8AWhL50cBKyNp9g0bvt8NurIB2ruy2RJMJBkP57r0tt0vJNUyCD8XEfFv4EpgHvA6ye95OoPz\nc5Er389C3p+RgZQ4MnVPsqWRNAT4P+AzEVHXXdFOlm0R50fSScDiiJieu7iTopFh3UBXBuwP/Cwi\n9iPpBqi7+r4t9lykt1NOBSYA2wK1dP6M2WD4XGTR1fvP+7wMpMSRpQuTLYqkcpKk8buIuCVdvEjS\nuHT9OGBxunxLPj+HAqdImkvSy/LRJFcgI9KuamDD99txLrbArmwWAAsi4l/p/J9IEslg/FwcC7wa\nEUsiohm4BXg7g/NzkSvfz0Len5GBlDiydGGyxZAkkifrX4yIq3JW5XbTcjZJ3Uf78rPSlhOHAKva\nL1cHuoi4NCLGR8ROJL/3v0fEB4H7SbqqgU3PxRbZlU1EvAHMl9Te0+kxwAwG4eeC5BbVIZJq0r+X\n9nMx6D4XG8n3szAVeKekkelV3DvTZV3r74qdPCuBpgCzSJ5k/1J/x1Pg93oYyeXis8DT6TSF5J7s\nfcDL6c+t0vIiaXX2CvAcSUuTfn8fBTgvRwJ3pq8nAo8Ds4E/ApXp8qp0fna6fmJ/x93L52BfYFr6\n2biN5EHdQfm5IOmEdSbwPHADUDmYPhfAjST1O80kVw4f3pzPAnBeel5mA+f2dFw/AGhmZnkZSLeq\nzMysCDhxmJlZXpw4zMwsL04cZmaWFycOMzPLixOHWR+SdGR7775mA5UTh5mZ5cWJw6wTks6U9Lik\npyVdnY4FskbS9yQ9Kek+SWPSsvtKeiwd4+DWnPEPdpF0r6Rn0m12Tnc/JGc8jd+lTz2bDRhOHGYb\nkbQ7cDpwaETsC7QCHyTpRO/JiNgfeJBkDAOA3wAXR8TeJE/kti//HfCTiNiHpA+l9q4+9gM+QzKu\nzESSvrjMBoyynouYDTrHAAcAT6QXA9UkHcW1AX9Iy/wWuEXScGBERDyYLv818EdJQ4HtIuJWgIho\nAEj393hELEjnnyYZT+Hhwr8ts97hxGG2KQG/johLN1go/fdG5brrr6e720+NOa9b8d+hDTC+VWW2\nqfuA90naGjrGcN6R5O+lvdfVDwAPR8QqYIWkw9PlHwIejGTslAWS3p3uo1JSTZ++C7MC8X86ZhuJ\niBmSLgP+KqmEpOfRT5AMmrSnpOkko8ednm5yNvDzNDHMAc5Nl38IuFrS19J9/Ecfvg2zgnHvuGYZ\nSVoTEUP6Ow6z/uZbVWZmlhdfcZiZWV58xWFmZnlx4jAzs7w4cZiZWV6cOMzMLC9OHGZmlpf/D2xQ\nBMFpKiylAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcf23c88390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
