{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 5\n",
    "N = 25\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.6959, Training Accuracy= 0.506\n",
      "Epoch: 10, Loss= 0.6949, Training Accuracy= 0.508\n",
      "Epoch: 20, Loss= 0.6942, Training Accuracy= 0.512\n",
      "Epoch: 30, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 40, Loss= 0.6934, Training Accuracy= 0.511\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 60, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 70, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 80, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 90, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 100, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 110, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 120, Loss= 0.6923, Training Accuracy= 0.513\n",
      "Epoch: 130, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 140, Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 150, Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 160, Loss= 0.6921, Training Accuracy= 0.516\n",
      "Epoch: 170, Loss= 0.6920, Training Accuracy= 0.517\n",
      "Epoch: 180, Loss= 0.6920, Training Accuracy= 0.518\n",
      "Epoch: 190, Loss= 0.6920, Training Accuracy= 0.515\n",
      "Epoch: 200, Loss= 0.6938, Training Accuracy= 0.510\n",
      "Epoch: 210, Loss= 0.6934, Training Accuracy= 0.513\n",
      "Epoch: 220, Loss= 0.6940, Training Accuracy= 0.513\n",
      "Epoch: 230, Loss= 0.6946, Training Accuracy= 0.510\n",
      "Epoch: 240, Loss= 0.6931, Training Accuracy= 0.518\n",
      "Epoch: 250, Loss= 0.6927, Training Accuracy= 0.520\n",
      "Epoch: 260, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 270, Loss= 0.6921, Training Accuracy= 0.517\n",
      "Epoch: 280, Loss= 0.6926, Training Accuracy= 0.518\n",
      "Epoch: 290, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 300, Loss= 0.6919, Training Accuracy= 0.520\n",
      "Epoch: 310, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 320, Loss= 0.6920, Training Accuracy= 0.520\n",
      "Epoch: 330, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 340, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 350, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 360, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 370, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 380, Loss= 0.6921, Training Accuracy= 0.517\n",
      "Epoch: 390, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 400, Loss= 0.6919, Training Accuracy= 0.518\n",
      "Epoch: 410, Loss= 0.6920, Training Accuracy= 0.518\n",
      "Epoch: 420, Loss= 0.6923, Training Accuracy= 0.521\n",
      "Epoch: 430, Loss= 0.6921, Training Accuracy= 0.522\n",
      "Epoch: 440, Loss= 0.6921, Training Accuracy= 0.518\n",
      "Epoch: 450, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 460, Loss= 0.6926, Training Accuracy= 0.520\n",
      "Epoch: 470, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 480, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 500, Loss= 0.6928, Training Accuracy= 0.518\n",
      "Epoch: 510, Loss= 0.6924, Training Accuracy= 0.521\n",
      "Epoch: 520, Loss= 0.6938, Training Accuracy= 0.518\n",
      "Epoch: 530, Loss= 0.6931, Training Accuracy= 0.514\n",
      "Epoch: 540, Loss= 0.6949, Training Accuracy= 0.511\n",
      "Epoch: 550, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 560, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 570, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 580, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 590, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 600, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 610, Loss= 0.6941, Training Accuracy= 0.507\n",
      "Epoch: 620, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 630, Loss= 0.6936, Training Accuracy= 0.521\n",
      "Epoch: 640, Loss= 0.6935, Training Accuracy= 0.517\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.526\n",
      "Epoch: 660, Loss= 0.6931, Training Accuracy= 0.521\n",
      "Epoch: 670, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 680, Loss= 0.6941, Training Accuracy= 0.504\n",
      "Epoch: 690, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 700, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 710, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 720, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 730, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 740, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 750, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 760, Loss= 0.6929, Training Accuracy= 0.519\n",
      "Epoch: 770, Loss= 0.6926, Training Accuracy= 0.520\n",
      "Epoch: 780, Loss= 0.6926, Training Accuracy= 0.522\n",
      "Epoch: 790, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 800, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 810, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 820, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 830, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 840, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 850, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 860, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 870, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 880, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 890, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 900, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 910, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 920, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 930, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 940, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 950, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 960, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 970, Loss= 0.6931, Training Accuracy= 0.513\n",
      "Epoch: 980, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 990, Loss= 0.6928, Training Accuracy= 0.517\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4971\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.6960, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.6947, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.6937, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.6936, Training Accuracy= 0.498\n",
      "Epoch: 50, Loss= 0.6935, Training Accuracy= 0.498\n",
      "Epoch: 60, Loss= 0.6934, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 80, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 90, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.6933, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 220, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 230, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 240, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 250, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 260, Loss= 0.6935, Training Accuracy= 0.500\n",
      "Epoch: 270, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 280, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 290, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 300, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 310, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 320, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 330, Loss= 0.6954, Training Accuracy= 0.500\n",
      "Epoch: 340, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 350, Loss= 0.6949, Training Accuracy= 0.507\n",
      "Epoch: 360, Loss= 0.6942, Training Accuracy= 0.505\n",
      "Epoch: 370, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 380, Loss= 0.6942, Training Accuracy= 0.506\n",
      "Epoch: 390, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 400, Loss= 0.6938, Training Accuracy= 0.511\n",
      "Epoch: 410, Loss= 0.6956, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.6948, Training Accuracy= 0.510\n",
      "Epoch: 430, Loss= 0.6941, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 450, Loss= 0.6973, Training Accuracy= 0.502\n",
      "Epoch: 460, Loss= 0.6953, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 490, Loss= 0.6938, Training Accuracy= 0.506\n",
      "Epoch: 500, Loss= 0.6940, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 520, Loss= 0.6946, Training Accuracy= 0.509\n",
      "Epoch: 530, Loss= 0.6953, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.6957, Training Accuracy= 0.502\n",
      "Epoch: 550, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 560, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 570, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 580, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 590, Loss= 0.6928, Training Accuracy= 0.518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 610, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 620, Loss= 0.6924, Training Accuracy= 0.520\n",
      "Epoch: 630, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 640, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 650, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 660, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 670, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 680, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 690, Loss= 0.6936, Training Accuracy= 0.501\n",
      "Epoch: 700, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 710, Loss= 0.6944, Training Accuracy= 0.499\n",
      "Epoch: 720, Loss= 0.6947, Training Accuracy= 0.499\n",
      "Epoch: 730, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 740, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 750, Loss= 0.6943, Training Accuracy= 0.502\n",
      "Epoch: 760, Loss= 0.6940, Training Accuracy= 0.506\n",
      "Epoch: 770, Loss= 0.6940, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 790, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 800, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 810, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 820, Loss= 0.6935, Training Accuracy= 0.500\n",
      "Epoch: 830, Loss= 0.6934, Training Accuracy= 0.500\n",
      "Epoch: 840, Loss= 0.6934, Training Accuracy= 0.500\n",
      "Epoch: 850, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 860, Loss= 0.6933, Training Accuracy= 0.501\n",
      "Epoch: 870, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 880, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 890, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 900, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 910, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 920, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 930, Loss= 0.6943, Training Accuracy= 0.494\n",
      "Epoch: 940, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 950, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 960, Loss= 0.6938, Training Accuracy= 0.505\n",
      "Epoch: 970, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 980, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 990, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4841\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.7380, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.7010, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6960, Training Accuracy= 0.509\n",
      "Epoch: 30, Loss= 0.6948, Training Accuracy= 0.510\n",
      "Epoch: 40, Loss= 0.6942, Training Accuracy= 0.513\n",
      "Epoch: 50, Loss= 0.6941, Training Accuracy= 0.517\n",
      "Epoch: 60, Loss= 0.6943, Training Accuracy= 0.516\n",
      "Epoch: 70, Loss= 0.6943, Training Accuracy= 0.515\n",
      "Epoch: 80, Loss= 0.6942, Training Accuracy= 0.516\n",
      "Epoch: 90, Loss= 0.6942, Training Accuracy= 0.516\n",
      "Epoch: 100, Loss= 0.6943, Training Accuracy= 0.516\n",
      "Epoch: 110, Loss= 0.6944, Training Accuracy= 0.516\n",
      "Epoch: 120, Loss= 0.6948, Training Accuracy= 0.516\n",
      "Epoch: 130, Loss= 0.6955, Training Accuracy= 0.514\n",
      "Epoch: 140, Loss= 0.6959, Training Accuracy= 0.517\n",
      "Epoch: 150, Loss= 0.6959, Training Accuracy= 0.519\n",
      "Epoch: 160, Loss= 0.6962, Training Accuracy= 0.518\n",
      "Epoch: 170, Loss= 0.6973, Training Accuracy= 0.516\n",
      "Epoch: 180, Loss= 0.6967, Training Accuracy= 0.511\n",
      "Epoch: 190, Loss= 0.6955, Training Accuracy= 0.512\n",
      "Epoch: 200, Loss= 0.6956, Training Accuracy= 0.513\n",
      "Epoch: 210, Loss= 0.6963, Training Accuracy= 0.514\n",
      "Epoch: 220, Loss= 0.6960, Training Accuracy= 0.514\n",
      "Epoch: 230, Loss= 0.6959, Training Accuracy= 0.513\n",
      "Epoch: 240, Loss= 0.6962, Training Accuracy= 0.511\n",
      "Epoch: 250, Loss= 0.6974, Training Accuracy= 0.510\n",
      "Epoch: 260, Loss= 0.6972, Training Accuracy= 0.503\n",
      "Epoch: 270, Loss= 0.6966, Training Accuracy= 0.505\n",
      "Epoch: 280, Loss= 0.6959, Training Accuracy= 0.511\n",
      "Epoch: 290, Loss= 0.6958, Training Accuracy= 0.511\n",
      "Epoch: 300, Loss= 0.6972, Training Accuracy= 0.502\n",
      "Epoch: 310, Loss= 0.6964, Training Accuracy= 0.509\n",
      "Epoch: 320, Loss= 0.6965, Training Accuracy= 0.504\n",
      "Epoch: 330, Loss= 0.6961, Training Accuracy= 0.505\n",
      "Epoch: 340, Loss= 0.6960, Training Accuracy= 0.503\n",
      "Epoch: 350, Loss= 0.6983, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.6985, Training Accuracy= 0.503\n",
      "Epoch: 370, Loss= 0.6969, Training Accuracy= 0.508\n",
      "Epoch: 380, Loss= 0.6966, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.6958, Training Accuracy= 0.503\n",
      "Epoch: 400, Loss= 0.6958, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6959, Training Accuracy= 0.503\n",
      "Epoch: 420, Loss= 0.6963, Training Accuracy= 0.503\n",
      "Epoch: 430, Loss= 0.6956, Training Accuracy= 0.509\n",
      "Epoch: 440, Loss= 0.6963, Training Accuracy= 0.504\n",
      "Epoch: 450, Loss= 0.6951, Training Accuracy= 0.515\n",
      "Epoch: 460, Loss= 0.6939, Training Accuracy= 0.520\n",
      "Epoch: 470, Loss= 0.6955, Training Accuracy= 0.516\n",
      "Epoch: 480, Loss= 0.6928, Training Accuracy= 0.525\n",
      "Epoch: 490, Loss= 0.6938, Training Accuracy= 0.525\n",
      "Epoch: 500, Loss= 0.6924, Training Accuracy= 0.523\n",
      "Epoch: 510, Loss= 0.6935, Training Accuracy= 0.521\n",
      "Epoch: 520, Loss= 0.6937, Training Accuracy= 0.526\n",
      "Epoch: 530, Loss= 0.7013, Training Accuracy= 0.500\n",
      "Epoch: 540, Loss= 0.7003, Training Accuracy= 0.505\n",
      "Epoch: 550, Loss= 0.6970, Training Accuracy= 0.503\n",
      "Epoch: 560, Loss= 0.6982, Training Accuracy= 0.504\n",
      "Epoch: 570, Loss= 0.6970, Training Accuracy= 0.506\n",
      "Epoch: 580, Loss= 0.6973, Training Accuracy= 0.507\n",
      "Epoch: 590, Loss= 0.6971, Training Accuracy= 0.504\n",
      "Epoch: 600, Loss= 0.6993, Training Accuracy= 0.502\n",
      "Epoch: 610, Loss= 0.6960, Training Accuracy= 0.505\n",
      "Epoch: 620, Loss= 0.6936, Training Accuracy= 0.513\n",
      "Epoch: 630, Loss= 0.7089, Training Accuracy= 0.503\n",
      "Epoch: 640, Loss= 0.7089, Training Accuracy= 0.503\n",
      "Epoch: 650, Loss= 0.7088, Training Accuracy= 0.503\n",
      "Epoch: 660, Loss= 0.7089, Training Accuracy= 0.503\n",
      "Epoch: 670, Loss= 0.7089, Training Accuracy= 0.503\n",
      "Epoch: 680, Loss= 0.7088, Training Accuracy= 0.503\n",
      "Epoch: 690, Loss= 0.7088, Training Accuracy= 0.503\n",
      "Epoch: 700, Loss= 0.7086, Training Accuracy= 0.503\n",
      "Epoch: 710, Loss= 0.7086, Training Accuracy= 0.503\n",
      "Epoch: 720, Loss= 0.7103, Training Accuracy= 0.503\n",
      "Epoch: 730, Loss= 0.7103, Training Accuracy= 0.503\n",
      "Epoch: 740, Loss= 0.7103, Training Accuracy= 0.503\n",
      "Epoch: 750, Loss= 0.7103, Training Accuracy= 0.503\n",
      "Epoch: 760, Loss= 0.7103, Training Accuracy= 0.503\n",
      "Epoch: 770, Loss= 0.7102, Training Accuracy= 0.503\n",
      "Epoch: 780, Loss= 0.7102, Training Accuracy= 0.503\n",
      "Epoch: 790, Loss= 0.7102, Training Accuracy= 0.503\n",
      "Epoch: 800, Loss= 0.7102, Training Accuracy= 0.503\n",
      "Epoch: 810, Loss= 0.7102, Training Accuracy= 0.503\n",
      "Epoch: 820, Loss= 0.7102, Training Accuracy= 0.503\n",
      "Epoch: 830, Loss= 0.7101, Training Accuracy= 0.503\n",
      "Epoch: 840, Loss= 0.7101, Training Accuracy= 0.503\n",
      "Epoch: 850, Loss= 0.7101, Training Accuracy= 0.503\n",
      "Epoch: 860, Loss= 0.7101, Training Accuracy= 0.503\n",
      "Epoch: 870, Loss= 0.7100, Training Accuracy= 0.503\n",
      "Epoch: 880, Loss= 0.7100, Training Accuracy= 0.503\n",
      "Epoch: 890, Loss= 0.7100, Training Accuracy= 0.503\n",
      "Epoch: 900, Loss= 0.7100, Training Accuracy= 0.503\n",
      "Epoch: 910, Loss= 0.7099, Training Accuracy= 0.503\n",
      "Epoch: 920, Loss= 0.7099, Training Accuracy= 0.503\n",
      "Epoch: 930, Loss= 0.7099, Training Accuracy= 0.503\n",
      "Epoch: 940, Loss= 0.7099, Training Accuracy= 0.503\n",
      "Epoch: 950, Loss= 0.7099, Training Accuracy= 0.503\n",
      "Epoch: 960, Loss= 0.7098, Training Accuracy= 0.503\n",
      "Epoch: 970, Loss= 0.7098, Training Accuracy= 0.503\n",
      "Epoch: 980, Loss= 0.7098, Training Accuracy= 0.503\n",
      "Epoch: 990, Loss= 0.7097, Training Accuracy= 0.503\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5061\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.7029, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6973, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6964, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6958, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6951, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 60, Loss= 0.6939, Training Accuracy= 0.511\n",
      "Epoch: 70, Loss= 0.6935, Training Accuracy= 0.515\n",
      "Epoch: 80, Loss= 0.6932, Training Accuracy= 0.515\n",
      "Epoch: 90, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 100, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 110, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 120, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 130, Loss= 0.6925, Training Accuracy= 0.513\n",
      "Epoch: 140, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 150, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 160, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 170, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 180, Loss= 0.6933, Training Accuracy= 0.514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.6936, Training Accuracy= 0.514\n",
      "Epoch: 200, Loss= 0.6940, Training Accuracy= 0.512\n",
      "Epoch: 210, Loss= 0.6941, Training Accuracy= 0.512\n",
      "Epoch: 220, Loss= 0.6943, Training Accuracy= 0.514\n",
      "Epoch: 230, Loss= 0.6939, Training Accuracy= 0.514\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 260, Loss= 0.6990, Training Accuracy= 0.507\n",
      "Epoch: 270, Loss= 0.6937, Training Accuracy= 0.511\n",
      "Epoch: 280, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 290, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 320, Loss= 0.6928, Training Accuracy= 0.518\n",
      "Epoch: 330, Loss= 0.6927, Training Accuracy= 0.520\n",
      "Epoch: 340, Loss= 0.6927, Training Accuracy= 0.520\n",
      "Epoch: 350, Loss= 0.6934, Training Accuracy= 0.515\n",
      "Epoch: 360, Loss= 0.6931, Training Accuracy= 0.519\n",
      "Epoch: 370, Loss= 0.6932, Training Accuracy= 0.516\n",
      "Epoch: 380, Loss= 0.6939, Training Accuracy= 0.518\n",
      "Epoch: 390, Loss= 0.6937, Training Accuracy= 0.516\n",
      "Epoch: 400, Loss= 0.6938, Training Accuracy= 0.520\n",
      "Epoch: 410, Loss= 0.6942, Training Accuracy= 0.519\n",
      "Epoch: 420, Loss= 0.6940, Training Accuracy= 0.517\n",
      "Epoch: 430, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 440, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 450, Loss= 0.6937, Training Accuracy= 0.523\n",
      "Epoch: 460, Loss= 0.6926, Training Accuracy= 0.521\n",
      "Epoch: 470, Loss= 0.6932, Training Accuracy= 0.515\n",
      "Epoch: 480, Loss= 0.6928, Training Accuracy= 0.517\n",
      "Epoch: 490, Loss= 0.6935, Training Accuracy= 0.500\n",
      "Epoch: 500, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 510, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 520, Loss= 0.6942, Training Accuracy= 0.509\n",
      "Epoch: 530, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 540, Loss= 0.6918, Training Accuracy= 0.517\n",
      "Epoch: 550, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 560, Loss= 0.6968, Training Accuracy= 0.504\n",
      "Epoch: 570, Loss= 0.7002, Training Accuracy= 0.501\n",
      "Epoch: 580, Loss= 0.6990, Training Accuracy= 0.500\n",
      "Epoch: 590, Loss= 0.6983, Training Accuracy= 0.506\n",
      "Epoch: 600, Loss= 0.6983, Training Accuracy= 0.509\n",
      "Epoch: 610, Loss= 0.6983, Training Accuracy= 0.510\n",
      "Epoch: 620, Loss= 0.6984, Training Accuracy= 0.510\n",
      "Epoch: 630, Loss= 0.6984, Training Accuracy= 0.512\n",
      "Epoch: 640, Loss= 0.6983, Training Accuracy= 0.513\n",
      "Epoch: 650, Loss= 0.6982, Training Accuracy= 0.511\n",
      "Epoch: 660, Loss= 0.6981, Training Accuracy= 0.512\n",
      "Epoch: 670, Loss= 0.6980, Training Accuracy= 0.512\n",
      "Epoch: 680, Loss= 0.6980, Training Accuracy= 0.512\n",
      "Epoch: 690, Loss= 0.6978, Training Accuracy= 0.511\n",
      "Epoch: 700, Loss= 0.6976, Training Accuracy= 0.511\n",
      "Epoch: 710, Loss= 0.6970, Training Accuracy= 0.512\n",
      "Epoch: 720, Loss= 0.6968, Training Accuracy= 0.512\n",
      "Epoch: 730, Loss= 0.6954, Training Accuracy= 0.512\n",
      "Epoch: 740, Loss= 0.6976, Training Accuracy= 0.500\n",
      "Epoch: 750, Loss= 0.6966, Training Accuracy= 0.500\n",
      "Epoch: 760, Loss= 0.6962, Training Accuracy= 0.500\n",
      "Epoch: 770, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 780, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 790, Loss= 0.6960, Training Accuracy= 0.500\n",
      "Epoch: 800, Loss= 0.6963, Training Accuracy= 0.500\n",
      "Epoch: 810, Loss= 0.6965, Training Accuracy= 0.504\n",
      "Epoch: 820, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 830, Loss= 0.6951, Training Accuracy= 0.508\n",
      "Epoch: 840, Loss= 0.6947, Training Accuracy= 0.509\n",
      "Epoch: 850, Loss= 0.6970, Training Accuracy= 0.500\n",
      "Epoch: 860, Loss= 0.6965, Training Accuracy= 0.500\n",
      "Epoch: 870, Loss= 0.6964, Training Accuracy= 0.500\n",
      "Epoch: 880, Loss= 0.6961, Training Accuracy= 0.500\n",
      "Epoch: 890, Loss= 0.7005, Training Accuracy= 0.500\n",
      "Epoch: 900, Loss= 0.7004, Training Accuracy= 0.500\n",
      "Epoch: 910, Loss= 0.7003, Training Accuracy= 0.500\n",
      "Epoch: 920, Loss= 0.7003, Training Accuracy= 0.500\n",
      "Epoch: 930, Loss= 0.7002, Training Accuracy= 0.500\n",
      "Epoch: 940, Loss= 0.7002, Training Accuracy= 0.500\n",
      "Epoch: 950, Loss= 0.7001, Training Accuracy= 0.500\n",
      "Epoch: 960, Loss= 0.7001, Training Accuracy= 0.500\n",
      "Epoch: 970, Loss= 0.7001, Training Accuracy= 0.500\n",
      "Epoch: 980, Loss= 0.7000, Training Accuracy= 0.500\n",
      "Epoch: 990, Loss= 0.7000, Training Accuracy= 0.500\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5115\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.6947, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 30, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 40, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 50, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 60, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 80, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 100, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 110, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 120, Loss= 0.6926, Training Accuracy= 0.517\n",
      "Epoch: 130, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 140, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 150, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 160, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 170, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 180, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 190, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 210, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 220, Loss= 0.6924, Training Accuracy= 0.521\n",
      "Epoch: 230, Loss= 0.6927, Training Accuracy= 0.520\n",
      "Epoch: 240, Loss= 0.6922, Training Accuracy= 0.520\n",
      "Epoch: 250, Loss= 0.6917, Training Accuracy= 0.527\n",
      "Epoch: 260, Loss= 0.6922, Training Accuracy= 0.517\n",
      "Epoch: 270, Loss= 0.6922, Training Accuracy= 0.524\n",
      "Epoch: 280, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 290, Loss= 0.6947, Training Accuracy= 0.495\n",
      "Epoch: 300, Loss= 0.6943, Training Accuracy= 0.495\n",
      "Epoch: 310, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 320, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 330, Loss= 0.6934, Training Accuracy= 0.500\n",
      "Epoch: 340, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 350, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 360, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 370, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 380, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 390, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 400, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 410, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 420, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 430, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 440, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 450, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 460, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 470, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 480, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 490, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 500, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 510, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 520, Loss= 0.6938, Training Accuracy= 0.505\n",
      "Epoch: 530, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 540, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 550, Loss= 0.6938, Training Accuracy= 0.510\n",
      "Epoch: 560, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 570, Loss= 0.6961, Training Accuracy= 0.502\n",
      "Epoch: 580, Loss= 0.6938, Training Accuracy= 0.494\n",
      "Epoch: 590, Loss= 0.6941, Training Accuracy= 0.501\n",
      "Epoch: 600, Loss= 0.6939, Training Accuracy= 0.499\n",
      "Epoch: 610, Loss= 0.6955, Training Accuracy= 0.506\n",
      "Epoch: 620, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 630, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 640, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 650, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 660, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 670, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 680, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 690, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 700, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 710, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 720, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 730, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 740, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 750, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 760, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 770, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 780, Loss= 0.6931, Training Accuracy= 0.504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 800, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 810, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 820, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 830, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 840, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 850, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 860, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 870, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 880, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 890, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 900, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 910, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 920, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 930, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 940, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 950, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 960, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 970, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 980, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 990, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5125\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.6991, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.6971, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.6960, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.6950, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 60, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 70, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 80, Loss= 0.6928, Training Accuracy= 0.516\n",
      "Epoch: 90, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 100, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 110, Loss= 0.6926, Training Accuracy= 0.517\n",
      "Epoch: 120, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 130, Loss= 0.6925, Training Accuracy= 0.519\n",
      "Epoch: 140, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 150, Loss= 0.6924, Training Accuracy= 0.521\n",
      "Epoch: 160, Loss= 0.6923, Training Accuracy= 0.521\n",
      "Epoch: 170, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 180, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 190, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 200, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 210, Loss= 0.6934, Training Accuracy= 0.516\n",
      "Epoch: 220, Loss= 0.6937, Training Accuracy= 0.514\n",
      "Epoch: 230, Loss= 0.6929, Training Accuracy= 0.519\n",
      "Epoch: 240, Loss= 0.6935, Training Accuracy= 0.514\n",
      "Epoch: 250, Loss= 0.6933, Training Accuracy= 0.519\n",
      "Epoch: 260, Loss= 0.6937, Training Accuracy= 0.517\n",
      "Epoch: 270, Loss= 0.6933, Training Accuracy= 0.518\n",
      "Epoch: 280, Loss= 0.6926, Training Accuracy= 0.519\n",
      "Epoch: 290, Loss= 0.6942, Training Accuracy= 0.513\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 310, Loss= 0.6934, Training Accuracy= 0.516\n",
      "Epoch: 320, Loss= 0.6932, Training Accuracy= 0.519\n",
      "Epoch: 330, Loss= 0.6927, Training Accuracy= 0.523\n",
      "Epoch: 340, Loss= 0.6932, Training Accuracy= 0.517\n",
      "Epoch: 350, Loss= 0.6928, Training Accuracy= 0.524\n",
      "Epoch: 360, Loss= 0.6930, Training Accuracy= 0.522\n",
      "Epoch: 370, Loss= 0.6931, Training Accuracy= 0.517\n",
      "Epoch: 380, Loss= 0.6928, Training Accuracy= 0.522\n",
      "Epoch: 390, Loss= 0.6931, Training Accuracy= 0.521\n",
      "Epoch: 400, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 410, Loss= 0.6946, Training Accuracy= 0.513\n",
      "Epoch: 420, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Epoch: 430, Loss= 0.6934, Training Accuracy= 0.517\n",
      "Epoch: 440, Loss= 0.6933, Training Accuracy= 0.518\n",
      "Epoch: 450, Loss= 0.6941, Training Accuracy= 0.515\n",
      "Epoch: 460, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 470, Loss= 0.6939, Training Accuracy= 0.518\n",
      "Epoch: 480, Loss= 0.6945, Training Accuracy= 0.517\n",
      "Epoch: 490, Loss= 0.6931, Training Accuracy= 0.521\n",
      "Epoch: 500, Loss= 0.6948, Training Accuracy= 0.515\n",
      "Epoch: 510, Loss= 0.6929, Training Accuracy= 0.519\n",
      "Epoch: 520, Loss= 0.6981, Training Accuracy= 0.501\n",
      "Epoch: 530, Loss= 0.6947, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.6945, Training Accuracy= 0.519\n",
      "Epoch: 550, Loss= 0.6946, Training Accuracy= 0.506\n",
      "Epoch: 560, Loss= 0.6937, Training Accuracy= 0.514\n",
      "Epoch: 570, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 580, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 590, Loss= 0.6926, Training Accuracy= 0.526\n",
      "Epoch: 600, Loss= 0.6928, Training Accuracy= 0.526\n",
      "Epoch: 610, Loss= 0.6918, Training Accuracy= 0.521\n",
      "Epoch: 620, Loss= 0.6926, Training Accuracy= 0.521\n",
      "Epoch: 630, Loss= 0.6916, Training Accuracy= 0.526\n",
      "Epoch: 640, Loss= 0.7016, Training Accuracy= 0.507\n",
      "Epoch: 650, Loss= 0.6973, Training Accuracy= 0.515\n",
      "Epoch: 660, Loss= 0.6950, Training Accuracy= 0.501\n",
      "Epoch: 670, Loss= 0.7004, Training Accuracy= 0.504\n",
      "Epoch: 680, Loss= 0.7188, Training Accuracy= 0.506\n",
      "Epoch: 690, Loss= 0.7110, Training Accuracy= 0.501\n",
      "Epoch: 700, Loss= 0.7067, Training Accuracy= 0.501\n",
      "Epoch: 710, Loss= 0.7037, Training Accuracy= 0.501\n",
      "Epoch: 720, Loss= 0.7016, Training Accuracy= 0.501\n",
      "Epoch: 730, Loss= 0.6985, Training Accuracy= 0.501\n",
      "Epoch: 740, Loss= 0.6975, Training Accuracy= 0.501\n",
      "Epoch: 750, Loss= 0.6957, Training Accuracy= 0.501\n",
      "Epoch: 760, Loss= 0.6983, Training Accuracy= 0.501\n",
      "Epoch: 770, Loss= 0.7000, Training Accuracy= 0.501\n",
      "Epoch: 780, Loss= 0.6968, Training Accuracy= 0.501\n",
      "Epoch: 790, Loss= 0.7107, Training Accuracy= 0.498\n",
      "Epoch: 800, Loss= 0.7121, Training Accuracy= 0.498\n",
      "Epoch: 810, Loss= 0.7127, Training Accuracy= 0.496\n",
      "Epoch: 820, Loss= 0.7127, Training Accuracy= 0.497\n",
      "Epoch: 830, Loss= 0.7115, Training Accuracy= 0.497\n",
      "Epoch: 840, Loss= 0.7120, Training Accuracy= 0.495\n",
      "Epoch: 850, Loss= 0.7128, Training Accuracy= 0.498\n",
      "Epoch: 860, Loss= 0.7082, Training Accuracy= 0.495\n",
      "Epoch: 870, Loss= 0.7150, Training Accuracy= 0.495\n",
      "Epoch: 880, Loss= 0.7152, Training Accuracy= 0.495\n",
      "Epoch: 890, Loss= 0.7155, Training Accuracy= 0.495\n",
      "Epoch: 900, Loss= 0.7162, Training Accuracy= 0.495\n",
      "Epoch: 910, Loss= 0.7055, Training Accuracy= 0.501\n",
      "Epoch: 920, Loss= 0.7056, Training Accuracy= 0.501\n",
      "Epoch: 930, Loss= 0.7056, Training Accuracy= 0.501\n",
      "Epoch: 940, Loss= 0.7056, Training Accuracy= 0.501\n",
      "Epoch: 950, Loss= 0.7054, Training Accuracy= 0.501\n",
      "Epoch: 960, Loss= 0.6955, Training Accuracy= 0.495\n",
      "Epoch: 970, Loss= 0.7024, Training Accuracy= 0.499\n",
      "Epoch: 980, Loss= 0.7011, Training Accuracy= 0.497\n",
      "Epoch: 990, Loss= 0.7011, Training Accuracy= 0.497\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4966\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.7108, Training Accuracy= 0.509\n",
      "Epoch: 10, Loss= 0.7047, Training Accuracy= 0.509\n",
      "Epoch: 20, Loss= 0.7032, Training Accuracy= 0.509\n",
      "Epoch: 30, Loss= 0.7020, Training Accuracy= 0.509\n",
      "Epoch: 40, Loss= 0.7006, Training Accuracy= 0.509\n",
      "Epoch: 50, Loss= 0.6989, Training Accuracy= 0.509\n",
      "Epoch: 60, Loss= 0.6973, Training Accuracy= 0.509\n",
      "Epoch: 70, Loss= 0.6961, Training Accuracy= 0.509\n",
      "Epoch: 80, Loss= 0.6952, Training Accuracy= 0.509\n",
      "Epoch: 90, Loss= 0.6948, Training Accuracy= 0.509\n",
      "Epoch: 100, Loss= 0.6946, Training Accuracy= 0.509\n",
      "Epoch: 110, Loss= 0.6945, Training Accuracy= 0.510\n",
      "Epoch: 120, Loss= 0.6944, Training Accuracy= 0.510\n",
      "Epoch: 130, Loss= 0.6943, Training Accuracy= 0.509\n",
      "Epoch: 140, Loss= 0.6943, Training Accuracy= 0.509\n",
      "Epoch: 150, Loss= 0.6942, Training Accuracy= 0.509\n",
      "Epoch: 160, Loss= 0.6942, Training Accuracy= 0.510\n",
      "Epoch: 170, Loss= 0.6941, Training Accuracy= 0.511\n",
      "Epoch: 180, Loss= 0.6941, Training Accuracy= 0.510\n",
      "Epoch: 190, Loss= 0.6939, Training Accuracy= 0.511\n",
      "Epoch: 200, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 210, Loss= 0.6936, Training Accuracy= 0.511\n",
      "Epoch: 220, Loss= 0.6935, Training Accuracy= 0.513\n",
      "Epoch: 230, Loss= 0.6938, Training Accuracy= 0.516\n",
      "Epoch: 240, Loss= 0.6943, Training Accuracy= 0.515\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.520\n",
      "Epoch: 260, Loss= 0.6947, Training Accuracy= 0.515\n",
      "Epoch: 270, Loss= 0.6933, Training Accuracy= 0.519\n",
      "Epoch: 280, Loss= 0.6955, Training Accuracy= 0.518\n",
      "Epoch: 290, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 300, Loss= 0.6937, Training Accuracy= 0.517\n",
      "Epoch: 310, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 320, Loss= 0.6923, Training Accuracy= 0.525\n",
      "Epoch: 330, Loss= 0.6927, Training Accuracy= 0.521\n",
      "Epoch: 340, Loss= 0.6984, Training Accuracy= 0.495\n",
      "Epoch: 350, Loss= 0.6971, Training Accuracy= 0.508\n",
      "Epoch: 360, Loss= 0.6971, Training Accuracy= 0.513\n",
      "Epoch: 370, Loss= 0.6971, Training Accuracy= 0.513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6967, Training Accuracy= 0.512\n",
      "Epoch: 390, Loss= 0.6962, Training Accuracy= 0.512\n",
      "Epoch: 400, Loss= 0.6959, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6964, Training Accuracy= 0.512\n",
      "Epoch: 420, Loss= 0.6968, Training Accuracy= 0.510\n",
      "Epoch: 430, Loss= 0.6971, Training Accuracy= 0.513\n",
      "Epoch: 440, Loss= 0.6971, Training Accuracy= 0.516\n",
      "Epoch: 450, Loss= 0.6948, Training Accuracy= 0.512\n",
      "Epoch: 460, Loss= 0.6938, Training Accuracy= 0.513\n",
      "Epoch: 470, Loss= 0.6939, Training Accuracy= 0.514\n",
      "Epoch: 480, Loss= 0.6941, Training Accuracy= 0.513\n",
      "Epoch: 490, Loss= 0.6938, Training Accuracy= 0.515\n",
      "Epoch: 500, Loss= 0.6938, Training Accuracy= 0.515\n",
      "Epoch: 510, Loss= 0.7003, Training Accuracy= 0.509\n",
      "Epoch: 520, Loss= 0.6999, Training Accuracy= 0.509\n",
      "Epoch: 530, Loss= 0.6997, Training Accuracy= 0.509\n",
      "Epoch: 540, Loss= 0.6994, Training Accuracy= 0.509\n",
      "Epoch: 550, Loss= 0.6993, Training Accuracy= 0.509\n",
      "Epoch: 560, Loss= 0.6992, Training Accuracy= 0.509\n",
      "Epoch: 570, Loss= 0.6998, Training Accuracy= 0.509\n",
      "Epoch: 580, Loss= 0.6998, Training Accuracy= 0.509\n",
      "Epoch: 590, Loss= 0.6994, Training Accuracy= 0.509\n",
      "Epoch: 600, Loss= 0.6995, Training Accuracy= 0.509\n",
      "Epoch: 610, Loss= 0.6992, Training Accuracy= 0.509\n",
      "Epoch: 620, Loss= 0.6987, Training Accuracy= 0.509\n",
      "Epoch: 630, Loss= 0.6983, Training Accuracy= 0.509\n",
      "Epoch: 640, Loss= 0.6978, Training Accuracy= 0.509\n",
      "Epoch: 650, Loss= 0.6977, Training Accuracy= 0.509\n",
      "Epoch: 660, Loss= 0.6968, Training Accuracy= 0.509\n",
      "Epoch: 670, Loss= 0.6964, Training Accuracy= 0.509\n",
      "Epoch: 680, Loss= 0.6965, Training Accuracy= 0.509\n",
      "Epoch: 690, Loss= 0.6963, Training Accuracy= 0.509\n",
      "Epoch: 700, Loss= 0.6963, Training Accuracy= 0.509\n",
      "Epoch: 710, Loss= 0.6965, Training Accuracy= 0.509\n",
      "Epoch: 720, Loss= 0.6963, Training Accuracy= 0.509\n",
      "Epoch: 730, Loss= 0.6965, Training Accuracy= 0.509\n",
      "Epoch: 740, Loss= 0.6964, Training Accuracy= 0.509\n",
      "Epoch: 750, Loss= 0.6963, Training Accuracy= 0.509\n",
      "Epoch: 760, Loss= 0.6963, Training Accuracy= 0.509\n",
      "Epoch: 770, Loss= 0.6963, Training Accuracy= 0.509\n",
      "Epoch: 780, Loss= 0.6946, Training Accuracy= 0.506\n",
      "Epoch: 790, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.6938, Training Accuracy= 0.505\n",
      "Epoch: 810, Loss= 0.6941, Training Accuracy= 0.510\n",
      "Epoch: 820, Loss= 0.6941, Training Accuracy= 0.508\n",
      "Epoch: 830, Loss= 0.6944, Training Accuracy= 0.509\n",
      "Epoch: 840, Loss= 0.6940, Training Accuracy= 0.513\n",
      "Epoch: 850, Loss= 0.6938, Training Accuracy= 0.512\n",
      "Epoch: 860, Loss= 0.6941, Training Accuracy= 0.511\n",
      "Epoch: 870, Loss= 0.6941, Training Accuracy= 0.511\n",
      "Epoch: 880, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 890, Loss= 0.6939, Training Accuracy= 0.510\n",
      "Epoch: 900, Loss= 0.6939, Training Accuracy= 0.512\n",
      "Epoch: 910, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Epoch: 920, Loss= 0.6938, Training Accuracy= 0.510\n",
      "Epoch: 930, Loss= 0.6940, Training Accuracy= 0.510\n",
      "Epoch: 940, Loss= 0.6947, Training Accuracy= 0.509\n",
      "Epoch: 950, Loss= 0.6939, Training Accuracy= 0.512\n",
      "Epoch: 960, Loss= 0.6942, Training Accuracy= 0.511\n",
      "Epoch: 970, Loss= 0.6936, Training Accuracy= 0.516\n",
      "Epoch: 980, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 990, Loss= 0.7010, Training Accuracy= 0.509\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.498\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.7058, Training Accuracy= 0.505\n",
      "Epoch: 10, Loss= 0.6963, Training Accuracy= 0.505\n",
      "Epoch: 20, Loss= 0.6950, Training Accuracy= 0.505\n",
      "Epoch: 30, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 40, Loss= 0.6941, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 60, Loss= 0.6938, Training Accuracy= 0.505\n",
      "Epoch: 70, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 90, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 100, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 110, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 130, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 150, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 160, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 170, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 180, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 190, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 200, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 210, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 220, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 230, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 240, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 250, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 260, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 270, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 280, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 290, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 300, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 310, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 320, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 330, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 340, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 350, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 360, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 370, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 380, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 390, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 400, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 410, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 420, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 430, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 440, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 450, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 460, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 470, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 480, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 490, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 500, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 510, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 520, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 530, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 540, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 550, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 560, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 570, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 580, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 590, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 600, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 610, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 620, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 630, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 640, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 650, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 660, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 670, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 680, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 690, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 700, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 710, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 720, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 730, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 740, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 750, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 760, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 770, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 780, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 790, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 800, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 810, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 820, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 830, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 840, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 850, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 860, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 870, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 880, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 890, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 900, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 910, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 920, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 930, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 940, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 950, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 960, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 970, Loss= 0.6934, Training Accuracy= 0.505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 990, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4937\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.7183, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.7009, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.6960, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.6953, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6952, Training Accuracy= 0.498\n",
      "Epoch: 50, Loss= 0.6951, Training Accuracy= 0.498\n",
      "Epoch: 60, Loss= 0.6950, Training Accuracy= 0.498\n",
      "Epoch: 70, Loss= 0.6950, Training Accuracy= 0.498\n",
      "Epoch: 80, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 90, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 130, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 140, Loss= 0.6948, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.6949, Training Accuracy= 0.502\n",
      "Epoch: 160, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6950, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.6949, Training Accuracy= 0.507\n",
      "Epoch: 200, Loss= 0.6947, Training Accuracy= 0.510\n",
      "Epoch: 210, Loss= 0.7132, Training Accuracy= 0.498\n",
      "Epoch: 220, Loss= 0.6948, Training Accuracy= 0.507\n",
      "Epoch: 230, Loss= 0.6949, Training Accuracy= 0.507\n",
      "Epoch: 240, Loss= 0.6946, Training Accuracy= 0.509\n",
      "Epoch: 250, Loss= 0.6944, Training Accuracy= 0.514\n",
      "Epoch: 260, Loss= 0.6945, Training Accuracy= 0.517\n",
      "Epoch: 270, Loss= 0.6940, Training Accuracy= 0.513\n",
      "Epoch: 280, Loss= 0.7208, Training Accuracy= 0.498\n",
      "Epoch: 290, Loss= 0.6934, Training Accuracy= 0.511\n",
      "Epoch: 300, Loss= 0.6943, Training Accuracy= 0.502\n",
      "Epoch: 310, Loss= 0.6943, Training Accuracy= 0.511\n",
      "Epoch: 320, Loss= 0.7199, Training Accuracy= 0.498\n",
      "Epoch: 330, Loss= 0.6952, Training Accuracy= 0.511\n",
      "Epoch: 340, Loss= 0.6932, Training Accuracy= 0.517\n",
      "Epoch: 350, Loss= 0.6939, Training Accuracy= 0.509\n",
      "Epoch: 360, Loss= 0.6935, Training Accuracy= 0.515\n",
      "Epoch: 370, Loss= 0.6944, Training Accuracy= 0.511\n",
      "Epoch: 380, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 390, Loss= 0.7202, Training Accuracy= 0.498\n",
      "Epoch: 400, Loss= 0.7152, Training Accuracy= 0.498\n",
      "Epoch: 410, Loss= 0.6937, Training Accuracy= 0.514\n",
      "Epoch: 420, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 430, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 450, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 460, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 470, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 490, Loss= 0.6977, Training Accuracy= 0.497\n",
      "Epoch: 500, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.7047, Training Accuracy= 0.498\n",
      "Epoch: 520, Loss= 0.7035, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.7029, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.7027, Training Accuracy= 0.498\n",
      "Epoch: 550, Loss= 0.7026, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.7025, Training Accuracy= 0.498\n",
      "Epoch: 570, Loss= 0.7025, Training Accuracy= 0.498\n",
      "Epoch: 580, Loss= 0.7029, Training Accuracy= 0.498\n",
      "Epoch: 590, Loss= 0.7034, Training Accuracy= 0.498\n",
      "Epoch: 600, Loss= 0.7034, Training Accuracy= 0.498\n",
      "Epoch: 610, Loss= 0.7036, Training Accuracy= 0.498\n",
      "Epoch: 620, Loss= 0.7041, Training Accuracy= 0.498\n",
      "Epoch: 630, Loss= 0.7040, Training Accuracy= 0.498\n",
      "Epoch: 640, Loss= 0.7038, Training Accuracy= 0.499\n",
      "Epoch: 650, Loss= 0.7038, Training Accuracy= 0.497\n",
      "Epoch: 660, Loss= 0.7025, Training Accuracy= 0.499\n",
      "Epoch: 670, Loss= 0.6996, Training Accuracy= 0.500\n",
      "Epoch: 680, Loss= 0.6996, Training Accuracy= 0.499\n",
      "Epoch: 690, Loss= 0.7006, Training Accuracy= 0.500\n",
      "Epoch: 700, Loss= 0.7024, Training Accuracy= 0.500\n",
      "Epoch: 710, Loss= 0.7014, Training Accuracy= 0.500\n",
      "Epoch: 720, Loss= 0.6990, Training Accuracy= 0.500\n",
      "Epoch: 730, Loss= 0.6989, Training Accuracy= 0.499\n",
      "Epoch: 740, Loss= 0.6973, Training Accuracy= 0.500\n",
      "Epoch: 750, Loss= 0.7010, Training Accuracy= 0.504\n",
      "Epoch: 760, Loss= 0.6979, Training Accuracy= 0.497\n",
      "Epoch: 770, Loss= 0.6964, Training Accuracy= 0.497\n",
      "Epoch: 780, Loss= 0.6955, Training Accuracy= 0.498\n",
      "Epoch: 790, Loss= 0.6945, Training Accuracy= 0.498\n",
      "Epoch: 800, Loss= 0.6980, Training Accuracy= 0.496\n",
      "Epoch: 810, Loss= 0.6979, Training Accuracy= 0.497\n",
      "Epoch: 820, Loss= 0.6976, Training Accuracy= 0.497\n",
      "Epoch: 830, Loss= 0.6981, Training Accuracy= 0.498\n",
      "Epoch: 840, Loss= 0.6985, Training Accuracy= 0.499\n",
      "Epoch: 850, Loss= 0.6994, Training Accuracy= 0.496\n",
      "Epoch: 860, Loss= 0.7012, Training Accuracy= 0.498\n",
      "Epoch: 870, Loss= 0.7014, Training Accuracy= 0.498\n",
      "Epoch: 880, Loss= 0.7009, Training Accuracy= 0.498\n",
      "Epoch: 890, Loss= 0.6988, Training Accuracy= 0.502\n",
      "Epoch: 900, Loss= 0.6962, Training Accuracy= 0.498\n",
      "Epoch: 910, Loss= 0.6961, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.6961, Training Accuracy= 0.499\n",
      "Epoch: 930, Loss= 0.6962, Training Accuracy= 0.502\n",
      "Epoch: 940, Loss= 0.6965, Training Accuracy= 0.498\n",
      "Epoch: 950, Loss= 0.6966, Training Accuracy= 0.498\n",
      "Epoch: 960, Loss= 0.6962, Training Accuracy= 0.498\n",
      "Epoch: 970, Loss= 0.6963, Training Accuracy= 0.499\n",
      "Epoch: 980, Loss= 0.6961, Training Accuracy= 0.498\n",
      "Epoch: 990, Loss= 0.6976, Training Accuracy= 0.510\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5091\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7128, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6952, Training Accuracy= 0.513\n",
      "Epoch: 20, Loss= 0.6937, Training Accuracy= 0.516\n",
      "Epoch: 30, Loss= 0.6930, Training Accuracy= 0.520\n",
      "Epoch: 40, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 50, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 60, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 70, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 80, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 90, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 100, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 110, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 120, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 130, Loss= 0.6925, Training Accuracy= 0.513\n",
      "Epoch: 140, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 150, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 160, Loss= 0.6922, Training Accuracy= 0.514\n",
      "Epoch: 170, Loss= 0.6922, Training Accuracy= 0.517\n",
      "Epoch: 180, Loss= 0.6921, Training Accuracy= 0.519\n",
      "Epoch: 190, Loss= 0.6920, Training Accuracy= 0.519\n",
      "Epoch: 200, Loss= 0.6919, Training Accuracy= 0.522\n",
      "Epoch: 210, Loss= 0.6918, Training Accuracy= 0.525\n",
      "Epoch: 220, Loss= 0.6916, Training Accuracy= 0.527\n",
      "Epoch: 230, Loss= 0.6916, Training Accuracy= 0.528\n",
      "Epoch: 240, Loss= 0.6915, Training Accuracy= 0.526\n",
      "Epoch: 250, Loss= 0.6915, Training Accuracy= 0.525\n",
      "Epoch: 260, Loss= 0.6916, Training Accuracy= 0.525\n",
      "Epoch: 270, Loss= 0.6916, Training Accuracy= 0.524\n",
      "Epoch: 280, Loss= 0.6916, Training Accuracy= 0.524\n",
      "Epoch: 290, Loss= 0.6916, Training Accuracy= 0.523\n",
      "Epoch: 300, Loss= 0.6916, Training Accuracy= 0.524\n",
      "Epoch: 310, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 320, Loss= 0.6917, Training Accuracy= 0.518\n",
      "Epoch: 330, Loss= 0.6917, Training Accuracy= 0.520\n",
      "Epoch: 340, Loss= 0.6916, Training Accuracy= 0.519\n",
      "Epoch: 350, Loss= 0.6916, Training Accuracy= 0.518\n",
      "Epoch: 360, Loss= 0.6908, Training Accuracy= 0.525\n",
      "Epoch: 370, Loss= 0.6911, Training Accuracy= 0.521\n",
      "Epoch: 380, Loss= 0.6911, Training Accuracy= 0.522\n",
      "Epoch: 390, Loss= 0.6907, Training Accuracy= 0.523\n",
      "Epoch: 400, Loss= 0.6912, Training Accuracy= 0.524\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 420, Loss= 0.6926, Training Accuracy= 0.517\n",
      "Epoch: 430, Loss= 0.6919, Training Accuracy= 0.524\n",
      "Epoch: 440, Loss= 0.6921, Training Accuracy= 0.519\n",
      "Epoch: 450, Loss= 0.6916, Training Accuracy= 0.523\n",
      "Epoch: 460, Loss= 0.6916, Training Accuracy= 0.523\n",
      "Epoch: 470, Loss= 0.6926, Training Accuracy= 0.518\n",
      "Epoch: 480, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 490, Loss= 0.6923, Training Accuracy= 0.521\n",
      "Epoch: 500, Loss= 0.6916, Training Accuracy= 0.522\n",
      "Epoch: 510, Loss= 0.6913, Training Accuracy= 0.525\n",
      "Epoch: 520, Loss= 0.6916, Training Accuracy= 0.520\n",
      "Epoch: 530, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 550, Loss= 0.6915, Training Accuracy= 0.523\n",
      "Epoch: 560, Loss= 0.6918, Training Accuracy= 0.510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.6914, Training Accuracy= 0.525\n",
      "Epoch: 580, Loss= 0.6915, Training Accuracy= 0.524\n",
      "Epoch: 590, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 600, Loss= 0.6916, Training Accuracy= 0.523\n",
      "Epoch: 610, Loss= 0.6912, Training Accuracy= 0.527\n",
      "Epoch: 620, Loss= 0.6924, Training Accuracy= 0.525\n",
      "Epoch: 630, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 640, Loss= 0.6933, Training Accuracy= 0.515\n",
      "Epoch: 650, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 660, Loss= 0.6940, Training Accuracy= 0.517\n",
      "Epoch: 670, Loss= 0.6934, Training Accuracy= 0.511\n",
      "Epoch: 680, Loss= 0.6944, Training Accuracy= 0.501\n",
      "Epoch: 690, Loss= 0.6929, Training Accuracy= 0.521\n",
      "Epoch: 700, Loss= 0.6945, Training Accuracy= 0.515\n",
      "Epoch: 710, Loss= 0.6929, Training Accuracy= 0.518\n",
      "Epoch: 720, Loss= 0.6934, Training Accuracy= 0.516\n",
      "Epoch: 730, Loss= 0.6919, Training Accuracy= 0.527\n",
      "Epoch: 740, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 750, Loss= 0.6919, Training Accuracy= 0.520\n",
      "Epoch: 760, Loss= 0.6944, Training Accuracy= 0.498\n",
      "Epoch: 770, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 780, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 790, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 800, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 810, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 820, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 830, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 840, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 850, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 860, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 870, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 880, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 890, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 900, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 910, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 920, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 930, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 940, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 950, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 960, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 970, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 980, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 990, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4914\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.3\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.4971, 0.48410001, 0.5061, 0.5115, 0.51249999, 0.4966, 0.498, 0.4937, 0.50910002, 0.4914]\n",
      "mean of test_accuracies_10replications:  0.50001\n",
      "standard deviation of test_accuracies_10replications_std_mean:  8.93839262426e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMDOzHnHiMDOzHvn/CvxivooBTWgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f603a78d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
