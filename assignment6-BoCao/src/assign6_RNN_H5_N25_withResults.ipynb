{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 5\n",
    "N = 25\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.7010, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.6978, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6967, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.6962, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6959, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6958, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6957, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6956, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6955, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6955, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6958, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6960, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6964, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6967, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6968, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.6967, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6965, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6964, Training Accuracy= 0.504\n",
      "Epoch: 180, Loss= 0.6963, Training Accuracy= 0.504\n",
      "Epoch: 190, Loss= 0.6964, Training Accuracy= 0.503\n",
      "Epoch: 200, Loss= 0.6965, Training Accuracy= 0.504\n",
      "Epoch: 210, Loss= 0.6960, Training Accuracy= 0.505\n",
      "Epoch: 220, Loss= 0.6955, Training Accuracy= 0.506\n",
      "Epoch: 230, Loss= 0.6952, Training Accuracy= 0.508\n",
      "Epoch: 240, Loss= 0.6950, Training Accuracy= 0.508\n",
      "Epoch: 250, Loss= 0.6949, Training Accuracy= 0.507\n",
      "Epoch: 260, Loss= 0.6949, Training Accuracy= 0.510\n",
      "Epoch: 270, Loss= 0.6950, Training Accuracy= 0.511\n",
      "Epoch: 280, Loss= 0.6950, Training Accuracy= 0.512\n",
      "Epoch: 290, Loss= 0.6951, Training Accuracy= 0.514\n",
      "Epoch: 300, Loss= 0.6950, Training Accuracy= 0.514\n",
      "Epoch: 310, Loss= 0.6949, Training Accuracy= 0.514\n",
      "Epoch: 320, Loss= 0.6950, Training Accuracy= 0.515\n",
      "Epoch: 330, Loss= 0.6949, Training Accuracy= 0.515\n",
      "Epoch: 340, Loss= 0.6947, Training Accuracy= 0.516\n",
      "Epoch: 350, Loss= 0.6945, Training Accuracy= 0.515\n",
      "Epoch: 360, Loss= 0.6947, Training Accuracy= 0.516\n",
      "Epoch: 370, Loss= 0.6951, Training Accuracy= 0.516\n",
      "Epoch: 380, Loss= 0.6952, Training Accuracy= 0.519\n",
      "Epoch: 390, Loss= 0.6950, Training Accuracy= 0.518\n",
      "Epoch: 400, Loss= 0.6951, Training Accuracy= 0.518\n",
      "Epoch: 410, Loss= 0.6953, Training Accuracy= 0.517\n",
      "Epoch: 420, Loss= 0.6951, Training Accuracy= 0.515\n",
      "Epoch: 430, Loss= 0.6952, Training Accuracy= 0.517\n",
      "Epoch: 440, Loss= 0.6949, Training Accuracy= 0.520\n",
      "Epoch: 450, Loss= 0.6947, Training Accuracy= 0.520\n",
      "Epoch: 460, Loss= 0.6951, Training Accuracy= 0.519\n",
      "Epoch: 470, Loss= 0.6947, Training Accuracy= 0.519\n",
      "Epoch: 480, Loss= 0.6945, Training Accuracy= 0.519\n",
      "Epoch: 490, Loss= 0.6944, Training Accuracy= 0.519\n",
      "Epoch: 500, Loss= 0.6938, Training Accuracy= 0.520\n",
      "Epoch: 510, Loss= 0.6942, Training Accuracy= 0.519\n",
      "Epoch: 520, Loss= 0.6940, Training Accuracy= 0.519\n",
      "Epoch: 530, Loss= 0.6939, Training Accuracy= 0.519\n",
      "Epoch: 540, Loss= 0.6939, Training Accuracy= 0.521\n",
      "Epoch: 550, Loss= 0.6941, Training Accuracy= 0.519\n",
      "Epoch: 560, Loss= 0.6935, Training Accuracy= 0.522\n",
      "Epoch: 570, Loss= 0.6931, Training Accuracy= 0.524\n",
      "Epoch: 580, Loss= 0.6947, Training Accuracy= 0.523\n",
      "Epoch: 590, Loss= 0.6931, Training Accuracy= 0.526\n",
      "Epoch: 600, Loss= 0.6932, Training Accuracy= 0.525\n",
      "Epoch: 610, Loss= 0.6981, Training Accuracy= 0.515\n",
      "Epoch: 620, Loss= 0.6938, Training Accuracy= 0.521\n",
      "Epoch: 630, Loss= 0.6932, Training Accuracy= 0.523\n",
      "Epoch: 640, Loss= 0.6932, Training Accuracy= 0.524\n",
      "Epoch: 650, Loss= 0.6936, Training Accuracy= 0.522\n",
      "Epoch: 660, Loss= 0.6927, Training Accuracy= 0.525\n",
      "Epoch: 670, Loss= 0.6945, Training Accuracy= 0.523\n",
      "Epoch: 680, Loss= 0.6941, Training Accuracy= 0.520\n",
      "Epoch: 690, Loss= 0.6937, Training Accuracy= 0.522\n",
      "Epoch: 700, Loss= 0.6934, Training Accuracy= 0.522\n",
      "Epoch: 710, Loss= 0.6928, Training Accuracy= 0.523\n",
      "Epoch: 720, Loss= 0.6946, Training Accuracy= 0.519\n",
      "Epoch: 730, Loss= 0.6937, Training Accuracy= 0.521\n",
      "Epoch: 740, Loss= 0.6931, Training Accuracy= 0.524\n",
      "Epoch: 750, Loss= 0.6935, Training Accuracy= 0.524\n",
      "Epoch: 760, Loss= 0.6934, Training Accuracy= 0.523\n",
      "Epoch: 770, Loss= 0.6940, Training Accuracy= 0.521\n",
      "Epoch: 780, Loss= 0.6944, Training Accuracy= 0.517\n",
      "Epoch: 790, Loss= 0.6943, Training Accuracy= 0.518\n",
      "Epoch: 800, Loss= 0.6948, Training Accuracy= 0.515\n",
      "Epoch: 810, Loss= 0.6937, Training Accuracy= 0.519\n",
      "Epoch: 820, Loss= 0.6945, Training Accuracy= 0.519\n",
      "Epoch: 830, Loss= 0.6938, Training Accuracy= 0.520\n",
      "Epoch: 840, Loss= 0.6963, Training Accuracy= 0.512\n",
      "Epoch: 850, Loss= 0.6952, Training Accuracy= 0.517\n",
      "Epoch: 860, Loss= 0.6954, Training Accuracy= 0.518\n",
      "Epoch: 870, Loss= 0.6939, Training Accuracy= 0.520\n",
      "Epoch: 880, Loss= 0.6958, Training Accuracy= 0.515\n",
      "Epoch: 890, Loss= 0.6945, Training Accuracy= 0.521\n",
      "Epoch: 900, Loss= 0.6951, Training Accuracy= 0.520\n",
      "Epoch: 910, Loss= 0.6940, Training Accuracy= 0.519\n",
      "Epoch: 920, Loss= 0.6946, Training Accuracy= 0.517\n",
      "Epoch: 930, Loss= 0.6951, Training Accuracy= 0.518\n",
      "Epoch: 940, Loss= 0.6964, Training Accuracy= 0.517\n",
      "Epoch: 950, Loss= 0.6938, Training Accuracy= 0.522\n",
      "Epoch: 960, Loss= 0.6957, Training Accuracy= 0.518\n",
      "Epoch: 970, Loss= 0.6958, Training Accuracy= 0.519\n",
      "Epoch: 980, Loss= 0.6945, Training Accuracy= 0.521\n",
      "Epoch: 990, Loss= 0.6952, Training Accuracy= 0.519\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4983\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6946, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6944, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6943, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 110, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 120, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 130, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 140, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 150, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 160, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 190, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 220, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 250, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 260, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 270, Loss= 0.6932, Training Accuracy= 0.516\n",
      "Epoch: 280, Loss= 0.6934, Training Accuracy= 0.511\n",
      "Epoch: 290, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 310, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 320, Loss= 0.6925, Training Accuracy= 0.510\n",
      "Epoch: 330, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 340, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 350, Loss= 0.6956, Training Accuracy= 0.502\n",
      "Epoch: 360, Loss= 0.6950, Training Accuracy= 0.502\n",
      "Epoch: 370, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 380, Loss= 0.6936, Training Accuracy= 0.513\n",
      "Epoch: 390, Loss= 0.6939, Training Accuracy= 0.509\n",
      "Epoch: 400, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 410, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 420, Loss= 0.6948, Training Accuracy= 0.505\n",
      "Epoch: 430, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 440, Loss= 0.6939, Training Accuracy= 0.506\n",
      "Epoch: 450, Loss= 0.6946, Training Accuracy= 0.502\n",
      "Epoch: 460, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 470, Loss= 0.6943, Training Accuracy= 0.508\n",
      "Epoch: 480, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 490, Loss= 0.6938, Training Accuracy= 0.505\n",
      "Epoch: 500, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 520, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 530, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 540, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 550, Loss= 0.6938, Training Accuracy= 0.510\n",
      "Epoch: 560, Loss= 0.6953, Training Accuracy= 0.504\n",
      "Epoch: 570, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 580, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 590, Loss= 0.6928, Training Accuracy= 0.513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 610, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 620, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 630, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 640, Loss= 0.6934, Training Accuracy= 0.511\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 660, Loss= 0.6933, Training Accuracy= 0.516\n",
      "Epoch: 670, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 680, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 690, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 700, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 710, Loss= 0.6983, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.6982, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.6979, Training Accuracy= 0.502\n",
      "Epoch: 740, Loss= 0.6976, Training Accuracy= 0.502\n",
      "Epoch: 750, Loss= 0.6974, Training Accuracy= 0.502\n",
      "Epoch: 760, Loss= 0.6972, Training Accuracy= 0.502\n",
      "Epoch: 770, Loss= 0.6972, Training Accuracy= 0.502\n",
      "Epoch: 780, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 790, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 800, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 810, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 820, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 830, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 840, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 850, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 860, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 870, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 880, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 890, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 900, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 910, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 920, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 930, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 940, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 950, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 960, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 970, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 980, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Epoch: 990, Loss= 0.6971, Training Accuracy= 0.502\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5059\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.6962, Training Accuracy= 0.499\n",
      "Epoch: 10, Loss= 0.6957, Training Accuracy= 0.499\n",
      "Epoch: 20, Loss= 0.6955, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.6953, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.6950, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.6946, Training Accuracy= 0.499\n",
      "Epoch: 60, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 70, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 80, Loss= 0.6936, Training Accuracy= 0.499\n",
      "Epoch: 90, Loss= 0.6935, Training Accuracy= 0.499\n",
      "Epoch: 100, Loss= 0.6934, Training Accuracy= 0.499\n",
      "Epoch: 110, Loss= 0.6933, Training Accuracy= 0.499\n",
      "Epoch: 120, Loss= 0.6933, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 160, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 170, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 180, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 190, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 200, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 210, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 220, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 230, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6933, Training Accuracy= 0.502\n",
      "Epoch: 260, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 270, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 280, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 290, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 300, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 310, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 320, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 330, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 340, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 350, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 360, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 370, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 380, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 390, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 400, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 420, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 430, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 440, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 450, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 460, Loss= 0.6931, Training Accuracy= 0.514\n",
      "Epoch: 470, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 480, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 490, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 500, Loss= 0.6967, Training Accuracy= 0.501\n",
      "Epoch: 510, Loss= 0.6966, Training Accuracy= 0.500\n",
      "Epoch: 520, Loss= 0.6965, Training Accuracy= 0.500\n",
      "Epoch: 530, Loss= 0.6965, Training Accuracy= 0.502\n",
      "Epoch: 540, Loss= 0.6962, Training Accuracy= 0.502\n",
      "Epoch: 550, Loss= 0.6962, Training Accuracy= 0.503\n",
      "Epoch: 560, Loss= 0.6959, Training Accuracy= 0.504\n",
      "Epoch: 570, Loss= 0.6957, Training Accuracy= 0.505\n",
      "Epoch: 580, Loss= 0.6954, Training Accuracy= 0.506\n",
      "Epoch: 590, Loss= 0.6954, Training Accuracy= 0.507\n",
      "Epoch: 600, Loss= 0.6953, Training Accuracy= 0.505\n",
      "Epoch: 610, Loss= 0.6952, Training Accuracy= 0.505\n",
      "Epoch: 620, Loss= 0.6950, Training Accuracy= 0.507\n",
      "Epoch: 630, Loss= 0.6950, Training Accuracy= 0.507\n",
      "Epoch: 640, Loss= 0.6950, Training Accuracy= 0.506\n",
      "Epoch: 650, Loss= 0.6964, Training Accuracy= 0.506\n",
      "Epoch: 660, Loss= 0.6958, Training Accuracy= 0.510\n",
      "Epoch: 670, Loss= 0.6957, Training Accuracy= 0.509\n",
      "Epoch: 680, Loss= 0.6955, Training Accuracy= 0.511\n",
      "Epoch: 690, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 700, Loss= 0.6957, Training Accuracy= 0.512\n",
      "Epoch: 710, Loss= 0.6958, Training Accuracy= 0.510\n",
      "Epoch: 720, Loss= 0.6963, Training Accuracy= 0.503\n",
      "Epoch: 730, Loss= 0.6958, Training Accuracy= 0.511\n",
      "Epoch: 740, Loss= 0.6965, Training Accuracy= 0.511\n",
      "Epoch: 750, Loss= 0.6961, Training Accuracy= 0.508\n",
      "Epoch: 760, Loss= 0.6960, Training Accuracy= 0.509\n",
      "Epoch: 770, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 780, Loss= 0.6945, Training Accuracy= 0.505\n",
      "Epoch: 790, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 800, Loss= 0.6956, Training Accuracy= 0.499\n",
      "Epoch: 810, Loss= 0.6957, Training Accuracy= 0.499\n",
      "Epoch: 820, Loss= 0.6957, Training Accuracy= 0.499\n",
      "Epoch: 830, Loss= 0.6957, Training Accuracy= 0.499\n",
      "Epoch: 840, Loss= 0.6956, Training Accuracy= 0.499\n",
      "Epoch: 850, Loss= 0.6956, Training Accuracy= 0.499\n",
      "Epoch: 860, Loss= 0.6955, Training Accuracy= 0.499\n",
      "Epoch: 870, Loss= 0.6955, Training Accuracy= 0.499\n",
      "Epoch: 880, Loss= 0.6954, Training Accuracy= 0.499\n",
      "Epoch: 890, Loss= 0.6954, Training Accuracy= 0.499\n",
      "Epoch: 900, Loss= 0.6953, Training Accuracy= 0.499\n",
      "Epoch: 910, Loss= 0.6953, Training Accuracy= 0.499\n",
      "Epoch: 920, Loss= 0.6953, Training Accuracy= 0.499\n",
      "Epoch: 930, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 940, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 950, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 960, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 970, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 980, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 990, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5062\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.7049, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6939, Training Accuracy= 0.504\n",
      "Epoch: 50, Loss= 0.6938, Training Accuracy= 0.505\n",
      "Epoch: 60, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 70, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 90, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 110, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 160, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 230, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 270, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 280, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 290, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 300, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 310, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 320, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 330, Loss= 0.6935, Training Accuracy= 0.511\n",
      "Epoch: 340, Loss= 0.6936, Training Accuracy= 0.510\n",
      "Epoch: 350, Loss= 0.6936, Training Accuracy= 0.511\n",
      "Epoch: 360, Loss= 0.6937, Training Accuracy= 0.511\n",
      "Epoch: 370, Loss= 0.6937, Training Accuracy= 0.511\n",
      "Epoch: 380, Loss= 0.6937, Training Accuracy= 0.511\n",
      "Epoch: 390, Loss= 0.6938, Training Accuracy= 0.510\n",
      "Epoch: 400, Loss= 0.6940, Training Accuracy= 0.510\n",
      "Epoch: 410, Loss= 0.6942, Training Accuracy= 0.509\n",
      "Epoch: 420, Loss= 0.6943, Training Accuracy= 0.509\n",
      "Epoch: 430, Loss= 0.6943, Training Accuracy= 0.511\n",
      "Epoch: 440, Loss= 0.6946, Training Accuracy= 0.509\n",
      "Epoch: 450, Loss= 0.6943, Training Accuracy= 0.509\n",
      "Epoch: 460, Loss= 0.6946, Training Accuracy= 0.508\n",
      "Epoch: 470, Loss= 0.6944, Training Accuracy= 0.507\n",
      "Epoch: 480, Loss= 0.6954, Training Accuracy= 0.503\n",
      "Epoch: 490, Loss= 0.6939, Training Accuracy= 0.514\n",
      "Epoch: 500, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6936, Training Accuracy= 0.515\n",
      "Epoch: 520, Loss= 0.6955, Training Accuracy= 0.509\n",
      "Epoch: 530, Loss= 0.6952, Training Accuracy= 0.505\n",
      "Epoch: 540, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 550, Loss= 0.6941, Training Accuracy= 0.509\n",
      "Epoch: 560, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 570, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 580, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 590, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 600, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 610, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 620, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 630, Loss= 0.6931, Training Accuracy= 0.514\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 650, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 660, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 670, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 680, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 690, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 700, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 710, Loss= 0.6949, Training Accuracy= 0.507\n",
      "Epoch: 720, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Epoch: 730, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 740, Loss= 0.6931, Training Accuracy= 0.520\n",
      "Epoch: 750, Loss= 0.6935, Training Accuracy= 0.516\n",
      "Epoch: 760, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 770, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 780, Loss= 0.6934, Training Accuracy= 0.514\n",
      "Epoch: 790, Loss= 0.6935, Training Accuracy= 0.515\n",
      "Epoch: 800, Loss= 0.6938, Training Accuracy= 0.511\n",
      "Epoch: 810, Loss= 0.6940, Training Accuracy= 0.515\n",
      "Epoch: 820, Loss= 0.6944, Training Accuracy= 0.511\n",
      "Epoch: 830, Loss= 0.6950, Training Accuracy= 0.509\n",
      "Epoch: 840, Loss= 0.6954, Training Accuracy= 0.505\n",
      "Epoch: 850, Loss= 0.6952, Training Accuracy= 0.510\n",
      "Epoch: 860, Loss= 0.6955, Training Accuracy= 0.507\n",
      "Epoch: 870, Loss= 0.6951, Training Accuracy= 0.507\n",
      "Epoch: 880, Loss= 0.6971, Training Accuracy= 0.505\n",
      "Epoch: 890, Loss= 0.6955, Training Accuracy= 0.510\n",
      "Epoch: 900, Loss= 0.6935, Training Accuracy= 0.518\n",
      "Epoch: 910, Loss= 0.6942, Training Accuracy= 0.506\n",
      "Epoch: 920, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 930, Loss= 0.6952, Training Accuracy= 0.503\n",
      "Epoch: 940, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 950, Loss= 0.6956, Training Accuracy= 0.506\n",
      "Epoch: 960, Loss= 0.6983, Training Accuracy= 0.502\n",
      "Epoch: 970, Loss= 0.7238, Training Accuracy= 0.501\n",
      "Epoch: 980, Loss= 0.7216, Training Accuracy= 0.501\n",
      "Epoch: 990, Loss= 0.7189, Training Accuracy= 0.501\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4898\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.8255, Training Accuracy= 0.506\n",
      "Epoch: 10, Loss= 0.7272, Training Accuracy= 0.506\n",
      "Epoch: 20, Loss= 0.7120, Training Accuracy= 0.506\n",
      "Epoch: 30, Loss= 0.7049, Training Accuracy= 0.506\n",
      "Epoch: 40, Loss= 0.7012, Training Accuracy= 0.506\n",
      "Epoch: 50, Loss= 0.6990, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6977, Training Accuracy= 0.506\n",
      "Epoch: 70, Loss= 0.6969, Training Accuracy= 0.506\n",
      "Epoch: 80, Loss= 0.6962, Training Accuracy= 0.506\n",
      "Epoch: 90, Loss= 0.6958, Training Accuracy= 0.506\n",
      "Epoch: 100, Loss= 0.6954, Training Accuracy= 0.506\n",
      "Epoch: 110, Loss= 0.6951, Training Accuracy= 0.506\n",
      "Epoch: 120, Loss= 0.6949, Training Accuracy= 0.506\n",
      "Epoch: 130, Loss= 0.6948, Training Accuracy= 0.506\n",
      "Epoch: 140, Loss= 0.6947, Training Accuracy= 0.506\n",
      "Epoch: 150, Loss= 0.6947, Training Accuracy= 0.506\n",
      "Epoch: 160, Loss= 0.6947, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6947, Training Accuracy= 0.506\n",
      "Epoch: 180, Loss= 0.6947, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.6947, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6946, Training Accuracy= 0.506\n",
      "Epoch: 210, Loss= 0.6946, Training Accuracy= 0.506\n",
      "Epoch: 220, Loss= 0.6944, Training Accuracy= 0.508\n",
      "Epoch: 230, Loss= 0.6940, Training Accuracy= 0.510\n",
      "Epoch: 240, Loss= 0.6943, Training Accuracy= 0.512\n",
      "Epoch: 250, Loss= 0.6939, Training Accuracy= 0.516\n",
      "Epoch: 260, Loss= 0.6940, Training Accuracy= 0.512\n",
      "Epoch: 270, Loss= 0.6940, Training Accuracy= 0.511\n",
      "Epoch: 280, Loss= 0.6940, Training Accuracy= 0.511\n",
      "Epoch: 290, Loss= 0.6939, Training Accuracy= 0.510\n",
      "Epoch: 300, Loss= 0.6939, Training Accuracy= 0.512\n",
      "Epoch: 310, Loss= 0.6938, Training Accuracy= 0.513\n",
      "Epoch: 320, Loss= 0.6938, Training Accuracy= 0.516\n",
      "Epoch: 330, Loss= 0.6938, Training Accuracy= 0.515\n",
      "Epoch: 340, Loss= 0.6937, Training Accuracy= 0.515\n",
      "Epoch: 350, Loss= 0.6937, Training Accuracy= 0.516\n",
      "Epoch: 360, Loss= 0.6937, Training Accuracy= 0.520\n",
      "Epoch: 370, Loss= 0.6937, Training Accuracy= 0.518\n",
      "Epoch: 380, Loss= 0.6937, Training Accuracy= 0.519\n",
      "Epoch: 390, Loss= 0.6932, Training Accuracy= 0.523\n",
      "Epoch: 400, Loss= 0.6928, Training Accuracy= 0.524\n",
      "Epoch: 410, Loss= 0.6932, Training Accuracy= 0.524\n",
      "Epoch: 420, Loss= 0.6939, Training Accuracy= 0.509\n",
      "Epoch: 430, Loss= 0.6932, Training Accuracy= 0.522\n",
      "Epoch: 440, Loss= 0.6939, Training Accuracy= 0.514\n",
      "Epoch: 450, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 460, Loss= 0.6995, Training Accuracy= 0.506\n",
      "Epoch: 470, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 480, Loss= 0.6947, Training Accuracy= 0.510\n",
      "Epoch: 490, Loss= 0.6940, Training Accuracy= 0.521\n",
      "Epoch: 500, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 510, Loss= 0.7037, Training Accuracy= 0.506\n",
      "Epoch: 520, Loss= 0.6995, Training Accuracy= 0.506\n",
      "Epoch: 530, Loss= 0.6934, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.7189, Training Accuracy= 0.506\n",
      "Epoch: 550, Loss= 0.7059, Training Accuracy= 0.506\n",
      "Epoch: 560, Loss= 0.6982, Training Accuracy= 0.506\n",
      "Epoch: 570, Loss= 0.6953, Training Accuracy= 0.508\n",
      "Epoch: 580, Loss= 0.6960, Training Accuracy= 0.509\n",
      "Epoch: 590, Loss= 0.6952, Training Accuracy= 0.509\n",
      "Epoch: 600, Loss= 0.6951, Training Accuracy= 0.521\n",
      "Epoch: 610, Loss= 0.6939, Training Accuracy= 0.521\n",
      "Epoch: 620, Loss= 0.6933, Training Accuracy= 0.522\n",
      "Epoch: 630, Loss= 0.6940, Training Accuracy= 0.520\n",
      "Epoch: 640, Loss= 0.6937, Training Accuracy= 0.521\n",
      "Epoch: 650, Loss= 0.6951, Training Accuracy= 0.515\n",
      "Epoch: 660, Loss= 0.6980, Training Accuracy= 0.506\n",
      "Epoch: 670, Loss= 0.6952, Training Accuracy= 0.514\n",
      "Epoch: 680, Loss= 0.6933, Training Accuracy= 0.520\n",
      "Epoch: 690, Loss= 0.6956, Training Accuracy= 0.507\n",
      "Epoch: 700, Loss= 0.6990, Training Accuracy= 0.506\n",
      "Epoch: 710, Loss= 0.7049, Training Accuracy= 0.506\n",
      "Epoch: 720, Loss= 0.7083, Training Accuracy= 0.506\n",
      "Epoch: 730, Loss= 0.7094, Training Accuracy= 0.506\n",
      "Epoch: 740, Loss= 0.7281, Training Accuracy= 0.504\n",
      "Epoch: 750, Loss= 0.7278, Training Accuracy= 0.506\n",
      "Epoch: 760, Loss= 0.7272, Training Accuracy= 0.506\n",
      "Epoch: 770, Loss= 0.7242, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.7208, Training Accuracy= 0.506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.7354, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.7339, Training Accuracy= 0.506\n",
      "Epoch: 810, Loss= 0.7222, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.7318, Training Accuracy= 0.506\n",
      "Epoch: 830, Loss= 0.7224, Training Accuracy= 0.506\n",
      "Epoch: 840, Loss= 0.7150, Training Accuracy= 0.506\n",
      "Epoch: 850, Loss= 0.7111, Training Accuracy= 0.506\n",
      "Epoch: 860, Loss= 0.7089, Training Accuracy= 0.506\n",
      "Epoch: 870, Loss= 0.7073, Training Accuracy= 0.506\n",
      "Epoch: 880, Loss= 0.7066, Training Accuracy= 0.506\n",
      "Epoch: 890, Loss= 0.7058, Training Accuracy= 0.506\n",
      "Epoch: 900, Loss= 0.7051, Training Accuracy= 0.506\n",
      "Epoch: 910, Loss= 0.7051, Training Accuracy= 0.506\n",
      "Epoch: 920, Loss= 0.7054, Training Accuracy= 0.506\n",
      "Epoch: 930, Loss= 0.7047, Training Accuracy= 0.506\n",
      "Epoch: 940, Loss= 0.7049, Training Accuracy= 0.506\n",
      "Epoch: 950, Loss= 0.7033, Training Accuracy= 0.507\n",
      "Epoch: 960, Loss= 0.7276, Training Accuracy= 0.506\n",
      "Epoch: 970, Loss= 0.7185, Training Accuracy= 0.506\n",
      "Epoch: 980, Loss= 0.7185, Training Accuracy= 0.506\n",
      "Epoch: 990, Loss= 0.7215, Training Accuracy= 0.506\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4968\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.7247, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.7066, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.7007, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6979, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6964, Training Accuracy= 0.498\n",
      "Epoch: 50, Loss= 0.6956, Training Accuracy= 0.498\n",
      "Epoch: 60, Loss= 0.6951, Training Accuracy= 0.498\n",
      "Epoch: 70, Loss= 0.6948, Training Accuracy= 0.498\n",
      "Epoch: 80, Loss= 0.6946, Training Accuracy= 0.498\n",
      "Epoch: 90, Loss= 0.6945, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.6944, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.6944, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 130, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 140, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 150, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 160, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 170, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 180, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 190, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 200, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 210, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 220, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 230, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 240, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 250, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 260, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 270, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 280, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 290, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 300, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 310, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 320, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 330, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 340, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 350, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 360, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 370, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 380, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 390, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 400, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 410, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 420, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 430, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 440, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 450, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 460, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 470, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 480, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 490, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 500, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 510, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 520, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 550, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 570, Loss= 0.6942, Training Accuracy= 0.497\n",
      "Epoch: 580, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 590, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 600, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 610, Loss= 0.6942, Training Accuracy= 0.498\n",
      "Epoch: 620, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 630, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 640, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 650, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 660, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 670, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 680, Loss= 0.6941, Training Accuracy= 0.501\n",
      "Epoch: 690, Loss= 0.6942, Training Accuracy= 0.501\n",
      "Epoch: 700, Loss= 0.6944, Training Accuracy= 0.501\n",
      "Epoch: 710, Loss= 0.6944, Training Accuracy= 0.501\n",
      "Epoch: 720, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 730, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 740, Loss= 0.6941, Training Accuracy= 0.501\n",
      "Epoch: 750, Loss= 0.6941, Training Accuracy= 0.501\n",
      "Epoch: 760, Loss= 0.6941, Training Accuracy= 0.501\n",
      "Epoch: 770, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 780, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 790, Loss= 0.6942, Training Accuracy= 0.501\n",
      "Epoch: 800, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 810, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 820, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 830, Loss= 0.6943, Training Accuracy= 0.502\n",
      "Epoch: 840, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 850, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 860, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 870, Loss= 0.6964, Training Accuracy= 0.503\n",
      "Epoch: 880, Loss= 0.6939, Training Accuracy= 0.504\n",
      "Epoch: 890, Loss= 0.6937, Training Accuracy= 0.506\n",
      "Epoch: 900, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 910, Loss= 0.6985, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 930, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 940, Loss= 0.6941, Training Accuracy= 0.504\n",
      "Epoch: 950, Loss= 0.6940, Training Accuracy= 0.513\n",
      "Epoch: 960, Loss= 0.6933, Training Accuracy= 0.515\n",
      "Epoch: 970, Loss= 0.6934, Training Accuracy= 0.516\n",
      "Epoch: 980, Loss= 0.6948, Training Accuracy= 0.516\n",
      "Epoch: 990, Loss= 0.6949, Training Accuracy= 0.516\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5027\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 10, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 20, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 30, Loss= 0.6926, Training Accuracy= 0.518\n",
      "Epoch: 40, Loss= 0.6926, Training Accuracy= 0.521\n",
      "Epoch: 50, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 60, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 70, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 80, Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 90, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 100, Loss= 0.6921, Training Accuracy= 0.520\n",
      "Epoch: 110, Loss= 0.6920, Training Accuracy= 0.519\n",
      "Epoch: 120, Loss= 0.6920, Training Accuracy= 0.519\n",
      "Epoch: 130, Loss= 0.6920, Training Accuracy= 0.525\n",
      "Epoch: 140, Loss= 0.6918, Training Accuracy= 0.524\n",
      "Epoch: 150, Loss= 0.6916, Training Accuracy= 0.523\n",
      "Epoch: 160, Loss= 0.6921, Training Accuracy= 0.518\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 180, Loss= 0.6912, Training Accuracy= 0.526\n",
      "Epoch: 190, Loss= 0.6918, Training Accuracy= 0.523\n",
      "Epoch: 200, Loss= 0.6910, Training Accuracy= 0.526\n",
      "Epoch: 210, Loss= 0.6913, Training Accuracy= 0.524\n",
      "Epoch: 220, Loss= 0.6920, Training Accuracy= 0.518\n",
      "Epoch: 230, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 240, Loss= 0.6942, Training Accuracy= 0.499\n",
      "Epoch: 250, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 260, Loss= 0.6945, Training Accuracy= 0.506\n",
      "Epoch: 270, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 280, Loss= 0.6932, Training Accuracy= 0.499\n",
      "Epoch: 290, Loss= 0.6934, Training Accuracy= 0.513\n",
      "Epoch: 300, Loss= 0.6928, Training Accuracy= 0.517\n",
      "Epoch: 310, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 320, Loss= 0.6921, Training Accuracy= 0.517\n",
      "Epoch: 330, Loss= 0.6966, Training Accuracy= 0.497\n",
      "Epoch: 340, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 350, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 360, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 370, Loss= 0.6932, Training Accuracy= 0.498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 390, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 400, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 410, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 420, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 430, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 440, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 450, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 460, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 470, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 480, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 490, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 500, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 510, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 520, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 550, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 570, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 580, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 590, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 600, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 610, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 620, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 630, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 640, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 650, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 660, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 670, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 680, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 690, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 700, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 710, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 720, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 730, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 740, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 750, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 760, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 770, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 780, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 790, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 800, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 810, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 820, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 830, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 840, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 850, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 860, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 870, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 880, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 890, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 900, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 910, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 930, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 940, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 950, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 960, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 970, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 980, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 990, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4861\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.6951, Training Accuracy= 0.508\n",
      "Epoch: 10, Loss= 0.6939, Training Accuracy= 0.509\n",
      "Epoch: 20, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 30, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 40, Loss= 0.6931, Training Accuracy= 0.513\n",
      "Epoch: 50, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 60, Loss= 0.6926, Training Accuracy= 0.519\n",
      "Epoch: 70, Loss= 0.6924, Training Accuracy= 0.521\n",
      "Epoch: 80, Loss= 0.6923, Training Accuracy= 0.521\n",
      "Epoch: 90, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 100, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 110, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 120, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 130, Loss= 0.6923, Training Accuracy= 0.523\n",
      "Epoch: 140, Loss= 0.6923, Training Accuracy= 0.523\n",
      "Epoch: 150, Loss= 0.6921, Training Accuracy= 0.521\n",
      "Epoch: 160, Loss= 0.6920, Training Accuracy= 0.522\n",
      "Epoch: 170, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 180, Loss= 0.6917, Training Accuracy= 0.528\n",
      "Epoch: 190, Loss= 0.6944, Training Accuracy= 0.509\n",
      "Epoch: 200, Loss= 0.6941, Training Accuracy= 0.508\n",
      "Epoch: 210, Loss= 0.6941, Training Accuracy= 0.508\n",
      "Epoch: 220, Loss= 0.6940, Training Accuracy= 0.508\n",
      "Epoch: 230, Loss= 0.6938, Training Accuracy= 0.506\n",
      "Epoch: 240, Loss= 0.6955, Training Accuracy= 0.508\n",
      "Epoch: 250, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 260, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 270, Loss= 0.6967, Training Accuracy= 0.508\n",
      "Epoch: 280, Loss= 0.6956, Training Accuracy= 0.508\n",
      "Epoch: 290, Loss= 0.6951, Training Accuracy= 0.508\n",
      "Epoch: 300, Loss= 0.6948, Training Accuracy= 0.508\n",
      "Epoch: 310, Loss= 0.6936, Training Accuracy= 0.510\n",
      "Epoch: 320, Loss= 0.6934, Training Accuracy= 0.510\n",
      "Epoch: 330, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 340, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 350, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 360, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 370, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 380, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 390, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 400, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 420, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 430, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 440, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 450, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 460, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 470, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 480, Loss= 0.6931, Training Accuracy= 0.514\n",
      "Epoch: 490, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 500, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 510, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 520, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 530, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 540, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 550, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 560, Loss= 0.6932, Training Accuracy= 0.515\n",
      "Epoch: 570, Loss= 0.6932, Training Accuracy= 0.515\n",
      "Epoch: 580, Loss= 0.6932, Training Accuracy= 0.515\n",
      "Epoch: 590, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 600, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 610, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 620, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 630, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 640, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 650, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 660, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 670, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 680, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 690, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 700, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 710, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 730, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 740, Loss= 0.6928, Training Accuracy= 0.519\n",
      "Epoch: 750, Loss= 0.6926, Training Accuracy= 0.521\n",
      "Epoch: 760, Loss= 0.6926, Training Accuracy= 0.524\n",
      "Epoch: 770, Loss= 0.6925, Training Accuracy= 0.524\n",
      "Epoch: 780, Loss= 0.6925, Training Accuracy= 0.525\n",
      "Epoch: 790, Loss= 0.6924, Training Accuracy= 0.524\n",
      "Epoch: 800, Loss= 0.6924, Training Accuracy= 0.522\n",
      "Epoch: 810, Loss= 0.6924, Training Accuracy= 0.523\n",
      "Epoch: 820, Loss= 0.6924, Training Accuracy= 0.523\n",
      "Epoch: 830, Loss= 0.6924, Training Accuracy= 0.522\n",
      "Epoch: 840, Loss= 0.6926, Training Accuracy= 0.526\n",
      "Epoch: 850, Loss= 0.6925, Training Accuracy= 0.526\n",
      "Epoch: 860, Loss= 0.6928, Training Accuracy= 0.528\n",
      "Epoch: 870, Loss= 0.6932, Training Accuracy= 0.527\n",
      "Epoch: 880, Loss= 0.6936, Training Accuracy= 0.527\n",
      "Epoch: 890, Loss= 0.6938, Training Accuracy= 0.527\n",
      "Epoch: 900, Loss= 0.6939, Training Accuracy= 0.528\n",
      "Epoch: 910, Loss= 0.6939, Training Accuracy= 0.528\n",
      "Epoch: 920, Loss= 0.6940, Training Accuracy= 0.530\n",
      "Epoch: 930, Loss= 0.6940, Training Accuracy= 0.526\n",
      "Epoch: 940, Loss= 0.6941, Training Accuracy= 0.526\n",
      "Epoch: 950, Loss= 0.6945, Training Accuracy= 0.524\n",
      "Epoch: 960, Loss= 0.6936, Training Accuracy= 0.526\n",
      "Epoch: 970, Loss= 0.6941, Training Accuracy= 0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.6939, Training Accuracy= 0.508\n",
      "Epoch: 990, Loss= 0.6939, Training Accuracy= 0.508\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5023\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.7365, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.7029, Training Accuracy= 0.495\n",
      "Epoch: 20, Loss= 0.6993, Training Accuracy= 0.495\n",
      "Epoch: 30, Loss= 0.6978, Training Accuracy= 0.495\n",
      "Epoch: 40, Loss= 0.6971, Training Accuracy= 0.494\n",
      "Epoch: 50, Loss= 0.6966, Training Accuracy= 0.494\n",
      "Epoch: 60, Loss= 0.6962, Training Accuracy= 0.495\n",
      "Epoch: 70, Loss= 0.6960, Training Accuracy= 0.495\n",
      "Epoch: 80, Loss= 0.6958, Training Accuracy= 0.495\n",
      "Epoch: 90, Loss= 0.6956, Training Accuracy= 0.496\n",
      "Epoch: 100, Loss= 0.6955, Training Accuracy= 0.496\n",
      "Epoch: 110, Loss= 0.6954, Training Accuracy= 0.496\n",
      "Epoch: 120, Loss= 0.6953, Training Accuracy= 0.497\n",
      "Epoch: 130, Loss= 0.6952, Training Accuracy= 0.498\n",
      "Epoch: 140, Loss= 0.6951, Training Accuracy= 0.499\n",
      "Epoch: 150, Loss= 0.6951, Training Accuracy= 0.498\n",
      "Epoch: 160, Loss= 0.6950, Training Accuracy= 0.499\n",
      "Epoch: 170, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 180, Loss= 0.6948, Training Accuracy= 0.498\n",
      "Epoch: 190, Loss= 0.6948, Training Accuracy= 0.499\n",
      "Epoch: 200, Loss= 0.6947, Training Accuracy= 0.499\n",
      "Epoch: 210, Loss= 0.6946, Training Accuracy= 0.501\n",
      "Epoch: 220, Loss= 0.6946, Training Accuracy= 0.502\n",
      "Epoch: 230, Loss= 0.6945, Training Accuracy= 0.502\n",
      "Epoch: 240, Loss= 0.6945, Training Accuracy= 0.504\n",
      "Epoch: 250, Loss= 0.6945, Training Accuracy= 0.507\n",
      "Epoch: 260, Loss= 0.6946, Training Accuracy= 0.508\n",
      "Epoch: 270, Loss= 0.6946, Training Accuracy= 0.511\n",
      "Epoch: 280, Loss= 0.6945, Training Accuracy= 0.510\n",
      "Epoch: 290, Loss= 0.6950, Training Accuracy= 0.508\n",
      "Epoch: 300, Loss= 0.6950, Training Accuracy= 0.508\n",
      "Epoch: 310, Loss= 0.7003, Training Accuracy= 0.495\n",
      "Epoch: 320, Loss= 0.6943, Training Accuracy= 0.506\n",
      "Epoch: 330, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 340, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 350, Loss= 0.6939, Training Accuracy= 0.510\n",
      "Epoch: 360, Loss= 0.6941, Training Accuracy= 0.507\n",
      "Epoch: 370, Loss= 0.6934, Training Accuracy= 0.519\n",
      "Epoch: 380, Loss= 0.6926, Training Accuracy= 0.520\n",
      "Epoch: 390, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 400, Loss= 0.6941, Training Accuracy= 0.516\n",
      "Epoch: 410, Loss= 0.6994, Training Accuracy= 0.495\n",
      "Epoch: 420, Loss= 0.6968, Training Accuracy= 0.500\n",
      "Epoch: 430, Loss= 0.6968, Training Accuracy= 0.506\n",
      "Epoch: 440, Loss= 0.6986, Training Accuracy= 0.495\n",
      "Epoch: 450, Loss= 0.6948, Training Accuracy= 0.507\n",
      "Epoch: 460, Loss= 0.6948, Training Accuracy= 0.514\n",
      "Epoch: 470, Loss= 0.6948, Training Accuracy= 0.515\n",
      "Epoch: 480, Loss= 0.6954, Training Accuracy= 0.517\n",
      "Epoch: 490, Loss= 0.6939, Training Accuracy= 0.519\n",
      "Epoch: 500, Loss= 0.6936, Training Accuracy= 0.515\n",
      "Epoch: 510, Loss= 0.6959, Training Accuracy= 0.513\n",
      "Epoch: 520, Loss= 0.6931, Training Accuracy= 0.518\n",
      "Epoch: 530, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 540, Loss= 0.6932, Training Accuracy= 0.519\n",
      "Epoch: 550, Loss= 0.6928, Training Accuracy= 0.521\n",
      "Epoch: 560, Loss= 0.6935, Training Accuracy= 0.519\n",
      "Epoch: 570, Loss= 0.6934, Training Accuracy= 0.520\n",
      "Epoch: 580, Loss= 0.6929, Training Accuracy= 0.520\n",
      "Epoch: 590, Loss= 0.6944, Training Accuracy= 0.517\n",
      "Epoch: 600, Loss= 0.6928, Training Accuracy= 0.522\n",
      "Epoch: 610, Loss= 0.6940, Training Accuracy= 0.519\n",
      "Epoch: 620, Loss= 0.6937, Training Accuracy= 0.518\n",
      "Epoch: 630, Loss= 0.6925, Training Accuracy= 0.519\n",
      "Epoch: 640, Loss= 0.6947, Training Accuracy= 0.517\n",
      "Epoch: 650, Loss= 0.6924, Training Accuracy= 0.523\n",
      "Epoch: 660, Loss= 0.6945, Training Accuracy= 0.514\n",
      "Epoch: 670, Loss= 0.6957, Training Accuracy= 0.511\n",
      "Epoch: 680, Loss= 0.6937, Training Accuracy= 0.516\n",
      "Epoch: 690, Loss= 0.6955, Training Accuracy= 0.508\n",
      "Epoch: 700, Loss= 0.6916, Training Accuracy= 0.526\n",
      "Epoch: 710, Loss= 0.6955, Training Accuracy= 0.513\n",
      "Epoch: 720, Loss= 0.6943, Training Accuracy= 0.513\n",
      "Epoch: 730, Loss= 0.6941, Training Accuracy= 0.515\n",
      "Epoch: 740, Loss= 0.6947, Training Accuracy= 0.519\n",
      "Epoch: 750, Loss= 0.6946, Training Accuracy= 0.510\n",
      "Epoch: 760, Loss= 0.6944, Training Accuracy= 0.512\n",
      "Epoch: 770, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 780, Loss= 0.6952, Training Accuracy= 0.502\n",
      "Epoch: 790, Loss= 0.7074, Training Accuracy= 0.495\n",
      "Epoch: 800, Loss= 0.7106, Training Accuracy= 0.495\n",
      "Epoch: 810, Loss= 0.7095, Training Accuracy= 0.495\n",
      "Epoch: 820, Loss= 0.7083, Training Accuracy= 0.495\n",
      "Epoch: 830, Loss= 0.6967, Training Accuracy= 0.495\n",
      "Epoch: 840, Loss= 0.6972, Training Accuracy= 0.495\n",
      "Epoch: 850, Loss= 0.6976, Training Accuracy= 0.495\n",
      "Epoch: 860, Loss= 0.6974, Training Accuracy= 0.495\n",
      "Epoch: 870, Loss= 0.6974, Training Accuracy= 0.497\n",
      "Epoch: 880, Loss= 0.6975, Training Accuracy= 0.498\n",
      "Epoch: 890, Loss= 0.6974, Training Accuracy= 0.497\n",
      "Epoch: 900, Loss= 0.6975, Training Accuracy= 0.498\n",
      "Epoch: 910, Loss= 0.6976, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.7162, Training Accuracy= 0.495\n",
      "Epoch: 930, Loss= 0.7118, Training Accuracy= 0.495\n",
      "Epoch: 940, Loss= 0.7145, Training Accuracy= 0.495\n",
      "Epoch: 950, Loss= 0.7065, Training Accuracy= 0.495\n",
      "Epoch: 960, Loss= 0.7140, Training Accuracy= 0.495\n",
      "Epoch: 970, Loss= 0.7094, Training Accuracy= 0.495\n",
      "Epoch: 980, Loss= 0.6995, Training Accuracy= 0.497\n",
      "Epoch: 990, Loss= 0.7103, Training Accuracy= 0.494\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5073\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7010, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.7003, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6984, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6975, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6970, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6967, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6965, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6963, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.6962, Training Accuracy= 0.500\n",
      "Epoch: 90, Loss= 0.6961, Training Accuracy= 0.500\n",
      "Epoch: 100, Loss= 0.6961, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6960, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.6960, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.6959, Training Accuracy= 0.500\n",
      "Epoch: 140, Loss= 0.6959, Training Accuracy= 0.500\n",
      "Epoch: 150, Loss= 0.6959, Training Accuracy= 0.500\n",
      "Epoch: 160, Loss= 0.6958, Training Accuracy= 0.500\n",
      "Epoch: 170, Loss= 0.6958, Training Accuracy= 0.500\n",
      "Epoch: 180, Loss= 0.6957, Training Accuracy= 0.500\n",
      "Epoch: 190, Loss= 0.6957, Training Accuracy= 0.500\n",
      "Epoch: 200, Loss= 0.6955, Training Accuracy= 0.500\n",
      "Epoch: 210, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 220, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 230, Loss= 0.6946, Training Accuracy= 0.501\n",
      "Epoch: 240, Loss= 0.6944, Training Accuracy= 0.501\n",
      "Epoch: 250, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 260, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 270, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 280, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 290, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 300, Loss= 0.6938, Training Accuracy= 0.500\n",
      "Epoch: 310, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 320, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 330, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 340, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 350, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 360, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 370, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 380, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 390, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 400, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 410, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 420, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 430, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.6940, Training Accuracy= 0.504\n",
      "Epoch: 450, Loss= 0.6946, Training Accuracy= 0.506\n",
      "Epoch: 460, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 470, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 480, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 490, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 500, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 510, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 520, Loss= 0.6922, Training Accuracy= 0.517\n",
      "Epoch: 530, Loss= 0.6919, Training Accuracy= 0.524\n",
      "Epoch: 540, Loss= 0.6920, Training Accuracy= 0.523\n",
      "Epoch: 550, Loss= 0.6921, Training Accuracy= 0.519\n",
      "Epoch: 560, Loss= 0.6916, Training Accuracy= 0.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.6921, Training Accuracy= 0.524\n",
      "Epoch: 580, Loss= 0.6930, Training Accuracy= 0.522\n",
      "Epoch: 590, Loss= 0.6920, Training Accuracy= 0.522\n",
      "Epoch: 600, Loss= 0.6917, Training Accuracy= 0.526\n",
      "Epoch: 610, Loss= 0.6927, Training Accuracy= 0.525\n",
      "Epoch: 620, Loss= 0.6924, Training Accuracy= 0.529\n",
      "Epoch: 630, Loss= 0.6932, Training Accuracy= 0.521\n",
      "Epoch: 640, Loss= 0.6924, Training Accuracy= 0.524\n",
      "Epoch: 650, Loss= 0.6922, Training Accuracy= 0.522\n",
      "Epoch: 660, Loss= 0.6923, Training Accuracy= 0.521\n",
      "Epoch: 670, Loss= 0.6947, Training Accuracy= 0.507\n",
      "Epoch: 680, Loss= 0.6943, Training Accuracy= 0.509\n",
      "Epoch: 690, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 700, Loss= 0.6925, Training Accuracy= 0.527\n",
      "Epoch: 710, Loss= 0.6918, Training Accuracy= 0.528\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.521\n",
      "Epoch: 730, Loss= 0.6940, Training Accuracy= 0.516\n",
      "Epoch: 740, Loss= 0.6960, Training Accuracy= 0.507\n",
      "Epoch: 750, Loss= 0.6983, Training Accuracy= 0.511\n",
      "Epoch: 760, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 770, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 780, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 790, Loss= 0.6959, Training Accuracy= 0.504\n",
      "Epoch: 800, Loss= 0.6956, Training Accuracy= 0.505\n",
      "Epoch: 810, Loss= 0.6955, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.6955, Training Accuracy= 0.508\n",
      "Epoch: 830, Loss= 0.6954, Training Accuracy= 0.508\n",
      "Epoch: 840, Loss= 0.6953, Training Accuracy= 0.508\n",
      "Epoch: 850, Loss= 0.6952, Training Accuracy= 0.509\n",
      "Epoch: 860, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 870, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 880, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 890, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 900, Loss= 0.6943, Training Accuracy= 0.512\n",
      "Epoch: 910, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 920, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 930, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 940, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 950, Loss= 0.6933, Training Accuracy= 0.501\n",
      "Epoch: 960, Loss= 0.7005, Training Accuracy= 0.500\n",
      "Epoch: 970, Loss= 0.7005, Training Accuracy= 0.500\n",
      "Epoch: 980, Loss= 0.7005, Training Accuracy= 0.500\n",
      "Epoch: 990, Loss= 0.7005, Training Accuracy= 0.500\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5017\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.3\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.49829999, 0.50590003, 0.50620002, 0.48980001, 0.49680001, 0.50269997, 0.48609999, 0.50230002, 0.50730002, 0.50169998]\n",
      "mean of test_accuracies_10replications:  0.49971\n",
      "standard deviation of test_accuracies_10replications_std_mean:  6.71974662691e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f998137df10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
