{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 5\n",
    "N = 50\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.7027, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 20, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 50, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 60, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 70, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 80, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 100, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 110, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 160, Loss= 0.6934, Training Accuracy= 0.500\n",
      "Epoch: 170, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 180, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 190, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 200, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 210, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 220, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 240, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 250, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 260, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 270, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 280, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 290, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 300, Loss= 0.6925, Training Accuracy= 0.513\n",
      "Epoch: 310, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 320, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 330, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 340, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 350, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 360, Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 370, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 380, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 400, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 420, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 430, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 450, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 460, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 470, Loss= 0.6954, Training Accuracy= 0.498\n",
      "Epoch: 480, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 490, Loss= 0.6939, Training Accuracy= 0.509\n",
      "Epoch: 500, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.6936, Training Accuracy= 0.508\n",
      "Epoch: 520, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 530, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 540, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 550, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 560, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 570, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 580, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 590, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 600, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 610, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 620, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 630, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 640, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 650, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 660, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 670, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 680, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 690, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 700, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 710, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 720, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 730, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 740, Loss= 0.6926, Training Accuracy= 0.517\n",
      "Epoch: 750, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 760, Loss= 0.6926, Training Accuracy= 0.518\n",
      "Epoch: 770, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 780, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 790, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 800, Loss= 0.6942, Training Accuracy= 0.504\n",
      "Epoch: 810, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 820, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 830, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 840, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 850, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 860, Loss= 0.6939, Training Accuracy= 0.504\n",
      "Epoch: 870, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 880, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 890, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 900, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 910, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 920, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 930, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 940, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 950, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 960, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 970, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 980, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 990, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5029\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7199, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.7028, Training Accuracy= 0.495\n",
      "Epoch: 20, Loss= 0.6997, Training Accuracy= 0.495\n",
      "Epoch: 30, Loss= 0.6983, Training Accuracy= 0.495\n",
      "Epoch: 40, Loss= 0.6974, Training Accuracy= 0.495\n",
      "Epoch: 50, Loss= 0.6967, Training Accuracy= 0.495\n",
      "Epoch: 60, Loss= 0.6963, Training Accuracy= 0.495\n",
      "Epoch: 70, Loss= 0.6959, Training Accuracy= 0.495\n",
      "Epoch: 80, Loss= 0.6956, Training Accuracy= 0.495\n",
      "Epoch: 90, Loss= 0.6952, Training Accuracy= 0.495\n",
      "Epoch: 100, Loss= 0.6950, Training Accuracy= 0.495\n",
      "Epoch: 110, Loss= 0.6947, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.6944, Training Accuracy= 0.499\n",
      "Epoch: 130, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 140, Loss= 0.6939, Training Accuracy= 0.499\n",
      "Epoch: 150, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 160, Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 170, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 180, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 190, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 200, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 210, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 220, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 230, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 250, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 270, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 290, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 300, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 310, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 320, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 330, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 340, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 350, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 370, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 380, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 400, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 410, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 420, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 430, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 440, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 450, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 460, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 490, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 500, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 510, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 520, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 530, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 540, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 550, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 560, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 570, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 580, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 590, Loss= 0.6932, Training Accuracy= 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 610, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 620, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 630, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 640, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 650, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 660, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 670, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 680, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 690, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 700, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 710, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 720, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 730, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 740, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 750, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 760, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 770, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 780, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 790, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 800, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 810, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 830, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 840, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 850, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 860, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 870, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 880, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 890, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 900, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 910, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 920, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 930, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 940, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 950, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 960, Loss= 0.6940, Training Accuracy= 0.504\n",
      "Epoch: 970, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 980, Loss= 0.6959, Training Accuracy= 0.496\n",
      "Epoch: 990, Loss= 0.6944, Training Accuracy= 0.504\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4941\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 10, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 90, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.6934, Training Accuracy= 0.496\n",
      "Epoch: 130, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 140, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 150, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 160, Loss= 0.6934, Training Accuracy= 0.500\n",
      "Epoch: 170, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 180, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 210, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 220, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 230, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 240, Loss= 0.6934, Training Accuracy= 0.516\n",
      "Epoch: 250, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 260, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 270, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 280, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 290, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 300, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 310, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 320, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 330, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 340, Loss= 0.6937, Training Accuracy= 0.499\n",
      "Epoch: 350, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 360, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 370, Loss= 0.6939, Training Accuracy= 0.498\n",
      "Epoch: 380, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 390, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 400, Loss= 0.6940, Training Accuracy= 0.500\n",
      "Epoch: 410, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 420, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 430, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 450, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 460, Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 470, Loss= 0.6936, Training Accuracy= 0.501\n",
      "Epoch: 480, Loss= 0.6936, Training Accuracy= 0.501\n",
      "Epoch: 490, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 500, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 510, Loss= 0.6937, Training Accuracy= 0.499\n",
      "Epoch: 520, Loss= 0.6937, Training Accuracy= 0.499\n",
      "Epoch: 530, Loss= 0.6937, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.6937, Training Accuracy= 0.499\n",
      "Epoch: 550, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 560, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 570, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 580, Loss= 0.6946, Training Accuracy= 0.501\n",
      "Epoch: 590, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 600, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 610, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 620, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 630, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 640, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 650, Loss= 0.6933, Training Accuracy= 0.501\n",
      "Epoch: 660, Loss= 0.6939, Training Accuracy= 0.507\n",
      "Epoch: 670, Loss= 0.6943, Training Accuracy= 0.505\n",
      "Epoch: 680, Loss= 0.6939, Training Accuracy= 0.510\n",
      "Epoch: 690, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 700, Loss= 0.6951, Training Accuracy= 0.504\n",
      "Epoch: 710, Loss= 0.6932, Training Accuracy= 0.497\n",
      "Epoch: 720, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 740, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 750, Loss= 0.6936, Training Accuracy= 0.501\n",
      "Epoch: 760, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 770, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 780, Loss= 0.6937, Training Accuracy= 0.506\n",
      "Epoch: 790, Loss= 0.6937, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.6937, Training Accuracy= 0.506\n",
      "Epoch: 810, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 820, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 830, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 840, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 850, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 860, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 870, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 880, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 890, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 900, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 910, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 920, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 930, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 940, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 950, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 960, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 970, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 980, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 990, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4946\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.6978, Training Accuracy= 0.507\n",
      "Epoch: 10, Loss= 0.6959, Training Accuracy= 0.509\n",
      "Epoch: 20, Loss= 0.6955, Training Accuracy= 0.509\n",
      "Epoch: 30, Loss= 0.6951, Training Accuracy= 0.509\n",
      "Epoch: 40, Loss= 0.6947, Training Accuracy= 0.509\n",
      "Epoch: 50, Loss= 0.6944, Training Accuracy= 0.509\n",
      "Epoch: 60, Loss= 0.6941, Training Accuracy= 0.509\n",
      "Epoch: 70, Loss= 0.6940, Training Accuracy= 0.509\n",
      "Epoch: 80, Loss= 0.6939, Training Accuracy= 0.509\n",
      "Epoch: 90, Loss= 0.6939, Training Accuracy= 0.509\n",
      "Epoch: 100, Loss= 0.6939, Training Accuracy= 0.509\n",
      "Epoch: 110, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 120, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 130, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 140, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 150, Loss= 0.6936, Training Accuracy= 0.510\n",
      "Epoch: 160, Loss= 0.6935, Training Accuracy= 0.513\n",
      "Epoch: 170, Loss= 0.6935, Training Accuracy= 0.513\n",
      "Epoch: 180, Loss= 0.6935, Training Accuracy= 0.514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.6934, Training Accuracy= 0.516\n",
      "Epoch: 200, Loss= 0.6934, Training Accuracy= 0.517\n",
      "Epoch: 210, Loss= 0.6933, Training Accuracy= 0.516\n",
      "Epoch: 220, Loss= 0.6933, Training Accuracy= 0.515\n",
      "Epoch: 230, Loss= 0.6933, Training Accuracy= 0.515\n",
      "Epoch: 240, Loss= 0.6933, Training Accuracy= 0.517\n",
      "Epoch: 250, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 260, Loss= 0.6937, Training Accuracy= 0.516\n",
      "Epoch: 270, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 280, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 290, Loss= 0.6940, Training Accuracy= 0.508\n",
      "Epoch: 300, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 310, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 320, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 330, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 340, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 350, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 360, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 370, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 380, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 390, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 400, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 420, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 430, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 440, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 450, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 460, Loss= 0.6946, Training Accuracy= 0.508\n",
      "Epoch: 470, Loss= 0.6942, Training Accuracy= 0.507\n",
      "Epoch: 480, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 490, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 500, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 510, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 520, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 530, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 540, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 550, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 560, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 570, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 580, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 590, Loss= 0.6934, Training Accuracy= 0.510\n",
      "Epoch: 600, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 610, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 620, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 630, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 640, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 650, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 670, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 680, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 700, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 710, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 720, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 730, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 740, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 750, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 760, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 770, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 780, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 790, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 800, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 810, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 820, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 830, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 840, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 850, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 860, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 870, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 880, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 890, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 900, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 910, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 920, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 930, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 940, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 950, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 960, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 970, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 980, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 990, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4976\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.7069, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.6984, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.6970, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.6963, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.6959, Training Accuracy= 0.498\n",
      "Epoch: 50, Loss= 0.6957, Training Accuracy= 0.498\n",
      "Epoch: 60, Loss= 0.6956, Training Accuracy= 0.498\n",
      "Epoch: 70, Loss= 0.6955, Training Accuracy= 0.498\n",
      "Epoch: 80, Loss= 0.6954, Training Accuracy= 0.498\n",
      "Epoch: 90, Loss= 0.6954, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.6953, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.6952, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.6952, Training Accuracy= 0.498\n",
      "Epoch: 130, Loss= 0.6951, Training Accuracy= 0.498\n",
      "Epoch: 140, Loss= 0.6950, Training Accuracy= 0.498\n",
      "Epoch: 150, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 160, Loss= 0.6949, Training Accuracy= 0.498\n",
      "Epoch: 170, Loss= 0.6948, Training Accuracy= 0.498\n",
      "Epoch: 180, Loss= 0.6948, Training Accuracy= 0.498\n",
      "Epoch: 190, Loss= 0.6947, Training Accuracy= 0.498\n",
      "Epoch: 200, Loss= 0.6946, Training Accuracy= 0.498\n",
      "Epoch: 210, Loss= 0.6945, Training Accuracy= 0.498\n",
      "Epoch: 220, Loss= 0.6943, Training Accuracy= 0.498\n",
      "Epoch: 230, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 240, Loss= 0.6939, Training Accuracy= 0.502\n",
      "Epoch: 250, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 270, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 280, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 290, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 300, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 310, Loss= 0.6936, Training Accuracy= 0.510\n",
      "Epoch: 320, Loss= 0.6936, Training Accuracy= 0.511\n",
      "Epoch: 330, Loss= 0.6936, Training Accuracy= 0.513\n",
      "Epoch: 340, Loss= 0.6935, Training Accuracy= 0.513\n",
      "Epoch: 350, Loss= 0.6934, Training Accuracy= 0.514\n",
      "Epoch: 360, Loss= 0.6932, Training Accuracy= 0.518\n",
      "Epoch: 370, Loss= 0.6931, Training Accuracy= 0.520\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.521\n",
      "Epoch: 390, Loss= 0.6931, Training Accuracy= 0.520\n",
      "Epoch: 400, Loss= 0.6932, Training Accuracy= 0.519\n",
      "Epoch: 410, Loss= 0.6931, Training Accuracy= 0.520\n",
      "Epoch: 420, Loss= 0.6930, Training Accuracy= 0.519\n",
      "Epoch: 430, Loss= 0.6928, Training Accuracy= 0.521\n",
      "Epoch: 440, Loss= 0.6926, Training Accuracy= 0.525\n",
      "Epoch: 450, Loss= 0.6924, Training Accuracy= 0.521\n",
      "Epoch: 460, Loss= 0.6923, Training Accuracy= 0.523\n",
      "Epoch: 470, Loss= 0.6922, Training Accuracy= 0.525\n",
      "Epoch: 480, Loss= 0.6922, Training Accuracy= 0.526\n",
      "Epoch: 490, Loss= 0.6918, Training Accuracy= 0.524\n",
      "Epoch: 500, Loss= 0.6935, Training Accuracy= 0.511\n",
      "Epoch: 510, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 520, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 530, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 540, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 550, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 560, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 570, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 580, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 590, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 600, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 610, Loss= 0.6941, Training Accuracy= 0.499\n",
      "Epoch: 620, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 630, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 640, Loss= 0.6935, Training Accuracy= 0.514\n",
      "Epoch: 650, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 660, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 680, Loss= 0.6929, Training Accuracy= 0.504\n",
      "Epoch: 690, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 700, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 710, Loss= 0.6927, Training Accuracy= 0.506\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.519\n",
      "Epoch: 730, Loss= 0.6947, Training Accuracy= 0.516\n",
      "Epoch: 740, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 750, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 760, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 770, Loss= 0.6922, Training Accuracy= 0.521\n",
      "Epoch: 780, Loss= 0.6939, Training Accuracy= 0.511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 800, Loss= 0.7024, Training Accuracy= 0.504\n",
      "Epoch: 810, Loss= 0.7103, Training Accuracy= 0.503\n",
      "Epoch: 820, Loss= 0.7096, Training Accuracy= 0.504\n",
      "Epoch: 830, Loss= 0.7074, Training Accuracy= 0.503\n",
      "Epoch: 840, Loss= 0.6967, Training Accuracy= 0.502\n",
      "Epoch: 850, Loss= 0.6955, Training Accuracy= 0.504\n",
      "Epoch: 860, Loss= 0.6976, Training Accuracy= 0.502\n",
      "Epoch: 870, Loss= 0.6971, Training Accuracy= 0.498\n",
      "Epoch: 880, Loss= 0.7001, Training Accuracy= 0.501\n",
      "Epoch: 890, Loss= 0.6998, Training Accuracy= 0.504\n",
      "Epoch: 900, Loss= 0.6996, Training Accuracy= 0.503\n",
      "Epoch: 910, Loss= 0.6995, Training Accuracy= 0.503\n",
      "Epoch: 920, Loss= 0.6994, Training Accuracy= 0.503\n",
      "Epoch: 930, Loss= 0.6993, Training Accuracy= 0.503\n",
      "Epoch: 940, Loss= 0.6992, Training Accuracy= 0.503\n",
      "Epoch: 950, Loss= 0.6993, Training Accuracy= 0.503\n",
      "Epoch: 960, Loss= 0.7002, Training Accuracy= 0.504\n",
      "Epoch: 970, Loss= 0.6992, Training Accuracy= 0.503\n",
      "Epoch: 980, Loss= 0.6989, Training Accuracy= 0.503\n",
      "Epoch: 990, Loss= 0.6988, Training Accuracy= 0.503\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4969\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.6989, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6951, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 40, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 70, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 90, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 140, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 160, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 220, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 230, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 240, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 250, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 260, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 270, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 280, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 290, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 300, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 310, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 320, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 330, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 340, Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 350, Loss= 0.6940, Training Accuracy= 0.509\n",
      "Epoch: 360, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 370, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 390, Loss= 0.6982, Training Accuracy= 0.502\n",
      "Epoch: 400, Loss= 0.6962, Training Accuracy= 0.503\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 420, Loss= 0.6954, Training Accuracy= 0.503\n",
      "Epoch: 430, Loss= 0.6940, Training Accuracy= 0.511\n",
      "Epoch: 440, Loss= 0.6940, Training Accuracy= 0.512\n",
      "Epoch: 450, Loss= 0.6939, Training Accuracy= 0.509\n",
      "Epoch: 460, Loss= 0.6949, Training Accuracy= 0.510\n",
      "Epoch: 470, Loss= 0.6938, Training Accuracy= 0.515\n",
      "Epoch: 480, Loss= 0.6947, Training Accuracy= 0.513\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 500, Loss= 0.6971, Training Accuracy= 0.496\n",
      "Epoch: 510, Loss= 0.7283, Training Accuracy= 0.502\n",
      "Epoch: 520, Loss= 0.7250, Training Accuracy= 0.502\n",
      "Epoch: 530, Loss= 0.7240, Training Accuracy= 0.502\n",
      "Epoch: 540, Loss= 0.7232, Training Accuracy= 0.502\n",
      "Epoch: 550, Loss= 0.7227, Training Accuracy= 0.502\n",
      "Epoch: 560, Loss= 0.7227, Training Accuracy= 0.502\n",
      "Epoch: 570, Loss= 0.7227, Training Accuracy= 0.502\n",
      "Epoch: 580, Loss= 0.7227, Training Accuracy= 0.502\n",
      "Epoch: 590, Loss= 0.7226, Training Accuracy= 0.502\n",
      "Epoch: 600, Loss= 0.7226, Training Accuracy= 0.502\n",
      "Epoch: 610, Loss= 0.7225, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 0.7225, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 0.7221, Training Accuracy= 0.502\n",
      "Epoch: 640, Loss= 0.7222, Training Accuracy= 0.502\n",
      "Epoch: 650, Loss= 0.7222, Training Accuracy= 0.502\n",
      "Epoch: 660, Loss= 0.7233, Training Accuracy= 0.502\n",
      "Epoch: 670, Loss= 0.7232, Training Accuracy= 0.502\n",
      "Epoch: 680, Loss= 0.7230, Training Accuracy= 0.502\n",
      "Epoch: 690, Loss= 0.7229, Training Accuracy= 0.502\n",
      "Epoch: 700, Loss= 0.7214, Training Accuracy= 0.502\n",
      "Epoch: 710, Loss= 0.7210, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.7208, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.7203, Training Accuracy= 0.502\n",
      "Epoch: 740, Loss= 0.7199, Training Accuracy= 0.502\n",
      "Epoch: 750, Loss= 0.7197, Training Accuracy= 0.502\n",
      "Epoch: 760, Loss= 0.7194, Training Accuracy= 0.502\n",
      "Epoch: 770, Loss= 0.7191, Training Accuracy= 0.502\n",
      "Epoch: 780, Loss= 0.7177, Training Accuracy= 0.502\n",
      "Epoch: 790, Loss= 0.7146, Training Accuracy= 0.502\n",
      "Epoch: 800, Loss= 0.7127, Training Accuracy= 0.502\n",
      "Epoch: 810, Loss= 0.7109, Training Accuracy= 0.502\n",
      "Epoch: 820, Loss= 0.7100, Training Accuracy= 0.502\n",
      "Epoch: 830, Loss= 0.7089, Training Accuracy= 0.502\n",
      "Epoch: 840, Loss= 0.7030, Training Accuracy= 0.502\n",
      "Epoch: 850, Loss= 0.7031, Training Accuracy= 0.502\n",
      "Epoch: 860, Loss= 0.7031, Training Accuracy= 0.502\n",
      "Epoch: 870, Loss= 0.7016, Training Accuracy= 0.502\n",
      "Epoch: 880, Loss= 0.7015, Training Accuracy= 0.502\n",
      "Epoch: 890, Loss= 0.7014, Training Accuracy= 0.502\n",
      "Epoch: 900, Loss= 0.7009, Training Accuracy= 0.502\n",
      "Epoch: 910, Loss= 0.7008, Training Accuracy= 0.502\n",
      "Epoch: 920, Loss= 0.7009, Training Accuracy= 0.502\n",
      "Epoch: 930, Loss= 0.7018, Training Accuracy= 0.502\n",
      "Epoch: 940, Loss= 0.7036, Training Accuracy= 0.502\n",
      "Epoch: 950, Loss= 0.7040, Training Accuracy= 0.502\n",
      "Epoch: 960, Loss= 0.7043, Training Accuracy= 0.502\n",
      "Epoch: 970, Loss= 0.7045, Training Accuracy= 0.502\n",
      "Epoch: 980, Loss= 0.7046, Training Accuracy= 0.502\n",
      "Epoch: 990, Loss= 0.7034, Training Accuracy= 0.502\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.495\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.7021, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.7011, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.7009, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.7009, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.7008, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.7007, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.7005, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.7004, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.7002, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.6999, Training Accuracy= 0.497\n",
      "Epoch: 100, Loss= 0.6997, Training Accuracy= 0.497\n",
      "Epoch: 110, Loss= 0.6994, Training Accuracy= 0.497\n",
      "Epoch: 120, Loss= 0.6991, Training Accuracy= 0.497\n",
      "Epoch: 130, Loss= 0.6990, Training Accuracy= 0.497\n",
      "Epoch: 140, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 150, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 160, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 170, Loss= 0.6987, Training Accuracy= 0.497\n",
      "Epoch: 180, Loss= 0.6986, Training Accuracy= 0.497\n",
      "Epoch: 190, Loss= 0.6985, Training Accuracy= 0.497\n",
      "Epoch: 200, Loss= 0.6985, Training Accuracy= 0.497\n",
      "Epoch: 210, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 220, Loss= 0.6984, Training Accuracy= 0.497\n",
      "Epoch: 230, Loss= 0.6983, Training Accuracy= 0.497\n",
      "Epoch: 240, Loss= 0.6982, Training Accuracy= 0.497\n",
      "Epoch: 250, Loss= 0.6980, Training Accuracy= 0.497\n",
      "Epoch: 260, Loss= 0.6978, Training Accuracy= 0.497\n",
      "Epoch: 270, Loss= 0.6976, Training Accuracy= 0.497\n",
      "Epoch: 280, Loss= 0.6973, Training Accuracy= 0.497\n",
      "Epoch: 290, Loss= 0.6971, Training Accuracy= 0.497\n",
      "Epoch: 300, Loss= 0.6968, Training Accuracy= 0.497\n",
      "Epoch: 310, Loss= 0.6966, Training Accuracy= 0.497\n",
      "Epoch: 320, Loss= 0.6965, Training Accuracy= 0.497\n",
      "Epoch: 330, Loss= 0.6963, Training Accuracy= 0.497\n",
      "Epoch: 340, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 350, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 360, Loss= 0.6960, Training Accuracy= 0.497\n",
      "Epoch: 370, Loss= 0.6959, Training Accuracy= 0.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 390, Loss= 0.6956, Training Accuracy= 0.497\n",
      "Epoch: 400, Loss= 0.6954, Training Accuracy= 0.498\n",
      "Epoch: 410, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 420, Loss= 0.6951, Training Accuracy= 0.501\n",
      "Epoch: 430, Loss= 0.6948, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.6946, Training Accuracy= 0.502\n",
      "Epoch: 450, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 460, Loss= 0.6940, Training Accuracy= 0.504\n",
      "Epoch: 470, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 480, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 490, Loss= 0.6946, Training Accuracy= 0.497\n",
      "Epoch: 500, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 520, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 530, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 540, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 550, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 560, Loss= 0.6950, Training Accuracy= 0.496\n",
      "Epoch: 570, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 580, Loss= 0.6965, Training Accuracy= 0.497\n",
      "Epoch: 590, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 600, Loss= 0.6955, Training Accuracy= 0.497\n",
      "Epoch: 610, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 620, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 0.6945, Training Accuracy= 0.498\n",
      "Epoch: 640, Loss= 0.6945, Training Accuracy= 0.491\n",
      "Epoch: 650, Loss= 0.6945, Training Accuracy= 0.497\n",
      "Epoch: 660, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 670, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 680, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 690, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 700, Loss= 0.6957, Training Accuracy= 0.497\n",
      "Epoch: 710, Loss= 0.6956, Training Accuracy= 0.497\n",
      "Epoch: 720, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 730, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 740, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 750, Loss= 0.6960, Training Accuracy= 0.497\n",
      "Epoch: 760, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 770, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 780, Loss= 0.6958, Training Accuracy= 0.497\n",
      "Epoch: 790, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 800, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 810, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 820, Loss= 0.6959, Training Accuracy= 0.497\n",
      "Epoch: 830, Loss= 0.6960, Training Accuracy= 0.497\n",
      "Epoch: 840, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 850, Loss= 0.6960, Training Accuracy= 0.497\n",
      "Epoch: 860, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 870, Loss= 0.6961, Training Accuracy= 0.497\n",
      "Epoch: 880, Loss= 0.6964, Training Accuracy= 0.497\n",
      "Epoch: 890, Loss= 0.6964, Training Accuracy= 0.497\n",
      "Epoch: 900, Loss= 0.6965, Training Accuracy= 0.497\n",
      "Epoch: 910, Loss= 0.6965, Training Accuracy= 0.497\n",
      "Epoch: 920, Loss= 0.6965, Training Accuracy= 0.497\n",
      "Epoch: 930, Loss= 0.6963, Training Accuracy= 0.497\n",
      "Epoch: 940, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 950, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 960, Loss= 0.6962, Training Accuracy= 0.497\n",
      "Epoch: 970, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 980, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 990, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.504\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.6976, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6960, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6954, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6950, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6945, Training Accuracy= 0.507\n",
      "Epoch: 60, Loss= 0.6943, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.6940, Training Accuracy= 0.507\n",
      "Epoch: 80, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 100, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 110, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 120, Loss= 0.6935, Training Accuracy= 0.511\n",
      "Epoch: 130, Loss= 0.6935, Training Accuracy= 0.510\n",
      "Epoch: 140, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 150, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 160, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 170, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 180, Loss= 0.6936, Training Accuracy= 0.510\n",
      "Epoch: 190, Loss= 0.6936, Training Accuracy= 0.510\n",
      "Epoch: 200, Loss= 0.6936, Training Accuracy= 0.514\n",
      "Epoch: 210, Loss= 0.6935, Training Accuracy= 0.514\n",
      "Epoch: 220, Loss= 0.6929, Training Accuracy= 0.520\n",
      "Epoch: 230, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 240, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6943, Training Accuracy= 0.511\n",
      "Epoch: 260, Loss= 0.6940, Training Accuracy= 0.513\n",
      "Epoch: 270, Loss= 0.6939, Training Accuracy= 0.515\n",
      "Epoch: 280, Loss= 0.6943, Training Accuracy= 0.515\n",
      "Epoch: 290, Loss= 0.6945, Training Accuracy= 0.517\n",
      "Epoch: 300, Loss= 0.6957, Training Accuracy= 0.500\n",
      "Epoch: 310, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 320, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 330, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 340, Loss= 0.6954, Training Accuracy= 0.500\n",
      "Epoch: 350, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 360, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 370, Loss= 0.6941, Training Accuracy= 0.512\n",
      "Epoch: 380, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 390, Loss= 0.6945, Training Accuracy= 0.502\n",
      "Epoch: 400, Loss= 0.6946, Training Accuracy= 0.505\n",
      "Epoch: 410, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.6940, Training Accuracy= 0.508\n",
      "Epoch: 430, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 440, Loss= 0.6951, Training Accuracy= 0.505\n",
      "Epoch: 450, Loss= 0.6947, Training Accuracy= 0.504\n",
      "Epoch: 460, Loss= 0.6943, Training Accuracy= 0.508\n",
      "Epoch: 470, Loss= 0.6938, Training Accuracy= 0.506\n",
      "Epoch: 480, Loss= 0.6952, Training Accuracy= 0.503\n",
      "Epoch: 490, Loss= 0.6951, Training Accuracy= 0.505\n",
      "Epoch: 500, Loss= 0.6946, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 520, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 530, Loss= 0.6935, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.6935, Training Accuracy= 0.511\n",
      "Epoch: 550, Loss= 0.6934, Training Accuracy= 0.513\n",
      "Epoch: 560, Loss= 0.6935, Training Accuracy= 0.514\n",
      "Epoch: 570, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 580, Loss= 0.6972, Training Accuracy= 0.502\n",
      "Epoch: 590, Loss= 0.6969, Training Accuracy= 0.502\n",
      "Epoch: 600, Loss= 0.6966, Training Accuracy= 0.502\n",
      "Epoch: 610, Loss= 0.6964, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 630, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 640, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 650, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 660, Loss= 0.6947, Training Accuracy= 0.501\n",
      "Epoch: 670, Loss= 0.6951, Training Accuracy= 0.500\n",
      "Epoch: 680, Loss= 0.6957, Training Accuracy= 0.499\n",
      "Epoch: 690, Loss= 0.6950, Training Accuracy= 0.500\n",
      "Epoch: 700, Loss= 0.6957, Training Accuracy= 0.500\n",
      "Epoch: 710, Loss= 0.6955, Training Accuracy= 0.500\n",
      "Epoch: 720, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 730, Loss= 0.6955, Training Accuracy= 0.500\n",
      "Epoch: 740, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 750, Loss= 0.6954, Training Accuracy= 0.500\n",
      "Epoch: 760, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 770, Loss= 0.6960, Training Accuracy= 0.499\n",
      "Epoch: 780, Loss= 0.6966, Training Accuracy= 0.500\n",
      "Epoch: 790, Loss= 0.6961, Training Accuracy= 0.500\n",
      "Epoch: 800, Loss= 0.6958, Training Accuracy= 0.500\n",
      "Epoch: 810, Loss= 0.6959, Training Accuracy= 0.500\n",
      "Epoch: 820, Loss= 0.6959, Training Accuracy= 0.500\n",
      "Epoch: 830, Loss= 0.6967, Training Accuracy= 0.499\n",
      "Epoch: 840, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 850, Loss= 0.6955, Training Accuracy= 0.500\n",
      "Epoch: 860, Loss= 0.6965, Training Accuracy= 0.500\n",
      "Epoch: 870, Loss= 0.6965, Training Accuracy= 0.500\n",
      "Epoch: 880, Loss= 0.6954, Training Accuracy= 0.500\n",
      "Epoch: 890, Loss= 0.6952, Training Accuracy= 0.500\n",
      "Epoch: 900, Loss= 0.6959, Training Accuracy= 0.500\n",
      "Epoch: 910, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 920, Loss= 0.6954, Training Accuracy= 0.500\n",
      "Epoch: 930, Loss= 0.6957, Training Accuracy= 0.500\n",
      "Epoch: 940, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 950, Loss= 0.6955, Training Accuracy= 0.500\n",
      "Epoch: 960, Loss= 0.6969, Training Accuracy= 0.500\n",
      "Epoch: 970, Loss= 0.6970, Training Accuracy= 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.6969, Training Accuracy= 0.500\n",
      "Epoch: 990, Loss= 0.6965, Training Accuracy= 0.500\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5031\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.7261, Training Accuracy= 0.493\n",
      "Epoch: 10, Loss= 0.7066, Training Accuracy= 0.493\n",
      "Epoch: 20, Loss= 0.7033, Training Accuracy= 0.493\n",
      "Epoch: 30, Loss= 0.7019, Training Accuracy= 0.493\n",
      "Epoch: 40, Loss= 0.7010, Training Accuracy= 0.493\n",
      "Epoch: 50, Loss= 0.7005, Training Accuracy= 0.493\n",
      "Epoch: 60, Loss= 0.7001, Training Accuracy= 0.493\n",
      "Epoch: 70, Loss= 0.6999, Training Accuracy= 0.493\n",
      "Epoch: 80, Loss= 0.6996, Training Accuracy= 0.493\n",
      "Epoch: 90, Loss= 0.6995, Training Accuracy= 0.493\n",
      "Epoch: 100, Loss= 0.6993, Training Accuracy= 0.493\n",
      "Epoch: 110, Loss= 0.6992, Training Accuracy= 0.493\n",
      "Epoch: 120, Loss= 0.6991, Training Accuracy= 0.493\n",
      "Epoch: 130, Loss= 0.6991, Training Accuracy= 0.493\n",
      "Epoch: 140, Loss= 0.6990, Training Accuracy= 0.493\n",
      "Epoch: 150, Loss= 0.6989, Training Accuracy= 0.493\n",
      "Epoch: 160, Loss= 0.6989, Training Accuracy= 0.494\n",
      "Epoch: 170, Loss= 0.6990, Training Accuracy= 0.494\n",
      "Epoch: 180, Loss= 0.6992, Training Accuracy= 0.494\n",
      "Epoch: 190, Loss= 0.6995, Training Accuracy= 0.493\n",
      "Epoch: 200, Loss= 0.6992, Training Accuracy= 0.493\n",
      "Epoch: 210, Loss= 0.6989, Training Accuracy= 0.493\n",
      "Epoch: 220, Loss= 0.6992, Training Accuracy= 0.494\n",
      "Epoch: 230, Loss= 0.6993, Training Accuracy= 0.494\n",
      "Epoch: 240, Loss= 0.6994, Training Accuracy= 0.494\n",
      "Epoch: 250, Loss= 0.6995, Training Accuracy= 0.493\n",
      "Epoch: 260, Loss= 0.6997, Training Accuracy= 0.493\n",
      "Epoch: 270, Loss= 0.6999, Training Accuracy= 0.493\n",
      "Epoch: 280, Loss= 0.6999, Training Accuracy= 0.494\n",
      "Epoch: 290, Loss= 0.7002, Training Accuracy= 0.493\n",
      "Epoch: 300, Loss= 0.7003, Training Accuracy= 0.493\n",
      "Epoch: 310, Loss= 0.7004, Training Accuracy= 0.493\n",
      "Epoch: 320, Loss= 0.7004, Training Accuracy= 0.493\n",
      "Epoch: 330, Loss= 0.7005, Training Accuracy= 0.493\n",
      "Epoch: 340, Loss= 0.7005, Training Accuracy= 0.493\n",
      "Epoch: 350, Loss= 0.7005, Training Accuracy= 0.493\n",
      "Epoch: 360, Loss= 0.7004, Training Accuracy= 0.493\n",
      "Epoch: 370, Loss= 0.7002, Training Accuracy= 0.493\n",
      "Epoch: 380, Loss= 0.6999, Training Accuracy= 0.493\n",
      "Epoch: 390, Loss= 0.6995, Training Accuracy= 0.493\n",
      "Epoch: 400, Loss= 0.6994, Training Accuracy= 0.493\n",
      "Epoch: 410, Loss= 0.6992, Training Accuracy= 0.493\n",
      "Epoch: 420, Loss= 0.7032, Training Accuracy= 0.499\n",
      "Epoch: 430, Loss= 0.7004, Training Accuracy= 0.493\n",
      "Epoch: 440, Loss= 0.7003, Training Accuracy= 0.493\n",
      "Epoch: 450, Loss= 0.7011, Training Accuracy= 0.494\n",
      "Epoch: 460, Loss= 0.6994, Training Accuracy= 0.493\n",
      "Epoch: 470, Loss= 0.6991, Training Accuracy= 0.493\n",
      "Epoch: 480, Loss= 0.6992, Training Accuracy= 0.493\n",
      "Epoch: 490, Loss= 0.6993, Training Accuracy= 0.495\n",
      "Epoch: 500, Loss= 0.6994, Training Accuracy= 0.496\n",
      "Epoch: 510, Loss= 0.6995, Training Accuracy= 0.496\n",
      "Epoch: 520, Loss= 0.6995, Training Accuracy= 0.496\n",
      "Epoch: 530, Loss= 0.6995, Training Accuracy= 0.496\n",
      "Epoch: 540, Loss= 0.6996, Training Accuracy= 0.496\n",
      "Epoch: 550, Loss= 0.6998, Training Accuracy= 0.496\n",
      "Epoch: 560, Loss= 0.7000, Training Accuracy= 0.495\n",
      "Epoch: 570, Loss= 0.7000, Training Accuracy= 0.497\n",
      "Epoch: 580, Loss= 0.7009, Training Accuracy= 0.495\n",
      "Epoch: 590, Loss= 0.6998, Training Accuracy= 0.495\n",
      "Epoch: 600, Loss= 0.6997, Training Accuracy= 0.495\n",
      "Epoch: 610, Loss= 0.6998, Training Accuracy= 0.496\n",
      "Epoch: 620, Loss= 0.6999, Training Accuracy= 0.496\n",
      "Epoch: 630, Loss= 0.7000, Training Accuracy= 0.496\n",
      "Epoch: 640, Loss= 0.7003, Training Accuracy= 0.496\n",
      "Epoch: 650, Loss= 0.7004, Training Accuracy= 0.496\n",
      "Epoch: 660, Loss= 0.7004, Training Accuracy= 0.496\n",
      "Epoch: 670, Loss= 0.7003, Training Accuracy= 0.497\n",
      "Epoch: 680, Loss= 0.7004, Training Accuracy= 0.496\n",
      "Epoch: 690, Loss= 0.7005, Training Accuracy= 0.496\n",
      "Epoch: 700, Loss= 0.7004, Training Accuracy= 0.496\n",
      "Epoch: 710, Loss= 0.7001, Training Accuracy= 0.496\n",
      "Epoch: 720, Loss= 0.6999, Training Accuracy= 0.496\n",
      "Epoch: 730, Loss= 0.6999, Training Accuracy= 0.496\n",
      "Epoch: 740, Loss= 0.7052, Training Accuracy= 0.493\n",
      "Epoch: 750, Loss= 0.7006, Training Accuracy= 0.493\n",
      "Epoch: 760, Loss= 0.6988, Training Accuracy= 0.494\n",
      "Epoch: 770, Loss= 0.6997, Training Accuracy= 0.496\n",
      "Epoch: 780, Loss= 0.6998, Training Accuracy= 0.497\n",
      "Epoch: 790, Loss= 0.6998, Training Accuracy= 0.497\n",
      "Epoch: 800, Loss= 0.7020, Training Accuracy= 0.493\n",
      "Epoch: 810, Loss= 0.7012, Training Accuracy= 0.493\n",
      "Epoch: 820, Loss= 0.7003, Training Accuracy= 0.494\n",
      "Epoch: 830, Loss= 0.6996, Training Accuracy= 0.495\n",
      "Epoch: 840, Loss= 0.6994, Training Accuracy= 0.496\n",
      "Epoch: 850, Loss= 0.6995, Training Accuracy= 0.496\n",
      "Epoch: 860, Loss= 0.7020, Training Accuracy= 0.493\n",
      "Epoch: 870, Loss= 0.7003, Training Accuracy= 0.493\n",
      "Epoch: 880, Loss= 0.7002, Training Accuracy= 0.493\n",
      "Epoch: 890, Loss= 0.6993, Training Accuracy= 0.496\n",
      "Epoch: 900, Loss= 0.6995, Training Accuracy= 0.497\n",
      "Epoch: 910, Loss= 0.7010, Training Accuracy= 0.493\n",
      "Epoch: 920, Loss= 0.6998, Training Accuracy= 0.495\n",
      "Epoch: 930, Loss= 0.6996, Training Accuracy= 0.498\n",
      "Epoch: 940, Loss= 0.6997, Training Accuracy= 0.497\n",
      "Epoch: 950, Loss= 0.6996, Training Accuracy= 0.497\n",
      "Epoch: 960, Loss= 0.7218, Training Accuracy= 0.497\n",
      "Epoch: 970, Loss= 0.7214, Training Accuracy= 0.497\n",
      "Epoch: 980, Loss= 0.7212, Training Accuracy= 0.497\n",
      "Epoch: 990, Loss= 0.7212, Training Accuracy= 0.497\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4945\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7290, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.7061, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.7014, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6986, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.6967, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6954, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6947, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.6944, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 120, Loss= 0.6944, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.6945, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.6946, Training Accuracy= 0.502\n",
      "Epoch: 150, Loss= 0.6947, Training Accuracy= 0.502\n",
      "Epoch: 160, Loss= 0.6949, Training Accuracy= 0.502\n",
      "Epoch: 170, Loss= 0.6950, Training Accuracy= 0.502\n",
      "Epoch: 180, Loss= 0.6951, Training Accuracy= 0.502\n",
      "Epoch: 190, Loss= 0.6952, Training Accuracy= 0.502\n",
      "Epoch: 200, Loss= 0.6952, Training Accuracy= 0.502\n",
      "Epoch: 210, Loss= 0.6953, Training Accuracy= 0.502\n",
      "Epoch: 220, Loss= 0.6953, Training Accuracy= 0.502\n",
      "Epoch: 230, Loss= 0.6953, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6952, Training Accuracy= 0.504\n",
      "Epoch: 250, Loss= 0.6952, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.6951, Training Accuracy= 0.504\n",
      "Epoch: 270, Loss= 0.6950, Training Accuracy= 0.504\n",
      "Epoch: 280, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 290, Loss= 0.6947, Training Accuracy= 0.504\n",
      "Epoch: 300, Loss= 0.6945, Training Accuracy= 0.506\n",
      "Epoch: 310, Loss= 0.6942, Training Accuracy= 0.508\n",
      "Epoch: 320, Loss= 0.6940, Training Accuracy= 0.507\n",
      "Epoch: 330, Loss= 0.6937, Training Accuracy= 0.510\n",
      "Epoch: 340, Loss= 0.6939, Training Accuracy= 0.512\n",
      "Epoch: 350, Loss= 0.6940, Training Accuracy= 0.513\n",
      "Epoch: 360, Loss= 0.6938, Training Accuracy= 0.514\n",
      "Epoch: 370, Loss= 0.6938, Training Accuracy= 0.514\n",
      "Epoch: 380, Loss= 0.6937, Training Accuracy= 0.515\n",
      "Epoch: 390, Loss= 0.6937, Training Accuracy= 0.515\n",
      "Epoch: 400, Loss= 0.6937, Training Accuracy= 0.514\n",
      "Epoch: 410, Loss= 0.6935, Training Accuracy= 0.515\n",
      "Epoch: 420, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 430, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 440, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 460, Loss= 0.6929, Training Accuracy= 0.517\n",
      "Epoch: 470, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 480, Loss= 0.6928, Training Accuracy= 0.518\n",
      "Epoch: 490, Loss= 0.6928, Training Accuracy= 0.520\n",
      "Epoch: 500, Loss= 0.6928, Training Accuracy= 0.517\n",
      "Epoch: 510, Loss= 0.6929, Training Accuracy= 0.521\n",
      "Epoch: 520, Loss= 0.6929, Training Accuracy= 0.521\n",
      "Epoch: 530, Loss= 0.6929, Training Accuracy= 0.521\n",
      "Epoch: 540, Loss= 0.6928, Training Accuracy= 0.519\n",
      "Epoch: 550, Loss= 0.6931, Training Accuracy= 0.517\n",
      "Epoch: 560, Loss= 0.6928, Training Accuracy= 0.521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 580, Loss= 0.6923, Training Accuracy= 0.523\n",
      "Epoch: 590, Loss= 0.6927, Training Accuracy= 0.520\n",
      "Epoch: 600, Loss= 0.6928, Training Accuracy= 0.524\n",
      "Epoch: 610, Loss= 0.6929, Training Accuracy= 0.523\n",
      "Epoch: 620, Loss= 0.6928, Training Accuracy= 0.524\n",
      "Epoch: 630, Loss= 0.6928, Training Accuracy= 0.524\n",
      "Epoch: 640, Loss= 0.6929, Training Accuracy= 0.520\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.525\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.524\n",
      "Epoch: 670, Loss= 0.6929, Training Accuracy= 0.523\n",
      "Epoch: 680, Loss= 0.6930, Training Accuracy= 0.522\n",
      "Epoch: 690, Loss= 0.6919, Training Accuracy= 0.525\n",
      "Epoch: 700, Loss= 0.6921, Training Accuracy= 0.524\n",
      "Epoch: 710, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 720, Loss= 0.7109, Training Accuracy= 0.509\n",
      "Epoch: 730, Loss= 0.7107, Training Accuracy= 0.509\n",
      "Epoch: 740, Loss= 0.7107, Training Accuracy= 0.509\n",
      "Epoch: 750, Loss= 0.7106, Training Accuracy= 0.509\n",
      "Epoch: 760, Loss= 0.7105, Training Accuracy= 0.510\n",
      "Epoch: 770, Loss= 0.7108, Training Accuracy= 0.510\n",
      "Epoch: 780, Loss= 0.7109, Training Accuracy= 0.509\n",
      "Epoch: 790, Loss= 0.7110, Training Accuracy= 0.510\n",
      "Epoch: 800, Loss= 0.7110, Training Accuracy= 0.510\n",
      "Epoch: 810, Loss= 0.7108, Training Accuracy= 0.510\n",
      "Epoch: 820, Loss= 0.7033, Training Accuracy= 0.509\n",
      "Epoch: 830, Loss= 0.7054, Training Accuracy= 0.508\n",
      "Epoch: 840, Loss= 0.7051, Training Accuracy= 0.510\n",
      "Epoch: 850, Loss= 0.7286, Training Accuracy= 0.510\n",
      "Epoch: 860, Loss= 0.6975, Training Accuracy= 0.508\n",
      "Epoch: 870, Loss= 0.6975, Training Accuracy= 0.508\n",
      "Epoch: 880, Loss= 0.6975, Training Accuracy= 0.508\n",
      "Epoch: 890, Loss= 0.6975, Training Accuracy= 0.508\n",
      "Epoch: 900, Loss= 0.6975, Training Accuracy= 0.508\n",
      "Epoch: 910, Loss= 0.6974, Training Accuracy= 0.508\n",
      "Epoch: 920, Loss= 0.6975, Training Accuracy= 0.511\n",
      "Epoch: 930, Loss= 0.7091, Training Accuracy= 0.507\n",
      "Epoch: 940, Loss= 0.7084, Training Accuracy= 0.507\n",
      "Epoch: 950, Loss= 0.7077, Training Accuracy= 0.507\n",
      "Epoch: 960, Loss= 0.7067, Training Accuracy= 0.507\n",
      "Epoch: 970, Loss= 0.7049, Training Accuracy= 0.505\n",
      "Epoch: 980, Loss= 0.7117, Training Accuracy= 0.502\n",
      "Epoch: 990, Loss= 0.7117, Training Accuracy= 0.502\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5002\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.275\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.5029, 0.4941, 0.4946, 0.49759999, 0.49689999, 0.495, 0.50400001, 0.50309998, 0.49450001, 0.50019997]\n",
      "mean of test_accuracies_10replications:  0.49829\n",
      "standard deviation of test_accuracies_10replications_std_mean:  3.73374763876e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh5mZFcWJw8zMivL/AbUqnVNNb5TgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9953355650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
