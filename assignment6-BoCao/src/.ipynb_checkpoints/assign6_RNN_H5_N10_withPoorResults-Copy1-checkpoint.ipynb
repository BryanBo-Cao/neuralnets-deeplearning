{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 5\n",
    "N = 10\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.7008, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.6953, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.6948, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.6945, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.6942, Training Accuracy= 0.501\n",
      "Epoch: 50, Loss= 0.6941, Training Accuracy= 0.504\n",
      "Epoch: 60, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.6936, Training Accuracy= 0.501\n",
      "Epoch: 130, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.6936, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.6936, Training Accuracy= 0.501\n",
      "Epoch: 160, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 190, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 200, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 210, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 230, Loss= 0.6922, Training Accuracy= 0.523\n",
      "Epoch: 240, Loss= 0.6888, Training Accuracy= 0.515\n",
      "Epoch: 250, Loss= 0.7226, Training Accuracy= 0.470\n",
      "Epoch: 260, Loss= 0.6993, Training Accuracy= 0.503\n",
      "Epoch: 270, Loss= 0.6964, Training Accuracy= 0.527\n",
      "Epoch: 280, Loss= 0.6949, Training Accuracy= 0.518\n",
      "Epoch: 290, Loss= 0.7005, Training Accuracy= 0.512\n",
      "Epoch: 300, Loss= 0.7295, Training Accuracy= 0.497\n",
      "Epoch: 310, Loss= 0.7057, Training Accuracy= 0.504\n",
      "Epoch: 320, Loss= 0.7266, Training Accuracy= 0.501\n",
      "Epoch: 330, Loss= 0.6958, Training Accuracy= 0.517\n",
      "Epoch: 340, Loss= 0.7067, Training Accuracy= 0.500\n",
      "Epoch: 350, Loss= 0.7007, Training Accuracy= 0.502\n",
      "Epoch: 360, Loss= 0.6954, Training Accuracy= 0.499\n",
      "Epoch: 370, Loss= 0.6977, Training Accuracy= 0.502\n",
      "Epoch: 380, Loss= 0.6974, Training Accuracy= 0.502\n",
      "Epoch: 390, Loss= 0.6972, Training Accuracy= 0.501\n",
      "Epoch: 400, Loss= 0.6976, Training Accuracy= 0.503\n",
      "Epoch: 410, Loss= 0.7000, Training Accuracy= 0.502\n",
      "Epoch: 420, Loss= 0.6980, Training Accuracy= 0.502\n",
      "Epoch: 430, Loss= 0.6986, Training Accuracy= 0.503\n",
      "Epoch: 440, Loss= 0.6979, Training Accuracy= 0.508\n",
      "Epoch: 450, Loss= 0.6985, Training Accuracy= 0.509\n",
      "Epoch: 460, Loss= 0.6987, Training Accuracy= 0.506\n",
      "Epoch: 470, Loss= 0.7009, Training Accuracy= 0.507\n",
      "Epoch: 480, Loss= 0.6966, Training Accuracy= 0.510\n",
      "Epoch: 490, Loss= 0.7016, Training Accuracy= 0.504\n",
      "Epoch: 500, Loss= 0.7015, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6971, Training Accuracy= 0.504\n",
      "Epoch: 520, Loss= 0.6970, Training Accuracy= 0.504\n",
      "Epoch: 530, Loss= 0.6978, Training Accuracy= 0.508\n",
      "Epoch: 540, Loss= 0.7001, Training Accuracy= 0.507\n",
      "Epoch: 550, Loss= 0.6997, Training Accuracy= 0.504\n",
      "Epoch: 560, Loss= 0.6971, Training Accuracy= 0.515\n",
      "Epoch: 570, Loss= 0.7005, Training Accuracy= 0.506\n",
      "Epoch: 580, Loss= 0.7038, Training Accuracy= 0.517\n",
      "Epoch: 590, Loss= 0.7110, Training Accuracy= 0.520\n",
      "Epoch: 600, Loss= 0.7011, Training Accuracy= 0.504\n",
      "Epoch: 610, Loss= 0.7027, Training Accuracy= 0.518\n",
      "Epoch: 620, Loss= 0.7007, Training Accuracy= 0.512\n",
      "Epoch: 630, Loss= 0.6984, Training Accuracy= 0.511\n",
      "Epoch: 640, Loss= 0.6931, Training Accuracy= 0.522\n",
      "Epoch: 650, Loss= 0.6945, Training Accuracy= 0.532\n",
      "Epoch: 660, Loss= 0.6995, Training Accuracy= 0.495\n",
      "Epoch: 670, Loss= 0.6758, Training Accuracy= 0.576\n",
      "Epoch: 680, Loss= 0.6997, Training Accuracy= 0.501\n",
      "Epoch: 690, Loss= 0.6978, Training Accuracy= 0.501\n",
      "Epoch: 700, Loss= 0.6953, Training Accuracy= 0.498\n",
      "Epoch: 710, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 720, Loss= 0.6983, Training Accuracy= 0.511\n",
      "Epoch: 730, Loss= 0.6998, Training Accuracy= 0.501\n",
      "Epoch: 740, Loss= 0.6999, Training Accuracy= 0.501\n",
      "Epoch: 750, Loss= 0.6989, Training Accuracy= 0.512\n",
      "Epoch: 760, Loss= 0.7010, Training Accuracy= 0.507\n",
      "Epoch: 770, Loss= 0.6989, Training Accuracy= 0.512\n",
      "Epoch: 780, Loss= 0.6979, Training Accuracy= 0.510\n",
      "Epoch: 790, Loss= 0.6970, Training Accuracy= 0.512\n",
      "Epoch: 800, Loss= 0.6978, Training Accuracy= 0.514\n",
      "Epoch: 810, Loss= 0.6988, Training Accuracy= 0.500\n",
      "Epoch: 820, Loss= 0.6960, Training Accuracy= 0.500\n",
      "Epoch: 830, Loss= 0.6976, Training Accuracy= 0.506\n",
      "Epoch: 840, Loss= 0.6975, Training Accuracy= 0.509\n",
      "Epoch: 850, Loss= 0.6963, Training Accuracy= 0.502\n",
      "Epoch: 860, Loss= 0.6938, Training Accuracy= 0.510\n",
      "Epoch: 870, Loss= 0.6940, Training Accuracy= 0.549\n",
      "Epoch: 880, Loss= 0.6991, Training Accuracy= 0.506\n",
      "Epoch: 890, Loss= 0.7087, Training Accuracy= 0.497\n",
      "Epoch: 900, Loss= 0.7081, Training Accuracy= 0.501\n",
      "Epoch: 910, Loss= 0.7081, Training Accuracy= 0.502\n",
      "Epoch: 920, Loss= 0.7035, Training Accuracy= 0.510\n",
      "Epoch: 930, Loss= 0.6941, Training Accuracy= 0.507\n",
      "Epoch: 940, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 950, Loss= 0.6947, Training Accuracy= 0.502\n",
      "Epoch: 960, Loss= 0.6946, Training Accuracy= 0.507\n",
      "Epoch: 970, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 980, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 990, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 1000, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 1010, Loss= 0.6943, Training Accuracy= 0.505\n",
      "Epoch: 1020, Loss= 0.6943, Training Accuracy= 0.506\n",
      "Epoch: 1030, Loss= 0.6945, Training Accuracy= 0.504\n",
      "Epoch: 1040, Loss= 0.6943, Training Accuracy= 0.507\n",
      "Epoch: 1050, Loss= 0.6994, Training Accuracy= 0.502\n",
      "Epoch: 1060, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 1070, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 1080, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 1090, Loss= 0.6975, Training Accuracy= 0.504\n",
      "Epoch: 1100, Loss= 0.7005, Training Accuracy= 0.500\n",
      "Epoch: 1110, Loss= 0.7022, Training Accuracy= 0.498\n",
      "Epoch: 1120, Loss= 0.7083, Training Accuracy= 0.500\n",
      "Epoch: 1130, Loss= 0.6929, Training Accuracy= 0.524\n",
      "Epoch: 1140, Loss= 0.7017, Training Accuracy= 0.501\n",
      "Epoch: 1150, Loss= 0.7013, Training Accuracy= 0.501\n",
      "Epoch: 1160, Loss= 0.7018, Training Accuracy= 0.501\n",
      "Epoch: 1170, Loss= 0.7055, Training Accuracy= 0.501\n",
      "Epoch: 1180, Loss= 0.7054, Training Accuracy= 0.501\n",
      "Epoch: 1190, Loss= 0.7051, Training Accuracy= 0.501\n",
      "Epoch: 1200, Loss= 0.7040, Training Accuracy= 0.501\n",
      "Epoch: 1210, Loss= 0.7013, Training Accuracy= 0.501\n",
      "Epoch: 1220, Loss= 0.6996, Training Accuracy= 0.502\n",
      "Epoch: 1230, Loss= 0.6998, Training Accuracy= 0.502\n",
      "Epoch: 1240, Loss= 0.6996, Training Accuracy= 0.500\n",
      "Epoch: 1250, Loss= 0.6995, Training Accuracy= 0.501\n",
      "Epoch: 1260, Loss= 0.6994, Training Accuracy= 0.501\n",
      "Epoch: 1270, Loss= 0.6994, Training Accuracy= 0.501\n",
      "Epoch: 1280, Loss= 0.6994, Training Accuracy= 0.501\n",
      "Epoch: 1290, Loss= 0.6993, Training Accuracy= 0.502\n",
      "Epoch: 1300, Loss= 0.6993, Training Accuracy= 0.502\n",
      "Epoch: 1310, Loss= 0.6993, Training Accuracy= 0.500\n",
      "Epoch: 1320, Loss= 0.6992, Training Accuracy= 0.500\n",
      "Epoch: 1330, Loss= 0.6991, Training Accuracy= 0.500\n",
      "Epoch: 1340, Loss= 0.6990, Training Accuracy= 0.500\n",
      "Epoch: 1350, Loss= 0.6990, Training Accuracy= 0.502\n",
      "Epoch: 1360, Loss= 0.6989, Training Accuracy= 0.502\n",
      "Epoch: 1370, Loss= 0.6992, Training Accuracy= 0.502\n",
      "Epoch: 1380, Loss= 0.6990, Training Accuracy= 0.502\n",
      "Epoch: 1390, Loss= 0.6992, Training Accuracy= 0.502\n",
      "Epoch: 1400, Loss= 0.6990, Training Accuracy= 0.502\n",
      "Epoch: 1410, Loss= 0.6991, Training Accuracy= 0.502\n",
      "Epoch: 1420, Loss= 0.6990, Training Accuracy= 0.502\n",
      "Epoch: 1430, Loss= 0.6993, Training Accuracy= 0.501\n",
      "Epoch: 1440, Loss= 0.6993, Training Accuracy= 0.501\n",
      "Epoch: 1450, Loss= 0.6995, Training Accuracy= 0.502\n",
      "Epoch: 1460, Loss= 0.7004, Training Accuracy= 0.503\n",
      "Epoch: 1470, Loss= 0.7011, Training Accuracy= 0.504\n",
      "Epoch: 1480, Loss= 0.7012, Training Accuracy= 0.505\n",
      "Epoch: 1490, Loss= 0.7011, Training Accuracy= 0.505\n",
      "Epoch: 1500, Loss= 0.7007, Training Accuracy= 0.504\n",
      "Epoch: 1510, Loss= 0.7006, Training Accuracy= 0.506\n",
      "Epoch: 1520, Loss= 0.7008, Training Accuracy= 0.505\n",
      "Epoch: 1530, Loss= 0.6994, Training Accuracy= 0.506\n",
      "Epoch: 1540, Loss= 0.6993, Training Accuracy= 0.507\n",
      "Epoch: 1550, Loss= 0.7000, Training Accuracy= 0.507\n",
      "Epoch: 1560, Loss= 0.6971, Training Accuracy= 0.501\n",
      "Epoch: 1570, Loss= 0.6971, Training Accuracy= 0.501\n",
      "Epoch: 1580, Loss= 0.6973, Training Accuracy= 0.501\n",
      "Epoch: 1590, Loss= 0.6982, Training Accuracy= 0.501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1600, Loss= 0.6983, Training Accuracy= 0.501\n",
      "Epoch: 1610, Loss= 0.6978, Training Accuracy= 0.506\n",
      "Epoch: 1620, Loss= 0.6979, Training Accuracy= 0.501\n",
      "Epoch: 1630, Loss= 0.6981, Training Accuracy= 0.501\n",
      "Epoch: 1640, Loss= 0.6984, Training Accuracy= 0.501\n",
      "Epoch: 1650, Loss= 0.6984, Training Accuracy= 0.501\n",
      "Epoch: 1660, Loss= 0.6985, Training Accuracy= 0.501\n",
      "Epoch: 1670, Loss= 0.6983, Training Accuracy= 0.501\n",
      "Epoch: 1680, Loss= 0.6987, Training Accuracy= 0.501\n",
      "Epoch: 1690, Loss= 0.6987, Training Accuracy= 0.501\n",
      "Epoch: 1700, Loss= 0.6987, Training Accuracy= 0.501\n",
      "Epoch: 1710, Loss= 0.6986, Training Accuracy= 0.501\n",
      "Epoch: 1720, Loss= 0.6986, Training Accuracy= 0.501\n",
      "Epoch: 1730, Loss= 0.6985, Training Accuracy= 0.501\n",
      "Epoch: 1740, Loss= 0.6985, Training Accuracy= 0.501\n",
      "Epoch: 1750, Loss= 0.6985, Training Accuracy= 0.503\n",
      "Epoch: 1760, Loss= 0.6983, Training Accuracy= 0.503\n",
      "Epoch: 1770, Loss= 0.6979, Training Accuracy= 0.503\n",
      "Epoch: 1780, Loss= 0.6984, Training Accuracy= 0.503\n",
      "Epoch: 1790, Loss= 0.6982, Training Accuracy= 0.501\n",
      "Epoch: 1800, Loss= 0.6979, Training Accuracy= 0.501\n",
      "Epoch: 1810, Loss= 0.6976, Training Accuracy= 0.491\n",
      "Epoch: 1820, Loss= 0.6981, Training Accuracy= 0.497\n",
      "Epoch: 1830, Loss= 0.6986, Training Accuracy= 0.501\n",
      "Epoch: 1840, Loss= 0.6987, Training Accuracy= 0.504\n",
      "Epoch: 1850, Loss= 0.6987, Training Accuracy= 0.504\n",
      "Epoch: 1860, Loss= 0.6987, Training Accuracy= 0.507\n",
      "Epoch: 1870, Loss= 0.6987, Training Accuracy= 0.506\n",
      "Epoch: 1880, Loss= 0.6986, Training Accuracy= 0.509\n",
      "Epoch: 1890, Loss= 0.6986, Training Accuracy= 0.508\n",
      "Epoch: 1900, Loss= 0.6994, Training Accuracy= 0.499\n",
      "Epoch: 1910, Loss= 0.6983, Training Accuracy= 0.501\n",
      "Epoch: 1920, Loss= 0.6984, Training Accuracy= 0.501\n",
      "Epoch: 1930, Loss= 0.6985, Training Accuracy= 0.499\n",
      "Epoch: 1940, Loss= 0.6988, Training Accuracy= 0.500\n",
      "Epoch: 1950, Loss= 0.6981, Training Accuracy= 0.505\n",
      "Epoch: 1960, Loss= 0.7018, Training Accuracy= 0.505\n",
      "Epoch: 1970, Loss= 0.7013, Training Accuracy= 0.508\n",
      "Epoch: 1980, Loss= 0.7011, Training Accuracy= 0.509\n",
      "Epoch: 1990, Loss= 0.7009, Training Accuracy= 0.510\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5031\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.6974, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.6940, Training Accuracy= 0.515\n",
      "Epoch: 20, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 30, Loss= 0.6937, Training Accuracy= 0.516\n",
      "Epoch: 40, Loss= 0.6936, Training Accuracy= 0.518\n",
      "Epoch: 50, Loss= 0.6936, Training Accuracy= 0.520\n",
      "Epoch: 60, Loss= 0.6936, Training Accuracy= 0.523\n",
      "Epoch: 70, Loss= 0.6936, Training Accuracy= 0.523\n",
      "Epoch: 80, Loss= 0.6936, Training Accuracy= 0.513\n",
      "Epoch: 90, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 100, Loss= 0.6936, Training Accuracy= 0.508\n",
      "Epoch: 110, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 120, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 130, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 150, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 160, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 180, Loss= 0.6937, Training Accuracy= 0.507\n",
      "Epoch: 190, Loss= 0.6937, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 210, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 220, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 230, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 240, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6936, Training Accuracy= 0.501\n",
      "Epoch: 260, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 270, Loss= 0.6936, Training Accuracy= 0.520\n",
      "Epoch: 280, Loss= 0.7070, Training Accuracy= 0.507\n",
      "Epoch: 290, Loss= 0.6851, Training Accuracy= 0.542\n",
      "Epoch: 300, Loss= 0.6972, Training Accuracy= 0.529\n",
      "Epoch: 310, Loss= 0.7022, Training Accuracy= 0.521\n",
      "Epoch: 320, Loss= 0.7059, Training Accuracy= 0.503\n",
      "Epoch: 330, Loss= 0.7038, Training Accuracy= 0.497\n",
      "Epoch: 340, Loss= 0.7006, Training Accuracy= 0.510\n",
      "Epoch: 350, Loss= 0.6952, Training Accuracy= 0.504\n",
      "Epoch: 360, Loss= 0.6949, Training Accuracy= 0.501\n",
      "Epoch: 370, Loss= 0.7060, Training Accuracy= 0.503\n",
      "Epoch: 380, Loss= 0.6947, Training Accuracy= 0.505\n",
      "Epoch: 390, Loss= 0.7061, Training Accuracy= 0.533\n",
      "Epoch: 400, Loss= 0.7045, Training Accuracy= 0.489\n",
      "Epoch: 410, Loss= 0.7121, Training Accuracy= 0.502\n",
      "Epoch: 420, Loss= 0.7072, Training Accuracy= 0.502\n",
      "Epoch: 430, Loss= 0.7022, Training Accuracy= 0.513\n",
      "Epoch: 440, Loss= 0.7153, Training Accuracy= 0.513\n",
      "Epoch: 450, Loss= 0.7061, Training Accuracy= 0.501\n",
      "Epoch: 460, Loss= 0.7074, Training Accuracy= 0.501\n",
      "Epoch: 470, Loss= 0.7079, Training Accuracy= 0.501\n",
      "Epoch: 480, Loss= 0.7081, Training Accuracy= 0.501\n",
      "Epoch: 490, Loss= 0.7083, Training Accuracy= 0.501\n",
      "Epoch: 500, Loss= 0.7084, Training Accuracy= 0.501\n",
      "Epoch: 510, Loss= 0.7084, Training Accuracy= 0.501\n",
      "Epoch: 520, Loss= 0.7084, Training Accuracy= 0.501\n",
      "Epoch: 530, Loss= 0.7084, Training Accuracy= 0.501\n",
      "Epoch: 540, Loss= 0.7083, Training Accuracy= 0.501\n",
      "Epoch: 550, Loss= 0.7081, Training Accuracy= 0.501\n",
      "Epoch: 560, Loss= 0.7080, Training Accuracy= 0.501\n",
      "Epoch: 570, Loss= 0.7092, Training Accuracy= 0.501\n",
      "Epoch: 580, Loss= 0.7100, Training Accuracy= 0.503\n",
      "Epoch: 590, Loss= 0.7104, Training Accuracy= 0.501\n",
      "Epoch: 600, Loss= 0.7020, Training Accuracy= 0.508\n",
      "Epoch: 610, Loss= 0.7092, Training Accuracy= 0.499\n",
      "Epoch: 620, Loss= 0.7307, Training Accuracy= 0.504\n",
      "Epoch: 630, Loss= 0.7313, Training Accuracy= 0.502\n",
      "Epoch: 640, Loss= 0.7322, Training Accuracy= 0.501\n",
      "Epoch: 650, Loss= 0.7322, Training Accuracy= 0.501\n",
      "Epoch: 660, Loss= 0.7322, Training Accuracy= 0.501\n",
      "Epoch: 670, Loss= 0.7321, Training Accuracy= 0.501\n",
      "Epoch: 680, Loss= 0.7317, Training Accuracy= 0.501\n",
      "Epoch: 690, Loss= 0.7304, Training Accuracy= 0.501\n",
      "Epoch: 700, Loss= 0.7165, Training Accuracy= 0.501\n",
      "Epoch: 710, Loss= 0.7065, Training Accuracy= 0.501\n",
      "Epoch: 720, Loss= 0.7064, Training Accuracy= 0.501\n",
      "Epoch: 730, Loss= 0.7066, Training Accuracy= 0.501\n",
      "Epoch: 740, Loss= 0.7068, Training Accuracy= 0.501\n",
      "Epoch: 750, Loss= 0.7066, Training Accuracy= 0.501\n",
      "Epoch: 760, Loss= 0.7061, Training Accuracy= 0.501\n",
      "Epoch: 770, Loss= 0.6962, Training Accuracy= 0.504\n",
      "Epoch: 780, Loss= 0.6971, Training Accuracy= 0.511\n",
      "Epoch: 790, Loss= 0.6940, Training Accuracy= 0.509\n",
      "Epoch: 800, Loss= 0.6989, Training Accuracy= 0.501\n",
      "Epoch: 810, Loss= 0.6919, Training Accuracy= 0.519\n",
      "Epoch: 820, Loss= 0.7254, Training Accuracy= 0.516\n",
      "Epoch: 830, Loss= 0.7184, Training Accuracy= 0.516\n",
      "Epoch: 840, Loss= 0.7199, Training Accuracy= 0.497\n",
      "Epoch: 850, Loss= 0.6900, Training Accuracy= 0.550\n",
      "Epoch: 860, Loss= 0.7627, Training Accuracy= 0.496\n",
      "Epoch: 870, Loss= 0.6963, Training Accuracy= 0.539\n",
      "Epoch: 880, Loss= 0.6091, Training Accuracy= 0.694\n",
      "Epoch: 890, Loss= 0.7051, Training Accuracy= 0.514\n",
      "Epoch: 900, Loss= 0.7049, Training Accuracy= 0.515\n",
      "Epoch: 910, Loss= 0.7050, Training Accuracy= 0.510\n",
      "Epoch: 920, Loss= 0.7246, Training Accuracy= 0.510\n",
      "Epoch: 930, Loss= 0.7146, Training Accuracy= 0.500\n",
      "Epoch: 940, Loss= 0.7058, Training Accuracy= 0.496\n",
      "Epoch: 950, Loss= 0.5417, Training Accuracy= 0.767\n",
      "Epoch: 960, Loss= 0.0201, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0053, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0030, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0021, Training Accuracy= 1.000\n",
      "Epoch: 1000, Loss= 0.0016, Training Accuracy= 1.000\n",
      "Epoch: 1010, Loss= 0.0013, Training Accuracy= 1.000\n",
      "Epoch: 1020, Loss= 0.0011, Training Accuracy= 1.000\n",
      "Epoch: 1030, Loss= 0.0009, Training Accuracy= 1.000\n",
      "Epoch: 1040, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 1050, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 1060, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 1070, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 1080, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 1090, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 1100, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 1110, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 1120, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 1130, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 1140, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 1150, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 1160, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 1170, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 1180, Loss= 0.0003, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1190, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 1200, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 1210, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 1220, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1230, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1240, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1250, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1260, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1270, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1280, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1290, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1300, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1310, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1320, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1330, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1340, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1350, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1360, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1370, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1380, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1390, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 1400, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1410, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1420, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1430, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1440, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1450, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1460, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1470, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1480, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1490, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1500, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1510, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1520, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1530, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1540, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1550, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1560, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1570, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1580, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1590, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1600, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1610, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1620, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1630, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1640, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1650, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1660, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1670, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1680, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1690, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1700, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1710, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1720, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1730, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1740, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1750, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1760, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1770, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1780, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1790, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1800, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1810, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1820, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1830, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1840, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1850, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1860, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1870, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1880, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1890, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1900, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1910, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1920, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1930, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1940, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1950, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1960, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1970, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1980, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 1990, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.6940, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6933, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.6936, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.6937, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 50, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 60, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 70, Loss= 0.6939, Training Accuracy= 0.498\n",
      "Epoch: 80, Loss= 0.6939, Training Accuracy= 0.498\n",
      "Epoch: 90, Loss= 0.6939, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.6939, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 130, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 140, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 150, Loss= 0.6941, Training Accuracy= 0.498\n",
      "Epoch: 160, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 170, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 180, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 190, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 200, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 210, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 220, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 230, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 240, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 250, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 260, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 270, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 280, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 290, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 300, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 310, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 320, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 330, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 340, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 350, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 360, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 370, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 380, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 390, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 400, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 410, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 420, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 430, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 440, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 450, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 460, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 470, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 480, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 490, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 500, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 510, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 520, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 550, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 570, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 580, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 590, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 600, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 610, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 620, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 630, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 640, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 650, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 660, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 670, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 680, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 690, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 700, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 710, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 720, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 730, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 740, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 750, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 760, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 770, Loss= 0.6940, Training Accuracy= 0.498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 780, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 790, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 800, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 810, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 820, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 830, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 840, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 850, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 860, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 870, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 880, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 890, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 900, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 910, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 930, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 940, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 950, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 960, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 970, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 980, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 990, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1000, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1010, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1020, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1030, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1040, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1050, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1060, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1070, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1080, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1090, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1100, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1110, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1120, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1130, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1140, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1150, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1160, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1170, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1180, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1190, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1200, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1210, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1220, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1230, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1240, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1250, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1260, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1270, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1280, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1290, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1300, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1310, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1320, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1330, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1340, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1350, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1360, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1370, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1380, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1390, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1400, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1410, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1420, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1430, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1440, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1450, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1460, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1470, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1480, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1490, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1500, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1510, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1520, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1530, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1540, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1550, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1560, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1570, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1580, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1590, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1600, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1610, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1620, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1630, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1640, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1650, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1660, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1670, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1680, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1690, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1700, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1710, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1720, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1730, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1740, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1750, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1760, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1770, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1780, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1790, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1800, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1810, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1820, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1830, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1840, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1850, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1860, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1870, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1880, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1890, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1900, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1910, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1920, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1930, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1940, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1950, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1960, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1970, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1980, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Epoch: 1990, Loss= 0.6940, Training Accuracy= 0.498\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5048\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.7700, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.7460, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.7330, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.7264, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.7252, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.7247, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.7245, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.7244, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.7244, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.7243, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.7243, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.7243, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.7242, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.7242, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.7241, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.7240, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.7239, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.7237, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.7235, Training Accuracy= 0.503\n",
      "Epoch: 190, Loss= 0.7232, Training Accuracy= 0.503\n",
      "Epoch: 200, Loss= 0.7228, Training Accuracy= 0.503\n",
      "Epoch: 210, Loss= 0.7221, Training Accuracy= 0.503\n",
      "Epoch: 220, Loss= 0.7211, Training Accuracy= 0.503\n",
      "Epoch: 230, Loss= 0.7189, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.7143, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.7107, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.7094, Training Accuracy= 0.503\n",
      "Epoch: 270, Loss= 0.7089, Training Accuracy= 0.503\n",
      "Epoch: 280, Loss= 0.7087, Training Accuracy= 0.503\n",
      "Epoch: 290, Loss= 0.7086, Training Accuracy= 0.503\n",
      "Epoch: 300, Loss= 0.7086, Training Accuracy= 0.503\n",
      "Epoch: 310, Loss= 0.7086, Training Accuracy= 0.503\n",
      "Epoch: 320, Loss= 0.7086, Training Accuracy= 0.503\n",
      "Epoch: 330, Loss= 0.7086, Training Accuracy= 0.503\n",
      "Epoch: 340, Loss= 0.7086, Training Accuracy= 0.503\n",
      "Epoch: 350, Loss= 0.7086, Training Accuracy= 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 360, Loss= 0.7086, Training Accuracy= 0.503\n",
      "Epoch: 370, Loss= 0.7085, Training Accuracy= 0.503\n",
      "Epoch: 380, Loss= 0.7085, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.7084, Training Accuracy= 0.503\n",
      "Epoch: 400, Loss= 0.7084, Training Accuracy= 0.503\n",
      "Epoch: 410, Loss= 0.7085, Training Accuracy= 0.503\n",
      "Epoch: 420, Loss= 0.7085, Training Accuracy= 0.503\n",
      "Epoch: 430, Loss= 0.7085, Training Accuracy= 0.503\n",
      "Epoch: 440, Loss= 0.7084, Training Accuracy= 0.503\n",
      "Epoch: 450, Loss= 0.7082, Training Accuracy= 0.503\n",
      "Epoch: 460, Loss= 0.7079, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.7076, Training Accuracy= 0.503\n",
      "Epoch: 480, Loss= 0.7075, Training Accuracy= 0.503\n",
      "Epoch: 490, Loss= 0.7073, Training Accuracy= 0.503\n",
      "Epoch: 500, Loss= 0.7072, Training Accuracy= 0.503\n",
      "Epoch: 510, Loss= 0.7069, Training Accuracy= 0.503\n",
      "Epoch: 520, Loss= 0.7066, Training Accuracy= 0.503\n",
      "Epoch: 530, Loss= 0.7063, Training Accuracy= 0.503\n",
      "Epoch: 540, Loss= 0.7059, Training Accuracy= 0.503\n",
      "Epoch: 550, Loss= 0.7053, Training Accuracy= 0.503\n",
      "Epoch: 560, Loss= 0.7047, Training Accuracy= 0.503\n",
      "Epoch: 570, Loss= 0.7039, Training Accuracy= 0.503\n",
      "Epoch: 580, Loss= 0.7033, Training Accuracy= 0.504\n",
      "Epoch: 590, Loss= 0.7028, Training Accuracy= 0.504\n",
      "Epoch: 600, Loss= 0.7025, Training Accuracy= 0.503\n",
      "Epoch: 610, Loss= 0.7024, Training Accuracy= 0.505\n",
      "Epoch: 620, Loss= 0.7025, Training Accuracy= 0.503\n",
      "Epoch: 630, Loss= 0.7024, Training Accuracy= 0.504\n",
      "Epoch: 640, Loss= 0.7021, Training Accuracy= 0.502\n",
      "Epoch: 650, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 660, Loss= 0.7011, Training Accuracy= 0.502\n",
      "Epoch: 670, Loss= 0.7006, Training Accuracy= 0.502\n",
      "Epoch: 680, Loss= 0.7004, Training Accuracy= 0.504\n",
      "Epoch: 690, Loss= 0.7005, Training Accuracy= 0.503\n",
      "Epoch: 700, Loss= 0.7011, Training Accuracy= 0.503\n",
      "Epoch: 710, Loss= 0.7020, Training Accuracy= 0.507\n",
      "Epoch: 720, Loss= 0.7032, Training Accuracy= 0.507\n",
      "Epoch: 730, Loss= 0.7040, Training Accuracy= 0.507\n",
      "Epoch: 740, Loss= 0.7068, Training Accuracy= 0.511\n",
      "Epoch: 750, Loss= 0.7072, Training Accuracy= 0.512\n",
      "Epoch: 760, Loss= 0.7074, Training Accuracy= 0.516\n",
      "Epoch: 770, Loss= 0.7550, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.7434, Training Accuracy= 0.503\n",
      "Epoch: 790, Loss= 0.7414, Training Accuracy= 0.503\n",
      "Epoch: 800, Loss= 0.7409, Training Accuracy= 0.503\n",
      "Epoch: 810, Loss= 0.7403, Training Accuracy= 0.503\n",
      "Epoch: 820, Loss= 0.7334, Training Accuracy= 0.504\n",
      "Epoch: 830, Loss= 0.7475, Training Accuracy= 0.503\n",
      "Epoch: 840, Loss= 0.7466, Training Accuracy= 0.503\n",
      "Epoch: 850, Loss= 0.7453, Training Accuracy= 0.503\n",
      "Epoch: 860, Loss= 0.7447, Training Accuracy= 0.503\n",
      "Epoch: 870, Loss= 0.7440, Training Accuracy= 0.503\n",
      "Epoch: 880, Loss= 0.7419, Training Accuracy= 0.503\n",
      "Epoch: 890, Loss= 0.7399, Training Accuracy= 0.503\n",
      "Epoch: 900, Loss= 0.7393, Training Accuracy= 0.506\n",
      "Epoch: 910, Loss= 0.7748, Training Accuracy= 0.505\n",
      "Epoch: 920, Loss= 0.8844, Training Accuracy= 0.503\n",
      "Epoch: 930, Loss= 0.7758, Training Accuracy= 0.507\n",
      "Epoch: 940, Loss= 0.7745, Training Accuracy= 0.505\n",
      "Epoch: 950, Loss= 0.7635, Training Accuracy= 0.510\n",
      "Epoch: 960, Loss= 0.7662, Training Accuracy= 0.515\n",
      "Epoch: 970, Loss= 0.7611, Training Accuracy= 0.511\n",
      "Epoch: 980, Loss= 0.7594, Training Accuracy= 0.511\n",
      "Epoch: 990, Loss= 0.7734, Training Accuracy= 0.507\n",
      "Epoch: 1000, Loss= 0.7734, Training Accuracy= 0.507\n",
      "Epoch: 1010, Loss= 0.7725, Training Accuracy= 0.507\n",
      "Epoch: 1020, Loss= 0.7723, Training Accuracy= 0.507\n",
      "Epoch: 1030, Loss= 0.7757, Training Accuracy= 0.503\n",
      "Epoch: 1040, Loss= 0.7695, Training Accuracy= 0.506\n",
      "Epoch: 1050, Loss= 0.7878, Training Accuracy= 0.507\n",
      "Epoch: 1060, Loss= 0.7742, Training Accuracy= 0.507\n",
      "Epoch: 1070, Loss= 0.7713, Training Accuracy= 0.507\n",
      "Epoch: 1080, Loss= 0.7751, Training Accuracy= 0.507\n",
      "Epoch: 1090, Loss= 0.7717, Training Accuracy= 0.508\n",
      "Epoch: 1100, Loss= 0.7715, Training Accuracy= 0.512\n",
      "Epoch: 1110, Loss= 0.7794, Training Accuracy= 0.508\n",
      "Epoch: 1120, Loss= 0.7752, Training Accuracy= 0.509\n",
      "Epoch: 1130, Loss= 0.7743, Training Accuracy= 0.509\n",
      "Epoch: 1140, Loss= 0.7749, Training Accuracy= 0.509\n",
      "Epoch: 1150, Loss= 0.7739, Training Accuracy= 0.509\n",
      "Epoch: 1160, Loss= 0.7815, Training Accuracy= 0.507\n",
      "Epoch: 1170, Loss= 0.7781, Training Accuracy= 0.507\n",
      "Epoch: 1180, Loss= 0.7771, Training Accuracy= 0.507\n",
      "Epoch: 1190, Loss= 0.7773, Training Accuracy= 0.507\n",
      "Epoch: 1200, Loss= 0.7768, Training Accuracy= 0.507\n",
      "Epoch: 1210, Loss= 0.7966, Training Accuracy= 0.503\n",
      "Epoch: 1220, Loss= 0.7959, Training Accuracy= 0.503\n",
      "Epoch: 1230, Loss= 0.7940, Training Accuracy= 0.503\n",
      "Epoch: 1240, Loss= 0.7745, Training Accuracy= 0.503\n",
      "Epoch: 1250, Loss= 0.7793, Training Accuracy= 0.503\n",
      "Epoch: 1260, Loss= 0.7769, Training Accuracy= 0.503\n",
      "Epoch: 1270, Loss= 0.7766, Training Accuracy= 0.503\n",
      "Epoch: 1280, Loss= 0.7764, Training Accuracy= 0.503\n",
      "Epoch: 1290, Loss= 0.7762, Training Accuracy= 0.503\n",
      "Epoch: 1300, Loss= 0.7758, Training Accuracy= 0.503\n",
      "Epoch: 1310, Loss= 0.7778, Training Accuracy= 0.503\n",
      "Epoch: 1320, Loss= 0.7777, Training Accuracy= 0.503\n",
      "Epoch: 1330, Loss= 0.7780, Training Accuracy= 0.503\n",
      "Epoch: 1340, Loss= 0.7776, Training Accuracy= 0.503\n",
      "Epoch: 1350, Loss= 0.7751, Training Accuracy= 0.503\n",
      "Epoch: 1360, Loss= 0.7706, Training Accuracy= 0.503\n",
      "Epoch: 1370, Loss= 0.7771, Training Accuracy= 0.503\n",
      "Epoch: 1380, Loss= 0.7772, Training Accuracy= 0.503\n",
      "Epoch: 1390, Loss= 0.7772, Training Accuracy= 0.503\n",
      "Epoch: 1400, Loss= 0.7772, Training Accuracy= 0.503\n",
      "Epoch: 1410, Loss= 0.7772, Training Accuracy= 0.503\n",
      "Epoch: 1420, Loss= 0.7707, Training Accuracy= 0.503\n",
      "Epoch: 1430, Loss= 0.7770, Training Accuracy= 0.503\n",
      "Epoch: 1440, Loss= 0.7711, Training Accuracy= 0.503\n",
      "Epoch: 1450, Loss= 0.7693, Training Accuracy= 0.503\n",
      "Epoch: 1460, Loss= 0.7695, Training Accuracy= 0.503\n",
      "Epoch: 1470, Loss= 0.7705, Training Accuracy= 0.503\n",
      "Epoch: 1480, Loss= 0.7689, Training Accuracy= 0.503\n",
      "Epoch: 1490, Loss= 0.7798, Training Accuracy= 0.507\n",
      "Epoch: 1500, Loss= 0.7799, Training Accuracy= 0.497\n",
      "Epoch: 1510, Loss= 0.7789, Training Accuracy= 0.497\n",
      "Epoch: 1520, Loss= 0.7778, Training Accuracy= 0.497\n",
      "Epoch: 1530, Loss= 0.7770, Training Accuracy= 0.497\n",
      "Epoch: 1540, Loss= 0.7758, Training Accuracy= 0.512\n",
      "Epoch: 1550, Loss= 0.7767, Training Accuracy= 0.512\n",
      "Epoch: 1560, Loss= 0.7785, Training Accuracy= 0.513\n",
      "Epoch: 1570, Loss= 0.7842, Training Accuracy= 0.511\n",
      "Epoch: 1580, Loss= 0.7906, Training Accuracy= 0.517\n",
      "Epoch: 1590, Loss= 0.7892, Training Accuracy= 0.516\n",
      "Epoch: 1600, Loss= 0.7881, Training Accuracy= 0.514\n",
      "Epoch: 1610, Loss= 0.7855, Training Accuracy= 0.503\n",
      "Epoch: 1620, Loss= 0.7799, Training Accuracy= 0.503\n",
      "Epoch: 1630, Loss= 0.7777, Training Accuracy= 0.503\n",
      "Epoch: 1640, Loss= 0.7854, Training Accuracy= 0.503\n",
      "Epoch: 1650, Loss= 0.7836, Training Accuracy= 0.503\n",
      "Epoch: 1660, Loss= 0.7838, Training Accuracy= 0.503\n",
      "Epoch: 1670, Loss= 0.7810, Training Accuracy= 0.503\n",
      "Epoch: 1680, Loss= 0.7691, Training Accuracy= 0.503\n",
      "Epoch: 1690, Loss= 0.8182, Training Accuracy= 0.503\n",
      "Epoch: 1700, Loss= 0.7753, Training Accuracy= 0.503\n",
      "Epoch: 1710, Loss= 0.8063, Training Accuracy= 0.503\n",
      "Epoch: 1720, Loss= 0.8039, Training Accuracy= 0.503\n",
      "Epoch: 1730, Loss= 0.7726, Training Accuracy= 0.503\n",
      "Epoch: 1740, Loss= 0.8183, Training Accuracy= 0.503\n",
      "Epoch: 1750, Loss= 0.7647, Training Accuracy= 0.524\n",
      "Epoch: 1760, Loss= 0.8178, Training Accuracy= 0.503\n",
      "Epoch: 1770, Loss= 0.7763, Training Accuracy= 0.503\n",
      "Epoch: 1780, Loss= 0.8169, Training Accuracy= 0.503\n",
      "Epoch: 1790, Loss= 0.8192, Training Accuracy= 0.503\n",
      "Epoch: 1800, Loss= 0.7775, Training Accuracy= 0.503\n",
      "Epoch: 1810, Loss= 0.8294, Training Accuracy= 0.501\n",
      "Epoch: 1820, Loss= 0.7786, Training Accuracy= 0.503\n",
      "Epoch: 1830, Loss= 0.7658, Training Accuracy= 0.511\n",
      "Epoch: 1840, Loss= 0.7809, Training Accuracy= 0.503\n",
      "Epoch: 1850, Loss= 0.7792, Training Accuracy= 0.503\n",
      "Epoch: 1860, Loss= 0.7781, Training Accuracy= 0.503\n",
      "Epoch: 1870, Loss= 0.7768, Training Accuracy= 0.503\n",
      "Epoch: 1880, Loss= 0.7770, Training Accuracy= 0.503\n",
      "Epoch: 1890, Loss= 0.7772, Training Accuracy= 0.503\n",
      "Epoch: 1900, Loss= 0.7775, Training Accuracy= 0.503\n",
      "Epoch: 1910, Loss= 0.7777, Training Accuracy= 0.503\n",
      "Epoch: 1920, Loss= 0.7780, Training Accuracy= 0.503\n",
      "Epoch: 1930, Loss= 0.7783, Training Accuracy= 0.503\n",
      "Epoch: 1940, Loss= 0.7788, Training Accuracy= 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1950, Loss= 0.7768, Training Accuracy= 0.503\n",
      "Epoch: 1960, Loss= 0.7828, Training Accuracy= 0.503\n",
      "Epoch: 1970, Loss= 0.7782, Training Accuracy= 0.503\n",
      "Epoch: 1980, Loss= 0.7776, Training Accuracy= 0.503\n",
      "Epoch: 1990, Loss= 0.7785, Training Accuracy= 0.503\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5026\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.7986, Training Accuracy= 0.493\n",
      "Epoch: 10, Loss= 0.7397, Training Accuracy= 0.493\n",
      "Epoch: 20, Loss= 0.7295, Training Accuracy= 0.493\n",
      "Epoch: 30, Loss= 0.7264, Training Accuracy= 0.493\n",
      "Epoch: 40, Loss= 0.7249, Training Accuracy= 0.493\n",
      "Epoch: 50, Loss= 0.7223, Training Accuracy= 0.493\n",
      "Epoch: 60, Loss= 0.7170, Training Accuracy= 0.493\n",
      "Epoch: 70, Loss= 0.7124, Training Accuracy= 0.493\n",
      "Epoch: 80, Loss= 0.7100, Training Accuracy= 0.493\n",
      "Epoch: 90, Loss= 0.7084, Training Accuracy= 0.493\n",
      "Epoch: 100, Loss= 0.7082, Training Accuracy= 0.493\n",
      "Epoch: 110, Loss= 0.7080, Training Accuracy= 0.493\n",
      "Epoch: 120, Loss= 0.7077, Training Accuracy= 0.493\n",
      "Epoch: 130, Loss= 0.7080, Training Accuracy= 0.493\n",
      "Epoch: 140, Loss= 0.7081, Training Accuracy= 0.493\n",
      "Epoch: 150, Loss= 0.7084, Training Accuracy= 0.493\n",
      "Epoch: 160, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 170, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 180, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 190, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 200, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 210, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 220, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 230, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 240, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 250, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 260, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 270, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 280, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 290, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 300, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 310, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 320, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 330, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 340, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 350, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 360, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 370, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 380, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 390, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 400, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 410, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 420, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 430, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 440, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 450, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 460, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 470, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 480, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 490, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 500, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 510, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 520, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 530, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 540, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 550, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 560, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 570, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 580, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 590, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 600, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 610, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 620, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 630, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 640, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 650, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 660, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 670, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 680, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 690, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 700, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 710, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 720, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 730, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 740, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 750, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 760, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 770, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 780, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 790, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 800, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 810, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 820, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 830, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 840, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 850, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 860, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 870, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 880, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 890, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 900, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 910, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 920, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 930, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 940, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 950, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 960, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 970, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 980, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 990, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 1000, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 1010, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1020, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1030, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1040, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1050, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1060, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1070, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1080, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1090, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 1100, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 1110, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 1120, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 1130, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 1140, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 1150, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 1160, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 1170, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 1180, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 1190, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 1200, Loss= 0.7086, Training Accuracy= 0.493\n",
      "Epoch: 1210, Loss= 0.7087, Training Accuracy= 0.493\n",
      "Epoch: 1220, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 1230, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1240, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1250, Loss= 0.7090, Training Accuracy= 0.493\n",
      "Epoch: 1260, Loss= 0.7090, Training Accuracy= 0.493\n",
      "Epoch: 1270, Loss= 0.7090, Training Accuracy= 0.493\n",
      "Epoch: 1280, Loss= 0.7090, Training Accuracy= 0.493\n",
      "Epoch: 1290, Loss= 0.7090, Training Accuracy= 0.493\n",
      "Epoch: 1300, Loss= 0.7090, Training Accuracy= 0.493\n",
      "Epoch: 1310, Loss= 0.7090, Training Accuracy= 0.493\n",
      "Epoch: 1320, Loss= 0.7090, Training Accuracy= 0.493\n",
      "Epoch: 1330, Loss= 0.7090, Training Accuracy= 0.493\n",
      "Epoch: 1340, Loss= 0.7091, Training Accuracy= 0.493\n",
      "Epoch: 1350, Loss= 0.7091, Training Accuracy= 0.493\n",
      "Epoch: 1360, Loss= 0.7092, Training Accuracy= 0.493\n",
      "Epoch: 1370, Loss= 0.7092, Training Accuracy= 0.493\n",
      "Epoch: 1380, Loss= 0.7092, Training Accuracy= 0.493\n",
      "Epoch: 1390, Loss= 0.7093, Training Accuracy= 0.493\n",
      "Epoch: 1400, Loss= 0.7093, Training Accuracy= 0.493\n",
      "Epoch: 1410, Loss= 0.7093, Training Accuracy= 0.493\n",
      "Epoch: 1420, Loss= 0.7093, Training Accuracy= 0.493\n",
      "Epoch: 1430, Loss= 0.7093, Training Accuracy= 0.493\n",
      "Epoch: 1440, Loss= 0.7094, Training Accuracy= 0.493\n",
      "Epoch: 1450, Loss= 0.7094, Training Accuracy= 0.493\n",
      "Epoch: 1460, Loss= 0.7094, Training Accuracy= 0.493\n",
      "Epoch: 1470, Loss= 0.7095, Training Accuracy= 0.493\n",
      "Epoch: 1480, Loss= 0.7095, Training Accuracy= 0.493\n",
      "Epoch: 1490, Loss= 0.7095, Training Accuracy= 0.493\n",
      "Epoch: 1500, Loss= 0.7095, Training Accuracy= 0.493\n",
      "Epoch: 1510, Loss= 0.7094, Training Accuracy= 0.493\n",
      "Epoch: 1520, Loss= 0.7090, Training Accuracy= 0.493\n",
      "Epoch: 1530, Loss= 0.7087, Training Accuracy= 0.493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1540, Loss= 0.7088, Training Accuracy= 0.493\n",
      "Epoch: 1550, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1560, Loss= 0.7089, Training Accuracy= 0.493\n",
      "Epoch: 1570, Loss= 0.7090, Training Accuracy= 0.493\n",
      "Epoch: 1580, Loss= 0.7099, Training Accuracy= 0.493\n",
      "Epoch: 1590, Loss= 0.7100, Training Accuracy= 0.493\n",
      "Epoch: 1600, Loss= 0.7085, Training Accuracy= 0.493\n",
      "Epoch: 1610, Loss= 0.7072, Training Accuracy= 0.493\n",
      "Epoch: 1620, Loss= 0.7065, Training Accuracy= 0.493\n",
      "Epoch: 1630, Loss= 0.7095, Training Accuracy= 0.494\n",
      "Epoch: 1640, Loss= 0.7124, Training Accuracy= 0.507\n",
      "Epoch: 1650, Loss= 0.6997, Training Accuracy= 0.504\n",
      "Epoch: 1660, Loss= 0.7509, Training Accuracy= 0.494\n",
      "Epoch: 1670, Loss= 0.7348, Training Accuracy= 0.513\n",
      "Epoch: 1680, Loss= 0.8070, Training Accuracy= 0.493\n",
      "Epoch: 1690, Loss= 0.7992, Training Accuracy= 0.493\n",
      "Epoch: 1700, Loss= 0.7954, Training Accuracy= 0.493\n",
      "Epoch: 1710, Loss= 0.7835, Training Accuracy= 0.493\n",
      "Epoch: 1720, Loss= 0.7531, Training Accuracy= 0.493\n",
      "Epoch: 1730, Loss= 0.7530, Training Accuracy= 0.493\n",
      "Epoch: 1740, Loss= 0.7720, Training Accuracy= 0.493\n",
      "Epoch: 1750, Loss= 0.7381, Training Accuracy= 0.493\n",
      "Epoch: 1760, Loss= 0.7387, Training Accuracy= 0.493\n",
      "Epoch: 1770, Loss= 0.7307, Training Accuracy= 0.493\n",
      "Epoch: 1780, Loss= 0.8128, Training Accuracy= 0.493\n",
      "Epoch: 1790, Loss= 0.8133, Training Accuracy= 0.493\n",
      "Epoch: 1800, Loss= 0.8149, Training Accuracy= 0.493\n",
      "Epoch: 1810, Loss= 0.7993, Training Accuracy= 0.493\n",
      "Epoch: 1820, Loss= 0.8107, Training Accuracy= 0.493\n",
      "Epoch: 1830, Loss= 0.7846, Training Accuracy= 0.493\n",
      "Epoch: 1840, Loss= 0.7841, Training Accuracy= 0.493\n",
      "Epoch: 1850, Loss= 0.7789, Training Accuracy= 0.493\n",
      "Epoch: 1860, Loss= 0.7776, Training Accuracy= 0.493\n",
      "Epoch: 1870, Loss= 0.7673, Training Accuracy= 0.493\n",
      "Epoch: 1880, Loss= 0.7527, Training Accuracy= 0.493\n",
      "Epoch: 1890, Loss= 0.7523, Training Accuracy= 0.493\n",
      "Epoch: 1900, Loss= 0.7435, Training Accuracy= 0.493\n",
      "Epoch: 1910, Loss= 0.7357, Training Accuracy= 0.493\n",
      "Epoch: 1920, Loss= 0.7339, Training Accuracy= 0.493\n",
      "Epoch: 1930, Loss= 0.7329, Training Accuracy= 0.493\n",
      "Epoch: 1940, Loss= 0.7319, Training Accuracy= 0.493\n",
      "Epoch: 1950, Loss= 0.7312, Training Accuracy= 0.493\n",
      "Epoch: 1960, Loss= 0.7309, Training Accuracy= 0.493\n",
      "Epoch: 1970, Loss= 0.7307, Training Accuracy= 0.493\n",
      "Epoch: 1980, Loss= 0.7306, Training Accuracy= 0.493\n",
      "Epoch: 1990, Loss= 0.7305, Training Accuracy= 0.493\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4991\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.7018, Training Accuracy= 0.499\n",
      "Epoch: 10, Loss= 0.6997, Training Accuracy= 0.499\n",
      "Epoch: 20, Loss= 0.6989, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.6982, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.6981, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.6982, Training Accuracy= 0.499\n",
      "Epoch: 60, Loss= 0.6981, Training Accuracy= 0.498\n",
      "Epoch: 70, Loss= 0.6980, Training Accuracy= 0.499\n",
      "Epoch: 80, Loss= 0.6980, Training Accuracy= 0.499\n",
      "Epoch: 90, Loss= 0.6982, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.6977, Training Accuracy= 0.501\n",
      "Epoch: 110, Loss= 0.6992, Training Accuracy= 0.512\n",
      "Epoch: 120, Loss= 0.6993, Training Accuracy= 0.509\n",
      "Epoch: 130, Loss= 0.6968, Training Accuracy= 0.508\n",
      "Epoch: 140, Loss= 0.6965, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.6957, Training Accuracy= 0.515\n",
      "Epoch: 160, Loss= 0.6962, Training Accuracy= 0.508\n",
      "Epoch: 170, Loss= 0.6973, Training Accuracy= 0.512\n",
      "Epoch: 180, Loss= 0.7004, Training Accuracy= 0.503\n",
      "Epoch: 190, Loss= 0.7009, Training Accuracy= 0.502\n",
      "Epoch: 200, Loss= 0.7025, Training Accuracy= 0.501\n",
      "Epoch: 210, Loss= 0.7016, Training Accuracy= 0.502\n",
      "Epoch: 220, Loss= 0.7008, Training Accuracy= 0.506\n",
      "Epoch: 230, Loss= 0.6994, Training Accuracy= 0.506\n",
      "Epoch: 240, Loss= 0.7045, Training Accuracy= 0.499\n",
      "Epoch: 250, Loss= 0.7049, Training Accuracy= 0.505\n",
      "Epoch: 260, Loss= 0.7050, Training Accuracy= 0.506\n",
      "Epoch: 270, Loss= 0.7014, Training Accuracy= 0.499\n",
      "Epoch: 280, Loss= 0.7019, Training Accuracy= 0.499\n",
      "Epoch: 290, Loss= 0.7020, Training Accuracy= 0.499\n",
      "Epoch: 300, Loss= 0.7018, Training Accuracy= 0.500\n",
      "Epoch: 310, Loss= 0.7011, Training Accuracy= 0.502\n",
      "Epoch: 320, Loss= 0.7006, Training Accuracy= 0.503\n",
      "Epoch: 330, Loss= 0.7013, Training Accuracy= 0.499\n",
      "Epoch: 340, Loss= 0.7013, Training Accuracy= 0.499\n",
      "Epoch: 350, Loss= 0.7013, Training Accuracy= 0.499\n",
      "Epoch: 360, Loss= 0.7014, Training Accuracy= 0.499\n",
      "Epoch: 370, Loss= 0.7014, Training Accuracy= 0.499\n",
      "Epoch: 380, Loss= 0.7014, Training Accuracy= 0.499\n",
      "Epoch: 390, Loss= 0.7015, Training Accuracy= 0.499\n",
      "Epoch: 400, Loss= 0.7016, Training Accuracy= 0.499\n",
      "Epoch: 410, Loss= 0.7019, Training Accuracy= 0.499\n",
      "Epoch: 420, Loss= 0.7027, Training Accuracy= 0.499\n",
      "Epoch: 430, Loss= 0.7061, Training Accuracy= 0.500\n",
      "Epoch: 440, Loss= 0.7056, Training Accuracy= 0.502\n",
      "Epoch: 450, Loss= 0.7057, Training Accuracy= 0.504\n",
      "Epoch: 460, Loss= 0.7058, Training Accuracy= 0.502\n",
      "Epoch: 470, Loss= 0.7057, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.7057, Training Accuracy= 0.501\n",
      "Epoch: 490, Loss= 0.7057, Training Accuracy= 0.499\n",
      "Epoch: 500, Loss= 0.7056, Training Accuracy= 0.499\n",
      "Epoch: 510, Loss= 0.7057, Training Accuracy= 0.501\n",
      "Epoch: 520, Loss= 0.7050, Training Accuracy= 0.499\n",
      "Epoch: 530, Loss= 0.7021, Training Accuracy= 0.499\n",
      "Epoch: 540, Loss= 0.7081, Training Accuracy= 0.498\n",
      "Epoch: 550, Loss= 0.6995, Training Accuracy= 0.503\n",
      "Epoch: 560, Loss= 0.7014, Training Accuracy= 0.499\n",
      "Epoch: 570, Loss= 0.7066, Training Accuracy= 0.501\n",
      "Epoch: 580, Loss= 0.7087, Training Accuracy= 0.499\n",
      "Epoch: 590, Loss= 0.6997, Training Accuracy= 0.505\n",
      "Epoch: 600, Loss= 0.6985, Training Accuracy= 0.509\n",
      "Epoch: 610, Loss= 0.7047, Training Accuracy= 0.504\n",
      "Epoch: 620, Loss= 0.7060, Training Accuracy= 0.503\n",
      "Epoch: 630, Loss= 0.7061, Training Accuracy= 0.503\n",
      "Epoch: 640, Loss= 0.7010, Training Accuracy= 0.500\n",
      "Epoch: 650, Loss= 0.7011, Training Accuracy= 0.499\n",
      "Epoch: 660, Loss= 0.7015, Training Accuracy= 0.499\n",
      "Epoch: 670, Loss= 0.6796, Training Accuracy= 0.599\n",
      "Epoch: 680, Loss= 0.6674, Training Accuracy= 0.648\n",
      "Epoch: 690, Loss= 0.6696, Training Accuracy= 0.623\n",
      "Epoch: 700, Loss= 0.6645, Training Accuracy= 0.654\n",
      "Epoch: 710, Loss= 0.7013, Training Accuracy= 0.499\n",
      "Epoch: 720, Loss= 0.7009, Training Accuracy= 0.499\n",
      "Epoch: 730, Loss= 0.7011, Training Accuracy= 0.499\n",
      "Epoch: 740, Loss= 0.7013, Training Accuracy= 0.499\n",
      "Epoch: 750, Loss= 0.7013, Training Accuracy= 0.499\n",
      "Epoch: 760, Loss= 0.7014, Training Accuracy= 0.499\n",
      "Epoch: 770, Loss= 0.7015, Training Accuracy= 0.499\n",
      "Epoch: 780, Loss= 0.7015, Training Accuracy= 0.501\n",
      "Epoch: 790, Loss= 0.7017, Training Accuracy= 0.502\n",
      "Epoch: 800, Loss= 0.7077, Training Accuracy= 0.503\n",
      "Epoch: 810, Loss= 0.7013, Training Accuracy= 0.499\n",
      "Epoch: 820, Loss= 0.7070, Training Accuracy= 0.505\n",
      "Epoch: 830, Loss= 0.6992, Training Accuracy= 0.505\n",
      "Epoch: 840, Loss= 0.7005, Training Accuracy= 0.499\n",
      "Epoch: 850, Loss= 0.7005, Training Accuracy= 0.499\n",
      "Epoch: 860, Loss= 0.7001, Training Accuracy= 0.499\n",
      "Epoch: 870, Loss= 0.6997, Training Accuracy= 0.499\n",
      "Epoch: 880, Loss= 0.6993, Training Accuracy= 0.499\n",
      "Epoch: 890, Loss= 0.6993, Training Accuracy= 0.499\n",
      "Epoch: 900, Loss= 0.6997, Training Accuracy= 0.499\n",
      "Epoch: 910, Loss= 0.7025, Training Accuracy= 0.505\n",
      "Epoch: 920, Loss= 0.7025, Training Accuracy= 0.505\n",
      "Epoch: 930, Loss= 0.7074, Training Accuracy= 0.511\n",
      "Epoch: 940, Loss= 0.7039, Training Accuracy= 0.512\n",
      "Epoch: 950, Loss= 0.7010, Training Accuracy= 0.512\n",
      "Epoch: 960, Loss= 0.6996, Training Accuracy= 0.512\n",
      "Epoch: 970, Loss= 0.6986, Training Accuracy= 0.517\n",
      "Epoch: 980, Loss= 0.6745, Training Accuracy= 0.532\n",
      "Epoch: 990, Loss= 0.6976, Training Accuracy= 0.500\n",
      "Epoch: 1000, Loss= 0.6975, Training Accuracy= 0.501\n",
      "Epoch: 1010, Loss= 0.6974, Training Accuracy= 0.501\n",
      "Epoch: 1020, Loss= 0.6601, Training Accuracy= 0.540\n",
      "Epoch: 1030, Loss= 0.6592, Training Accuracy= 0.540\n",
      "Epoch: 1040, Loss= 0.6589, Training Accuracy= 0.540\n",
      "Epoch: 1050, Loss= 0.6587, Training Accuracy= 0.540\n",
      "Epoch: 1060, Loss= 0.6585, Training Accuracy= 0.540\n",
      "Epoch: 1070, Loss= 0.6584, Training Accuracy= 0.540\n",
      "Epoch: 1080, Loss= 0.6584, Training Accuracy= 0.540\n",
      "Epoch: 1090, Loss= 0.6583, Training Accuracy= 0.540\n",
      "Epoch: 1100, Loss= 0.6583, Training Accuracy= 0.540\n",
      "Epoch: 1110, Loss= 0.6582, Training Accuracy= 0.540\n",
      "Epoch: 1120, Loss= 0.6582, Training Accuracy= 0.540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1130, Loss= 0.6582, Training Accuracy= 0.540\n",
      "Epoch: 1140, Loss= 0.6581, Training Accuracy= 0.540\n",
      "Epoch: 1150, Loss= 0.6581, Training Accuracy= 0.540\n",
      "Epoch: 1160, Loss= 0.6581, Training Accuracy= 0.540\n",
      "Epoch: 1170, Loss= 0.6580, Training Accuracy= 0.540\n",
      "Epoch: 1180, Loss= 0.6580, Training Accuracy= 0.540\n",
      "Epoch: 1190, Loss= 0.7075, Training Accuracy= 0.503\n",
      "Epoch: 1200, Loss= 0.7035, Training Accuracy= 0.500\n",
      "Epoch: 1210, Loss= 0.6996, Training Accuracy= 0.499\n",
      "Epoch: 1220, Loss= 0.6995, Training Accuracy= 0.499\n",
      "Epoch: 1230, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1240, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1250, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1260, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1270, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1280, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1290, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1300, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1310, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1320, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1330, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1340, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1350, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1360, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1370, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1380, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1390, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 1400, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1410, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1420, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1430, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1440, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1450, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1460, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1470, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1480, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1490, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1500, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1510, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1520, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1530, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1540, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1550, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1560, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1570, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1580, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1590, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1600, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1610, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1620, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1630, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1640, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1650, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1660, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1670, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1680, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1690, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1700, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1710, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1720, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1730, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1740, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1750, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1760, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1770, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1780, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1790, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1800, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1810, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1820, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1830, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1840, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1850, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1860, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1870, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1880, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1890, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1900, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1910, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1920, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1930, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1940, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1950, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1960, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1970, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1980, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 1990, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4907\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.6967, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 30, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 40, Loss= 0.6931, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6931, Training Accuracy= 0.491\n",
      "Epoch: 60, Loss= 0.6931, Training Accuracy= 0.491\n",
      "Epoch: 70, Loss= 0.6931, Training Accuracy= 0.493\n",
      "Epoch: 80, Loss= 0.6931, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.6931, Training Accuracy= 0.499\n",
      "Epoch: 100, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 110, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 140, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 150, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 160, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 180, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 190, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 200, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 210, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 220, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 230, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 240, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 250, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 270, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 280, Loss= 0.6934, Training Accuracy= 0.514\n",
      "Epoch: 290, Loss= 0.6935, Training Accuracy= 0.516\n",
      "Epoch: 300, Loss= 0.6936, Training Accuracy= 0.519\n",
      "Epoch: 310, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 320, Loss= 0.6935, Training Accuracy= 0.500\n",
      "Epoch: 330, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 340, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 350, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 360, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 370, Loss= 0.6932, Training Accuracy= 0.496\n",
      "Epoch: 380, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 390, Loss= 0.6923, Training Accuracy= 0.511\n",
      "Epoch: 400, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 410, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 420, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 430, Loss= 0.6929, Training Accuracy= 0.520\n",
      "Epoch: 440, Loss= 0.6931, Training Accuracy= 0.521\n",
      "Epoch: 450, Loss= 0.6934, Training Accuracy= 0.523\n",
      "Epoch: 460, Loss= 0.6985, Training Accuracy= 0.516\n",
      "Epoch: 470, Loss= 0.7072, Training Accuracy= 0.516\n",
      "Epoch: 480, Loss= 0.6873, Training Accuracy= 0.554\n",
      "Epoch: 490, Loss= 0.6874, Training Accuracy= 0.550\n",
      "Epoch: 500, Loss= 0.6965, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.6991, Training Accuracy= 0.507\n",
      "Epoch: 520, Loss= 0.7001, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.6985, Training Accuracy= 0.504\n",
      "Epoch: 540, Loss= 0.6954, Training Accuracy= 0.505\n",
      "Epoch: 550, Loss= 0.6917, Training Accuracy= 0.519\n",
      "Epoch: 560, Loss= 0.6792, Training Accuracy= 0.562\n",
      "Epoch: 570, Loss= 0.7083, Training Accuracy= 0.497\n",
      "Epoch: 580, Loss= 0.7015, Training Accuracy= 0.523\n",
      "Epoch: 590, Loss= 0.7111, Training Accuracy= 0.514\n",
      "Epoch: 600, Loss= 0.6973, Training Accuracy= 0.513\n",
      "Epoch: 610, Loss= 0.6974, Training Accuracy= 0.513\n",
      "Epoch: 620, Loss= 0.6960, Training Accuracy= 0.508\n",
      "Epoch: 630, Loss= 0.6956, Training Accuracy= 0.506\n",
      "Epoch: 640, Loss= 0.6956, Training Accuracy= 0.507\n",
      "Epoch: 650, Loss= 0.6965, Training Accuracy= 0.522\n",
      "Epoch: 660, Loss= 0.6988, Training Accuracy= 0.504\n",
      "Epoch: 670, Loss= 0.6812, Training Accuracy= 0.557\n",
      "Epoch: 680, Loss= 0.6746, Training Accuracy= 0.583\n",
      "Epoch: 690, Loss= 0.7043, Training Accuracy= 0.502\n",
      "Epoch: 700, Loss= 0.7034, Training Accuracy= 0.502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 710, Loss= 0.7026, Training Accuracy= 0.498\n",
      "Epoch: 720, Loss= 0.7027, Training Accuracy= 0.498\n",
      "Epoch: 730, Loss= 0.7039, Training Accuracy= 0.514\n",
      "Epoch: 740, Loss= 0.6950, Training Accuracy= 0.495\n",
      "Epoch: 750, Loss= 0.6950, Training Accuracy= 0.497\n",
      "Epoch: 760, Loss= 0.6949, Training Accuracy= 0.495\n",
      "Epoch: 770, Loss= 0.6948, Training Accuracy= 0.501\n",
      "Epoch: 780, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 790, Loss= 0.6948, Training Accuracy= 0.499\n",
      "Epoch: 800, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 810, Loss= 0.6960, Training Accuracy= 0.505\n",
      "Epoch: 820, Loss= 0.6948, Training Accuracy= 0.497\n",
      "Epoch: 830, Loss= 0.6960, Training Accuracy= 0.504\n",
      "Epoch: 840, Loss= 0.6931, Training Accuracy= 0.514\n",
      "Epoch: 850, Loss= 0.6952, Training Accuracy= 0.508\n",
      "Epoch: 860, Loss= 0.6971, Training Accuracy= 0.519\n",
      "Epoch: 870, Loss= 0.7018, Training Accuracy= 0.504\n",
      "Epoch: 880, Loss= 0.6952, Training Accuracy= 0.499\n",
      "Epoch: 890, Loss= 0.6954, Training Accuracy= 0.498\n",
      "Epoch: 900, Loss= 0.6945, Training Accuracy= 0.502\n",
      "Epoch: 910, Loss= 0.6944, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.6945, Training Accuracy= 0.499\n",
      "Epoch: 930, Loss= 0.6945, Training Accuracy= 0.499\n",
      "Epoch: 940, Loss= 0.6945, Training Accuracy= 0.501\n",
      "Epoch: 950, Loss= 0.6945, Training Accuracy= 0.495\n",
      "Epoch: 960, Loss= 0.6945, Training Accuracy= 0.495\n",
      "Epoch: 970, Loss= 0.6945, Training Accuracy= 0.494\n",
      "Epoch: 980, Loss= 0.6945, Training Accuracy= 0.494\n",
      "Epoch: 990, Loss= 0.6944, Training Accuracy= 0.498\n",
      "Epoch: 1000, Loss= 0.6944, Training Accuracy= 0.495\n",
      "Epoch: 1010, Loss= 0.7133, Training Accuracy= 0.507\n",
      "Epoch: 1020, Loss= 0.7011, Training Accuracy= 0.496\n",
      "Epoch: 1030, Loss= 0.7055, Training Accuracy= 0.508\n",
      "Epoch: 1040, Loss= 0.7051, Training Accuracy= 0.510\n",
      "Epoch: 1050, Loss= 0.7016, Training Accuracy= 0.502\n",
      "Epoch: 1060, Loss= 0.6997, Training Accuracy= 0.499\n",
      "Epoch: 1070, Loss= 0.6965, Training Accuracy= 0.496\n",
      "Epoch: 1080, Loss= 0.6998, Training Accuracy= 0.502\n",
      "Epoch: 1090, Loss= 0.6996, Training Accuracy= 0.505\n",
      "Epoch: 1100, Loss= 0.6996, Training Accuracy= 0.505\n",
      "Epoch: 1110, Loss= 0.6996, Training Accuracy= 0.504\n",
      "Epoch: 1120, Loss= 0.6996, Training Accuracy= 0.505\n",
      "Epoch: 1130, Loss= 0.6995, Training Accuracy= 0.503\n",
      "Epoch: 1140, Loss= 0.6995, Training Accuracy= 0.503\n",
      "Epoch: 1150, Loss= 0.6994, Training Accuracy= 0.503\n",
      "Epoch: 1160, Loss= 0.6993, Training Accuracy= 0.503\n",
      "Epoch: 1170, Loss= 0.6992, Training Accuracy= 0.503\n",
      "Epoch: 1180, Loss= 0.6991, Training Accuracy= 0.502\n",
      "Epoch: 1190, Loss= 0.6988, Training Accuracy= 0.497\n",
      "Epoch: 1200, Loss= 0.6986, Training Accuracy= 0.502\n",
      "Epoch: 1210, Loss= 0.6975, Training Accuracy= 0.503\n",
      "Epoch: 1220, Loss= 0.6962, Training Accuracy= 0.505\n",
      "Epoch: 1230, Loss= 0.6961, Training Accuracy= 0.506\n",
      "Epoch: 1240, Loss= 0.6961, Training Accuracy= 0.506\n",
      "Epoch: 1250, Loss= 0.6965, Training Accuracy= 0.506\n",
      "Epoch: 1260, Loss= 0.6965, Training Accuracy= 0.506\n",
      "Epoch: 1270, Loss= 0.6964, Training Accuracy= 0.506\n",
      "Epoch: 1280, Loss= 0.6964, Training Accuracy= 0.506\n",
      "Epoch: 1290, Loss= 0.6964, Training Accuracy= 0.506\n",
      "Epoch: 1300, Loss= 0.6964, Training Accuracy= 0.506\n",
      "Epoch: 1310, Loss= 0.6964, Training Accuracy= 0.506\n",
      "Epoch: 1320, Loss= 0.6964, Training Accuracy= 0.505\n",
      "Epoch: 1330, Loss= 0.6964, Training Accuracy= 0.506\n",
      "Epoch: 1340, Loss= 0.6964, Training Accuracy= 0.506\n",
      "Epoch: 1350, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 1360, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 1370, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 1380, Loss= 0.6984, Training Accuracy= 0.504\n",
      "Epoch: 1390, Loss= 0.6979, Training Accuracy= 0.506\n",
      "Epoch: 1400, Loss= 0.6973, Training Accuracy= 0.493\n",
      "Epoch: 1410, Loss= 0.6980, Training Accuracy= 0.493\n",
      "Epoch: 1420, Loss= 0.6968, Training Accuracy= 0.495\n",
      "Epoch: 1430, Loss= 0.6948, Training Accuracy= 0.506\n",
      "Epoch: 1440, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 1450, Loss= 0.6955, Training Accuracy= 0.492\n",
      "Epoch: 1460, Loss= 0.6947, Training Accuracy= 0.505\n",
      "Epoch: 1470, Loss= 0.6958, Training Accuracy= 0.496\n",
      "Epoch: 1480, Loss= 0.6948, Training Accuracy= 0.503\n",
      "Epoch: 1490, Loss= 0.6937, Training Accuracy= 0.521\n",
      "Epoch: 1500, Loss= 0.6935, Training Accuracy= 0.514\n",
      "Epoch: 1510, Loss= 0.6939, Training Accuracy= 0.519\n",
      "Epoch: 1520, Loss= 0.6933, Training Accuracy= 0.527\n",
      "Epoch: 1530, Loss= 0.6935, Training Accuracy= 0.524\n",
      "Epoch: 1540, Loss= 0.6946, Training Accuracy= 0.512\n",
      "Epoch: 1550, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 1560, Loss= 0.7003, Training Accuracy= 0.517\n",
      "Epoch: 1570, Loss= 0.6974, Training Accuracy= 0.506\n",
      "Epoch: 1580, Loss= 0.6973, Training Accuracy= 0.506\n",
      "Epoch: 1590, Loss= 0.6973, Training Accuracy= 0.506\n",
      "Epoch: 1600, Loss= 0.6972, Training Accuracy= 0.505\n",
      "Epoch: 1610, Loss= 0.6973, Training Accuracy= 0.507\n",
      "Epoch: 1620, Loss= 0.6971, Training Accuracy= 0.509\n",
      "Epoch: 1630, Loss= 0.6972, Training Accuracy= 0.508\n",
      "Epoch: 1640, Loss= 0.6971, Training Accuracy= 0.509\n",
      "Epoch: 1650, Loss= 0.6970, Training Accuracy= 0.510\n",
      "Epoch: 1660, Loss= 0.6967, Training Accuracy= 0.509\n",
      "Epoch: 1670, Loss= 0.6965, Training Accuracy= 0.509\n",
      "Epoch: 1680, Loss= 0.6964, Training Accuracy= 0.509\n",
      "Epoch: 1690, Loss= 0.6963, Training Accuracy= 0.509\n",
      "Epoch: 1700, Loss= 0.6963, Training Accuracy= 0.511\n",
      "Epoch: 1710, Loss= 0.6962, Training Accuracy= 0.511\n",
      "Epoch: 1720, Loss= 0.6962, Training Accuracy= 0.512\n",
      "Epoch: 1730, Loss= 0.6962, Training Accuracy= 0.512\n",
      "Epoch: 1740, Loss= 0.6962, Training Accuracy= 0.512\n",
      "Epoch: 1750, Loss= 0.6962, Training Accuracy= 0.512\n",
      "Epoch: 1760, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1770, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1780, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1790, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1800, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1810, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1820, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1830, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1840, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1850, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1860, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1870, Loss= 0.6961, Training Accuracy= 0.512\n",
      "Epoch: 1880, Loss= 0.6961, Training Accuracy= 0.511\n",
      "Epoch: 1890, Loss= 0.6961, Training Accuracy= 0.511\n",
      "Epoch: 1900, Loss= 0.6960, Training Accuracy= 0.511\n",
      "Epoch: 1910, Loss= 0.6960, Training Accuracy= 0.511\n",
      "Epoch: 1920, Loss= 0.6960, Training Accuracy= 0.511\n",
      "Epoch: 1930, Loss= 0.6960, Training Accuracy= 0.511\n",
      "Epoch: 1940, Loss= 0.6959, Training Accuracy= 0.511\n",
      "Epoch: 1950, Loss= 0.6959, Training Accuracy= 0.512\n",
      "Epoch: 1960, Loss= 0.6960, Training Accuracy= 0.512\n",
      "Epoch: 1970, Loss= 0.6960, Training Accuracy= 0.513\n",
      "Epoch: 1980, Loss= 0.6961, Training Accuracy= 0.514\n",
      "Epoch: 1990, Loss= 0.6993, Training Accuracy= 0.506\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4916\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.6977, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.6964, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6958, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.6954, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 120, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 130, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 140, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 150, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 160, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.496\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.500\n",
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 230, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 270, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 280, Loss= 0.6930, Training Accuracy= 0.516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 290, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 320, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 330, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 340, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 350, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 360, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 400, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 420, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 430, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 470, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 500, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 510, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 520, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 530, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 540, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 550, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 560, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 570, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 580, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 590, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 600, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 610, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 620, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 630, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 640, Loss= 0.6927, Training Accuracy= 0.506\n",
      "Epoch: 650, Loss= 0.6926, Training Accuracy= 0.505\n",
      "Epoch: 660, Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 670, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 680, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 690, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 700, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 710, Loss= 0.6924, Training Accuracy= 0.521\n",
      "Epoch: 720, Loss= 0.6924, Training Accuracy= 0.521\n",
      "Epoch: 730, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 740, Loss= 0.6923, Training Accuracy= 0.511\n",
      "Epoch: 750, Loss= 0.6920, Training Accuracy= 0.504\n",
      "Epoch: 760, Loss= 0.6894, Training Accuracy= 0.513\n",
      "Epoch: 770, Loss= 0.7024, Training Accuracy= 0.507\n",
      "Epoch: 780, Loss= 0.6976, Training Accuracy= 0.504\n",
      "Epoch: 790, Loss= 0.6957, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.6924, Training Accuracy= 0.512\n",
      "Epoch: 810, Loss= 0.6965, Training Accuracy= 0.504\n",
      "Epoch: 820, Loss= 0.6956, Training Accuracy= 0.506\n",
      "Epoch: 830, Loss= 0.6951, Training Accuracy= 0.505\n",
      "Epoch: 840, Loss= 0.6941, Training Accuracy= 0.507\n",
      "Epoch: 850, Loss= 0.6912, Training Accuracy= 0.504\n",
      "Epoch: 860, Loss= 0.6868, Training Accuracy= 0.505\n",
      "Epoch: 870, Loss= 0.6833, Training Accuracy= 0.507\n",
      "Epoch: 880, Loss= 0.6824, Training Accuracy= 0.513\n",
      "Epoch: 890, Loss= 0.6815, Training Accuracy= 0.514\n",
      "Epoch: 900, Loss= 0.6796, Training Accuracy= 0.518\n",
      "Epoch: 910, Loss= 0.6791, Training Accuracy= 0.518\n",
      "Epoch: 920, Loss= 0.6994, Training Accuracy= 0.503\n",
      "Epoch: 930, Loss= 0.6995, Training Accuracy= 0.503\n",
      "Epoch: 940, Loss= 0.6995, Training Accuracy= 0.503\n",
      "Epoch: 950, Loss= 0.6996, Training Accuracy= 0.503\n",
      "Epoch: 960, Loss= 0.6996, Training Accuracy= 0.503\n",
      "Epoch: 970, Loss= 0.6997, Training Accuracy= 0.503\n",
      "Epoch: 980, Loss= 0.6997, Training Accuracy= 0.503\n",
      "Epoch: 990, Loss= 0.6997, Training Accuracy= 0.503\n",
      "Epoch: 1000, Loss= 0.6997, Training Accuracy= 0.503\n",
      "Epoch: 1010, Loss= 0.6996, Training Accuracy= 0.503\n",
      "Epoch: 1020, Loss= 0.6996, Training Accuracy= 0.503\n",
      "Epoch: 1030, Loss= 0.6996, Training Accuracy= 0.503\n",
      "Epoch: 1040, Loss= 0.6996, Training Accuracy= 0.503\n",
      "Epoch: 1050, Loss= 0.6996, Training Accuracy= 0.503\n",
      "Epoch: 1060, Loss= 0.6995, Training Accuracy= 0.503\n",
      "Epoch: 1070, Loss= 0.6995, Training Accuracy= 0.503\n",
      "Epoch: 1080, Loss= 0.6995, Training Accuracy= 0.503\n",
      "Epoch: 1090, Loss= 0.6994, Training Accuracy= 0.503\n",
      "Epoch: 1100, Loss= 0.6994, Training Accuracy= 0.503\n",
      "Epoch: 1110, Loss= 0.6992, Training Accuracy= 0.503\n",
      "Epoch: 1120, Loss= 0.6989, Training Accuracy= 0.503\n",
      "Epoch: 1130, Loss= 0.6987, Training Accuracy= 0.503\n",
      "Epoch: 1140, Loss= 0.6987, Training Accuracy= 0.503\n",
      "Epoch: 1150, Loss= 0.6988, Training Accuracy= 0.503\n",
      "Epoch: 1160, Loss= 0.6993, Training Accuracy= 0.503\n",
      "Epoch: 1170, Loss= 0.6991, Training Accuracy= 0.503\n",
      "Epoch: 1180, Loss= 0.6992, Training Accuracy= 0.503\n",
      "Epoch: 1190, Loss= 0.7043, Training Accuracy= 0.503\n",
      "Epoch: 1200, Loss= 0.7022, Training Accuracy= 0.503\n",
      "Epoch: 1210, Loss= 0.7021, Training Accuracy= 0.503\n",
      "Epoch: 1220, Loss= 0.7021, Training Accuracy= 0.503\n",
      "Epoch: 1230, Loss= 0.7020, Training Accuracy= 0.503\n",
      "Epoch: 1240, Loss= 0.7020, Training Accuracy= 0.503\n",
      "Epoch: 1250, Loss= 0.7020, Training Accuracy= 0.503\n",
      "Epoch: 1260, Loss= 0.7020, Training Accuracy= 0.503\n",
      "Epoch: 1270, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1280, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1290, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1300, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1310, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1320, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1330, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1340, Loss= 0.7020, Training Accuracy= 0.503\n",
      "Epoch: 1350, Loss= 0.7022, Training Accuracy= 0.503\n",
      "Epoch: 1360, Loss= 0.7020, Training Accuracy= 0.503\n",
      "Epoch: 1370, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1380, Loss= 0.7018, Training Accuracy= 0.503\n",
      "Epoch: 1390, Loss= 0.7028, Training Accuracy= 0.503\n",
      "Epoch: 1400, Loss= 0.7026, Training Accuracy= 0.503\n",
      "Epoch: 1410, Loss= 0.7024, Training Accuracy= 0.503\n",
      "Epoch: 1420, Loss= 0.7022, Training Accuracy= 0.503\n",
      "Epoch: 1430, Loss= 0.7022, Training Accuracy= 0.503\n",
      "Epoch: 1440, Loss= 0.7022, Training Accuracy= 0.503\n",
      "Epoch: 1450, Loss= 0.7021, Training Accuracy= 0.503\n",
      "Epoch: 1460, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1470, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1480, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1490, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1500, Loss= 0.7022, Training Accuracy= 0.503\n",
      "Epoch: 1510, Loss= 0.7017, Training Accuracy= 0.503\n",
      "Epoch: 1520, Loss= 0.7021, Training Accuracy= 0.503\n",
      "Epoch: 1530, Loss= 0.7014, Training Accuracy= 0.503\n",
      "Epoch: 1540, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1550, Loss= 0.7021, Training Accuracy= 0.503\n",
      "Epoch: 1560, Loss= 0.7021, Training Accuracy= 0.503\n",
      "Epoch: 1570, Loss= 0.7014, Training Accuracy= 0.503\n",
      "Epoch: 1580, Loss= 0.7020, Training Accuracy= 0.503\n",
      "Epoch: 1590, Loss= 0.7013, Training Accuracy= 0.503\n",
      "Epoch: 1600, Loss= 0.7014, Training Accuracy= 0.503\n",
      "Epoch: 1610, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1620, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1630, Loss= 0.7019, Training Accuracy= 0.503\n",
      "Epoch: 1640, Loss= 0.7013, Training Accuracy= 0.503\n",
      "Epoch: 1650, Loss= 0.7013, Training Accuracy= 0.503\n",
      "Epoch: 1660, Loss= 0.7013, Training Accuracy= 0.503\n",
      "Epoch: 1670, Loss= 0.7021, Training Accuracy= 0.503\n",
      "Epoch: 1680, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1690, Loss= 0.7011, Training Accuracy= 0.503\n",
      "Epoch: 1700, Loss= 0.7020, Training Accuracy= 0.503\n",
      "Epoch: 1710, Loss= 0.7020, Training Accuracy= 0.503\n",
      "Epoch: 1720, Loss= 0.7014, Training Accuracy= 0.503\n",
      "Epoch: 1730, Loss= 0.7014, Training Accuracy= 0.503\n",
      "Epoch: 1740, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1750, Loss= 0.7014, Training Accuracy= 0.503\n",
      "Epoch: 1760, Loss= 0.7013, Training Accuracy= 0.503\n",
      "Epoch: 1770, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1780, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1790, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1800, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1810, Loss= 0.7011, Training Accuracy= 0.503\n",
      "Epoch: 1820, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1830, Loss= 0.7020, Training Accuracy= 0.503\n",
      "Epoch: 1840, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1850, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1860, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1870, Loss= 0.7015, Training Accuracy= 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1880, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1890, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1900, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1910, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1920, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1930, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1940, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1950, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1960, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1970, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1980, Loss= 0.7016, Training Accuracy= 0.503\n",
      "Epoch: 1990, Loss= 0.7013, Training Accuracy= 0.503\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5039\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.7154, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.7075, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.7045, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.7032, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.7028, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.7026, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.7025, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.7024, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.7024, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.7023, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.7023, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.7022, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.7022, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.7022, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.7022, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.7021, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.7014, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.5389, Training Accuracy= 0.741\n",
      "Epoch: 180, Loss= 0.7321, Training Accuracy= 0.510\n",
      "Epoch: 190, Loss= 0.7132, Training Accuracy= 0.507\n",
      "Epoch: 200, Loss= 0.7131, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.7130, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.7130, Training Accuracy= 0.507\n",
      "Epoch: 230, Loss= 0.7130, Training Accuracy= 0.507\n",
      "Epoch: 240, Loss= 0.7130, Training Accuracy= 0.507\n",
      "Epoch: 250, Loss= 0.7130, Training Accuracy= 0.507\n",
      "Epoch: 260, Loss= 0.7130, Training Accuracy= 0.507\n",
      "Epoch: 270, Loss= 0.7130, Training Accuracy= 0.507\n",
      "Epoch: 280, Loss= 0.7129, Training Accuracy= 0.507\n",
      "Epoch: 290, Loss= 0.7129, Training Accuracy= 0.507\n",
      "Epoch: 300, Loss= 0.7129, Training Accuracy= 0.507\n",
      "Epoch: 310, Loss= 0.7129, Training Accuracy= 0.507\n",
      "Epoch: 320, Loss= 0.7129, Training Accuracy= 0.507\n",
      "Epoch: 330, Loss= 0.7129, Training Accuracy= 0.507\n",
      "Epoch: 340, Loss= 0.7130, Training Accuracy= 0.507\n",
      "Epoch: 350, Loss= 0.7130, Training Accuracy= 0.507\n",
      "Epoch: 360, Loss= 0.7130, Training Accuracy= 0.507\n",
      "Epoch: 370, Loss= 0.7131, Training Accuracy= 0.507\n",
      "Epoch: 380, Loss= 0.7132, Training Accuracy= 0.507\n",
      "Epoch: 390, Loss= 0.7132, Training Accuracy= 0.507\n",
      "Epoch: 400, Loss= 0.7133, Training Accuracy= 0.507\n",
      "Epoch: 410, Loss= 0.7134, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.7135, Training Accuracy= 0.507\n",
      "Epoch: 430, Loss= 0.7136, Training Accuracy= 0.507\n",
      "Epoch: 440, Loss= 0.7136, Training Accuracy= 0.507\n",
      "Epoch: 450, Loss= 0.7136, Training Accuracy= 0.507\n",
      "Epoch: 460, Loss= 0.7134, Training Accuracy= 0.507\n",
      "Epoch: 470, Loss= 0.7131, Training Accuracy= 0.507\n",
      "Epoch: 480, Loss= 0.7125, Training Accuracy= 0.507\n",
      "Epoch: 490, Loss= 0.7119, Training Accuracy= 0.507\n",
      "Epoch: 500, Loss= 0.7114, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.7109, Training Accuracy= 0.507\n",
      "Epoch: 520, Loss= 0.7104, Training Accuracy= 0.507\n",
      "Epoch: 530, Loss= 0.7100, Training Accuracy= 0.507\n",
      "Epoch: 540, Loss= 0.7097, Training Accuracy= 0.507\n",
      "Epoch: 550, Loss= 0.7094, Training Accuracy= 0.507\n",
      "Epoch: 560, Loss= 0.7092, Training Accuracy= 0.505\n",
      "Epoch: 570, Loss= 0.7090, Training Accuracy= 0.506\n",
      "Epoch: 580, Loss= 0.7089, Training Accuracy= 0.505\n",
      "Epoch: 590, Loss= 0.7088, Training Accuracy= 0.505\n",
      "Epoch: 600, Loss= 0.7087, Training Accuracy= 0.505\n",
      "Epoch: 610, Loss= 0.7086, Training Accuracy= 0.505\n",
      "Epoch: 620, Loss= 0.7086, Training Accuracy= 0.506\n",
      "Epoch: 630, Loss= 0.7085, Training Accuracy= 0.506\n",
      "Epoch: 640, Loss= 0.7085, Training Accuracy= 0.505\n",
      "Epoch: 650, Loss= 0.7084, Training Accuracy= 0.505\n",
      "Epoch: 660, Loss= 0.7084, Training Accuracy= 0.505\n",
      "Epoch: 670, Loss= 0.7084, Training Accuracy= 0.505\n",
      "Epoch: 680, Loss= 0.7083, Training Accuracy= 0.505\n",
      "Epoch: 690, Loss= 0.7083, Training Accuracy= 0.505\n",
      "Epoch: 700, Loss= 0.7083, Training Accuracy= 0.505\n",
      "Epoch: 710, Loss= 0.7082, Training Accuracy= 0.505\n",
      "Epoch: 720, Loss= 0.7082, Training Accuracy= 0.507\n",
      "Epoch: 730, Loss= 0.7082, Training Accuracy= 0.506\n",
      "Epoch: 740, Loss= 0.7081, Training Accuracy= 0.506\n",
      "Epoch: 750, Loss= 0.7081, Training Accuracy= 0.506\n",
      "Epoch: 760, Loss= 0.7080, Training Accuracy= 0.506\n",
      "Epoch: 770, Loss= 0.7080, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.7079, Training Accuracy= 0.506\n",
      "Epoch: 790, Loss= 0.7078, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.7076, Training Accuracy= 0.506\n",
      "Epoch: 810, Loss= 0.7073, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.7076, Training Accuracy= 0.507\n",
      "Epoch: 830, Loss= 0.7071, Training Accuracy= 0.505\n",
      "Epoch: 840, Loss= 0.7091, Training Accuracy= 0.502\n",
      "Epoch: 850, Loss= 0.7083, Training Accuracy= 0.504\n",
      "Epoch: 860, Loss= 0.7121, Training Accuracy= 0.498\n",
      "Epoch: 870, Loss= 0.7226, Training Accuracy= 0.503\n",
      "Epoch: 880, Loss= 0.7209, Training Accuracy= 0.503\n",
      "Epoch: 890, Loss= 0.7221, Training Accuracy= 0.503\n",
      "Epoch: 900, Loss= 0.7166, Training Accuracy= 0.503\n",
      "Epoch: 910, Loss= 0.7192, Training Accuracy= 0.503\n",
      "Epoch: 920, Loss= 0.7214, Training Accuracy= 0.503\n",
      "Epoch: 930, Loss= 0.7175, Training Accuracy= 0.503\n",
      "Epoch: 940, Loss= 0.7163, Training Accuracy= 0.503\n",
      "Epoch: 950, Loss= 0.7447, Training Accuracy= 0.502\n",
      "Epoch: 960, Loss= 0.7200, Training Accuracy= 0.507\n",
      "Epoch: 970, Loss= 0.7110, Training Accuracy= 0.499\n",
      "Epoch: 980, Loss= 0.7094, Training Accuracy= 0.499\n",
      "Epoch: 990, Loss= 0.7093, Training Accuracy= 0.498\n",
      "Epoch: 1000, Loss= 0.7092, Training Accuracy= 0.500\n",
      "Epoch: 1010, Loss= 0.6993, Training Accuracy= 0.513\n",
      "Epoch: 1020, Loss= 0.6958, Training Accuracy= 0.507\n",
      "Epoch: 1030, Loss= 0.7186, Training Accuracy= 0.503\n",
      "Epoch: 1040, Loss= 0.7193, Training Accuracy= 0.503\n",
      "Epoch: 1050, Loss= 0.7229, Training Accuracy= 0.504\n",
      "Epoch: 1060, Loss= 0.7224, Training Accuracy= 0.505\n",
      "Epoch: 1070, Loss= 0.7224, Training Accuracy= 0.507\n",
      "Epoch: 1080, Loss= 0.7042, Training Accuracy= 0.507\n",
      "Epoch: 1090, Loss= 0.7010, Training Accuracy= 0.511\n",
      "Epoch: 1100, Loss= 0.6987, Training Accuracy= 0.498\n",
      "Epoch: 1110, Loss= 0.7050, Training Accuracy= 0.491\n",
      "Epoch: 1120, Loss= 0.6987, Training Accuracy= 0.502\n",
      "Epoch: 1130, Loss= 0.6984, Training Accuracy= 0.506\n",
      "Epoch: 1140, Loss= 0.6988, Training Accuracy= 0.507\n",
      "Epoch: 1150, Loss= 0.7006, Training Accuracy= 0.507\n",
      "Epoch: 1160, Loss= 0.7005, Training Accuracy= 0.523\n",
      "Epoch: 1170, Loss= 0.7025, Training Accuracy= 0.504\n",
      "Epoch: 1180, Loss= 0.7118, Training Accuracy= 0.498\n",
      "Epoch: 1190, Loss= 0.6911, Training Accuracy= 0.527\n",
      "Epoch: 1200, Loss= 0.6898, Training Accuracy= 0.538\n",
      "Epoch: 1210, Loss= 0.7256, Training Accuracy= 0.504\n",
      "Epoch: 1220, Loss= 0.7063, Training Accuracy= 0.516\n",
      "Epoch: 1230, Loss= 0.7090, Training Accuracy= 0.518\n",
      "Epoch: 1240, Loss= 0.7027, Training Accuracy= 0.503\n",
      "Epoch: 1250, Loss= 0.6998, Training Accuracy= 0.504\n",
      "Epoch: 1260, Loss= 0.7023, Training Accuracy= 0.503\n",
      "Epoch: 1270, Loss= 0.7015, Training Accuracy= 0.503\n",
      "Epoch: 1280, Loss= 0.7022, Training Accuracy= 0.503\n",
      "Epoch: 1290, Loss= 0.7031, Training Accuracy= 0.503\n",
      "Epoch: 1300, Loss= 0.7062, Training Accuracy= 0.503\n",
      "Epoch: 1310, Loss= 0.7064, Training Accuracy= 0.503\n",
      "Epoch: 1320, Loss= 0.7064, Training Accuracy= 0.503\n",
      "Epoch: 1330, Loss= 0.7060, Training Accuracy= 0.503\n",
      "Epoch: 1340, Loss= 0.7060, Training Accuracy= 0.503\n",
      "Epoch: 1350, Loss= 0.7062, Training Accuracy= 0.503\n",
      "Epoch: 1360, Loss= 0.7058, Training Accuracy= 0.503\n",
      "Epoch: 1370, Loss= 0.7065, Training Accuracy= 0.503\n",
      "Epoch: 1380, Loss= 0.7067, Training Accuracy= 0.503\n",
      "Epoch: 1390, Loss= 0.7067, Training Accuracy= 0.503\n",
      "Epoch: 1400, Loss= 0.7068, Training Accuracy= 0.503\n",
      "Epoch: 1410, Loss= 0.7069, Training Accuracy= 0.503\n",
      "Epoch: 1420, Loss= 0.7070, Training Accuracy= 0.503\n",
      "Epoch: 1430, Loss= 0.7070, Training Accuracy= 0.503\n",
      "Epoch: 1440, Loss= 0.7071, Training Accuracy= 0.503\n",
      "Epoch: 1450, Loss= 0.7072, Training Accuracy= 0.503\n",
      "Epoch: 1460, Loss= 0.7074, Training Accuracy= 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1470, Loss= 0.7075, Training Accuracy= 0.503\n",
      "Epoch: 1480, Loss= 0.7076, Training Accuracy= 0.503\n",
      "Epoch: 1490, Loss= 0.7077, Training Accuracy= 0.503\n",
      "Epoch: 1500, Loss= 0.7077, Training Accuracy= 0.503\n",
      "Epoch: 1510, Loss= 0.7077, Training Accuracy= 0.503\n",
      "Epoch: 1520, Loss= 0.7077, Training Accuracy= 0.503\n",
      "Epoch: 1530, Loss= 0.7077, Training Accuracy= 0.503\n",
      "Epoch: 1540, Loss= 0.7077, Training Accuracy= 0.503\n",
      "Epoch: 1550, Loss= 0.7078, Training Accuracy= 0.503\n",
      "Epoch: 1560, Loss= 0.7105, Training Accuracy= 0.507\n",
      "Epoch: 1570, Loss= 0.7120, Training Accuracy= 0.503\n",
      "Epoch: 1580, Loss= 0.7122, Training Accuracy= 0.501\n",
      "Epoch: 1590, Loss= 0.7120, Training Accuracy= 0.502\n",
      "Epoch: 1600, Loss= 0.7119, Training Accuracy= 0.499\n",
      "Epoch: 1610, Loss= 0.7118, Training Accuracy= 0.500\n",
      "Epoch: 1620, Loss= 0.7115, Training Accuracy= 0.507\n",
      "Epoch: 1630, Loss= 0.7106, Training Accuracy= 0.503\n",
      "Epoch: 1640, Loss= 0.7106, Training Accuracy= 0.504\n",
      "Epoch: 1650, Loss= 0.7110, Training Accuracy= 0.503\n",
      "Epoch: 1660, Loss= 0.7108, Training Accuracy= 0.510\n",
      "Epoch: 1670, Loss= 0.7094, Training Accuracy= 0.510\n",
      "Epoch: 1680, Loss= 0.7091, Training Accuracy= 0.510\n",
      "Epoch: 1690, Loss= 0.7118, Training Accuracy= 0.503\n",
      "Epoch: 1700, Loss= 0.7104, Training Accuracy= 0.510\n",
      "Epoch: 1710, Loss= 0.7115, Training Accuracy= 0.504\n",
      "Epoch: 1720, Loss= 0.7103, Training Accuracy= 0.510\n",
      "Epoch: 1730, Loss= 0.7101, Training Accuracy= 0.510\n",
      "Epoch: 1740, Loss= 0.7107, Training Accuracy= 0.510\n",
      "Epoch: 1750, Loss= 0.7114, Training Accuracy= 0.501\n",
      "Epoch: 1760, Loss= 0.7113, Training Accuracy= 0.509\n",
      "Epoch: 1770, Loss= 0.7106, Training Accuracy= 0.509\n",
      "Epoch: 1780, Loss= 0.7200, Training Accuracy= 0.503\n",
      "Epoch: 1790, Loss= 0.7152, Training Accuracy= 0.500\n",
      "Epoch: 1800, Loss= 0.7146, Training Accuracy= 0.503\n",
      "Epoch: 1810, Loss= 0.7141, Training Accuracy= 0.503\n",
      "Epoch: 1820, Loss= 0.7218, Training Accuracy= 0.503\n",
      "Epoch: 1830, Loss= 0.7218, Training Accuracy= 0.504\n",
      "Epoch: 1840, Loss= 0.7117, Training Accuracy= 0.495\n",
      "Epoch: 1850, Loss= 0.7111, Training Accuracy= 0.499\n",
      "Epoch: 1860, Loss= 0.7113, Training Accuracy= 0.499\n",
      "Epoch: 1870, Loss= 0.7115, Training Accuracy= 0.499\n",
      "Epoch: 1880, Loss= 0.7116, Training Accuracy= 0.500\n",
      "Epoch: 1890, Loss= 0.7118, Training Accuracy= 0.500\n",
      "Epoch: 1900, Loss= 0.7119, Training Accuracy= 0.500\n",
      "Epoch: 1910, Loss= 0.7120, Training Accuracy= 0.500\n",
      "Epoch: 1920, Loss= 0.7121, Training Accuracy= 0.500\n",
      "Epoch: 1930, Loss= 0.7121, Training Accuracy= 0.499\n",
      "Epoch: 1940, Loss= 0.7121, Training Accuracy= 0.499\n",
      "Epoch: 1950, Loss= 0.7120, Training Accuracy= 0.499\n",
      "Epoch: 1960, Loss= 0.7118, Training Accuracy= 0.499\n",
      "Epoch: 1970, Loss= 0.7116, Training Accuracy= 0.499\n",
      "Epoch: 1980, Loss= 0.7115, Training Accuracy= 0.499\n",
      "Epoch: 1990, Loss= 0.7113, Training Accuracy= 0.499\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4964\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.6947, Training Accuracy= 0.499\n",
      "Epoch: 10, Loss= 0.6942, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6934, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.6933, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.6931, Training Accuracy= 0.498\n",
      "Epoch: 90, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 120, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 130, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 140, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 160, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.500\n",
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 230, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.520\n",
      "Epoch: 270, Loss= 0.6930, Training Accuracy= 0.519\n",
      "Epoch: 280, Loss= 0.6930, Training Accuracy= 0.519\n",
      "Epoch: 290, Loss= 0.6930, Training Accuracy= 0.521\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.518\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.519\n",
      "Epoch: 320, Loss= 0.6930, Training Accuracy= 0.519\n",
      "Epoch: 330, Loss= 0.6930, Training Accuracy= 0.519\n",
      "Epoch: 340, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 350, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 360, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 400, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 420, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 430, Loss= 0.6930, Training Accuracy= 0.517\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.520\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.521\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.520\n",
      "Epoch: 470, Loss= 0.6930, Training Accuracy= 0.520\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.520\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.519\n",
      "Epoch: 500, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 510, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 520, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 530, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 540, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 550, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 560, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 570, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 580, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 590, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 600, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 610, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 620, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 630, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 680, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 700, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 710, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 730, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 740, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 750, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 770, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 780, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 790, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 800, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 810, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 820, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 830, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 840, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 850, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 860, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 870, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 880, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 890, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 900, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 910, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 920, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 930, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 940, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 950, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 960, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 970, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 990, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1000, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1010, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1020, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1030, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1040, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1050, Loss= 0.6929, Training Accuracy= 0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1060, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1070, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1080, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1090, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1100, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1110, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1120, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1130, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1140, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1150, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 1160, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 1170, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 1180, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 1190, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1200, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1210, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1220, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1230, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1240, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1250, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1260, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1270, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1280, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1290, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 1300, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 1310, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 1320, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 1330, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 1340, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 1350, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 1360, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 1370, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1380, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 1390, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1400, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1410, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1420, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 1430, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 1440, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 1450, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 1460, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 1470, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 1480, Loss= 0.6929, Training Accuracy= 0.517\n",
      "Epoch: 1490, Loss= 0.6929, Training Accuracy= 0.520\n",
      "Epoch: 1500, Loss= 0.6929, Training Accuracy= 0.520\n",
      "Epoch: 1510, Loss= 0.6929, Training Accuracy= 0.520\n",
      "Epoch: 1520, Loss= 0.6929, Training Accuracy= 0.521\n",
      "Epoch: 1530, Loss= 0.6929, Training Accuracy= 0.521\n",
      "Epoch: 1540, Loss= 0.6929, Training Accuracy= 0.520\n",
      "Epoch: 1550, Loss= 0.6929, Training Accuracy= 0.517\n",
      "Epoch: 1560, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 1570, Loss= 0.6929, Training Accuracy= 0.518\n",
      "Epoch: 1580, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 1590, Loss= 0.6929, Training Accuracy= 0.518\n",
      "Epoch: 1600, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 1610, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 1620, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 1630, Loss= 0.6928, Training Accuracy= 0.505\n",
      "Epoch: 1640, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 1650, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 1660, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 1670, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 1680, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 1690, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 1700, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 1710, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 1720, Loss= 0.6926, Training Accuracy= 0.508\n",
      "Epoch: 1730, Loss= 0.6926, Training Accuracy= 0.506\n",
      "Epoch: 1740, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 1750, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 1760, Loss= 0.6925, Training Accuracy= 0.507\n",
      "Epoch: 1770, Loss= 0.6925, Training Accuracy= 0.506\n",
      "Epoch: 1780, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 1790, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 1800, Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 1810, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 1820, Loss= 0.6924, Training Accuracy= 0.502\n",
      "Epoch: 1830, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 1840, Loss= 0.6923, Training Accuracy= 0.513\n",
      "Epoch: 1850, Loss= 0.6923, Training Accuracy= 0.515\n",
      "Epoch: 1860, Loss= 0.6923, Training Accuracy= 0.525\n",
      "Epoch: 1870, Loss= 0.6923, Training Accuracy= 0.528\n",
      "Epoch: 1880, Loss= 0.6922, Training Accuracy= 0.517\n",
      "Epoch: 1890, Loss= 0.6922, Training Accuracy= 0.508\n",
      "Epoch: 1900, Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 1910, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 1920, Loss= 0.6922, Training Accuracy= 0.511\n",
      "Epoch: 1930, Loss= 0.6921, Training Accuracy= 0.520\n",
      "Epoch: 1940, Loss= 0.6921, Training Accuracy= 0.514\n",
      "Epoch: 1950, Loss= 0.6916, Training Accuracy= 0.521\n",
      "Epoch: 1960, Loss= 0.6943, Training Accuracy= 0.512\n",
      "Epoch: 1970, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 1980, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 1990, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5097\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.45\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 2000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.50309998, 1.0, 0.50480002, 0.50260001, 0.4991, 0.49070001, 0.49160001, 0.50389999, 0.4964, 0.5097]\n",
      "mean of test_accuracies_10replications:  0.55019\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.00150043055415\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4sUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf4d276890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
