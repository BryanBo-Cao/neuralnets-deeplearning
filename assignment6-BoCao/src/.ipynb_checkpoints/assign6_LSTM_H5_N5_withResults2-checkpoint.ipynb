{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 5\n",
    "N = 5\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6938, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6938, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 90, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 100, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 140, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 150, Loss= 0.6937, Training Accuracy= 0.500\n",
      "Epoch: 160, Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 170, Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 180, Loss= 0.6933, Training Accuracy= 0.469\n",
      "Epoch: 190, Loss= 0.3872, Training Accuracy= 0.909\n",
      "Epoch: 200, Loss= 0.0857, Training Accuracy= 0.970\n",
      "Epoch: 210, Loss= 0.0219, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.0097, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.0060, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.0043, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.0033, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.0027, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0023, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0020, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0017, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0015, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0014, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0012, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0011, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0010, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0010, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0009, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7007, Training Accuracy= 0.510\n",
      "Epoch: 10, Loss= 0.6977, Training Accuracy= 0.510\n",
      "Epoch: 20, Loss= 0.6970, Training Accuracy= 0.510\n",
      "Epoch: 30, Loss= 0.6967, Training Accuracy= 0.510\n",
      "Epoch: 40, Loss= 0.6966, Training Accuracy= 0.510\n",
      "Epoch: 50, Loss= 0.6964, Training Accuracy= 0.510\n",
      "Epoch: 60, Loss= 0.6963, Training Accuracy= 0.510\n",
      "Epoch: 70, Loss= 0.6962, Training Accuracy= 0.510\n",
      "Epoch: 80, Loss= 0.6962, Training Accuracy= 0.510\n",
      "Epoch: 90, Loss= 0.6961, Training Accuracy= 0.510\n",
      "Epoch: 100, Loss= 0.6961, Training Accuracy= 0.510\n",
      "Epoch: 110, Loss= 0.6960, Training Accuracy= 0.510\n",
      "Epoch: 120, Loss= 0.6960, Training Accuracy= 0.510\n",
      "Epoch: 130, Loss= 0.6959, Training Accuracy= 0.510\n",
      "Epoch: 140, Loss= 0.6959, Training Accuracy= 0.510\n",
      "Epoch: 150, Loss= 0.6959, Training Accuracy= 0.510\n",
      "Epoch: 160, Loss= 0.6959, Training Accuracy= 0.510\n",
      "Epoch: 170, Loss= 0.6958, Training Accuracy= 0.510\n",
      "Epoch: 180, Loss= 0.6958, Training Accuracy= 0.510\n",
      "Epoch: 190, Loss= 0.6958, Training Accuracy= 0.510\n",
      "Epoch: 200, Loss= 0.6958, Training Accuracy= 0.510\n",
      "Epoch: 210, Loss= 0.6958, Training Accuracy= 0.510\n",
      "Epoch: 220, Loss= 0.6958, Training Accuracy= 0.510\n",
      "Epoch: 230, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 240, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 250, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 260, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 270, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 280, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 290, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 300, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 310, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 320, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 330, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 340, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 350, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 360, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 370, Loss= 0.6957, Training Accuracy= 0.510\n",
      "Epoch: 380, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 390, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 400, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 410, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 420, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 430, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 440, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 450, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 460, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 470, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 480, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 490, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 500, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 510, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 520, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 530, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 550, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 560, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 570, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 580, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 590, Loss= 0.6956, Training Accuracy= 0.510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 610, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 620, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 630, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 640, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 650, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 660, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 670, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 680, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 690, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 700, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 710, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 720, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 730, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 740, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 750, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 760, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 770, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 780, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 790, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 800, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 810, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 820, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 830, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 840, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 850, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 860, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 870, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 880, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 890, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 900, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 910, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 920, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 930, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 940, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 950, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 960, Loss= 0.6955, Training Accuracy= 0.510\n",
      "Epoch: 970, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 980, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 990, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5012\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.7298, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.7017, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.6987, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.6976, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.6970, Training Accuracy= 0.501\n",
      "Epoch: 50, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 60, Loss= 0.6964, Training Accuracy= 0.501\n",
      "Epoch: 70, Loss= 0.6962, Training Accuracy= 0.501\n",
      "Epoch: 80, Loss= 0.6961, Training Accuracy= 0.501\n",
      "Epoch: 90, Loss= 0.6960, Training Accuracy= 0.501\n",
      "Epoch: 100, Loss= 0.6959, Training Accuracy= 0.501\n",
      "Epoch: 110, Loss= 0.6958, Training Accuracy= 0.501\n",
      "Epoch: 120, Loss= 0.6957, Training Accuracy= 0.501\n",
      "Epoch: 130, Loss= 0.6957, Training Accuracy= 0.501\n",
      "Epoch: 140, Loss= 0.6956, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.6956, Training Accuracy= 0.501\n",
      "Epoch: 160, Loss= 0.6956, Training Accuracy= 0.501\n",
      "Epoch: 170, Loss= 0.6955, Training Accuracy= 0.501\n",
      "Epoch: 180, Loss= 0.6955, Training Accuracy= 0.501\n",
      "Epoch: 190, Loss= 0.6955, Training Accuracy= 0.501\n",
      "Epoch: 200, Loss= 0.6954, Training Accuracy= 0.501\n",
      "Epoch: 210, Loss= 0.6954, Training Accuracy= 0.501\n",
      "Epoch: 220, Loss= 0.6954, Training Accuracy= 0.501\n",
      "Epoch: 230, Loss= 0.6954, Training Accuracy= 0.501\n",
      "Epoch: 240, Loss= 0.6954, Training Accuracy= 0.501\n",
      "Epoch: 250, Loss= 0.6954, Training Accuracy= 0.501\n",
      "Epoch: 260, Loss= 0.6953, Training Accuracy= 0.501\n",
      "Epoch: 270, Loss= 0.6953, Training Accuracy= 0.501\n",
      "Epoch: 280, Loss= 0.6953, Training Accuracy= 0.501\n",
      "Epoch: 290, Loss= 0.6953, Training Accuracy= 0.501\n",
      "Epoch: 300, Loss= 0.6950, Training Accuracy= 0.501\n",
      "Epoch: 310, Loss= 0.6853, Training Accuracy= 0.501\n",
      "Epoch: 320, Loss= 0.6372, Training Accuracy= 0.655\n",
      "Epoch: 330, Loss= 0.0074, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0025, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0015, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0010, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0000, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0000, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0000, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0000, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0000, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.7078, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6999, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6982, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6974, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6970, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6968, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6966, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6965, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.6964, Training Accuracy= 0.500\n",
      "Epoch: 90, Loss= 0.6963, Training Accuracy= 0.500\n",
      "Epoch: 100, Loss= 0.6963, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6962, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.6962, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.6961, Training Accuracy= 0.500\n",
      "Epoch: 140, Loss= 0.6961, Training Accuracy= 0.500\n",
      "Epoch: 150, Loss= 0.6960, Training Accuracy= 0.500\n",
      "Epoch: 160, Loss= 0.6958, Training Accuracy= 0.500\n",
      "Epoch: 170, Loss= 0.4734, Training Accuracy= 0.784\n",
      "Epoch: 180, Loss= 0.0690, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.0126, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.0053, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.0033, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.0024, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.0019, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.0015, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.0013, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.0011, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0010, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0009, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.6934, Training Accuracy= 0.535\n",
      "Epoch: 10, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 20, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 30, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 40, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 50, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 60, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 70, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 80, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 90, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 100, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 110, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 120, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 130, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 140, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 150, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 160, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 170, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 180, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 190, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 200, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 210, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 220, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 230, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 240, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 250, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 260, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 270, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 280, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 290, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 300, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 310, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 320, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 330, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 340, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 350, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 360, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 370, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 380, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 390, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 400, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 410, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 420, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 430, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 440, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 450, Loss= 0.6926, Training Accuracy= 0.482\n",
      "Epoch: 460, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 470, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 480, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 490, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 500, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 510, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 520, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 530, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 540, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 550, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 560, Loss= 0.6925, Training Accuracy= 0.482\n",
      "Epoch: 570, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 580, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 590, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 600, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 610, Loss= 0.6922, Training Accuracy= 0.514\n",
      "Epoch: 620, Loss= 0.5953, Training Accuracy= 0.725\n",
      "Epoch: 630, Loss= 0.0927, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0128, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0055, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0035, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0026, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0020, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0017, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0014, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0013, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0011, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0010, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0009, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0006, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.7077, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.7020, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.7004, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.6997, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.6992, Training Accuracy= 0.501\n",
      "Epoch: 50, Loss= 0.6989, Training Accuracy= 0.501\n",
      "Epoch: 60, Loss= 0.6986, Training Accuracy= 0.501\n",
      "Epoch: 70, Loss= 0.6984, Training Accuracy= 0.501\n",
      "Epoch: 80, Loss= 0.6983, Training Accuracy= 0.501\n",
      "Epoch: 90, Loss= 0.6981, Training Accuracy= 0.501\n",
      "Epoch: 100, Loss= 0.6980, Training Accuracy= 0.501\n",
      "Epoch: 110, Loss= 0.6979, Training Accuracy= 0.501\n",
      "Epoch: 120, Loss= 0.6978, Training Accuracy= 0.501\n",
      "Epoch: 130, Loss= 0.6977, Training Accuracy= 0.501\n",
      "Epoch: 140, Loss= 0.6976, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.6975, Training Accuracy= 0.501\n",
      "Epoch: 160, Loss= 0.6975, Training Accuracy= 0.501\n",
      "Epoch: 170, Loss= 0.6974, Training Accuracy= 0.501\n",
      "Epoch: 180, Loss= 0.6974, Training Accuracy= 0.501\n",
      "Epoch: 190, Loss= 0.6973, Training Accuracy= 0.501\n",
      "Epoch: 200, Loss= 0.6973, Training Accuracy= 0.501\n",
      "Epoch: 210, Loss= 0.6972, Training Accuracy= 0.501\n",
      "Epoch: 220, Loss= 0.6972, Training Accuracy= 0.501\n",
      "Epoch: 230, Loss= 0.6971, Training Accuracy= 0.501\n",
      "Epoch: 240, Loss= 0.6971, Training Accuracy= 0.501\n",
      "Epoch: 250, Loss= 0.6971, Training Accuracy= 0.501\n",
      "Epoch: 260, Loss= 0.6970, Training Accuracy= 0.501\n",
      "Epoch: 270, Loss= 0.6970, Training Accuracy= 0.501\n",
      "Epoch: 280, Loss= 0.6970, Training Accuracy= 0.501\n",
      "Epoch: 290, Loss= 0.6969, Training Accuracy= 0.501\n",
      "Epoch: 300, Loss= 0.6969, Training Accuracy= 0.501\n",
      "Epoch: 310, Loss= 0.6969, Training Accuracy= 0.501\n",
      "Epoch: 320, Loss= 0.6969, Training Accuracy= 0.501\n",
      "Epoch: 330, Loss= 0.6968, Training Accuracy= 0.501\n",
      "Epoch: 340, Loss= 0.6968, Training Accuracy= 0.501\n",
      "Epoch: 350, Loss= 0.6968, Training Accuracy= 0.501\n",
      "Epoch: 360, Loss= 0.6968, Training Accuracy= 0.501\n",
      "Epoch: 370, Loss= 0.6968, Training Accuracy= 0.501\n",
      "Epoch: 380, Loss= 0.6968, Training Accuracy= 0.501\n",
      "Epoch: 390, Loss= 0.6967, Training Accuracy= 0.501\n",
      "Epoch: 400, Loss= 0.6967, Training Accuracy= 0.501\n",
      "Epoch: 410, Loss= 0.6967, Training Accuracy= 0.501\n",
      "Epoch: 420, Loss= 0.6967, Training Accuracy= 0.501\n",
      "Epoch: 430, Loss= 0.6967, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.6967, Training Accuracy= 0.501\n",
      "Epoch: 450, Loss= 0.6967, Training Accuracy= 0.501\n",
      "Epoch: 460, Loss= 0.6967, Training Accuracy= 0.501\n",
      "Epoch: 470, Loss= 0.6967, Training Accuracy= 0.501\n",
      "Epoch: 480, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 490, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 500, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 510, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 520, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 530, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 540, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 550, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 560, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 570, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 580, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 590, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 600, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 610, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 620, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 630, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 640, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 650, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 660, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 670, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 680, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 690, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 700, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 710, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 720, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 730, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 740, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 750, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 760, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 770, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 780, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 790, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 800, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 810, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 820, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 830, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 840, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 850, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 860, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 870, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 880, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 890, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 900, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 910, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 920, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 930, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 940, Loss= 0.6965, Training Accuracy= 0.501\n",
      "Epoch: 950, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 960, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 970, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 980, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 990, Loss= 0.6966, Training Accuracy= 0.501\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4995\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.7118, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.7037, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.7026, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.7019, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.7014, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.7010, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.7007, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.7004, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.7001, Training Accuracy= 0.500\n",
      "Epoch: 90, Loss= 0.6999, Training Accuracy= 0.500\n",
      "Epoch: 100, Loss= 0.6997, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6995, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.6993, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.6992, Training Accuracy= 0.500\n",
      "Epoch: 140, Loss= 0.6990, Training Accuracy= 0.500\n",
      "Epoch: 150, Loss= 0.6989, Training Accuracy= 0.500\n",
      "Epoch: 160, Loss= 0.6988, Training Accuracy= 0.500\n",
      "Epoch: 170, Loss= 0.6987, Training Accuracy= 0.500\n",
      "Epoch: 180, Loss= 0.6986, Training Accuracy= 0.500\n",
      "Epoch: 190, Loss= 0.6985, Training Accuracy= 0.500\n",
      "Epoch: 200, Loss= 0.6984, Training Accuracy= 0.500\n",
      "Epoch: 210, Loss= 0.6983, Training Accuracy= 0.500\n",
      "Epoch: 220, Loss= 0.6983, Training Accuracy= 0.500\n",
      "Epoch: 230, Loss= 0.6982, Training Accuracy= 0.500\n",
      "Epoch: 240, Loss= 0.6981, Training Accuracy= 0.500\n",
      "Epoch: 250, Loss= 0.6981, Training Accuracy= 0.500\n",
      "Epoch: 260, Loss= 0.6980, Training Accuracy= 0.500\n",
      "Epoch: 270, Loss= 0.6977, Training Accuracy= 0.500\n",
      "Epoch: 280, Loss= 0.6598, Training Accuracy= 0.689\n",
      "Epoch: 290, Loss= 0.1198, Training Accuracy= 0.971\n",
      "Epoch: 300, Loss= 0.0344, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0136, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0075, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0050, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0037, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0030, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0025, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0021, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.0018, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0016, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0014, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0013, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0012, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0011, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0010, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0009, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0009, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.6970, Training Accuracy= 0.507\n",
      "Epoch: 10, Loss= 0.6957, Training Accuracy= 0.507\n",
      "Epoch: 20, Loss= 0.6952, Training Accuracy= 0.507\n",
      "Epoch: 30, Loss= 0.6949, Training Accuracy= 0.507\n",
      "Epoch: 40, Loss= 0.6947, Training Accuracy= 0.507\n",
      "Epoch: 50, Loss= 0.6946, Training Accuracy= 0.507\n",
      "Epoch: 60, Loss= 0.6945, Training Accuracy= 0.507\n",
      "Epoch: 70, Loss= 0.6944, Training Accuracy= 0.507\n",
      "Epoch: 80, Loss= 0.6944, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6943, Training Accuracy= 0.507\n",
      "Epoch: 100, Loss= 0.6943, Training Accuracy= 0.507\n",
      "Epoch: 110, Loss= 0.6943, Training Accuracy= 0.507\n",
      "Epoch: 120, Loss= 0.6942, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6942, Training Accuracy= 0.507\n",
      "Epoch: 140, Loss= 0.6942, Training Accuracy= 0.507\n",
      "Epoch: 150, Loss= 0.6939, Training Accuracy= 0.507\n",
      "Epoch: 160, Loss= 0.4867, Training Accuracy= 0.783\n",
      "Epoch: 170, Loss= 0.0870, Training Accuracy= 0.970\n",
      "Epoch: 180, Loss= 0.0214, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.0089, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.0053, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.0038, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.0029, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.0023, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.0020, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.0017, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.0015, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0013, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0012, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0011, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0010, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0009, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0001, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.6957, Training Accuracy= 0.505\n",
      "Epoch: 10, Loss= 0.6943, Training Accuracy= 0.505\n",
      "Epoch: 20, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 30, Loss= 0.6938, Training Accuracy= 0.505\n",
      "Epoch: 40, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.6937, Training Accuracy= 0.505\n",
      "Epoch: 60, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 70, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 90, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 100, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 110, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 130, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 150, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 160, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 170, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 180, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 190, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 200, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 210, Loss= 0.6933, Training Accuracy= 0.475\n",
      "Epoch: 220, Loss= 0.6932, Training Accuracy= 0.475\n",
      "Epoch: 230, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 240, Loss= 0.6177, Training Accuracy= 0.756\n",
      "Epoch: 250, Loss= 0.0674, Training Accuracy= 0.969\n",
      "Epoch: 260, Loss= 0.0199, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0058, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0041, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0032, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0026, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0022, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0019, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0016, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0015, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0013, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0012, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0011, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0010, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0009, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0009, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 570, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0001, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7133, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.7012, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6991, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6981, Training Accuracy= 0.500\n",
      "Epoch: 40, Loss= 0.6975, Training Accuracy= 0.500\n",
      "Epoch: 50, Loss= 0.6971, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6968, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6966, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.6963, Training Accuracy= 0.500\n",
      "Epoch: 90, Loss= 0.6962, Training Accuracy= 0.500\n",
      "Epoch: 100, Loss= 0.6960, Training Accuracy= 0.500\n",
      "Epoch: 110, Loss= 0.6959, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.6958, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 140, Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 150, Loss= 0.6955, Training Accuracy= 0.500\n",
      "Epoch: 160, Loss= 0.6954, Training Accuracy= 0.500\n",
      "Epoch: 170, Loss= 0.6953, Training Accuracy= 0.500\n",
      "Epoch: 180, Loss= 0.6952, Training Accuracy= 0.500\n",
      "Epoch: 190, Loss= 0.6952, Training Accuracy= 0.500\n",
      "Epoch: 200, Loss= 0.6951, Training Accuracy= 0.500\n",
      "Epoch: 210, Loss= 0.6950, Training Accuracy= 0.500\n",
      "Epoch: 220, Loss= 0.6950, Training Accuracy= 0.500\n",
      "Epoch: 230, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 240, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 250, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 260, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 270, Loss= 0.6948, Training Accuracy= 0.500\n",
      "Epoch: 280, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 290, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 300, Loss= 0.6947, Training Accuracy= 0.500\n",
      "Epoch: 310, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 320, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 330, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 340, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 350, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 360, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 370, Loss= 0.6945, Training Accuracy= 0.500\n",
      "Epoch: 380, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 390, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 400, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 410, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 420, Loss= 0.6943, Training Accuracy= 0.500\n",
      "Epoch: 430, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 440, Loss= 0.6938, Training Accuracy= 0.500\n",
      "Epoch: 450, Loss= 0.6841, Training Accuracy= 0.503\n",
      "Epoch: 460, Loss= 0.1886, Training Accuracy= 0.937\n",
      "Epoch: 470, Loss= 0.1009, Training Accuracy= 0.967\n",
      "Epoch: 480, Loss= 0.4217, Training Accuracy= 0.843\n",
      "Epoch: 490, Loss= 0.0158, Training Accuracy= 1.000\n",
      "Epoch: 500, Loss= 0.0082, Training Accuracy= 1.000\n",
      "Epoch: 510, Loss= 0.0054, Training Accuracy= 1.000\n",
      "Epoch: 520, Loss= 0.0040, Training Accuracy= 1.000\n",
      "Epoch: 530, Loss= 0.0031, Training Accuracy= 1.000\n",
      "Epoch: 540, Loss= 0.0025, Training Accuracy= 1.000\n",
      "Epoch: 550, Loss= 0.0020, Training Accuracy= 1.000\n",
      "Epoch: 560, Loss= 0.0017, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.0014, Training Accuracy= 1.000\n",
      "Epoch: 580, Loss= 0.0012, Training Accuracy= 1.000\n",
      "Epoch: 590, Loss= 0.0011, Training Accuracy= 1.000\n",
      "Epoch: 600, Loss= 0.0009, Training Accuracy= 1.000\n",
      "Epoch: 610, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 620, Loss= 0.0008, Training Accuracy= 1.000\n",
      "Epoch: 630, Loss= 0.0007, Training Accuracy= 1.000\n",
      "Epoch: 640, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 650, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 660, Loss= 0.0006, Training Accuracy= 1.000\n",
      "Epoch: 670, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 680, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 690, Loss= 0.0005, Training Accuracy= 1.000\n",
      "Epoch: 700, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 710, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 720, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 730, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 740, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 750, Loss= 0.0004, Training Accuracy= 1.000\n",
      "Epoch: 760, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 770, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 780, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 790, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 800, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 810, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 820, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 830, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 840, Loss= 0.0003, Training Accuracy= 1.000\n",
      "Epoch: 850, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 860, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 870, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 880, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 890, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 900, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 910, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 920, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 930, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 940, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 950, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 960, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 970, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 980, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Epoch: 990, Loss= 0.0002, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.35\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [1.0, 0.50120002, 1.0, 1.0, 1.0, 0.49950001, 1.0, 1.0, 1.0, 1.0]\n",
      "mean of test_accuracies_10replications:  0.90007\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.00199860364199\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZoPixGFmZoPy/wGV1uUvNBHVTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff31b4b1350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
