{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 5\n",
    "N = 2\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Minibatch Loss= 1.0602, Training Accuracy= 0.502\n",
      "Epoch: 10, Minibatch Loss= 0.6946, Training Accuracy= 0.502\n",
      "Epoch: 20, Minibatch Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 30, Minibatch Loss= 0.6927, Training Accuracy= 0.504\n",
      "Epoch: 40, Minibatch Loss= 0.6922, Training Accuracy= 0.504\n",
      "Epoch: 50, Minibatch Loss= 0.6916, Training Accuracy= 0.504\n",
      "Epoch: 60, Minibatch Loss= 0.6909, Training Accuracy= 0.504\n",
      "Epoch: 70, Minibatch Loss= 0.6900, Training Accuracy= 0.504\n",
      "Epoch: 80, Minibatch Loss= 0.6891, Training Accuracy= 0.504\n",
      "Epoch: 90, Minibatch Loss= 0.6880, Training Accuracy= 0.752\n",
      "Epoch: 100, Minibatch Loss= 0.6866, Training Accuracy= 0.752\n",
      "Epoch: 110, Minibatch Loss= 0.6849, Training Accuracy= 0.752\n",
      "Epoch: 120, Minibatch Loss= 0.6827, Training Accuracy= 0.752\n",
      "Epoch: 130, Minibatch Loss= 0.6801, Training Accuracy= 0.752\n",
      "Epoch: 140, Minibatch Loss= 0.6767, Training Accuracy= 0.752\n",
      "Epoch: 150, Minibatch Loss= 0.6724, Training Accuracy= 0.752\n",
      "Epoch: 160, Minibatch Loss= 0.6667, Training Accuracy= 0.752\n",
      "Epoch: 170, Minibatch Loss= 0.6592, Training Accuracy= 0.752\n",
      "Epoch: 180, Minibatch Loss= 0.6491, Training Accuracy= 0.752\n",
      "Epoch: 190, Minibatch Loss= 0.6356, Training Accuracy= 0.752\n",
      "Epoch: 200, Minibatch Loss= 0.6175, Training Accuracy= 0.752\n",
      "Epoch: 210, Minibatch Loss= 0.5938, Training Accuracy= 0.752\n",
      "Epoch: 220, Minibatch Loss= 0.5637, Training Accuracy= 0.752\n",
      "Epoch: 230, Minibatch Loss= 0.5268, Training Accuracy= 0.752\n",
      "Epoch: 240, Minibatch Loss= 0.4827, Training Accuracy= 0.752\n",
      "Epoch: 250, Minibatch Loss= 0.4306, Training Accuracy= 0.752\n",
      "Epoch: 260, Minibatch Loss= 0.3702, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.3046, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.2412, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.1880, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.1477, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.1185, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0972, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0815, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0695, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0603, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0529, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0470, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0422, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0381, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0347, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0318, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0293, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0272, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0253, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0236, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0221, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0208, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0196, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0186, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 1: \n",
      "Epoch: 0, Minibatch Loss= 0.7199, Training Accuracy= 0.502\n",
      "Epoch: 10, Minibatch Loss= 0.6907, Training Accuracy= 0.502\n",
      "Epoch: 20, Minibatch Loss= 0.6896, Training Accuracy= 0.753\n",
      "Epoch: 30, Minibatch Loss= 0.6886, Training Accuracy= 0.753\n",
      "Epoch: 40, Minibatch Loss= 0.6875, Training Accuracy= 0.753\n",
      "Epoch: 50, Minibatch Loss= 0.6862, Training Accuracy= 0.753\n",
      "Epoch: 60, Minibatch Loss= 0.6847, Training Accuracy= 0.753\n",
      "Epoch: 70, Minibatch Loss= 0.6830, Training Accuracy= 0.753\n",
      "Epoch: 80, Minibatch Loss= 0.6808, Training Accuracy= 0.753\n",
      "Epoch: 90, Minibatch Loss= 0.6783, Training Accuracy= 0.753\n",
      "Epoch: 100, Minibatch Loss= 0.6751, Training Accuracy= 0.753\n",
      "Epoch: 110, Minibatch Loss= 0.6711, Training Accuracy= 0.753\n",
      "Epoch: 120, Minibatch Loss= 0.6661, Training Accuracy= 0.753\n",
      "Epoch: 130, Minibatch Loss= 0.6596, Training Accuracy= 0.753\n",
      "Epoch: 140, Minibatch Loss= 0.6513, Training Accuracy= 0.753\n",
      "Epoch: 150, Minibatch Loss= 0.6404, Training Accuracy= 0.753\n",
      "Epoch: 160, Minibatch Loss= 0.6260, Training Accuracy= 0.753\n",
      "Epoch: 170, Minibatch Loss= 0.6072, Training Accuracy= 0.753\n",
      "Epoch: 180, Minibatch Loss= 0.5828, Training Accuracy= 0.753\n",
      "Epoch: 190, Minibatch Loss= 0.5519, Training Accuracy= 0.753\n",
      "Epoch: 200, Minibatch Loss= 0.5142, Training Accuracy= 0.753\n",
      "Epoch: 210, Minibatch Loss= 0.4701, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.4202, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.3653, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.3069, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.2486, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.1954, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.1513, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.1174, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0925, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0743, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0610, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0512, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0437, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0379, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0333, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0297, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0266, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0242, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0221, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0203, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0187, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0174, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0162, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0152, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0143, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0135, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0128, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0121, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0115, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 2: \n",
      "Epoch: 0, Minibatch Loss= 0.7916, Training Accuracy= 0.501\n",
      "Epoch: 10, Minibatch Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 20, Minibatch Loss= 0.6926, Training Accuracy= 0.503\n",
      "Epoch: 30, Minibatch Loss= 0.6918, Training Accuracy= 0.503\n",
      "Epoch: 40, Minibatch Loss= 0.6909, Training Accuracy= 0.752\n",
      "Epoch: 50, Minibatch Loss= 0.6901, Training Accuracy= 0.752\n",
      "Epoch: 60, Minibatch Loss= 0.6890, Training Accuracy= 0.752\n",
      "Epoch: 70, Minibatch Loss= 0.6879, Training Accuracy= 0.752\n",
      "Epoch: 80, Minibatch Loss= 0.6866, Training Accuracy= 0.752\n",
      "Epoch: 90, Minibatch Loss= 0.6849, Training Accuracy= 0.752\n",
      "Epoch: 100, Minibatch Loss= 0.6830, Training Accuracy= 0.752\n",
      "Epoch: 110, Minibatch Loss= 0.6806, Training Accuracy= 0.752\n",
      "Epoch: 120, Minibatch Loss= 0.6776, Training Accuracy= 0.752\n",
      "Epoch: 130, Minibatch Loss= 0.6738, Training Accuracy= 0.752\n",
      "Epoch: 140, Minibatch Loss= 0.6691, Training Accuracy= 0.752\n",
      "Epoch: 150, Minibatch Loss= 0.6629, Training Accuracy= 0.752\n",
      "Epoch: 160, Minibatch Loss= 0.6548, Training Accuracy= 0.752\n",
      "Epoch: 170, Minibatch Loss= 0.6440, Training Accuracy= 0.752\n",
      "Epoch: 180, Minibatch Loss= 0.6291, Training Accuracy= 0.752\n",
      "Epoch: 190, Minibatch Loss= 0.6085, Training Accuracy= 0.752\n",
      "Epoch: 200, Minibatch Loss= 0.5795, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.5391, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.4846, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.4159, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.3382, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.2615, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.1961, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.1464, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.1112, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0868, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0697, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0575, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0485, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330, Minibatch Loss= 0.0416, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0363, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0321, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0287, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0259, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0235, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0215, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0198, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0184, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0171, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0160, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0141, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0133, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0119, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 3: \n",
      "Epoch: 0, Minibatch Loss= 0.7023, Training Accuracy= 0.255\n",
      "Epoch: 10, Minibatch Loss= 0.6957, Training Accuracy= 0.504\n",
      "Epoch: 20, Minibatch Loss= 0.6938, Training Accuracy= 0.504\n",
      "Epoch: 30, Minibatch Loss= 0.6923, Training Accuracy= 0.504\n",
      "Epoch: 40, Minibatch Loss= 0.6908, Training Accuracy= 0.504\n",
      "Epoch: 50, Minibatch Loss= 0.6891, Training Accuracy= 0.504\n",
      "Epoch: 60, Minibatch Loss= 0.6872, Training Accuracy= 0.504\n",
      "Epoch: 70, Minibatch Loss= 0.6846, Training Accuracy= 0.504\n",
      "Epoch: 80, Minibatch Loss= 0.6814, Training Accuracy= 0.751\n",
      "Epoch: 90, Minibatch Loss= 0.6770, Training Accuracy= 0.751\n",
      "Epoch: 100, Minibatch Loss= 0.6710, Training Accuracy= 0.751\n",
      "Epoch: 110, Minibatch Loss= 0.6625, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.6500, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.6311, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.6016, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.5550, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.4837, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.3873, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.2848, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.2020, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.1459, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.1098, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0860, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0697, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0581, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0494, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0429, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0377, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0336, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0302, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0274, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0250, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0230, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0213, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0198, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0184, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0173, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0162, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0153, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0145, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0137, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0119, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0109, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0104, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 4: \n",
      "Epoch: 0, Minibatch Loss= 0.7914, Training Accuracy= 0.503\n",
      "Epoch: 10, Minibatch Loss= 0.6953, Training Accuracy= 0.759\n",
      "Epoch: 20, Minibatch Loss= 0.6945, Training Accuracy= 0.759\n",
      "Epoch: 30, Minibatch Loss= 0.6940, Training Accuracy= 0.759\n",
      "Epoch: 40, Minibatch Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 50, Minibatch Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 60, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 70, Minibatch Loss= 0.6929, Training Accuracy= 0.503\n",
      "Epoch: 80, Minibatch Loss= 0.6927, Training Accuracy= 0.503\n",
      "Epoch: 90, Minibatch Loss= 0.6925, Training Accuracy= 0.503\n",
      "Epoch: 100, Minibatch Loss= 0.6923, Training Accuracy= 0.503\n",
      "Epoch: 110, Minibatch Loss= 0.6921, Training Accuracy= 0.503\n",
      "Epoch: 120, Minibatch Loss= 0.6918, Training Accuracy= 0.503\n",
      "Epoch: 130, Minibatch Loss= 0.6916, Training Accuracy= 0.503\n",
      "Epoch: 140, Minibatch Loss= 0.6913, Training Accuracy= 0.503\n",
      "Epoch: 150, Minibatch Loss= 0.6909, Training Accuracy= 0.503\n",
      "Epoch: 160, Minibatch Loss= 0.6905, Training Accuracy= 0.503\n",
      "Epoch: 170, Minibatch Loss= 0.6901, Training Accuracy= 0.759\n",
      "Epoch: 180, Minibatch Loss= 0.6895, Training Accuracy= 0.759\n",
      "Epoch: 190, Minibatch Loss= 0.6889, Training Accuracy= 0.759\n",
      "Epoch: 200, Minibatch Loss= 0.6881, Training Accuracy= 0.759\n",
      "Epoch: 210, Minibatch Loss= 0.6871, Training Accuracy= 0.508\n",
      "Epoch: 220, Minibatch Loss= 0.6860, Training Accuracy= 0.508\n",
      "Epoch: 230, Minibatch Loss= 0.6846, Training Accuracy= 0.508\n",
      "Epoch: 240, Minibatch Loss= 0.6828, Training Accuracy= 0.508\n",
      "Epoch: 250, Minibatch Loss= 0.6808, Training Accuracy= 0.508\n",
      "Epoch: 260, Minibatch Loss= 0.6782, Training Accuracy= 0.508\n",
      "Epoch: 270, Minibatch Loss= 0.6750, Training Accuracy= 0.508\n",
      "Epoch: 280, Minibatch Loss= 0.6710, Training Accuracy= 0.508\n",
      "Epoch: 290, Minibatch Loss= 0.6660, Training Accuracy= 0.508\n",
      "Epoch: 300, Minibatch Loss= 0.6594, Training Accuracy= 0.759\n",
      "Epoch: 310, Minibatch Loss= 0.6510, Training Accuracy= 0.759\n",
      "Epoch: 320, Minibatch Loss= 0.6399, Training Accuracy= 0.759\n",
      "Epoch: 330, Minibatch Loss= 0.6254, Training Accuracy= 0.759\n",
      "Epoch: 340, Minibatch Loss= 0.6064, Training Accuracy= 0.759\n",
      "Epoch: 350, Minibatch Loss= 0.5815, Training Accuracy= 0.759\n",
      "Epoch: 360, Minibatch Loss= 0.5492, Training Accuracy= 0.759\n",
      "Epoch: 370, Minibatch Loss= 0.5083, Training Accuracy= 0.759\n",
      "Epoch: 380, Minibatch Loss= 0.4580, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.3992, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.3348, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.2702, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.2120, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.1648, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.1292, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.1031, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0842, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0702, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0596, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0515, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 5: \n",
      "Epoch: 0, Minibatch Loss= 0.7154, Training Accuracy= 0.251\n",
      "Epoch: 10, Minibatch Loss= 0.6982, Training Accuracy= 0.251\n",
      "Epoch: 20, Minibatch Loss= 0.6923, Training Accuracy= 0.251\n",
      "Epoch: 30, Minibatch Loss= 0.6879, Training Accuracy= 0.494\n",
      "Epoch: 40, Minibatch Loss= 0.6842, Training Accuracy= 0.494\n",
      "Epoch: 50, Minibatch Loss= 0.6807, Training Accuracy= 0.494\n",
      "Epoch: 60, Minibatch Loss= 0.6767, Training Accuracy= 0.494\n",
      "Epoch: 70, Minibatch Loss= 0.6719, Training Accuracy= 0.756\n",
      "Epoch: 80, Minibatch Loss= 0.6659, Training Accuracy= 0.756\n",
      "Epoch: 90, Minibatch Loss= 0.6577, Training Accuracy= 0.756\n",
      "Epoch: 100, Minibatch Loss= 0.6466, Training Accuracy= 0.756\n",
      "Epoch: 110, Minibatch Loss= 0.6311, Training Accuracy= 0.756\n",
      "Epoch: 120, Minibatch Loss= 0.6096, Training Accuracy= 0.756\n",
      "Epoch: 130, Minibatch Loss= 0.5800, Training Accuracy= 0.756\n",
      "Epoch: 140, Minibatch Loss= 0.5403, Training Accuracy= 0.756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Minibatch Loss= 0.4894, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.4277, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.3574, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.2838, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.2153, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.1597, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.1192, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0912, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0720, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0586, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0489, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0416, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0361, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0318, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0283, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0255, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0231, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0211, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0194, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0180, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0156, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0146, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0137, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0122, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0110, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0105, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0088, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0085, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0081, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 6: \n",
      "Epoch: 0, Minibatch Loss= 0.8061, Training Accuracy= 0.497\n",
      "Epoch: 10, Minibatch Loss= 0.6940, Training Accuracy= 0.246\n",
      "Epoch: 20, Minibatch Loss= 0.6933, Training Accuracy= 0.501\n",
      "Epoch: 30, Minibatch Loss= 0.6929, Training Accuracy= 0.501\n",
      "Epoch: 40, Minibatch Loss= 0.6926, Training Accuracy= 0.501\n",
      "Epoch: 50, Minibatch Loss= 0.6922, Training Accuracy= 0.501\n",
      "Epoch: 60, Minibatch Loss= 0.6919, Training Accuracy= 0.501\n",
      "Epoch: 70, Minibatch Loss= 0.6915, Training Accuracy= 0.501\n",
      "Epoch: 80, Minibatch Loss= 0.6910, Training Accuracy= 0.501\n",
      "Epoch: 90, Minibatch Loss= 0.6905, Training Accuracy= 0.501\n",
      "Epoch: 100, Minibatch Loss= 0.6899, Training Accuracy= 0.501\n",
      "Epoch: 110, Minibatch Loss= 0.6891, Training Accuracy= 0.501\n",
      "Epoch: 120, Minibatch Loss= 0.6880, Training Accuracy= 0.750\n",
      "Epoch: 130, Minibatch Loss= 0.6868, Training Accuracy= 0.750\n",
      "Epoch: 140, Minibatch Loss= 0.6851, Training Accuracy= 0.750\n",
      "Epoch: 150, Minibatch Loss= 0.6829, Training Accuracy= 0.750\n",
      "Epoch: 160, Minibatch Loss= 0.6800, Training Accuracy= 0.750\n",
      "Epoch: 170, Minibatch Loss= 0.6761, Training Accuracy= 0.750\n",
      "Epoch: 180, Minibatch Loss= 0.6709, Training Accuracy= 0.750\n",
      "Epoch: 190, Minibatch Loss= 0.6635, Training Accuracy= 0.750\n",
      "Epoch: 200, Minibatch Loss= 0.6528, Training Accuracy= 0.750\n",
      "Epoch: 210, Minibatch Loss= 0.6366, Training Accuracy= 0.750\n",
      "Epoch: 220, Minibatch Loss= 0.6112, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.5709, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.5095, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.4261, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.3336, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.2520, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.1906, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.1475, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.1173, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0959, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0801, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0683, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0591, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0519, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0461, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0413, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0374, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0340, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0312, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0287, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0266, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0248, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0231, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0217, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0204, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0192, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0182, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0173, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 7: \n",
      "Epoch: 0, Minibatch Loss= 0.7302, Training Accuracy= 0.503\n",
      "Epoch: 10, Minibatch Loss= 0.6922, Training Accuracy= 0.251\n",
      "Epoch: 20, Minibatch Loss= 0.6900, Training Accuracy= 0.251\n",
      "Epoch: 30, Minibatch Loss= 0.6880, Training Accuracy= 0.251\n",
      "Epoch: 40, Minibatch Loss= 0.6861, Training Accuracy= 0.497\n",
      "Epoch: 50, Minibatch Loss= 0.6839, Training Accuracy= 0.497\n",
      "Epoch: 60, Minibatch Loss= 0.6814, Training Accuracy= 0.497\n",
      "Epoch: 70, Minibatch Loss= 0.6782, Training Accuracy= 0.497\n",
      "Epoch: 80, Minibatch Loss= 0.6741, Training Accuracy= 0.497\n",
      "Epoch: 90, Minibatch Loss= 0.6687, Training Accuracy= 0.497\n",
      "Epoch: 100, Minibatch Loss= 0.6615, Training Accuracy= 0.497\n",
      "Epoch: 110, Minibatch Loss= 0.6516, Training Accuracy= 0.497\n",
      "Epoch: 120, Minibatch Loss= 0.6377, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.6176, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.5880, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.5438, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.4797, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.3947, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.3004, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.2175, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.1573, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.1173, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0910, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0731, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0604, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0511, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0441, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0386, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0343, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0307, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0278, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0253, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0232, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0214, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0199, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0185, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0173, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0163, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0153, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0145, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0137, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0130, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0104, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0099, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 480, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 8: \n",
      "Epoch: 0, Minibatch Loss= 0.7089, Training Accuracy= 0.505\n",
      "Epoch: 10, Minibatch Loss= 0.6833, Training Accuracy= 0.506\n",
      "Epoch: 20, Minibatch Loss= 0.6797, Training Accuracy= 0.749\n",
      "Epoch: 30, Minibatch Loss= 0.6757, Training Accuracy= 0.749\n",
      "Epoch: 40, Minibatch Loss= 0.6706, Training Accuracy= 0.749\n",
      "Epoch: 50, Minibatch Loss= 0.6638, Training Accuracy= 1.000\n",
      "Epoch: 60, Minibatch Loss= 0.6542, Training Accuracy= 1.000\n",
      "Epoch: 70, Minibatch Loss= 0.6401, Training Accuracy= 1.000\n",
      "Epoch: 80, Minibatch Loss= 0.6186, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.5852, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.5345, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.4643, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.3815, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.2990, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.2281, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.1738, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.1347, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.1069, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0870, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0724, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0614, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0529, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0463, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0410, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0366, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0330, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0300, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0274, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0252, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0233, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0217, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0202, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0189, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0178, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0168, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0158, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0143, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0136, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0109, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0105, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0101, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0087, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 9: \n",
      "Epoch: 0, Minibatch Loss= 0.7171, Training Accuracy= 0.505\n",
      "Epoch: 10, Minibatch Loss= 0.6897, Training Accuracy= 0.505\n",
      "Epoch: 20, Minibatch Loss= 0.6889, Training Accuracy= 0.753\n",
      "Epoch: 30, Minibatch Loss= 0.6881, Training Accuracy= 0.753\n",
      "Epoch: 40, Minibatch Loss= 0.6870, Training Accuracy= 0.753\n",
      "Epoch: 50, Minibatch Loss= 0.6855, Training Accuracy= 0.498\n",
      "Epoch: 60, Minibatch Loss= 0.6837, Training Accuracy= 0.498\n",
      "Epoch: 70, Minibatch Loss= 0.6814, Training Accuracy= 0.498\n",
      "Epoch: 80, Minibatch Loss= 0.6783, Training Accuracy= 0.745\n",
      "Epoch: 90, Minibatch Loss= 0.6740, Training Accuracy= 0.745\n",
      "Epoch: 100, Minibatch Loss= 0.6680, Training Accuracy= 0.745\n",
      "Epoch: 110, Minibatch Loss= 0.6595, Training Accuracy= 0.745\n",
      "Epoch: 120, Minibatch Loss= 0.6469, Training Accuracy= 0.745\n",
      "Epoch: 130, Minibatch Loss= 0.6275, Training Accuracy= 0.745\n",
      "Epoch: 140, Minibatch Loss= 0.5969, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.5483, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.4751, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.3810, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.2858, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.2098, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.1566, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.1207, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0960, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0786, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0659, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0562, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0488, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0429, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0382, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0343, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0311, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0283, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0260, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0240, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0223, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0207, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0194, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0182, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0171, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0162, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0153, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0145, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0138, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0115, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0110, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0102, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "train_accuracies_1st_replication:  [0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.50160003, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.5036, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 0.7525, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "test_accuracies_1st_replication:  [0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.50019997, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.49610001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 0.74790001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "test_accuracies_10replications:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "mean of test_accuracies_10replications:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.005\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 500\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "minibatch_losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    \n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                minibatch_losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "mean of test_accuracies_10replications:  1.0\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.0\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEoCAYAAACpaN3LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XeYVOXZx/Hvbxu7LL2KKFLEgiWW\nVWxREQsYW4wmGluixhQ1MZLE3o0paoxRY4nGHturGLEiCMYoKqAixYYEBUFAOruw9X7/OGdhl5nd\nOVum7Oz9ua5zzZznnDlz72y59ynneWRmOOecc43JSXcAzjnnMp8nC+eccwl5snDOOZeQJwvnnHMJ\nebJwzjmXkCcL55xzCXmycM45l5AnC+eccwnlJTpB0v7A1cA24fkCzMwGJzc055xzmUKJ7uCW9DHw\na2A6UF1bbmbLkxuac865TJGwZgGsNrOXkh6Jc865jBWlZvFHIBd4BiivLTez95IbmnPOuUwRJVlM\nilNsZnZIckJyzjmXaRImC+eccy7h0FlJfSXdJ+mlcH+YpLOSH5pzzrlMEeU+iweAV4Atw/1PgQuS\nFZBzzrnMEyVZ9DKzJ4EaADOros4QWuecc9kvSrIoldQTMABJ+wCrkxqVc865jBLlPosLgeeAIZLe\nBHoDJyQ1Kueccxkl0mgoSXnA9gRTfXxiZpXJDsw551zmiDIaqiNwMXCBmc0CBko6KumROeecyxhR\n+izuByqAfcP9hcD1SYvIOedcxomSLIaY2Z+BSgAzW0/QHOWcq0PSwZIW1tmfLengJLzPS5LOaO3r\nOteYKMmiQlIRm0ZDDaHOHFHOJZuk8yRNk1Qu6YEmvG6+pEOTGFqjzGwnM5vckmtIulrSI5tdd7SZ\nPdii4Jxroiijoa4CXga2lvQosD/wo2QG5dxmFhE0fR4BFCXrTSTlhfcROec202jNQpKAj4HjCRLE\nY0BJS/9bcq4pzOwZM3sWiFlDRVIvSc9LWiVphaQ3JOVIehgYAIyTtE7S7+K89mBJCyVdJOlrgv45\nJB0l6YPwmm9J2rXOa+ZLukTSHEkrJd0vqTBe3HVrNpJyJV0q6XNJayVNl7R1eOxWSQskrQnLvx2W\njwIuBX4Qfg0zwvLJks4On+dIulzSF5KWSnpIUtfw2EBJJukMSV9K+kbSZc3/Trj2rNFkYcG42mfN\nbLmZvWBmz5vZNymKzbkoxhAMuugN9CX442pmdhrwJXC0mXUK+93i2QLoQbAS5DmS9gD+CfwU6Anc\nDTwnqUOd15xCUMsZAmwHXB4hzguBk4EjgS7AmUBZeGwqsFsYx7+ApyQVmtnLwA3AE+HX8K041/1R\nuI0ABgOdgNs3O+cAgqHvI4ErJe0YIV7n6onSZ/G2pL2SHolzzVMJ9AO2MbNKM3vDmjaVcg1wlZmV\nh4M3fgLcbWbvmFl12DdQDuxT5zW3m9kCM1sB/J4gCSRyNnC5mX1igRm1q02a2SPhP2RVZnYz0IHg\nj3sUpwB/MbN5ZrYOuAQ4Kbw3qtY1ZrbezGYAM4B4Sce5RkVJFiOAKWH1+UNJMyV9mOzAnIvoRmAu\nMF7SPEkXN/H1y8xsQ539bYAxYRPUKkmrgK3ZNJEmwII6z7/Y7FhDtgY+j3dA0hhJH0laHb5fV6BX\nxPi3DGOoG08eQS2r1td1npcR1D6ca5IoHdyjkx6Fc81kZmsJmqLGSNoJmCRpqplNJBzBl+gSm+0v\nAH5vZr9v5DVb13k+gKADPpEFBM1Ws+oWhv0TFxE0Ec02sxpJK9k0PD3R17CIIMHVjacKWAJsFSEu\n5yKJUrNYG2eL8svhXKuQlBd2IucCuZIKa5tZws7obcPBGGsIZkSunRV5CUE7flP8A/iZpOEKFEv6\njqTOdc45V9JWknoQ9JE8EeG69wLXSRoaXnfXcILOzgR/3JcBeZKuJOjTqLWEYNaEhn5XHwN+LWmQ\npE5s6uPwUV2uVUVJFu8R/CB/CnwWPv+fpPck7ZnM4JwLXQ6sJ5h25tTweW2n8lBgArAOmAL8vc5o\nvT8Al4fNSb+J8kZmNo2g3+J2YCVBE9ePNjvtX8B4YF64RZnR4C/Ak+Hr1gD3EQwDfgV4ieD36wtg\nA/WbuZ4KH5dLirfu/T+Bh4H/AP8LX39+hHica5Ioa3DfBYw1s1fC/cOBUQQ/+Lea2fCkR+lchpA0\nHzjbzCakOxbnUilKzaKkNlEAmNl44EAze5tg1IZzzrksFyVZrAhvWtom3H4HrJSUS7h6XjyS/hne\nJDSrgeOnhKOrPgxvfPLhfM45l6GiNEP1Ipjy44Cw6L/AtQSr5Q0ws7kNvO5Agnbkh8xs5zjH9wM+\nMrOVkkYDV3uTlnPOZaZIix81++LSQOD5eMlis/O6A7PMrH/SgnHOOddsUZqhUuEsghEhzjnnMlCU\nm/KSStIIgmRxQCPnnAOcA1BcXLznDjvskKLonHMuO0yfPv0bM+vd3NenNVmEs3neC4yunScnHjO7\nB7gHoKSkxKZNm5aiCJ1zLjtI+iLxWQ1LmCwk9Sa4SWlg3fPN7MyWvLGkAcAzwGlm9mlLruWccy65\notQs/g28QXCXbHWCczeS9BhwMNBLwVKTVwH5AGZ2F3AlwRTQfw9maqDKzEqaErxzzrnUiJIsOprZ\nRU29sJk1Om2zmZ1NMG2zc865DBdlNNTzko5MeiTOOecyVpRk8SuChLE+XPZxraQ1yQ7MOedc5kjY\nDGVmnROd45xzLrs1mCwk7WBmH4drEscws3jTJTvnnMtCjdUsLiS4Ee7mOMcMOCQpETnnnMs4DSYL\nMzsnfByRunCcc85lokyZG8o551wG82ThnHMuIU8WzjnnEkqYLCTtL6k4fH6qpL9I2ib5oTnnnMsU\nUWoWdwJl4bKnvwO+AB5KalTOOecySpRkUWXBcnrHArea2a2A36jnnHPtSJSJBNdKugQ4FThQUi7h\n7LHOOefahyg1ix8A5cBZZvY10B+4MalROeecyyiRahYEzU/VkrYDdgAeS25YzjnnMkmUmsV/gA6S\n+gMTgR8DDyQzKOecc5klSrKQmZUBxwO3mdl3gZ2SG5ZzzrlMEilZSNoXOAV4ISzLTV5IiZkZVTVV\n6QzBOefalSjJ4gLgEmCsmc2WNBiYlNywGvbe4vfIuTaH08eenq4QnHOu3Ymy+NHrwOuSOkvqZGbz\ngF8mP7QG4wGgoroiXSE451y7E2W6j10kvQ/MAuZImi4p7X0W5dXl6Q7BOefajSjNUHcDF5rZNmY2\nABgD/CO5YSVWXuXJwjnnUiVKsig2s419FGY2GShOWkQRec3COedSJ8pNefMkXQE8HO6fCvwveSFF\n4zUL55xLnSg1izOB3sAzwNjw+Y+TGVQUXrNwzrnUiTIaaiVpHP3UEK9ZOOdc6jSYLCSNA6yh42Z2\nTFIiishrFs45lzqN1SxuSlkUzeA1C9cWrFsHNTXpjsK5lmswWYQ34zWbpH8CRwFLzWznOMcF3Aoc\nCZQBPzKz96Je32sWLlOZwR//CA88AJ9+mu5onGsdUUZDNdcDwO00vATraGBouA0nWL51eNSL+x3c\nLlNddhn84Q/B846Ukkt1egNyjmCtiZZIWrIws/9IGtjIKccCD4VLtr4tqZukfma2OMr1vRnKZaLK\nSrj9dgDjGY7nuzyb7pCcA0AtfH2UobPJ0h9YUGd/YVgWQ9I5kqZJmlZbVl5dvnGeKOcyxcyZsHYt\n7MJMTxQuqySsWYSr4/0W2Kbu+WZ2SAvfO16ii/vX38zuAe4B0JbaeE5lTSUFuQUtDMO51rN+ffDY\nuU6lv5ocStM/6YFr91rWEBWlGeop4C6C+aBas/F1IbB1nf2tgEVNuUB5VbknC5dRqsJlVur2U7zJ\n/hzEf9IUkXO1WtYQFSVZVJnZnS16l/ieA86T9DhBx/bqqP0Vtcqry+lM5ySE5lzz1CaLPDYtzlUV\n/prl5ECxVzBcmqxtYQ93Yzfl9QifjpP0C4KpPjb2KpvZisYuLOkx4GCgl6SFwFVAfvjau4AXCYbN\nziUYOtvkKUS8k9tlmuqwQhEvWRx6KLzySjqicg7Uwh7uxmoW0wn6EGrf4rd1jhkwuLELm9nJCY4b\ncG6EGBvk91q4TNNYzSI3rYsRO9cyjd2UNwhAUqGZbah7TFJhsgOLwmsWLtPE67OoDpesz0vmXU3O\nJVmUobNvRSxLOb8xz2WaxmoWnixcW9ZYn8UWBPc9FEnanU3NUV2AjimILSFvhnKZxpOFy1aN/fge\nAfyIYEjrzWxKFmuAS5MbVjTeDOUyjScLl60a67N4EHhQ0vfM7OkUxhSZ1yxcpqkdDVW3z8I7uF02\niNJnsaekbrU7krpLuj6JMUXmNQuXaeLVLLyD22WDKMlitJmtqt0JV847MnkhRec1C5dpvBnKZaso\nySJXUofaHUlFQIdGzk8Zr1m4TOPJwmWrKD++jwATJd1PcDPemcCDSY0qog1VGxKf5FwKxbvPwpOF\nywYJf3zN7M+SZgIjCUZEXWdmGTFpwfqq9ekOwbl6vM/CZatIP75m9hLwUpJjabLSitJ0h+BcPY3N\nDeWjoVxblrDPQtI+kqZKWiepQlK1pDWpCC6RssqydIfgXD3eZ+GyVZQO7tuBk4HPgCLgbOC2ZAYV\nlScLl2k8WbhsFbUZaq6kXDOrBu6XlBFzQ3mycJnGJxJ02SrKj2+ZpALgA0l/BhZDZqwR6cnCZRqv\nWbhsFaUZ6rTwvPOAUoKlUL+XzKCiKqvyZOEyi69n4bJVlKGzX4Q1i4HAM8AnZpYRc4P7aCiXaRob\nDeU1C9eWJfzxlfQd4C7gc4L7LAZJ+mk4nDatvBnKZRrvs3DZKsqP783ACDObCyBpCPACGXDfhScL\nl2m8z8Jlqyh9FktrE0VoHrA0SfE0iScLl2k8Wbhs1dhKeceHT2dLehF4kmBuqBOBqSmILSFPFi7T\neAe3y1aN/a9zdJ3nS4CDwufLgO5Ji6gJPFm4TON9Fi5bNbZS3o9TGUhzeLJwmcaboVy2aqwZ6nfh\njLO3ETQ/1WNmv0xqZBGUVvrQWZdZfOisy1aN/fh+FD5OS0UgzVFRXUFVTRV5Of5b6DKD1yxctmqs\nGWpc+JgRCx01ZF3FOroVdkt8onMp4H0WLltFuSlvO+A3BHdwbzzfzA5JXljRrdqwypOFyxg+Gspl\nqyj/6zxFcAf3vVDn36UIJI0CbgVygXvN7I+bHR9AsERrt/Cci83sxaa8x6oNq5pyunNJ5c1QLltF\n+fGtMrM7m3phSbnAHcBhwEJgqqTnzGxOndMuB540szslDQNeJKjBRObJwmUSTxYuW0W5g3ucpF9I\n6iepR+0W4XV7A3PNbF448eDjwLGbnWNAl/B5V2BR5MhDnixcJqkdDeV9Fi7bRPnxPSN8/G2dMgMG\nJ3hdf2BBnf2FwPDNzrkaGC/pfII1Mg6NEE89nixcJvGahctWUaYoH9TMayve5TbbPxl4wMxulrQv\n8LCknc2spt6FpHOAcwDoV/8CnixcJvEObpetGrsp7xAze63OHFH1mNkzCa69kGChpFpbEdvMdBYw\nKrzeFEmFQC82m6jQzO4B7gHQlqqXcFauX5kgDOdSx2sWLls19uN7EPAa9eeIqmUECyE1ZiowVNIg\n4CvgJOCHm53zJTASeEDSjkAhwdxTkXnNwmUSv8/CZavGbsq7Knxs1hxRZlYl6TzgFYJhsf80s9mS\nrgWmmdlzwBjgH5J+TZCAfmRmMVOLNGZVuScLlzm8ZuGyVZSb8roBpxN7U17CuaHCeyZe3KzsyjrP\n5wD7Rw831or1K1ry8oxUVQVz5kB5ebojcU21bl3w6MnCZZsoP74vAm8DM4GaBOem3OK1i9MdQqsx\ng6uvhr/+FdasSXc0riU8WbhsE+XHt9DMLkx6JM301dqv0h1Cq7nxRrj22nRH4VpDvD4LHw3l2rIo\nyeJhST8Bngc2NoyYWUa0/yxZt4TK6kryc/PTHUqL3X9/8NiRUr7DC3TE1+toq7qzaZSe1yxcNojy\n41sB3Ahcxqb7JKLclJcUeTl5VNWp4hvG4nWLGdB1QDrCaTWlpfDxx8Hz1ziE4byb3oBcq6kiDwm6\n+XyXrg2LkiwuBLY1s2+SHUwU+bn59ZIFwMI1C9t8sqisDB47sdYTRRZZQXeW0Jc994TuGbEYsXPN\nEyVZzIbMaQ8pyC1gPevrlS1cszBN0bSeeEMuK8jnXzG3pri2opwOPMxp5BYXcfPN6Y7GuZaJkiyq\ngQ8kTaJ+n0VallUtyC2IKZuzbE6cM9uWeMliNV35MQ+Qlwe77ZamwFyz5efDPvvAbafB7runOxrn\nWiZKsng23DJCUX5RTNkHX3+QhkhaV2N3/vbtC1OnpiMq55wLRJlIMKOWVe2Y1zGmLJuShY/Pd85l\noijrWWSUovwiclQ/7C9Wf8HcFXPTFFHrqF0HwZOFcy4TtblkkaMcdt8itgH4oRkPpSGa1uNTWzvn\nMlmb/J/1hGEnMH3x9Hplv3/j98xaOot+nfrRqaATHfM7kpeTF7Pl5uTGLa93jiKc08h1OuR2IDen\naX/hfbZS51wma/KfIUk3AKuBe81seeuHlNhJO5/EZa9dRk2dNZJqrIaxH49NRzhx5SqXDnkd6JDb\ngcK8wo3PO+SF++Hz2sf1C7cD/hC3ZrGyYim3v/vkxmsV5RfRuaAznTt03vjYpUMXOhd0zoo72Z1z\nmac5/7O+CwwBbiGYjTblBnYbyLl7nctt796WjrePpNqqKasso6wy4i0qi/agoWSxuHQB5790fqTL\ndMjtsDGJdOnQpV5C2VhW0Jluhd3o1bFXzNapoBNSvEUOnXPtWZOThZllxDDaP4z8A1MXTeXthW+n\nO5TWURN2ZsdJFuRUxXtFXOXV5ZSXlfNNWfNuuC/ILdiYOLbotAX9O/cPti792bLzlhuf9ynuEzPQ\nwDmXvRpbVvU2YtfM3ihdN+XVKi4oZuLpE7ly0pXcOe3O6P/BZ6qacGbSOn0WG5OFquO9IikqqitY\ntHYRi9Yu4sMlHzZ4XkFuAQO7DWRI9yEM6T6Ewd0HM6THEIb2GMrQnkPJy/GOFueySWO/0dPCx/2B\nYcAT4f6JwPS4r0ixjvkduenwm7huxHW8//X7fPLNJ6ytWEtpRSlllWVUWzVVNVVU1wSP9TZroDzc\nal8bU57gNRXVFZRXlWMN59n44tQsaju4m1KzSJWK6go+Xf4pny7/NOZYQW4BO/bakZ377MwufXZh\nl767sGvfXenfub83cTnXRjW2rOqDAJJ+BIwws8pw/y5gfEqii6gov4j9tt6P/bbeL92hAGBmVNVU\nBU1CVeWUV5ezoWrDxuflVeF+nePT3+zKHx+I3wy1dfd+HF3yi42vKassY23FWtaWr2VtxVrWlK/Z\n+Lxup3+6VFRXMGPJDGYsmVGvvF+nfuyz1T7ss9U+DO8/nJItSyguKE5TlM65pojSVrAl0BmoXb+i\nU1jmGiCJ/Nx88nPz6VTQKdJruiyEPxI/WezQZyh3fOeOhNcwM9ZXrWdteZhA6iSUuollTfkaVq5f\nyTfrv+Gbsk3bstJllFcnby3XxesWM/bjsRtHreXn5LPf1vtx2ODDOHTwoZRsWdLkIcfOudSIkiz+\nCLwfTiQIcBBwddIiaqdaY7oPSXTM70jH/I707dS3yTGYGWWVZXxT9g1LS5eyaO0ivlr7FV+t+YpF\n6xbx1ZqvNu6vLl/d5OtvrrKmkte/eJ3Xv3idyyddTrfCbhwx5AhOGHYCo7cd7bUO5zJIlLmh7pf0\nEjA8LLrYzL5ObljtT+10H+lcjlMSxQXFFBcUs023bRo9d+X6lXy+8nPmrZzH5ys+5/OVwTZ76WyW\nlS1r1vuv2rCKJ2Y/wROzn6Aor4hR247ipJ1P4tjtj6VDXodmXdM51zoSJgsFPZKHAoPN7FpJAyTt\nbWa+Qk8ramsTCXYv6k5JUQklW5bEHFuybgmzls5i5tKZzFwykw+XfsiMr2dQWVMZ+frrq9ZvbLLq\nWdSTU3c9lbN2P4td+u7Sml+Gcy6iKH+G/g7UAIcA1wJrgaeBvZIYV7vT1pJFY/p26kvfTn0ZOXjk\nxrINVRv44OsPeHvh27y98G3e+PINFq1dFOl6y9cv59Z3buXWd25l3632Zcy+Yzhuh+O8f8O5FIry\nZ2i4me0h6X0AM1spKXYFItci2ZQs4inMK9w4EgqC/pGPv/mYV+e9yoR5E5g0fxLrKtYlvM6UhVM4\n4akTGNJ9CBfueyFn7n4mhXmFyQ7fuXYvyi24lZJyCW/Qk9SboKbhWlF7m0hQEjv23pFfDv8lz538\nHMt/t5yXT3mZs3c/m14deyV8/ecrP+fcF89l6G1Dufe9e6mqybx7UZzLJlGSxd+AsUAfSb8H/gvc\nkNSo2qH2PkV5QW4BR2x7BP845h8sHrOYCadN4JRdTklYa1i4ZiE/GfcTht0xjKfnPI1ZE2+GdM5F\nkjBZmNmjwO+APwCLgePM7KlkB9be+OJHm+Tl5DFy8EgeOf4RFl24iDuOvINv9f1Wo6/5bMVnnPDU\nCRzxyBF88s0nKYrUufaj0WQhKUfSLDP72MzuMLPbzeyjqBeXNErSJ5LmSrq4gXO+L2mOpNmS/tXU\nLyBbZHufRXN1L+rOL/b6Be//9H0mnDaBUduOavT8V+e9yi537sIlEy5hfeX6FEXpXPZrNFmYWQ0w\nQ9KApl447Oe4AxhNMLfUyZKGbXbOUOASYH8z2wm4oKnvky3aW59FU0li5OCRvHTKS8z42QyO2+G4\nBs+trKnkj2/+kT3u2YOpX01NYZTOZa8ofRb9gNmSJkp6rnaL8Lq9gblmNs/MKoDHgWM3O+cnwB1m\nthLAzJY2Jfhs4jWL6HbtuytjfzCWd85+h5GDRjZ43sfffMy+9+3LlZOupKK6IoUROpd9ovwZuqaZ\n1+4PLKizv5BNd4HX2g5A0ptALnC1mb3czPdr09p7B3dz7N1/byacPoFxn4zjly//kvmr5secU23V\nXPef63jl81d46sSnGNC1yZVk5xzROrhfj7dFuHa8uag3H6qSBwwFDgZOBu6V1C3mQtI5kqZJmrZs\nWfOmksh03sHdfEdvfzRzfjGHKw+8koLc+LcAvfvVu+xx9x68MveVFEfnXHZI5lJnC4Gt6+xvBWx+\ny+5C4N9mVmlm/wM+IUge9ZjZPWZWYmYlvXv3TlrA6eR9Fi1TlF/ENSOu4b1z3mOPfnvEPWf5+uWM\nfnQ010y+JiOmcneuLUlmspgKDJU0KLzj+yRg876OZ4ERAJJ6ETRLzUtiTBnL+yxax059duLts97m\nygOvJFex7XeGcfXrV/PDp3/IhqoNaYjQubYpacnCzKqA84BXgI+AJ81stqRrJR0TnvYKsFzSHGAS\n8FszW56smDKZ91m0nvzcfK4ZcQ1v/PgN+nfuH/ecJ2Y/wciHRrKsNDubNZ1rbQmThaT9Jb0q6VNJ\n8yT9T1Kk//7N7EUz287MhpjZ78OyK83sufC5mdmFZjbMzHYxs8db9uW0XV6zaH37br0v7//0fQ4d\nfGjc428teIt97tsn7tKwzrn6otQs7gP+AhxAMNNsCT7jbKvzPovk6F3cm5dPeZnLvn1Z3OPzVs7j\nwPsP5MMlH6Y4MufalijJYrWZvWRmS81see2W9MjaGR8NlTy5Oblcf8j1PPzdh8nPyY85vqR0CQc/\ncDDvfuVLtDjXkCjJYpKkGyXtK2mP2i3pkbUz3gyVfKfueioTTp9Aj6IeMcdWbljJyIdG8vr8KKPC\nnWt/oiSL4QRNTzcAN4fbTckMqj3yDu7UOHCbA5ly1pS4N+etq1jHkf86kje/fDMNkTmX2aLclDci\nznZIKoJrT+L1WXjNIjm267kdb/z4DYb2iLmlh7LKMo7815FMWzQtDZE5l7kaTBaSTg0fL4y3pS7E\n9iFezcI7uJNnQNcB/OfH/2HnPjvHHFtTvoYjHjmCmUtmpiEy5zJTYzWL4vCxcwOba0XewZ16W3Ta\ngslnTGb3LXaPObZi/QoOe/gw5q6Ym4bInMs8Df4ZMrO7w8fmTiTomsA7uNOjZ8eejD9tPAc9cBBz\nls2pd2xJ6RJGPzqaKWdNibTUq3PZLKv/DK1cCWVl6Y4imrVrg0fv4E69Xh17MeG0CRz4wIExNYm5\nK+ZyzGPHMPH0iRTlF6UpQufSr80li08+gREjGj9n2TJYsADWrElNTK3Jb8pLj36d+zHx9Il8+/5v\n8+XqL+sdm7JwCqeOPZUnT3iS3BzP3K59anN/htatg8mT0x1F8ngzVPoM6DqAl095mf3+uR+rNqyq\nd+yZj57hogkXcdPhPmrctU8J/wxJugH4s5mtCve7A2PM7PJkBxdPPxZzNtdGOvdDduXfHEv8pTUy\nkyeL9Nqx9478+6R/c9jDh8WsrnfzlJvZbYvdOHXXU9MUnXPpI7PN1yPa7ATpfTPbfbOy98wsLXdx\nl0jWlBHwS+nNHIYlPjFD7MJMerICgKN5juc5msmT4aCD0htXe/PYzMf44TM/jCkvzCvkzTPfbHDN\nDOcylaTpZlbS3NdH+Z81V1IHMysP37AI6NDcN0y1PiyjD21zCocKCigqguGbL0brku7kXU7mi9Vf\ncMnES+qVb6jawHGPH8e0c6bRp7hPmqJzLvWiJItHgImS7idYFvVM4MGkRtWIxfTjWs5u9Jw8qjiL\n++jL0hRF1foWswX/5QCuuQYKC9MdTft00f4XMWPJDB6fVX/m/AVrFvD9p77PhNMnkJfjbYSufUjY\nDAUgaRRwKEHj/3gzS9tCxlKJQeKGqFyq2JPpFLGeHt0hJ5lrArYyUw5WshfHn1LEqd48nlZllWXs\nd99+zFgyI+bYZd++jOsPuT4NUTnXdC1thorSZzEIWGxmG8L9IqCvmc1v7pu2xPbbl9hdd0Xrtdhu\nO+gff6E05yKbv2o+JfeUsHx9/Zn5hRh/2vgGF1dyLpOkIllMA/Yzs4pwvwB408zSsgBSSUmJTZvm\nk7y51Jo4byKHP3I4NVZTr7xvcV9m/GwGfTv1TVNkzkXT0mQRpXEmrzZRAITPC5r7hs61RSMHj+Sq\ng66KKV9SuoTTnz09Jok4l22iJItlko6p3ZF0LPBN8kJyLjNd9u3LGDEwdvqA8Z+P5y9T/pKGiJxL\nnSjJ4mfApZK+lLQAuAj4aXIlVSttAAAY/ElEQVTDci7z5Obk8sjxj9C7Y++YY5e9dhmzl85OQ1TO\npUaUxY8+N7N9gGHAMDPbz8x83mbXLm3ZeUse+u5DMeUV1RWc/uzpVFZXpiEq55Iv0oBSSd8BfgH8\nWtKVkq5MbljOZa5R247iguEXxJS/t/g9bnjjhjRE5FzyJUwWku4CfgCcT3CfxYnANkmOy7mMdsPI\nG9ih1w4x5de/cT3TF01PQ0TOJVeUmsV+ZnY6sDJcCGlfYOvkhuVcZivKL+LB4x4kV/WnLK+qqeKM\nZ8+ImYTQubYuSrJYHz6WSdoSqAQGJS8k59qGvfvvzSUHXBJTPnvZbG5888Y0RORc8kRJFs9L6gbc\nCLwHzAceS2ZQzrUVVxx0Rdw1vK/7z3V8uvzTNETkXHJEGQ11nZmtMrOnCfoqdjCzSB3ckkZJ+kTS\nXEkXN3LeCZJMUrPvLnQuHQpyC/jnsf+MaY4qry7nZ8//jChzrznXFjRpej0zKzez1VHOlZQL3AGM\nJhh2e7KkmIUlJHUGfgm805RYnMsUu22xGxfue2FM+aT5k3hwRtomaHauVSVzLta9gblmNi+cIuRx\n4Ng4510H/BnYkMRYnEuqqw66ikHdYrvyxowfw7LSZWmIyLnWlcxk0R9YUGd/YVi2kaTdga3N7Pkk\nxuFc0hUXFHPnd+6MKV+xfgWXTrw0DRE517qi3GcxMUpZvJfGKdvYgCspB7gFGBMhhnMkTZM0bdky\n/y/NZaYjtj2CH+4SuxTrfe/fx7RFPlOya9saTBaSCiX1AHpJ6i6pR7gNBLaMcO2F1L8fYytgUZ39\nzsDOwGRJ84F9gOfidXKb2T1mVmJmJb17x87L41ymuOWIW+jaoWu9MsM478XzfGZa16Y1VrP4KTAd\n2CF8rN3+TdBxnchUYKikQeEaGCcBz9UeNLPVZtbLzAaa2UDgbeAYM/N/wVyb1ae4D9ccfE1M+Ttf\nvcNDM2LnlHKurWgwWZjZrWY2CPiNmQ02s0Hh9i0zuz3Rhc2sCjgPeAX4CHjSzGZLurbulOfOZZtf\n7PULduq9U0z5RRMuYvWGSIMJncs4UTq4vw6HtyLpcknPSNojysXN7EUz287MhpjZ78OyK83suTjn\nHuy1CpcN8nPzuW30bTHlS0uXcvXkq1MfkHOtIEqyuMLM1ko6ADgCeBCIHfbhnNtoxKARnDjsxJjy\n2969jTnL5qQhIudaJkqyqA4fvwPcaWb/xpdVdS6hmw6/iaK8onpl1VbNr1/5td/Z7dqcKMniK0l3\nA98HXpTUIeLrnGvXBnQdwKXfjr3HYvzn43nhsxfSEJFzzRflj/73CTqpR5nZKqAH8NukRuVclvjN\nfr9hYLeBMeUXvnKhT2Pu2pQoEwmWAUuBA8KiKuCzZAblXLYozCvkpsNuiin/bMVn3P5uwkGFzmWM\nKHdwXwVcBNRO3J8PPJLMoJzLJsfveDwHbXNQTPk1r1/D0tKlaYjIuaaL0gz1XeAYoBTAzBYR3H3t\nnItAEn8d9Ve02Qw4a8rXcMVrV6QpKueaJkqyqLBg6IYBSCpObkjOZZ/dttiNn+zxk5jye9+/lxlf\nz0hDRM41TZRk8WQ4GqqbpJ8AE4B7kxuWc9nnukOuo0uHLvXKaqyGC165wIfSuowXpYP7JuD/gKeB\n7YErzexvyQ7MuWzTp7gPVx4Yu8jk5PmTGfvx2DRE5Fx0UTq4/2Rmr5rZb83sN2b2qqQ/pSI457LN\n+cPPZ2iPoTHlY8aPYUOVr//lMleUZqjD4pSNbu1AnGsPCnILuPnwm2PK56+azy1TbklDRM5F09h6\nFj+XNBPYXtKHdbb/AR+mLkTnsstR2x3F4UMOjyn//Ru/Z/HaxWmIyLnEGqtZ/As4mmANiqPrbHua\n2akpiM25rCSJW464hVzl1isvrSzl0td8CVaXmRpbz2K1mc03s5PN7Is624pUBuhcNhrWexg/L/l5\nTPkDHzzA1K+mpiEi5xrnEwI6lyZXH3w13Qu7x5T7UFqXiTxZOJcmPTv25NoR18aUv7XgLR6f9Xga\nInKuYZ4snEujn5X8jGG9h8WU/27C7yirLEtDRM7F1+RkIWmCpJckHZWMgJxrT/Jy8rjliNghswvX\nLOTGN29MQ0TOxdecmsXpwOXANq0ci3Pt0uFDDueo7WL/9/rTm39iweoFaYjIuViRkoWkIknbQzDr\nrJlNN7M7khuac+3HzYffTH5Ofr2y9VXruWjCRWmKyLn6okz3cTTwAfByuL+bpOeSHZhz7cl2Pbfj\nl8N/GVP+2KzHeGvBW2mIyLn6otQsrgb2BlYBmNkHwMDkheRc+3TFgVfQu2PvmPJfvfwraqwmDRE5\nt0mUZFFlZquTHolz7VzXwq5cf8j1MeXTFk3joRkPpSEi5zaJkixmSfohkCtpqKTbAK8XO5cEZ+1+\nFt/q+62Y8jHjx/D1uq/TEJFzgSjJ4nxgJ6AceAxYA1yQzKCca69yc3K5ddStMeUr1q/g5y/83O/s\ndmkTZfGjMjO7zMz2MrOS8LlPvO9ckhw08CBO2vmkmPJnP36WJ2Y/kYaInIs2GmqSpNc231IRnHPt\n1d9G/S1uZ/d5L57nzVEuLaI0Q/0G+G24XUEwjHZalItLGiXpE0lzJV0c5/iFkuaE62RMlOQ3+jkH\n9C7uzR1Hxt7KtHz9cs549gwfHeVSLkoz1PQ625tmdiEwPNHrJOUCdxCsqjcMOFnS5pPgvA+UmNmu\nBOt8/7nJX4FzWerEnU7khGEnxJSP/3w8N78Vu9qec8kUpRmqR52tl6QjgC0iXHtvYK6ZzTOzCuBx\n4Ni6J5jZJDOrnS3tbWCrJsbvXFa748g76FPcJ6b80tcu9XUvXEpFaYaaTtDsNB2YAowBzorwuv5A\n3YltFoZlDTkLeCneAUnnSJomadqyZcsivLVz2aFPcR8ePO7BmPKqmiq+/3/f55uyb9IQlWuPojRD\nDTKzweHjUDM73Mz+G+Haine5uCdKpwIlQNxpNs3snnAkVknv3rGdfs5ls1HbjmLMvmNiyuevms/3\nn/o+ldWVaYjKtTd5DR2QdHxjLzSzZxJceyGwdZ39rYBFcd7nUOAy4CAzK09wTefapRtG3sDk+ZOZ\nvnh6vfJJ8ycxZvwY/jb6b2mKzLUXDSYL4OhGjhmQKFlMBYZKGgR8BZwE/LDuCZJ2B+4GRpnZ0sTh\nOtc+FeQW8MQJT7DXP/Zi5YaV9Y7d9u5t7NhrR36+V+ya3s61lgaThZn9uCUXNrMqSecBrwC5wD/N\nbLaka4FpZvYcQbNTJ+ApSQBfmtkxLXlf57LVkB5DeOKEJxj16KiYobPnvnguvTr24sSdTkxTdC7b\nKdH0AZJ6AlcBBxDUKP4LXGtmy5MfXqySkhKbNi3SbR7OZaW/vv1Xfv3Kr2PK83PyefGUFzl08KFp\niMplOknTzaykua+PMhrqcWAZ8D3ghPC5zzngXJr8aviv+PFusRX/yppKjnv8OCbPn5z6oFzWi5Is\nepjZdWb2v3C7HuiW7MCcc/FJ4u6j7uY7Q78Tc6y0spTRj47m5bkvpyEyl82iJItJkk6SlBNu3wde\nSHZgzrmG5efm8+SJT3LAgANijm2o2sAxjx3D03OeTkNkLls1mCwkrZW0Bvgp8C+CKcrLCZqlYhtM\nnXMp1TG/I+NOHhd3/YvKmkpOfOpE/vTfP/m05q5VNJgszKyzmXUJH3PMLD/ccsysSyqDdM7F162w\nG6+d8Rp7bblXzDHDuHjixZz53JlsqPJVBVzLRGmGcs5lsB5FPZhw+oS4TVIAD3zwAPvdtx+fLv80\nxZG5bOLJwrks0KVDF14+5WWO2u6ouMff//p99rxnTx6a8ZA3S7lm8WThXJYoLijm2R88G3ceKYB1\nFes449kzOPqxo1mwekHcc5xrSKRkISlX0paSBtRuyQ7MOdd0uTm53HT4Tfzj6H9QkFsQ95wXPnuB\nnf6+E7dMuYWK6ooUR+jaqijrWZwPLAFeJRgy+wLwfJLjcs61wNl7nM2Us6awbY9t4x5fW7GWC8df\nyE5/34lnP37Wm6ZcQlFqFr8Ctjezncxsl3DbNdmBOedaZo9+ezD9nOmcssspDZ4zd8VcvvvEdzng\n/gN4ee7LnjRcg6IkiwXA6mQH4pxrfV06dOGR4x9h7A/GskWnhhe4fGvBW4x+dDR737s3z3z0DFU1\nVSmM0rUFUSYSvA/YnqD5aeN6E2b2l+SGFp9PJOhc86xcv5KLJlzEve/di8Vfh2yjAV0H8LM9f8bZ\ne5xN72JfcCwbpGIiwS8J+isKgM51NudcG9K9qDv3HH0P7//0fQ4ZdEij5365+ksufe1StrplK45/\n4nie+egZyqt8bbL2LGHNItN4zcK5ljMzxn06jsteu4xZS2dFek23wm58b8fvcdwOxzFy0EiK8ouS\nHKVrTS2tWTSYLCT91cwukDSOOGtnp2uRIk8WzrWeGqth3CfjuP6N65m2KPrvVVFeEYcNOYyjtzua\n0duOpn+X/kmM0rWGZCaLPc1suqSD4h03s9eb+6Yt4cnCudZnZkyYN4Hbp97OuE/GJezT2NzQHkMZ\nMXAEIwaN4OCBBzfame7SI2nJIlN5snAuueavms9d0+7igQ8eYEnpkmZdY1C3Qezdf2+G9x/O3v33\nZvd+u9Mxv2MrR+qawpOFcy4pqmqqeO1/r/HIh4/wzEfPUFpZ2uxr5SqXHXvvyC59dgm2vsHjgK4D\nkNSKUbuGeLJwziVdaUUpL819iXGfjuOFT19g+frlrXLdrh26skOvHdi2x7YM7TGUoT2HMrTHULbt\nsS3di7q3ynu4gCcL51xKVddUM2XhFMZ9Mo5X573KB19/0OQ+jih6FvVkSI8hDOg6gK27bF3/sevW\n9CnuQ458LtSokp4sJL0KnGhmq8L97sDjZnZEc9+0JTxZOJdZVqxfwRtfvMGk+ZOYPH8yM5fOpMZq\nkv6+BbkF9O/cn76d+rJFpy3oW7zpcfOy4oLipMeT6VKRLN43s90TlaWKJwvnMtu6inVMXzSdd796\nl3e+eod3v3qXBWvSOyV6x/yO9CjqQY+iHvQs6rnxebz9HkU96FrYlS4dutC5oDO5Oblpjb21tDRZ\n5EU4p0bSADP7MnzDbYhz34VzzgF0KujEQQMP4qCBm0bdLytdxsylM5m5ZGbwuHQms5bOoqyyLCUx\nlVWWUVZZxsI1C5v82uL8Yrp06BJpK84vpmN+Rzrmd6S4IHi+eVlhXmGbbD6LkiwuA/4rqfa+igOB\nc5IXknMu2/Qu7s0hgw6pN81IjdXw5eov+Wz5Z3y24jM+W/4Zc1fO5bPlnzFv5TwqayrTGPEmpZWl\nlFaWsnjd4la75sbkESe5FOUVUZhXSIe8DhTmFm56nldIh9wO9fabUtZSkTq4JfUC9gEETDGzb1r8\nzs3kzVDOZb+qmioWrF7AF6u/4MvVX7Jg9YLgcU3w+OXqL1lbsTbdYbYtV5PcZihJ3wVeM7Pnw/1u\nko4zs2eb+6bOOdeYvJw8BnUfxKDugxo8Z9WGVXy97mu+Xvc1S9YtCR5LlwTPS4Oy2v1MqaW0ZVGa\noa4ys7G1O2a2StJVQMJkIWkUcCuQC9xrZn/c7HgH4CFgT2A58AMzmx89fOdce9WtsBvdCruxQ68d\nGj3PzFhTvoYV61ds3JavX77pedlyVmxYUe/4mvI1rClfw7qKdSn6ajJflGQRrycmSo0kF7gDOAxY\nCEyV9JyZzalz2lnASjPbVtJJwJ+AH0SIyTnnIpFE18KudC3s2mhNJZ7qmmrWVazbmDzWVqzd+Dze\nVlZZRmllafBYUbqxY722rKyyjA1VG5L0lSZXlGQxTdJfCP7wG3A+MD3C6/YG5prZPABJjwPHAnWT\nxbHA1eHz/wNulyRra3cKOueyUm5O7sZE01qqa6pZX7U+bjIprShlQ9UGyqvL2VC1IXheVV6vrHZ/\nQ3Ujx+KUraVlfTxRksX5wBXAEwQd3OOBcyO8rj/Bkqy1FgLDGzrHzKokrQZ6AmnrQHfOuWTKzcml\nU0EnOhV0Sun76tKWzcGVMFmYWSlwcTOuHS+yzWsMUc5B0jlsGq5bLinaai3ZrxeeWGv5Z7GJfxab\n+GexyfYteXGUvofewO+AnYCNg3XNrPF1GYOaxNZ19rcCFjVwzkJJeUBXYMXmFzKze4B7wnimtWT4\nVzbxz2IT/yw28c9iE/8sNpHUonsOotxG+CjwMTAIuAaYD0yN8LqpwFBJgyQVACcBz212znPAGeHz\nEwiG6Hp/hXPOZZgoyaKnmd0HVJrZ62Z2JsENeo0ysyrgPOAV4CPgSTObLelaSbVLst4H9JQ0F7iQ\n5jV3OeecS7IoHdy1d7MslvQdgqakraJc3MxeBF7crOzKOs83ACdGC3Wje5p4fjbzz2IT/yw28c9i\nE/8sNmnRZxFl1tmjgDcI+hZuA7oA15jZ5k1KzjnnslSbW/zIOedc6rWpeXIljZL0iaS5krK+f0PS\nPyUtrTtUWFIPSa9K+ix87B6WS9Lfws/mQ0l7pC/y1iVpa0mTJH0kabakX4Xl7fGzKJT0rqQZ4Wdx\nTVg+SNI74WfxRDioBEkdwv254fGB6Yw/GSTlSnpfUu38de3ys5A0X9JMSR/Ujnxqzd+RNpMs6kwf\nMhoYBpwsaVh6o0q6B4BRm5VdDEw0s6HARDYNChgNDA23c4A7UxRjKlQBY8xsR4LBFeeG3/v2+FmU\nA4eY2beA3YBRkvYhmCrnlvCzWEkwlQ7UmVIHuCU8L9v8imAQTa32/FmMMLPd6gwXbr3fETNrExuw\nL/BKnf1LgEvSHVcKvu6BwKw6+58A/cLn/YBPwud3AyfHOy/bNuDfBHOOtevPAugIvEcwM8I3QF5Y\nvvF3hWA04r7h87zwPKU79lb8DLYK/wgeAjxPcKNve/0s5gO9Nitrtd+RKDfldQC+F/7R2ni+mV2b\n6LWtLMr0Ie1BXzNbDGBmiyX1CcvjfT79gdZbsSUDhE0HuwPv0E4/i7CWPR3YlqC2/TmwyoLh6rDp\n64Xsn1LnrwQ3DXcO93vSfj8LA8ZLMuBuC25mbrXfkShDZ/8NrCb44SxvevytJtLUIO1Y1n8+kjoB\nTwMXmNkaqcG5brL6szCzamA3Sd2AscCO8U4LH7P2swhHai41s+mSDq4tjnNq1n8Wof3NbFGYEF6V\n9HEj5zb5s4iSLLYys83bzdMhyvQh7cESSf3C/xL6AUvD8qz+fCTlEySKR83smbC4XX4WtSxYW2Yy\nQT9ON0l54X/Udb/eSFPqtFH7A8dIOpJgKqIuBDWN9vhZYGaLwselksYSzPzdar8jUTq435K0S9ND\nb3VRpg9pD+pOkXIGQc2vtvz0cJTDPsDq2upnW6egCnEf8JGZ/aXOofb4WfQOaxRIKgIOJejcnUQw\nZQ7EfhZZOaWOmV1iZluZ2UCCvwevmdkptMPPQlKxpM61z4HDgVm05u9IhE6TOUAFQQfIh8BM4MM0\ndeAcCXxK0EZ7Wbo7lFLw9T5G0IZYSfCfwFkEbawTgc/Cxx7huWJT+/VMoCTd8bfi53AAQRX5Q+CD\ncDuynX4WuwLvh5/FLODKsHww8C4wF3gK6BCWF4b7c8Pjg9P9NSTpczkYeL69fhbh1zwj3GbX/n1s\nzd+RKHdwbxOv3My+aPSFzjnnskaDfRaSupjZGmjh8krOOefavAZrFpKeN7OjJP2PoAmgbu+5mdng\nVATonHMu/XxuKOeccwlFGTpLOJ/IUOqvlPefZAXlnHMus0S5g/tsgrlXtiIYhbIPMIXg9nrnnHPt\nQJT7LH4F7AV8YWYjCKZaWJbUqJzLUpIOrp0d1bm2JEqy2GDBinZI6mBmHwPbJzcs55xzmSRKslgY\n3jH6LMF8I/8mC6dOcK4uSaeG60Z8IOnucM2EdZJulvSepImSeofn7ibp7XBdgLF11gzYVtIEBWtP\nvCdpSHj5TpL+T9LHkh5VI5NcOZcpEiYLM/uuma0ys6uBKwimXTgu2YE5ly6SdgR+QDAx225ANXAK\nUAy8Z2Z7AK8DV4UveQi4yMx2Jbgbtrb8UeAOC9ae2I9NM3ruDlxAsC7LYII5jpzLaI12cEvKIZja\nY2cAM3s9JVE5l14jgT2BqeE//UUEE7DVAE+E5zwCPCOpK9Ctzu/Gg8BT4Tw9/c1sLECdplyAd81s\nYbj/AcH0//9N/pflXPM1WrMwsxpghqQBKYrHuUwg4EELVhzbzcy2D2vWm2vsJqXGmpbqTvVfTcQh\n7M6lU5Q+i37A7LCN9rnaLdmBOZdGE4ETaheKCdcx3obg96V2NtMfAv81s9XASknfDstPA14Pp8pZ\nKOm48BodJHVM6VfhXCuK8h/NNUmPwrkMYmZzJF1OsOpYDsGsv+cCpcBOkqYTLAj2g/AlZwB3hclg\nHvDjsPw04G5J14bXODGFX4ZzrSrKrLN/MrOLEpU5l+0krTOzTumOw7l0iNIMdVicstGtHYhzzrnM\n1dgU5T8HfgEMlvRhnUOdgTeTHZhzmcZrFa49a2yK8q5Ad+APwMV1Dq01s6xZt9Y551xiPkW5c865\nhKL0WTjnnGvnPFk455xLyJOFc865hDxZOOecS8iThXPOuYT+H/+gjYzmvVNIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcd3273aa50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minibatch_losses_1st_replication\n",
    "plt.plot(minibatch_losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, minibacth loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
