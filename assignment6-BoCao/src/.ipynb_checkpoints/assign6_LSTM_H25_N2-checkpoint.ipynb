{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 25\n",
    "N = 2\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Minibatch Loss= 0.7830, Training Accuracy= 0.495\n",
      "Epoch: 10, Minibatch Loss= 0.6961, Training Accuracy= 0.500\n",
      "Epoch: 20, Minibatch Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 30, Minibatch Loss= 0.6926, Training Accuracy= 0.500\n",
      "Epoch: 40, Minibatch Loss= 0.6917, Training Accuracy= 0.251\n",
      "Epoch: 50, Minibatch Loss= 0.6910, Training Accuracy= 0.251\n",
      "Epoch: 60, Minibatch Loss= 0.6903, Training Accuracy= 0.504\n",
      "Epoch: 70, Minibatch Loss= 0.6897, Training Accuracy= 0.504\n",
      "Epoch: 80, Minibatch Loss= 0.6891, Training Accuracy= 0.504\n",
      "Epoch: 90, Minibatch Loss= 0.6884, Training Accuracy= 0.504\n",
      "Epoch: 100, Minibatch Loss= 0.6877, Training Accuracy= 0.753\n",
      "Epoch: 110, Minibatch Loss= 0.6869, Training Accuracy= 0.753\n",
      "Epoch: 120, Minibatch Loss= 0.6860, Training Accuracy= 0.753\n",
      "Epoch: 130, Minibatch Loss= 0.6850, Training Accuracy= 0.753\n",
      "Epoch: 140, Minibatch Loss= 0.6839, Training Accuracy= 0.753\n",
      "Epoch: 150, Minibatch Loss= 0.6826, Training Accuracy= 0.753\n",
      "Epoch: 160, Minibatch Loss= 0.6811, Training Accuracy= 0.753\n",
      "Epoch: 170, Minibatch Loss= 0.6794, Training Accuracy= 0.753\n",
      "Epoch: 180, Minibatch Loss= 0.6773, Training Accuracy= 0.753\n",
      "Epoch: 190, Minibatch Loss= 0.6747, Training Accuracy= 0.753\n",
      "Epoch: 200, Minibatch Loss= 0.6717, Training Accuracy= 0.753\n",
      "Epoch: 210, Minibatch Loss= 0.6679, Training Accuracy= 0.753\n",
      "Epoch: 220, Minibatch Loss= 0.6634, Training Accuracy= 0.753\n",
      "Epoch: 230, Minibatch Loss= 0.6579, Training Accuracy= 0.753\n",
      "Epoch: 240, Minibatch Loss= 0.6512, Training Accuracy= 0.753\n",
      "Epoch: 250, Minibatch Loss= 0.6432, Training Accuracy= 0.753\n",
      "Epoch: 260, Minibatch Loss= 0.6335, Training Accuracy= 0.753\n",
      "Epoch: 270, Minibatch Loss= 0.6216, Training Accuracy= 0.753\n",
      "Epoch: 280, Minibatch Loss= 0.6071, Training Accuracy= 0.753\n",
      "Epoch: 290, Minibatch Loss= 0.5893, Training Accuracy= 0.753\n",
      "Epoch: 300, Minibatch Loss= 0.5678, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.5420, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.5119, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.4776, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.4396, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.3990, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.3569, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.3146, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.2740, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.2364, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.2030, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.1744, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.1505, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.1307, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.1144, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.1010, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0898, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0805, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0727, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0660, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 1: \n",
      "Epoch: 0, Minibatch Loss= 0.7166, Training Accuracy= 0.501\n",
      "Epoch: 10, Minibatch Loss= 0.6970, Training Accuracy= 0.247\n",
      "Epoch: 20, Minibatch Loss= 0.6949, Training Accuracy= 0.247\n",
      "Epoch: 30, Minibatch Loss= 0.6937, Training Accuracy= 0.247\n",
      "Epoch: 40, Minibatch Loss= 0.6928, Training Accuracy= 0.501\n",
      "Epoch: 50, Minibatch Loss= 0.6919, Training Accuracy= 0.501\n",
      "Epoch: 60, Minibatch Loss= 0.6911, Training Accuracy= 0.752\n",
      "Epoch: 70, Minibatch Loss= 0.6902, Training Accuracy= 0.752\n",
      "Epoch: 80, Minibatch Loss= 0.6892, Training Accuracy= 0.752\n",
      "Epoch: 90, Minibatch Loss= 0.6882, Training Accuracy= 0.752\n",
      "Epoch: 100, Minibatch Loss= 0.6871, Training Accuracy= 0.752\n",
      "Epoch: 110, Minibatch Loss= 0.6859, Training Accuracy= 0.752\n",
      "Epoch: 120, Minibatch Loss= 0.6844, Training Accuracy= 0.752\n",
      "Epoch: 130, Minibatch Loss= 0.6828, Training Accuracy= 0.752\n",
      "Epoch: 140, Minibatch Loss= 0.6810, Training Accuracy= 0.752\n",
      "Epoch: 150, Minibatch Loss= 0.6789, Training Accuracy= 0.752\n",
      "Epoch: 160, Minibatch Loss= 0.6764, Training Accuracy= 0.752\n",
      "Epoch: 170, Minibatch Loss= 0.6735, Training Accuracy= 0.752\n",
      "Epoch: 180, Minibatch Loss= 0.6701, Training Accuracy= 0.752\n",
      "Epoch: 190, Minibatch Loss= 0.6662, Training Accuracy= 0.752\n",
      "Epoch: 200, Minibatch Loss= 0.6614, Training Accuracy= 0.752\n",
      "Epoch: 210, Minibatch Loss= 0.6558, Training Accuracy= 0.752\n",
      "Epoch: 220, Minibatch Loss= 0.6490, Training Accuracy= 0.752\n",
      "Epoch: 230, Minibatch Loss= 0.6409, Training Accuracy= 0.752\n",
      "Epoch: 240, Minibatch Loss= 0.6309, Training Accuracy= 0.752\n",
      "Epoch: 250, Minibatch Loss= 0.6187, Training Accuracy= 0.752\n",
      "Epoch: 260, Minibatch Loss= 0.6037, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.5852, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.5626, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.5350, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.5022, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.4642, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.4220, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.3771, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.3319, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.2884, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.2487, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.2137, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.1838, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.1587, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.1378, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.1205, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.1061, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0942, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0841, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0756, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0684, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0623, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0569, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0523, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 2: \n",
      "Epoch: 0, Minibatch Loss= 0.6966, Training Accuracy= 0.501\n",
      "Epoch: 10, Minibatch Loss= 0.6919, Training Accuracy= 0.749\n",
      "Epoch: 20, Minibatch Loss= 0.6909, Training Accuracy= 0.501\n",
      "Epoch: 30, Minibatch Loss= 0.6900, Training Accuracy= 0.501\n",
      "Epoch: 40, Minibatch Loss= 0.6890, Training Accuracy= 0.501\n",
      "Epoch: 50, Minibatch Loss= 0.6880, Training Accuracy= 0.501\n",
      "Epoch: 60, Minibatch Loss= 0.6869, Training Accuracy= 0.501\n",
      "Epoch: 70, Minibatch Loss= 0.6856, Training Accuracy= 0.501\n",
      "Epoch: 80, Minibatch Loss= 0.6841, Training Accuracy= 0.752\n",
      "Epoch: 90, Minibatch Loss= 0.6823, Training Accuracy= 0.752\n",
      "Epoch: 100, Minibatch Loss= 0.6803, Training Accuracy= 0.752\n",
      "Epoch: 110, Minibatch Loss= 0.6780, Training Accuracy= 0.752\n",
      "Epoch: 120, Minibatch Loss= 0.6752, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.6719, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.6679, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.6632, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.6575, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.6505, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.6420, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.6316, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.6189, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.6031, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.5837, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.5596, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.5299, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.4939, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.4511, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.4023, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.3496, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.2965, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.2468, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.2035, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.1677, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330, Minibatch Loss= 0.1392, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.1167, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0991, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0853, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0742, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0653, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0580, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0520, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0470, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0428, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0392, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0360, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0333, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0310, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0289, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0270, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0254, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 3: \n",
      "Epoch: 0, Minibatch Loss= 0.6927, Training Accuracy= 0.500\n",
      "Epoch: 10, Minibatch Loss= 0.6916, Training Accuracy= 0.500\n",
      "Epoch: 20, Minibatch Loss= 0.6911, Training Accuracy= 0.500\n",
      "Epoch: 30, Minibatch Loss= 0.6906, Training Accuracy= 0.500\n",
      "Epoch: 40, Minibatch Loss= 0.6901, Training Accuracy= 0.749\n",
      "Epoch: 50, Minibatch Loss= 0.6895, Training Accuracy= 0.749\n",
      "Epoch: 60, Minibatch Loss= 0.6890, Training Accuracy= 0.749\n",
      "Epoch: 70, Minibatch Loss= 0.6884, Training Accuracy= 0.749\n",
      "Epoch: 80, Minibatch Loss= 0.6878, Training Accuracy= 0.749\n",
      "Epoch: 90, Minibatch Loss= 0.6871, Training Accuracy= 0.749\n",
      "Epoch: 100, Minibatch Loss= 0.6864, Training Accuracy= 0.749\n",
      "Epoch: 110, Minibatch Loss= 0.6856, Training Accuracy= 0.749\n",
      "Epoch: 120, Minibatch Loss= 0.6847, Training Accuracy= 0.749\n",
      "Epoch: 130, Minibatch Loss= 0.6838, Training Accuracy= 0.749\n",
      "Epoch: 140, Minibatch Loss= 0.6827, Training Accuracy= 0.749\n",
      "Epoch: 150, Minibatch Loss= 0.6815, Training Accuracy= 0.749\n",
      "Epoch: 160, Minibatch Loss= 0.6801, Training Accuracy= 0.749\n",
      "Epoch: 170, Minibatch Loss= 0.6786, Training Accuracy= 0.749\n",
      "Epoch: 180, Minibatch Loss= 0.6768, Training Accuracy= 0.749\n",
      "Epoch: 190, Minibatch Loss= 0.6748, Training Accuracy= 0.749\n",
      "Epoch: 200, Minibatch Loss= 0.6724, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.6697, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.6665, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.6627, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.6582, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.6527, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.6460, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.6378, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.6276, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.6149, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.5991, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.5793, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.5546, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.5239, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.4867, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.4426, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.3928, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.3400, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.2880, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.2402, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.1990, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.1652, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.1380, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.1166, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0996, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0861, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0753, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0665, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0592, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0532, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 4: \n",
      "Epoch: 0, Minibatch Loss= 0.8798, Training Accuracy= 0.504\n",
      "Epoch: 10, Minibatch Loss= 0.6990, Training Accuracy= 0.503\n",
      "Epoch: 20, Minibatch Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 30, Minibatch Loss= 0.6912, Training Accuracy= 0.503\n",
      "Epoch: 40, Minibatch Loss= 0.6895, Training Accuracy= 0.503\n",
      "Epoch: 50, Minibatch Loss= 0.6882, Training Accuracy= 0.503\n",
      "Epoch: 60, Minibatch Loss= 0.6868, Training Accuracy= 0.503\n",
      "Epoch: 70, Minibatch Loss= 0.6854, Training Accuracy= 0.758\n",
      "Epoch: 80, Minibatch Loss= 0.6837, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.6817, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.6795, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.6768, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.6737, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.6700, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.6655, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.6601, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.6536, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.6455, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.6355, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.6230, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.6072, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.5872, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.5617, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.5298, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.4905, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.4439, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.3915, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.3365, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.2832, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.2352, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.1946, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.1616, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.1354, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.1147, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0984, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0854, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0749, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0664, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0593, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0535, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0485, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0443, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0407, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0376, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0349, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0325, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0304, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0285, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0268, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0253, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 5: \n",
      "Epoch: 0, Minibatch Loss= 0.7057, Training Accuracy= 0.252\n",
      "Epoch: 10, Minibatch Loss= 0.6952, Training Accuracy= 0.500\n",
      "Epoch: 20, Minibatch Loss= 0.6938, Training Accuracy= 0.758\n",
      "Epoch: 30, Minibatch Loss= 0.6931, Training Accuracy= 0.758\n",
      "Epoch: 40, Minibatch Loss= 0.6925, Training Accuracy= 0.758\n",
      "Epoch: 50, Minibatch Loss= 0.6922, Training Accuracy= 0.758\n",
      "Epoch: 60, Minibatch Loss= 0.6918, Training Accuracy= 0.758\n",
      "Epoch: 70, Minibatch Loss= 0.6914, Training Accuracy= 0.758\n",
      "Epoch: 80, Minibatch Loss= 0.6911, Training Accuracy= 0.758\n",
      "Epoch: 90, Minibatch Loss= 0.6907, Training Accuracy= 0.758\n",
      "Epoch: 100, Minibatch Loss= 0.6903, Training Accuracy= 0.758\n",
      "Epoch: 110, Minibatch Loss= 0.6899, Training Accuracy= 0.758\n",
      "Epoch: 120, Minibatch Loss= 0.6895, Training Accuracy= 0.758\n",
      "Epoch: 130, Minibatch Loss= 0.6890, Training Accuracy= 0.758\n",
      "Epoch: 140, Minibatch Loss= 0.6884, Training Accuracy= 0.758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Minibatch Loss= 0.6878, Training Accuracy= 0.758\n",
      "Epoch: 160, Minibatch Loss= 0.6872, Training Accuracy= 0.758\n",
      "Epoch: 170, Minibatch Loss= 0.6865, Training Accuracy= 0.758\n",
      "Epoch: 180, Minibatch Loss= 0.6857, Training Accuracy= 0.758\n",
      "Epoch: 190, Minibatch Loss= 0.6849, Training Accuracy= 0.758\n",
      "Epoch: 200, Minibatch Loss= 0.6839, Training Accuracy= 0.758\n",
      "Epoch: 210, Minibatch Loss= 0.6828, Training Accuracy= 0.758\n",
      "Epoch: 220, Minibatch Loss= 0.6816, Training Accuracy= 0.758\n",
      "Epoch: 230, Minibatch Loss= 0.6802, Training Accuracy= 0.758\n",
      "Epoch: 240, Minibatch Loss= 0.6787, Training Accuracy= 0.758\n",
      "Epoch: 250, Minibatch Loss= 0.6769, Training Accuracy= 0.758\n",
      "Epoch: 260, Minibatch Loss= 0.6749, Training Accuracy= 0.758\n",
      "Epoch: 270, Minibatch Loss= 0.6726, Training Accuracy= 0.758\n",
      "Epoch: 280, Minibatch Loss= 0.6699, Training Accuracy= 0.758\n",
      "Epoch: 290, Minibatch Loss= 0.6668, Training Accuracy= 0.758\n",
      "Epoch: 300, Minibatch Loss= 0.6633, Training Accuracy= 0.758\n",
      "Epoch: 310, Minibatch Loss= 0.6591, Training Accuracy= 0.758\n",
      "Epoch: 320, Minibatch Loss= 0.6543, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.6485, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.6416, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.6334, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.6234, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.6112, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.5963, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.5779, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.5552, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.5272, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.4934, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.4534, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.4082, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.3598, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.3114, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.2659, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.2255, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.1911, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 6: \n",
      "Epoch: 0, Minibatch Loss= 0.7393, Training Accuracy= 0.507\n",
      "Epoch: 10, Minibatch Loss= 0.6909, Training Accuracy= 0.258\n",
      "Epoch: 20, Minibatch Loss= 0.6898, Training Accuracy= 0.258\n",
      "Epoch: 30, Minibatch Loss= 0.6888, Training Accuracy= 0.258\n",
      "Epoch: 40, Minibatch Loss= 0.6878, Training Accuracy= 0.258\n",
      "Epoch: 50, Minibatch Loss= 0.6867, Training Accuracy= 0.258\n",
      "Epoch: 60, Minibatch Loss= 0.6854, Training Accuracy= 0.258\n",
      "Epoch: 70, Minibatch Loss= 0.6838, Training Accuracy= 0.258\n",
      "Epoch: 80, Minibatch Loss= 0.6818, Training Accuracy= 0.502\n",
      "Epoch: 90, Minibatch Loss= 0.6794, Training Accuracy= 0.502\n",
      "Epoch: 100, Minibatch Loss= 0.6764, Training Accuracy= 0.502\n",
      "Epoch: 110, Minibatch Loss= 0.6725, Training Accuracy= 0.752\n",
      "Epoch: 120, Minibatch Loss= 0.6675, Training Accuracy= 0.752\n",
      "Epoch: 130, Minibatch Loss= 0.6610, Training Accuracy= 0.752\n",
      "Epoch: 140, Minibatch Loss= 0.6526, Training Accuracy= 0.752\n",
      "Epoch: 150, Minibatch Loss= 0.6415, Training Accuracy= 0.752\n",
      "Epoch: 160, Minibatch Loss= 0.6271, Training Accuracy= 0.752\n",
      "Epoch: 170, Minibatch Loss= 0.6083, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.5838, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.5524, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.5133, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.4666, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.4138, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.3577, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.3023, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.2513, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.2073, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.1712, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.1426, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.1202, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.1027, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0890, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0780, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0692, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0620, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0560, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0510, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0467, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0431, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0399, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0372, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0347, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0326, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0307, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0290, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0274, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0260, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0248, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0236, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0225, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 7: \n",
      "Epoch: 0, Minibatch Loss= 0.6982, Training Accuracy= 0.503\n",
      "Epoch: 10, Minibatch Loss= 0.6937, Training Accuracy= 0.247\n",
      "Epoch: 20, Minibatch Loss= 0.6928, Training Accuracy= 0.749\n",
      "Epoch: 30, Minibatch Loss= 0.6922, Training Accuracy= 0.501\n",
      "Epoch: 40, Minibatch Loss= 0.6916, Training Accuracy= 0.501\n",
      "Epoch: 50, Minibatch Loss= 0.6909, Training Accuracy= 0.501\n",
      "Epoch: 60, Minibatch Loss= 0.6903, Training Accuracy= 0.501\n",
      "Epoch: 70, Minibatch Loss= 0.6897, Training Accuracy= 0.501\n",
      "Epoch: 80, Minibatch Loss= 0.6890, Training Accuracy= 0.501\n",
      "Epoch: 90, Minibatch Loss= 0.6882, Training Accuracy= 0.501\n",
      "Epoch: 100, Minibatch Loss= 0.6874, Training Accuracy= 0.501\n",
      "Epoch: 110, Minibatch Loss= 0.6864, Training Accuracy= 0.501\n",
      "Epoch: 120, Minibatch Loss= 0.6853, Training Accuracy= 0.753\n",
      "Epoch: 130, Minibatch Loss= 0.6841, Training Accuracy= 0.753\n",
      "Epoch: 140, Minibatch Loss= 0.6827, Training Accuracy= 0.753\n",
      "Epoch: 150, Minibatch Loss= 0.6811, Training Accuracy= 0.753\n",
      "Epoch: 160, Minibatch Loss= 0.6793, Training Accuracy= 0.753\n",
      "Epoch: 170, Minibatch Loss= 0.6772, Training Accuracy= 0.753\n",
      "Epoch: 180, Minibatch Loss= 0.6747, Training Accuracy= 0.753\n",
      "Epoch: 190, Minibatch Loss= 0.6718, Training Accuracy= 0.753\n",
      "Epoch: 200, Minibatch Loss= 0.6684, Training Accuracy= 0.753\n",
      "Epoch: 210, Minibatch Loss= 0.6644, Training Accuracy= 0.753\n",
      "Epoch: 220, Minibatch Loss= 0.6595, Training Accuracy= 0.753\n",
      "Epoch: 230, Minibatch Loss= 0.6537, Training Accuracy= 0.753\n",
      "Epoch: 240, Minibatch Loss= 0.6468, Training Accuracy= 0.753\n",
      "Epoch: 250, Minibatch Loss= 0.6387, Training Accuracy= 0.753\n",
      "Epoch: 260, Minibatch Loss= 0.6290, Training Accuracy= 0.753\n",
      "Epoch: 270, Minibatch Loss= 0.6177, Training Accuracy= 0.753\n",
      "Epoch: 280, Minibatch Loss= 0.6043, Training Accuracy= 0.753\n",
      "Epoch: 290, Minibatch Loss= 0.5886, Training Accuracy= 0.753\n",
      "Epoch: 300, Minibatch Loss= 0.5703, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.5489, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.5241, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.4960, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.4646, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.4302, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.3935, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.3555, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.3172, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.2799, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.2446, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.2125, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.1839, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.1593, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.1383, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.1207, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.1060, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0938, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 480, Minibatch Loss= 0.0835, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0749, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 8: \n",
      "Epoch: 0, Minibatch Loss= 0.6987, Training Accuracy= 0.493\n",
      "Epoch: 10, Minibatch Loss= 0.6951, Training Accuracy= 0.493\n",
      "Epoch: 20, Minibatch Loss= 0.6933, Training Accuracy= 0.255\n",
      "Epoch: 30, Minibatch Loss= 0.6924, Training Accuracy= 0.255\n",
      "Epoch: 40, Minibatch Loss= 0.6918, Training Accuracy= 0.255\n",
      "Epoch: 50, Minibatch Loss= 0.6914, Training Accuracy= 0.511\n",
      "Epoch: 60, Minibatch Loss= 0.6910, Training Accuracy= 0.511\n",
      "Epoch: 70, Minibatch Loss= 0.6907, Training Accuracy= 0.511\n",
      "Epoch: 80, Minibatch Loss= 0.6903, Training Accuracy= 0.511\n",
      "Epoch: 90, Minibatch Loss= 0.6899, Training Accuracy= 0.511\n",
      "Epoch: 100, Minibatch Loss= 0.6895, Training Accuracy= 0.511\n",
      "Epoch: 110, Minibatch Loss= 0.6890, Training Accuracy= 0.511\n",
      "Epoch: 120, Minibatch Loss= 0.6885, Training Accuracy= 0.511\n",
      "Epoch: 130, Minibatch Loss= 0.6880, Training Accuracy= 0.511\n",
      "Epoch: 140, Minibatch Loss= 0.6874, Training Accuracy= 0.511\n",
      "Epoch: 150, Minibatch Loss= 0.6866, Training Accuracy= 0.511\n",
      "Epoch: 160, Minibatch Loss= 0.6858, Training Accuracy= 0.511\n",
      "Epoch: 170, Minibatch Loss= 0.6849, Training Accuracy= 0.511\n",
      "Epoch: 180, Minibatch Loss= 0.6839, Training Accuracy= 0.762\n",
      "Epoch: 190, Minibatch Loss= 0.6826, Training Accuracy= 0.762\n",
      "Epoch: 200, Minibatch Loss= 0.6812, Training Accuracy= 0.762\n",
      "Epoch: 210, Minibatch Loss= 0.6794, Training Accuracy= 0.762\n",
      "Epoch: 220, Minibatch Loss= 0.6774, Training Accuracy= 0.762\n",
      "Epoch: 230, Minibatch Loss= 0.6749, Training Accuracy= 0.762\n",
      "Epoch: 240, Minibatch Loss= 0.6720, Training Accuracy= 0.762\n",
      "Epoch: 250, Minibatch Loss= 0.6685, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.6643, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.6592, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.6531, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.6457, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.6369, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.6260, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.6128, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.5967, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.5770, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.5532, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.5248, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.4915, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.4536, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.4117, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.3677, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.3234, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.2812, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.2427, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.2091, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.1803, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.1562, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.1362, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.1196, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.1058, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 9: \n",
      "Epoch: 0, Minibatch Loss= 0.6947, Training Accuracy= 0.505\n",
      "Epoch: 10, Minibatch Loss= 0.6913, Training Accuracy= 0.756\n",
      "Epoch: 20, Minibatch Loss= 0.6908, Training Accuracy= 0.505\n",
      "Epoch: 30, Minibatch Loss= 0.6904, Training Accuracy= 0.505\n",
      "Epoch: 40, Minibatch Loss= 0.6899, Training Accuracy= 0.505\n",
      "Epoch: 50, Minibatch Loss= 0.6894, Training Accuracy= 0.505\n",
      "Epoch: 60, Minibatch Loss= 0.6888, Training Accuracy= 0.505\n",
      "Epoch: 70, Minibatch Loss= 0.6882, Training Accuracy= 0.505\n",
      "Epoch: 80, Minibatch Loss= 0.6875, Training Accuracy= 0.505\n",
      "Epoch: 90, Minibatch Loss= 0.6867, Training Accuracy= 0.505\n",
      "Epoch: 100, Minibatch Loss= 0.6859, Training Accuracy= 0.505\n",
      "Epoch: 110, Minibatch Loss= 0.6849, Training Accuracy= 0.749\n",
      "Epoch: 120, Minibatch Loss= 0.6838, Training Accuracy= 0.749\n",
      "Epoch: 130, Minibatch Loss= 0.6826, Training Accuracy= 0.749\n",
      "Epoch: 140, Minibatch Loss= 0.6812, Training Accuracy= 0.749\n",
      "Epoch: 150, Minibatch Loss= 0.6797, Training Accuracy= 0.749\n",
      "Epoch: 160, Minibatch Loss= 0.6779, Training Accuracy= 0.749\n",
      "Epoch: 170, Minibatch Loss= 0.6759, Training Accuracy= 0.749\n",
      "Epoch: 180, Minibatch Loss= 0.6736, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.6709, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.6679, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.6644, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.6604, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.6557, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.6501, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.6436, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.6359, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.6268, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.6157, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.6025, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.5866, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.5674, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.5443, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.5168, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.4842, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.4465, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.4042, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.3588, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.3123, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.2674, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.2263, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.1905, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.1604, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.1357, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.1156, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0993, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0862, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0754, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0666, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0594, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.0025\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 500\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "mean of test_accuracies_10replications:  1.0\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.0\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEoCAYAAACpaN3LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc07ea0d6d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
