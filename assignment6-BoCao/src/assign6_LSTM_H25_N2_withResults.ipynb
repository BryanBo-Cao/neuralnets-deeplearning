{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 25\n",
    "N = 2\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.7377, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 0.6944, Training Accuracy= 0.504\n",
      "Epoch: 20, Loss= 0.6928, Training Accuracy= 0.504\n",
      "Epoch: 30, Loss= 0.6916, Training Accuracy= 0.504\n",
      "Epoch: 40, Loss= 0.6906, Training Accuracy= 0.504\n",
      "Epoch: 50, Loss= 0.6895, Training Accuracy= 0.504\n",
      "Epoch: 60, Loss= 0.6883, Training Accuracy= 0.504\n",
      "Epoch: 70, Loss= 0.6869, Training Accuracy= 0.754\n",
      "Epoch: 80, Loss= 0.6853, Training Accuracy= 0.754\n",
      "Epoch: 90, Loss= 0.6835, Training Accuracy= 0.754\n",
      "Epoch: 100, Loss= 0.6815, Training Accuracy= 0.754\n",
      "Epoch: 110, Loss= 0.6791, Training Accuracy= 0.754\n",
      "Epoch: 120, Loss= 0.6762, Training Accuracy= 0.754\n",
      "Epoch: 130, Loss= 0.6730, Training Accuracy= 0.754\n",
      "Epoch: 140, Loss= 0.6691, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.6646, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.6593, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.6530, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.6455, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.6366, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.6258, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.6128, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.5969, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.5777, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.5546, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.5269, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.4943, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.4571, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.4158, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.3722, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.3282, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.2860, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.2474, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.2133, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.1841, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.1595, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.1389, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.1218, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.1076, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0957, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0857, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0772, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0699, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0637, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0584, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0537, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0496, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0460, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0429, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0400, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7238, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.6926, Training Accuracy= 0.757\n",
      "Epoch: 20, Loss= 0.6915, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6908, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6901, Training Accuracy= 0.757\n",
      "Epoch: 50, Loss= 0.6894, Training Accuracy= 0.757\n",
      "Epoch: 60, Loss= 0.6888, Training Accuracy= 0.757\n",
      "Epoch: 70, Loss= 0.6881, Training Accuracy= 0.757\n",
      "Epoch: 80, Loss= 0.6873, Training Accuracy= 0.757\n",
      "Epoch: 90, Loss= 0.6864, Training Accuracy= 0.757\n",
      "Epoch: 100, Loss= 0.6855, Training Accuracy= 0.757\n",
      "Epoch: 110, Loss= 0.6844, Training Accuracy= 0.757\n",
      "Epoch: 120, Loss= 0.6832, Training Accuracy= 0.757\n",
      "Epoch: 130, Loss= 0.6819, Training Accuracy= 0.757\n",
      "Epoch: 140, Loss= 0.6803, Training Accuracy= 0.757\n",
      "Epoch: 150, Loss= 0.6786, Training Accuracy= 0.757\n",
      "Epoch: 160, Loss= 0.6765, Training Accuracy= 0.757\n",
      "Epoch: 170, Loss= 0.6741, Training Accuracy= 0.757\n",
      "Epoch: 180, Loss= 0.6714, Training Accuracy= 0.757\n",
      "Epoch: 190, Loss= 0.6681, Training Accuracy= 0.757\n",
      "Epoch: 200, Loss= 0.6643, Training Accuracy= 0.757\n",
      "Epoch: 210, Loss= 0.6599, Training Accuracy= 0.757\n",
      "Epoch: 220, Loss= 0.6546, Training Accuracy= 0.757\n",
      "Epoch: 230, Loss= 0.6484, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.6410, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.6320, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.6211, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.6078, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.5914, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.5710, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.5458, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.5147, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.4772, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.4332, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.3842, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.3328, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.2828, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.2374, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.1984, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.1663, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.1404, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.1196, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.1031, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0897, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0789, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0700, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0627, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0565, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0513, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0469, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.6990, Training Accuracy= 0.750\n",
      "Epoch: 10, Loss= 0.6951, Training Accuracy= 0.249\n",
      "Epoch: 20, Loss= 0.6935, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6923, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6913, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.6903, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.6892, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.6880, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.6866, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.6851, Training Accuracy= 0.497\n",
      "Epoch: 100, Loss= 0.6833, Training Accuracy= 0.497\n",
      "Epoch: 110, Loss= 0.6812, Training Accuracy= 0.497\n",
      "Epoch: 120, Loss= 0.6787, Training Accuracy= 0.497\n",
      "Epoch: 130, Loss= 0.6758, Training Accuracy= 0.497\n",
      "Epoch: 140, Loss= 0.6724, Training Accuracy= 0.497\n",
      "Epoch: 150, Loss= 0.6682, Training Accuracy= 0.497\n",
      "Epoch: 160, Loss= 0.6634, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.6576, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.6507, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.6424, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.6324, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.6204, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.6057, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.5878, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.5659, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.5393, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.5073, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.4698, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.4272, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.3808, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.3328, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.2859, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.2426, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.2045, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.1723, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.1458, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.1242, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.1068, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0927, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0812, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0718, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0640, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0575, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0520, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0473, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0433, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0398, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0368, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0342, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0319, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.8439, Training Accuracy= 0.494\n",
      "Epoch: 10, Loss= 0.6946, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6910, Training Accuracy= 0.752\n",
      "Epoch: 30, Loss= 0.6888, Training Accuracy= 0.752\n",
      "Epoch: 40, Loss= 0.6869, Training Accuracy= 0.752\n",
      "Epoch: 50, Loss= 0.6848, Training Accuracy= 0.752\n",
      "Epoch: 60, Loss= 0.6823, Training Accuracy= 0.752\n",
      "Epoch: 70, Loss= 0.6793, Training Accuracy= 0.752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80, Loss= 0.6755, Training Accuracy= 0.752\n",
      "Epoch: 90, Loss= 0.6710, Training Accuracy= 0.752\n",
      "Epoch: 100, Loss= 0.6654, Training Accuracy= 0.752\n",
      "Epoch: 110, Loss= 0.6584, Training Accuracy= 0.752\n",
      "Epoch: 120, Loss= 0.6494, Training Accuracy= 1.000\n",
      "Epoch: 130, Loss= 0.6380, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.6234, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.6045, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.5805, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.5503, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.5134, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.4700, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.4215, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.3701, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.3190, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.2712, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.2288, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.1928, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.1632, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.1391, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.1197, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.1040, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0912, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0807, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0720, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0647, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0586, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0534, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0489, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0450, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0416, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0386, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0360, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0336, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0315, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0297, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0280, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0265, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0251, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0238, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0227, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0216, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.6995, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 0.6933, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6926, Training Accuracy= 0.500\n",
      "Epoch: 30, Loss= 0.6921, Training Accuracy= 0.756\n",
      "Epoch: 40, Loss= 0.6916, Training Accuracy= 0.508\n",
      "Epoch: 50, Loss= 0.6911, Training Accuracy= 0.508\n",
      "Epoch: 60, Loss= 0.6906, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.6900, Training Accuracy= 0.508\n",
      "Epoch: 80, Loss= 0.6894, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.6888, Training Accuracy= 0.508\n",
      "Epoch: 100, Loss= 0.6881, Training Accuracy= 0.508\n",
      "Epoch: 110, Loss= 0.6873, Training Accuracy= 0.508\n",
      "Epoch: 120, Loss= 0.6864, Training Accuracy= 0.508\n",
      "Epoch: 130, Loss= 0.6855, Training Accuracy= 0.508\n",
      "Epoch: 140, Loss= 0.6844, Training Accuracy= 0.508\n",
      "Epoch: 150, Loss= 0.6832, Training Accuracy= 0.508\n",
      "Epoch: 160, Loss= 0.6818, Training Accuracy= 0.508\n",
      "Epoch: 170, Loss= 0.6803, Training Accuracy= 0.508\n",
      "Epoch: 180, Loss= 0.6785, Training Accuracy= 0.508\n",
      "Epoch: 190, Loss= 0.6765, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6743, Training Accuracy= 0.508\n",
      "Epoch: 210, Loss= 0.6717, Training Accuracy= 0.756\n",
      "Epoch: 220, Loss= 0.6688, Training Accuracy= 0.756\n",
      "Epoch: 230, Loss= 0.6654, Training Accuracy= 0.756\n",
      "Epoch: 240, Loss= 0.6616, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.6571, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.6519, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.6457, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.6384, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.6297, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.6192, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.6066, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.5911, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.5723, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.5493, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.5214, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.4880, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.4488, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.4046, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.3568, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.3083, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.2622, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.2208, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.1854, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.1562, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.1325, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.1134, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0981, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0856, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0754, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.7522, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 20, Loss= 0.6923, Training Accuracy= 0.509\n",
      "Epoch: 30, Loss= 0.6913, Training Accuracy= 0.754\n",
      "Epoch: 40, Loss= 0.6903, Training Accuracy= 0.754\n",
      "Epoch: 50, Loss= 0.6893, Training Accuracy= 0.754\n",
      "Epoch: 60, Loss= 0.6882, Training Accuracy= 0.754\n",
      "Epoch: 70, Loss= 0.6871, Training Accuracy= 0.754\n",
      "Epoch: 80, Loss= 0.6858, Training Accuracy= 0.754\n",
      "Epoch: 90, Loss= 0.6845, Training Accuracy= 0.754\n",
      "Epoch: 100, Loss= 0.6829, Training Accuracy= 0.754\n",
      "Epoch: 110, Loss= 0.6812, Training Accuracy= 0.754\n",
      "Epoch: 120, Loss= 0.6792, Training Accuracy= 0.754\n",
      "Epoch: 130, Loss= 0.6769, Training Accuracy= 0.754\n",
      "Epoch: 140, Loss= 0.6741, Training Accuracy= 0.754\n",
      "Epoch: 150, Loss= 0.6709, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.6671, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.6625, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.6569, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.6502, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.6419, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.6318, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.6195, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.6045, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.5863, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.5641, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.5374, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.5056, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.4685, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.4264, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.3806, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.3333, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.2870, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.2441, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.2064, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.1743, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.1479, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.1263, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.1088, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0946, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0830, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0735, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0655, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0589, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0533, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0485, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0444, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0408, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0377, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0350, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 1.0191, Training Accuracy= 0.505\n",
      "Epoch: 10, Loss= 0.6934, Training Accuracy= 0.257\n",
      "Epoch: 20, Loss= 0.6929, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.6925, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6921, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6918, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6915, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6911, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6908, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6905, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6901, Training Accuracy= 0.257\n",
      "Epoch: 110, Loss= 0.6897, Training Accuracy= 0.257\n",
      "Epoch: 120, Loss= 0.6894, Training Accuracy= 0.257\n",
      "Epoch: 130, Loss= 0.6889, Training Accuracy= 0.257\n",
      "Epoch: 140, Loss= 0.6885, Training Accuracy= 0.257\n",
      "Epoch: 150, Loss= 0.6880, Training Accuracy= 0.257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160, Loss= 0.6875, Training Accuracy= 0.257\n",
      "Epoch: 170, Loss= 0.6870, Training Accuracy= 0.507\n",
      "Epoch: 180, Loss= 0.6864, Training Accuracy= 0.507\n",
      "Epoch: 190, Loss= 0.6857, Training Accuracy= 0.507\n",
      "Epoch: 200, Loss= 0.6850, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.6843, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6835, Training Accuracy= 0.507\n",
      "Epoch: 230, Loss= 0.6825, Training Accuracy= 0.507\n",
      "Epoch: 240, Loss= 0.6815, Training Accuracy= 0.507\n",
      "Epoch: 250, Loss= 0.6805, Training Accuracy= 0.507\n",
      "Epoch: 260, Loss= 0.6793, Training Accuracy= 0.507\n",
      "Epoch: 270, Loss= 0.6779, Training Accuracy= 0.507\n",
      "Epoch: 280, Loss= 0.6765, Training Accuracy= 0.754\n",
      "Epoch: 290, Loss= 0.6748, Training Accuracy= 0.754\n",
      "Epoch: 300, Loss= 0.6730, Training Accuracy= 0.754\n",
      "Epoch: 310, Loss= 0.6710, Training Accuracy= 0.754\n",
      "Epoch: 320, Loss= 0.6687, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.6662, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.6633, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.6600, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.6563, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.6521, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.6472, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.6416, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.6350, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.6273, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.6183, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.6075, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.5947, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.5793, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.5608, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.5387, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.5123, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.4813, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.7084, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6926, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.6918, Training Accuracy= 0.252\n",
      "Epoch: 30, Loss= 0.6910, Training Accuracy= 0.504\n",
      "Epoch: 40, Loss= 0.6902, Training Accuracy= 0.504\n",
      "Epoch: 50, Loss= 0.6892, Training Accuracy= 0.504\n",
      "Epoch: 60, Loss= 0.6881, Training Accuracy= 0.504\n",
      "Epoch: 70, Loss= 0.6868, Training Accuracy= 0.504\n",
      "Epoch: 80, Loss= 0.6852, Training Accuracy= 0.504\n",
      "Epoch: 90, Loss= 0.6832, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.6807, Training Accuracy= 0.504\n",
      "Epoch: 110, Loss= 0.6777, Training Accuracy= 0.504\n",
      "Epoch: 120, Loss= 0.6741, Training Accuracy= 0.504\n",
      "Epoch: 130, Loss= 0.6698, Training Accuracy= 0.504\n",
      "Epoch: 140, Loss= 0.6646, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.6583, Training Accuracy= 0.504\n",
      "Epoch: 160, Loss= 0.6508, Training Accuracy= 0.750\n",
      "Epoch: 170, Loss= 0.6415, Training Accuracy= 0.750\n",
      "Epoch: 180, Loss= 0.6301, Training Accuracy= 0.750\n",
      "Epoch: 190, Loss= 0.6160, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.5986, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.5775, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.5520, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.5220, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.4873, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.4485, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.4065, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.3629, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.3197, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.2789, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.2419, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.2097, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.1824, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.1595, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.1404, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.1245, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.1111, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0999, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0904, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0822, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0752, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0692, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0639, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0592, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0551, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0514, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0482, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0452, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0426, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0402, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.7488, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.6954, Training Accuracy= 0.758\n",
      "Epoch: 20, Loss= 0.6927, Training Accuracy= 0.758\n",
      "Epoch: 30, Loss= 0.6912, Training Accuracy= 0.758\n",
      "Epoch: 40, Loss= 0.6902, Training Accuracy= 0.758\n",
      "Epoch: 50, Loss= 0.6893, Training Accuracy= 0.758\n",
      "Epoch: 60, Loss= 0.6884, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6875, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6864, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6852, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6838, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6821, Training Accuracy= 0.744\n",
      "Epoch: 120, Loss= 0.6802, Training Accuracy= 0.744\n",
      "Epoch: 130, Loss= 0.6779, Training Accuracy= 0.744\n",
      "Epoch: 140, Loss= 0.6751, Training Accuracy= 0.744\n",
      "Epoch: 150, Loss= 0.6719, Training Accuracy= 0.744\n",
      "Epoch: 160, Loss= 0.6680, Training Accuracy= 0.744\n",
      "Epoch: 170, Loss= 0.6634, Training Accuracy= 0.744\n",
      "Epoch: 180, Loss= 0.6579, Training Accuracy= 0.744\n",
      "Epoch: 190, Loss= 0.6514, Training Accuracy= 0.744\n",
      "Epoch: 200, Loss= 0.6437, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.6344, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.6233, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.6100, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.5939, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.5748, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.5522, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.5260, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.4962, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.4631, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.4273, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.3898, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.3515, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.3137, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.2773, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.2436, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.2131, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.1863, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.1632, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.1434, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.1267, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.1126, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.1006, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0904, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0817, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0742, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0678, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0622, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0573, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0529, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.6951, Training Accuracy= 0.743\n",
      "Epoch: 10, Loss= 0.6928, Training Accuracy= 0.494\n",
      "Epoch: 20, Loss= 0.6917, Training Accuracy= 0.494\n",
      "Epoch: 30, Loss= 0.6910, Training Accuracy= 0.494\n",
      "Epoch: 40, Loss= 0.6905, Training Accuracy= 0.508\n",
      "Epoch: 50, Loss= 0.6901, Training Accuracy= 0.508\n",
      "Epoch: 60, Loss= 0.6896, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.6891, Training Accuracy= 0.508\n",
      "Epoch: 80, Loss= 0.6887, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.6882, Training Accuracy= 0.508\n",
      "Epoch: 100, Loss= 0.6876, Training Accuracy= 0.508\n",
      "Epoch: 110, Loss= 0.6870, Training Accuracy= 0.508\n",
      "Epoch: 120, Loss= 0.6863, Training Accuracy= 0.508\n",
      "Epoch: 130, Loss= 0.6856, Training Accuracy= 0.508\n",
      "Epoch: 140, Loss= 0.6848, Training Accuracy= 0.508\n",
      "Epoch: 150, Loss= 0.6839, Training Accuracy= 0.508\n",
      "Epoch: 160, Loss= 0.6830, Training Accuracy= 0.508\n",
      "Epoch: 170, Loss= 0.6819, Training Accuracy= 0.508\n",
      "Epoch: 180, Loss= 0.6807, Training Accuracy= 0.508\n",
      "Epoch: 190, Loss= 0.6793, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6778, Training Accuracy= 0.508\n",
      "Epoch: 210, Loss= 0.6761, Training Accuracy= 0.508\n",
      "Epoch: 220, Loss= 0.6741, Training Accuracy= 0.508\n",
      "Epoch: 230, Loss= 0.6720, Training Accuracy= 0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240, Loss= 0.6695, Training Accuracy= 0.508\n",
      "Epoch: 250, Loss= 0.6666, Training Accuracy= 0.508\n",
      "Epoch: 260, Loss= 0.6634, Training Accuracy= 0.758\n",
      "Epoch: 270, Loss= 0.6597, Training Accuracy= 0.758\n",
      "Epoch: 280, Loss= 0.6554, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.6505, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.6447, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.6381, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.6304, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.6213, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.6107, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.5981, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.5833, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.5657, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.5449, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.5204, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.4917, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.4586, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.4212, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.3802, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.3371, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.2937, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.2523, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.2145, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.1816, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.1537, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.0025\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 500\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "mean of test_accuracies_10replications:  1.0\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.0\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQjwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c28f56810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
