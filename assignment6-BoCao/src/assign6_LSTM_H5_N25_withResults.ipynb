{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 5\n",
    "N = 25\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.6932, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 30, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 40, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 50, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 70, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 80, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 90, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 100, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 110, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 120, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 130, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 140, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 160, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 230, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 270, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 280, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 290, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 300, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 310, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 320, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 330, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 340, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 350, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 360, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 370, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 380, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 390, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 400, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 420, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 430, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 440, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 450, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 460, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 470, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 480, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 490, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 500, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 520, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 530, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 540, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 550, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 560, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 570, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 580, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 590, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 600, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 610, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 620, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 630, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 640, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 650, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 660, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 670, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 680, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 690, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 700, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 710, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 720, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 730, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 740, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 750, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 760, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 770, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 790, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 810, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 820, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 830, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 840, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 850, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 860, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 870, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 880, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 890, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 900, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 910, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 920, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 930, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 940, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 950, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 960, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 970, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 990, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4998\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.6952, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.6933, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 70, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 110, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 130, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 160, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 230, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 270, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 280, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 290, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 320, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 330, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 340, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 350, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 360, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 400, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 420, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 430, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 470, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 500, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 510, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 520, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 530, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 550, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 560, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 570, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 580, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 590, Loss= 0.6930, Training Accuracy= 0.510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 610, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 620, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 630, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 640, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 650, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 660, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 670, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 680, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 690, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 700, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 710, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 720, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 730, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 740, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 750, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 760, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 770, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 780, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 790, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 800, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 810, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 820, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 830, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 840, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 850, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 860, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 870, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 880, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 890, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 900, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 910, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 920, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 930, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 940, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 950, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 960, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 970, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 990, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4947\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.6941, Training Accuracy= 0.493\n",
      "Epoch: 10, Loss= 0.6936, Training Accuracy= 0.499\n",
      "Epoch: 20, Loss= 0.6935, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.6933, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 50, Loss= 0.6933, Training Accuracy= 0.514\n",
      "Epoch: 60, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 70, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 80, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 90, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 150, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 160, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 190, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 200, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 210, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 220, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 270, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 290, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 300, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 310, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 320, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 330, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 340, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 350, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 360, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 370, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 380, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 390, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 400, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 410, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 420, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 430, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 440, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 450, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 460, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 470, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 480, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 500, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 510, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 520, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 530, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 540, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 550, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 560, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 570, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 580, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 590, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 600, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 610, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 620, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 630, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 680, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 700, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 710, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 730, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 740, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 750, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 770, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 790, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 810, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 830, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 840, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 850, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 860, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 870, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 880, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 890, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 900, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 910, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 920, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 930, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 940, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 950, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 960, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 970, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 990, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4981\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 10, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 20, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 30, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 40, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 50, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 60, Loss= 0.6929, Training Accuracy= 0.518\n",
      "Epoch: 70, Loss= 0.6929, Training Accuracy= 0.518\n",
      "Epoch: 80, Loss= 0.6928, Training Accuracy= 0.518\n",
      "Epoch: 90, Loss= 0.6928, Training Accuracy= 0.520\n",
      "Epoch: 100, Loss= 0.6928, Training Accuracy= 0.519\n",
      "Epoch: 110, Loss= 0.6928, Training Accuracy= 0.520\n",
      "Epoch: 120, Loss= 0.6928, Training Accuracy= 0.521\n",
      "Epoch: 130, Loss= 0.6927, Training Accuracy= 0.520\n",
      "Epoch: 140, Loss= 0.6927, Training Accuracy= 0.519\n",
      "Epoch: 150, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 160, Loss= 0.6927, Training Accuracy= 0.519\n",
      "Epoch: 170, Loss= 0.6927, Training Accuracy= 0.519\n",
      "Epoch: 180, Loss= 0.6927, Training Accuracy= 0.519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.6926, Training Accuracy= 0.519\n",
      "Epoch: 200, Loss= 0.6926, Training Accuracy= 0.519\n",
      "Epoch: 210, Loss= 0.6926, Training Accuracy= 0.519\n",
      "Epoch: 220, Loss= 0.6926, Training Accuracy= 0.519\n",
      "Epoch: 230, Loss= 0.6926, Training Accuracy= 0.520\n",
      "Epoch: 240, Loss= 0.6926, Training Accuracy= 0.518\n",
      "Epoch: 250, Loss= 0.6926, Training Accuracy= 0.517\n",
      "Epoch: 260, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 270, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 280, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 290, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 300, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 310, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 320, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 330, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 340, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 350, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 360, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 370, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 380, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 390, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 400, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 410, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 420, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 430, Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 440, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 450, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 460, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 470, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 480, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 490, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 500, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 510, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 520, Loss= 0.6924, Training Accuracy= 0.518\n",
      "Epoch: 530, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 540, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 550, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 560, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 570, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 580, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 590, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 600, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 610, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 620, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 630, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 640, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 650, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 660, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 670, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 680, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 690, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 700, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 710, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 720, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 730, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 740, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 750, Loss= 0.6923, Training Accuracy= 0.520\n",
      "Epoch: 760, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 770, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 780, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 790, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 800, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 810, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 820, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 830, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 840, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 850, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 860, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 870, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 880, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 890, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 900, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 910, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 920, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 930, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 940, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 950, Loss= 0.6922, Training Accuracy= 0.518\n",
      "Epoch: 960, Loss= 0.6921, Training Accuracy= 0.518\n",
      "Epoch: 970, Loss= 0.6921, Training Accuracy= 0.518\n",
      "Epoch: 980, Loss= 0.6921, Training Accuracy= 0.518\n",
      "Epoch: 990, Loss= 0.6921, Training Accuracy= 0.518\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5105\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.7416, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.7073, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.7021, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.6999, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6987, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6979, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6973, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6969, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6966, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6963, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6961, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6959, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6958, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6957, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6955, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.6954, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6954, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6953, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6952, Training Accuracy= 0.503\n",
      "Epoch: 190, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 200, Loss= 0.6951, Training Accuracy= 0.503\n",
      "Epoch: 210, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 220, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 230, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6948, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.6948, Training Accuracy= 0.503\n",
      "Epoch: 270, Loss= 0.6948, Training Accuracy= 0.503\n",
      "Epoch: 280, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 290, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 300, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 310, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 320, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 330, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 340, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 350, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 370, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 380, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 400, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 410, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 420, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 430, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 440, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 450, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 460, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 480, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 490, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 500, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 510, Loss= 0.6943, Training Accuracy= 0.504\n",
      "Epoch: 520, Loss= 0.6943, Training Accuracy= 0.504\n",
      "Epoch: 530, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 540, Loss= 0.6942, Training Accuracy= 0.504\n",
      "Epoch: 550, Loss= 0.6942, Training Accuracy= 0.504\n",
      "Epoch: 560, Loss= 0.6942, Training Accuracy= 0.504\n",
      "Epoch: 570, Loss= 0.6942, Training Accuracy= 0.504\n",
      "Epoch: 580, Loss= 0.6942, Training Accuracy= 0.505\n",
      "Epoch: 590, Loss= 0.6942, Training Accuracy= 0.505\n",
      "Epoch: 600, Loss= 0.6942, Training Accuracy= 0.505\n",
      "Epoch: 610, Loss= 0.6942, Training Accuracy= 0.506\n",
      "Epoch: 620, Loss= 0.6942, Training Accuracy= 0.506\n",
      "Epoch: 630, Loss= 0.6942, Training Accuracy= 0.506\n",
      "Epoch: 640, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 650, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 660, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 670, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 680, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 690, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 700, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 710, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 720, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 730, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 740, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 750, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 760, Loss= 0.6940, Training Accuracy= 0.506\n",
      "Epoch: 770, Loss= 0.6940, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.6940, Training Accuracy= 0.506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 800, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 810, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 820, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 830, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 840, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 850, Loss= 0.6940, Training Accuracy= 0.506\n",
      "Epoch: 860, Loss= 0.6940, Training Accuracy= 0.506\n",
      "Epoch: 870, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 880, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 890, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 900, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 910, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 920, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 930, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 940, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 950, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 960, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 970, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 980, Loss= 0.6939, Training Accuracy= 0.505\n",
      "Epoch: 990, Loss= 0.6939, Training Accuracy= 0.504\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5005\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.6974, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6937, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.6937, Training Accuracy= 0.495\n",
      "Epoch: 30, Loss= 0.6937, Training Accuracy= 0.494\n",
      "Epoch: 40, Loss= 0.6937, Training Accuracy= 0.495\n",
      "Epoch: 50, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 60, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 70, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 80, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 90, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 100, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 110, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 120, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 130, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 140, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 150, Loss= 0.6936, Training Accuracy= 0.495\n",
      "Epoch: 160, Loss= 0.6935, Training Accuracy= 0.495\n",
      "Epoch: 170, Loss= 0.6935, Training Accuracy= 0.495\n",
      "Epoch: 180, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 190, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 200, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 210, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 220, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 230, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 240, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 250, Loss= 0.6935, Training Accuracy= 0.495\n",
      "Epoch: 260, Loss= 0.6935, Training Accuracy= 0.495\n",
      "Epoch: 270, Loss= 0.6935, Training Accuracy= 0.495\n",
      "Epoch: 280, Loss= 0.6935, Training Accuracy= 0.495\n",
      "Epoch: 290, Loss= 0.6935, Training Accuracy= 0.495\n",
      "Epoch: 300, Loss= 0.6935, Training Accuracy= 0.495\n",
      "Epoch: 310, Loss= 0.6935, Training Accuracy= 0.495\n",
      "Epoch: 320, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 330, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 340, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 350, Loss= 0.6935, Training Accuracy= 0.496\n",
      "Epoch: 360, Loss= 0.6934, Training Accuracy= 0.496\n",
      "Epoch: 370, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 380, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 390, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 400, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 410, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 420, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 430, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 440, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 450, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 460, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 470, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 480, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 490, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 500, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 510, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 520, Loss= 0.6934, Training Accuracy= 0.499\n",
      "Epoch: 530, Loss= 0.6934, Training Accuracy= 0.499\n",
      "Epoch: 540, Loss= 0.6934, Training Accuracy= 0.499\n",
      "Epoch: 550, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.6934, Training Accuracy= 0.499\n",
      "Epoch: 570, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 580, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 590, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 600, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 610, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 620, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 630, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 640, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 650, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 660, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 670, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 680, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 690, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 700, Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 710, Loss= 0.6934, Training Accuracy= 0.496\n",
      "Epoch: 720, Loss= 0.6934, Training Accuracy= 0.496\n",
      "Epoch: 730, Loss= 0.6934, Training Accuracy= 0.496\n",
      "Epoch: 740, Loss= 0.6933, Training Accuracy= 0.495\n",
      "Epoch: 750, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 760, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 770, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 780, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 790, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 800, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 810, Loss= 0.6933, Training Accuracy= 0.498\n",
      "Epoch: 820, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 830, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 840, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 850, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 860, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 870, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 880, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 890, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 900, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 910, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 920, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 930, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 940, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 950, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Epoch: 960, Loss= 0.6933, Training Accuracy= 0.498\n",
      "Epoch: 970, Loss= 0.6933, Training Accuracy= 0.498\n",
      "Epoch: 980, Loss= 0.6933, Training Accuracy= 0.498\n",
      "Epoch: 990, Loss= 0.6933, Training Accuracy= 0.497\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.497\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.6964, Training Accuracy= 0.500\n",
      "Epoch: 10, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 20, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 130, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 160, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 190, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 200, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 210, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 220, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 240, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 250, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 260, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 270, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 290, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 300, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 310, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 320, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 330, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 340, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 350, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 360, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 400, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 420, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 430, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 470, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 500, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 520, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 530, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 540, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 550, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 560, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 570, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 580, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 590, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 600, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 610, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 620, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 630, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 680, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 700, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 710, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 730, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 740, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 750, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 770, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 780, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 790, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 800, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 810, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 820, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 830, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 840, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 850, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 860, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 870, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 880, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 890, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 900, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 910, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 920, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 930, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 940, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 950, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 960, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 970, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 990, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.498\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.6954, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 20, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 30, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 40, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 50, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 60, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 70, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 80, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 100, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 110, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 120, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 140, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 150, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 160, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 170, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 180, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 190, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 200, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 210, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 220, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 230, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 240, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 250, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 260, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 270, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 280, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 290, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 300, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 310, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 320, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 330, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 340, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 350, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 360, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 370, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 380, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 390, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 400, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 410, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 420, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 430, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 440, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 450, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 460, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 470, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 480, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 490, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 500, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 510, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 520, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 530, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 540, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 550, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 560, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 570, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 580, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 590, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 600, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 610, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 620, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 630, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 640, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 650, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 660, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 670, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 680, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 690, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 700, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 710, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 720, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 730, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 740, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 750, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 760, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 770, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 780, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 790, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 800, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 810, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 820, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 830, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 840, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 850, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 860, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 870, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 880, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 890, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 900, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 910, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 920, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 930, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 940, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 950, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 960, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 970, Loss= 0.6927, Training Accuracy= 0.513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 990, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5049\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.6988, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 20, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 30, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 40, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 60, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 70, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 90, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 100, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 110, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 130, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 150, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 160, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 170, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 180, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 190, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 200, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 210, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 220, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 230, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 240, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 250, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 260, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 270, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 280, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 290, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 300, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 310, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 320, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 330, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 340, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 350, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 360, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 370, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 380, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 390, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 400, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 410, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 430, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 440, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 450, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 460, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 470, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 480, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 490, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 500, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 520, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 530, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 540, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 550, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 560, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 570, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 580, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 590, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 600, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 610, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 620, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 630, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 680, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 700, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 710, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 730, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 740, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 750, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 770, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 780, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 790, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 800, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 810, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 820, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 830, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 840, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 850, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 860, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 870, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 880, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 890, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 900, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 910, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 920, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 930, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 940, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 950, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 960, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 970, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 980, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 990, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4964\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.6955, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.6941, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 50, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 70, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 80, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 90, Loss= 0.6935, Training Accuracy= 0.504\n",
      "Epoch: 100, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 110, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 120, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 130, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 150, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 160, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 170, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 180, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 210, Loss= 0.6934, Training Accuracy= 0.506\n",
      "Epoch: 220, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 230, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 240, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 250, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 260, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 270, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 280, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 290, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 300, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 310, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 320, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 330, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 340, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 350, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 360, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 370, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 380, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 390, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 400, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 420, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 430, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 440, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 450, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 460, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 470, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 480, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 490, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 500, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 520, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 530, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 540, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 550, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 560, Loss= 0.6932, Training Accuracy= 0.506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 580, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 590, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 600, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 610, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 620, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 630, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 640, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 650, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 660, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 670, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 680, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 690, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 700, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 710, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 720, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 730, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 740, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 750, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 760, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 770, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 780, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 790, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 800, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 810, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 820, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 830, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 840, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 850, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 860, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 870, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 880, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 890, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 900, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 910, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 920, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 930, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 940, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 950, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 960, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 970, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 980, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 990, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4976\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.06\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.4998, 0.49470001, 0.49810001, 0.51050001, 0.50050002, 0.49700001, 0.498, 0.50489998, 0.4964, 0.49759999]\n",
      "mean of test_accuracies_10replications:  0.49975\n",
      "standard deviation of test_accuracies_10replications_std_mean:  4.4423523359e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9cdb0a7750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
