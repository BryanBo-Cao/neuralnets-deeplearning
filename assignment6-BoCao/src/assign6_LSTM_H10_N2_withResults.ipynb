{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 10\n",
    "N = 2\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.8077, Training Accuracy= 0.488\n",
      "Epoch: 10, Loss= 0.6913, Training Accuracy= 0.764\n",
      "Epoch: 20, Loss= 0.6898, Training Accuracy= 0.764\n",
      "Epoch: 30, Loss= 0.6882, Training Accuracy= 0.764\n",
      "Epoch: 40, Loss= 0.6864, Training Accuracy= 0.764\n",
      "Epoch: 50, Loss= 0.6842, Training Accuracy= 0.764\n",
      "Epoch: 60, Loss= 0.6815, Training Accuracy= 0.764\n",
      "Epoch: 70, Loss= 0.6781, Training Accuracy= 0.764\n",
      "Epoch: 80, Loss= 0.6738, Training Accuracy= 0.764\n",
      "Epoch: 90, Loss= 0.6683, Training Accuracy= 0.764\n",
      "Epoch: 100, Loss= 0.6610, Training Accuracy= 0.764\n",
      "Epoch: 110, Loss= 0.6514, Training Accuracy= 0.764\n",
      "Epoch: 120, Loss= 0.6384, Training Accuracy= 0.764\n",
      "Epoch: 130, Loss= 0.6206, Training Accuracy= 0.764\n",
      "Epoch: 140, Loss= 0.5960, Training Accuracy= 0.764\n",
      "Epoch: 150, Loss= 0.5621, Training Accuracy= 0.764\n",
      "Epoch: 160, Loss= 0.5159, Training Accuracy= 0.764\n",
      "Epoch: 170, Loss= 0.4551, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.3799, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.2968, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.2195, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.1593, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.1174, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.0892, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.0701, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.0568, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.0472, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0400, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0345, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0302, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0267, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0239, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0216, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0196, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0179, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0165, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0153, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0142, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0133, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0117, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0110, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0104, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0099, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0094, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0089, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0085, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0081, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0078, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0075, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7654, Training Accuracy= 0.510\n",
      "Epoch: 10, Loss= 0.6954, Training Accuracy= 0.504\n",
      "Epoch: 20, Loss= 0.6940, Training Accuracy= 0.504\n",
      "Epoch: 30, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 40, Loss= 0.6923, Training Accuracy= 0.758\n",
      "Epoch: 50, Loss= 0.6917, Training Accuracy= 0.758\n",
      "Epoch: 60, Loss= 0.6913, Training Accuracy= 0.758\n",
      "Epoch: 70, Loss= 0.6907, Training Accuracy= 0.510\n",
      "Epoch: 80, Loss= 0.6902, Training Accuracy= 0.510\n",
      "Epoch: 90, Loss= 0.6896, Training Accuracy= 0.510\n",
      "Epoch: 100, Loss= 0.6890, Training Accuracy= 0.510\n",
      "Epoch: 110, Loss= 0.6883, Training Accuracy= 0.510\n",
      "Epoch: 120, Loss= 0.6874, Training Accuracy= 0.510\n",
      "Epoch: 130, Loss= 0.6865, Training Accuracy= 0.758\n",
      "Epoch: 140, Loss= 0.6853, Training Accuracy= 0.758\n",
      "Epoch: 150, Loss= 0.6840, Training Accuracy= 0.758\n",
      "Epoch: 160, Loss= 0.6822, Training Accuracy= 0.758\n",
      "Epoch: 170, Loss= 0.6801, Training Accuracy= 0.758\n",
      "Epoch: 180, Loss= 0.6775, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.6741, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.6697, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.6639, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.6561, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.6455, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.6307, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.6097, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.5795, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.5361, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.4756, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.3978, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.3110, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.2310, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.1692, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.1262, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0971, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0772, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0631, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0528, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0451, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0392, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0345, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0307, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0276, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0250, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0228, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0209, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0193, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0179, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0167, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0156, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.7040, Training Accuracy= 0.510\n",
      "Epoch: 10, Loss= 0.6924, Training Accuracy= 0.510\n",
      "Epoch: 20, Loss= 0.6894, Training Accuracy= 0.510\n",
      "Epoch: 30, Loss= 0.6863, Training Accuracy= 0.757\n",
      "Epoch: 40, Loss= 0.6815, Training Accuracy= 0.757\n",
      "Epoch: 50, Loss= 0.6737, Training Accuracy= 0.757\n",
      "Epoch: 60, Loss= 0.6618, Training Accuracy= 0.757\n",
      "Epoch: 70, Loss= 0.6434, Training Accuracy= 0.757\n",
      "Epoch: 80, Loss= 0.6136, Training Accuracy= 1.000\n",
      "Epoch: 90, Loss= 0.5633, Training Accuracy= 1.000\n",
      "Epoch: 100, Loss= 0.4807, Training Accuracy= 1.000\n",
      "Epoch: 110, Loss= 0.3656, Training Accuracy= 1.000\n",
      "Epoch: 120, Loss= 0.2508, Training Accuracy= 1.000\n",
      "Epoch: 130, Loss= 0.1698, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.1209, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.0911, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.0721, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.0591, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.0498, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.0429, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.0376, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.0334, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.0300, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.0272, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.0248, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.0228, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.0211, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0196, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0183, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0172, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0162, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0152, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0144, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0137, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0130, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0119, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0109, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0104, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0100, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0097, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0090, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0087, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0084, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0081, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0079, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0076, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0074, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.7510, Training Accuracy= 0.499\n",
      "Epoch: 10, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6898, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.6874, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.6853, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.6831, Training Accuracy= 1.000\n",
      "Epoch: 60, Loss= 0.6806, Training Accuracy= 1.000\n",
      "Epoch: 70, Loss= 0.6774, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80, Loss= 0.6732, Training Accuracy= 1.000\n",
      "Epoch: 90, Loss= 0.6676, Training Accuracy= 1.000\n",
      "Epoch: 100, Loss= 0.6598, Training Accuracy= 1.000\n",
      "Epoch: 110, Loss= 0.6487, Training Accuracy= 1.000\n",
      "Epoch: 120, Loss= 0.6321, Training Accuracy= 1.000\n",
      "Epoch: 130, Loss= 0.6070, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.5682, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.5095, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.4280, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.3333, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.2469, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.1816, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.1365, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.1057, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.0844, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.0692, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.0580, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.0495, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.0429, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0377, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0335, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0300, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0271, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0247, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0226, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0209, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0193, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0180, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0168, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0157, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0148, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0139, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0132, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0125, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0119, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0108, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0103, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0099, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0094, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0091, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0087, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.8165, Training Accuracy= 0.506\n",
      "Epoch: 10, Loss= 0.6910, Training Accuracy= 0.508\n",
      "Epoch: 20, Loss= 0.6900, Training Accuracy= 0.508\n",
      "Epoch: 30, Loss= 0.6887, Training Accuracy= 0.508\n",
      "Epoch: 40, Loss= 0.6871, Training Accuracy= 0.508\n",
      "Epoch: 50, Loss= 0.6852, Training Accuracy= 0.753\n",
      "Epoch: 60, Loss= 0.6827, Training Accuracy= 0.753\n",
      "Epoch: 70, Loss= 0.6795, Training Accuracy= 0.753\n",
      "Epoch: 80, Loss= 0.6754, Training Accuracy= 0.753\n",
      "Epoch: 90, Loss= 0.6698, Training Accuracy= 0.753\n",
      "Epoch: 100, Loss= 0.6622, Training Accuracy= 0.753\n",
      "Epoch: 110, Loss= 0.6517, Training Accuracy= 0.753\n",
      "Epoch: 120, Loss= 0.6367, Training Accuracy= 0.753\n",
      "Epoch: 130, Loss= 0.6149, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.5830, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.5368, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.4728, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.3918, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.3033, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.2233, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.1628, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.1215, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.0940, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.0753, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.0620, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.0523, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.0450, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0393, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0347, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0310, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0280, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0255, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0233, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0214, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0198, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0185, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0172, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0161, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0152, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0143, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0135, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0128, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0122, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0116, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0111, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0106, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0101, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0097, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0090, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.7102, Training Accuracy= 0.507\n",
      "Epoch: 10, Loss= 0.6930, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.6921, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.6913, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.6905, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.6897, Training Accuracy= 0.755\n",
      "Epoch: 60, Loss= 0.6887, Training Accuracy= 0.755\n",
      "Epoch: 70, Loss= 0.6877, Training Accuracy= 0.755\n",
      "Epoch: 80, Loss= 0.6864, Training Accuracy= 0.755\n",
      "Epoch: 90, Loss= 0.6850, Training Accuracy= 0.755\n",
      "Epoch: 100, Loss= 0.6831, Training Accuracy= 0.755\n",
      "Epoch: 110, Loss= 0.6809, Training Accuracy= 0.755\n",
      "Epoch: 120, Loss= 0.6780, Training Accuracy= 0.755\n",
      "Epoch: 130, Loss= 0.6743, Training Accuracy= 0.755\n",
      "Epoch: 140, Loss= 0.6695, Training Accuracy= 0.755\n",
      "Epoch: 150, Loss= 0.6632, Training Accuracy= 0.755\n",
      "Epoch: 160, Loss= 0.6547, Training Accuracy= 0.755\n",
      "Epoch: 170, Loss= 0.6432, Training Accuracy= 0.755\n",
      "Epoch: 180, Loss= 0.6275, Training Accuracy= 0.755\n",
      "Epoch: 190, Loss= 0.6056, Training Accuracy= 0.755\n",
      "Epoch: 200, Loss= 0.5749, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.5328, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.4776, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.4105, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.3363, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.2631, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.1994, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.1500, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.1145, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0896, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0720, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0593, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0499, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0427, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0372, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0327, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0292, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0262, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0238, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0217, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0199, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0184, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0170, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0159, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0148, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0139, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0131, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0117, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0111, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.7239, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.6908, Training Accuracy= 0.253\n",
      "Epoch: 20, Loss= 0.6891, Training Accuracy= 0.253\n",
      "Epoch: 30, Loss= 0.6871, Training Accuracy= 0.253\n",
      "Epoch: 40, Loss= 0.6847, Training Accuracy= 0.253\n",
      "Epoch: 50, Loss= 0.6817, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6776, Training Accuracy= 0.506\n",
      "Epoch: 70, Loss= 0.6721, Training Accuracy= 0.749\n",
      "Epoch: 80, Loss= 0.6644, Training Accuracy= 0.749\n",
      "Epoch: 90, Loss= 0.6534, Training Accuracy= 0.749\n",
      "Epoch: 100, Loss= 0.6373, Training Accuracy= 0.749\n",
      "Epoch: 110, Loss= 0.6138, Training Accuracy= 0.749\n",
      "Epoch: 120, Loss= 0.5796, Training Accuracy= 1.000\n",
      "Epoch: 130, Loss= 0.5317, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.4694, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.3955, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160, Loss= 0.3173, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.2439, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.1837, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.1395, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.1086, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.0871, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.0718, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.0605, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.0520, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.0454, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.0401, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0359, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0324, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0294, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0270, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0248, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0230, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0214, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0200, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0187, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0176, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0166, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0157, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0149, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0141, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0135, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0129, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0123, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0118, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0108, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0104, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0100, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0097, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.7019, Training Accuracy= 0.494\n",
      "Epoch: 10, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.6921, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.6905, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.6887, Training Accuracy= 0.254\n",
      "Epoch: 50, Loss= 0.6866, Training Accuracy= 0.254\n",
      "Epoch: 60, Loss= 0.6841, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6810, Training Accuracy= 0.500\n",
      "Epoch: 80, Loss= 0.6773, Training Accuracy= 0.500\n",
      "Epoch: 90, Loss= 0.6726, Training Accuracy= 0.752\n",
      "Epoch: 100, Loss= 0.6665, Training Accuracy= 0.752\n",
      "Epoch: 110, Loss= 0.6584, Training Accuracy= 0.752\n",
      "Epoch: 120, Loss= 0.6471, Training Accuracy= 0.752\n",
      "Epoch: 130, Loss= 0.6311, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.6084, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.5763, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.5327, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.4768, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.4115, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.3423, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.2759, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.2182, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.1721, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.1372, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.1113, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.0920, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.0774, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0662, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0574, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0504, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0446, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0400, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0360, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0327, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0299, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0275, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0254, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0236, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0220, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0205, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0193, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0181, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0171, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0162, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0153, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0146, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0139, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0132, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0126, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0121, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.7034, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 0.6901, Training Accuracy= 0.755\n",
      "Epoch: 20, Loss= 0.6885, Training Accuracy= 0.755\n",
      "Epoch: 30, Loss= 0.6868, Training Accuracy= 0.755\n",
      "Epoch: 40, Loss= 0.6849, Training Accuracy= 0.755\n",
      "Epoch: 50, Loss= 0.6825, Training Accuracy= 0.755\n",
      "Epoch: 60, Loss= 0.6793, Training Accuracy= 0.755\n",
      "Epoch: 70, Loss= 0.6751, Training Accuracy= 0.755\n",
      "Epoch: 80, Loss= 0.6694, Training Accuracy= 0.755\n",
      "Epoch: 90, Loss= 0.6615, Training Accuracy= 1.000\n",
      "Epoch: 100, Loss= 0.6504, Training Accuracy= 1.000\n",
      "Epoch: 110, Loss= 0.6345, Training Accuracy= 1.000\n",
      "Epoch: 120, Loss= 0.6108, Training Accuracy= 1.000\n",
      "Epoch: 130, Loss= 0.5750, Training Accuracy= 1.000\n",
      "Epoch: 140, Loss= 0.5211, Training Accuracy= 1.000\n",
      "Epoch: 150, Loss= 0.4452, Training Accuracy= 1.000\n",
      "Epoch: 160, Loss= 0.3532, Training Accuracy= 1.000\n",
      "Epoch: 170, Loss= 0.2644, Training Accuracy= 1.000\n",
      "Epoch: 180, Loss= 0.1949, Training Accuracy= 1.000\n",
      "Epoch: 190, Loss= 0.1463, Training Accuracy= 1.000\n",
      "Epoch: 200, Loss= 0.1134, Training Accuracy= 1.000\n",
      "Epoch: 210, Loss= 0.0906, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.0743, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.0624, Training Accuracy= 1.000\n",
      "Epoch: 240, Loss= 0.0534, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.0464, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.0408, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.0363, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.0326, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.0295, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0269, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0247, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0228, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0211, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0196, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0184, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0172, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0162, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0153, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0144, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0137, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0130, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0118, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0108, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0104, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0099, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0096, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0092, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7060, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.6939, Training Accuracy= 0.252\n",
      "Epoch: 20, Loss= 0.6928, Training Accuracy= 0.252\n",
      "Epoch: 30, Loss= 0.6918, Training Accuracy= 0.252\n",
      "Epoch: 40, Loss= 0.6910, Training Accuracy= 0.252\n",
      "Epoch: 50, Loss= 0.6902, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.6893, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.6883, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.6873, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.6861, Training Accuracy= 0.497\n",
      "Epoch: 100, Loss= 0.6847, Training Accuracy= 0.497\n",
      "Epoch: 110, Loss= 0.6830, Training Accuracy= 0.497\n",
      "Epoch: 120, Loss= 0.6809, Training Accuracy= 0.497\n",
      "Epoch: 130, Loss= 0.6783, Training Accuracy= 0.748\n",
      "Epoch: 140, Loss= 0.6750, Training Accuracy= 0.748\n",
      "Epoch: 150, Loss= 0.6708, Training Accuracy= 0.748\n",
      "Epoch: 160, Loss= 0.6653, Training Accuracy= 0.748\n",
      "Epoch: 170, Loss= 0.6580, Training Accuracy= 0.748\n",
      "Epoch: 180, Loss= 0.6479, Training Accuracy= 0.748\n",
      "Epoch: 190, Loss= 0.6337, Training Accuracy= 0.748\n",
      "Epoch: 200, Loss= 0.6134, Training Accuracy= 0.748\n",
      "Epoch: 210, Loss= 0.5838, Training Accuracy= 1.000\n",
      "Epoch: 220, Loss= 0.5412, Training Accuracy= 1.000\n",
      "Epoch: 230, Loss= 0.4822, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240, Loss= 0.4077, Training Accuracy= 1.000\n",
      "Epoch: 250, Loss= 0.3256, Training Accuracy= 1.000\n",
      "Epoch: 260, Loss= 0.2485, Training Accuracy= 1.000\n",
      "Epoch: 270, Loss= 0.1857, Training Accuracy= 1.000\n",
      "Epoch: 280, Loss= 0.1393, Training Accuracy= 1.000\n",
      "Epoch: 290, Loss= 0.1066, Training Accuracy= 1.000\n",
      "Epoch: 300, Loss= 0.0838, Training Accuracy= 1.000\n",
      "Epoch: 310, Loss= 0.0677, Training Accuracy= 1.000\n",
      "Epoch: 320, Loss= 0.0560, Training Accuracy= 1.000\n",
      "Epoch: 330, Loss= 0.0473, Training Accuracy= 1.000\n",
      "Epoch: 340, Loss= 0.0407, Training Accuracy= 1.000\n",
      "Epoch: 350, Loss= 0.0355, Training Accuracy= 1.000\n",
      "Epoch: 360, Loss= 0.0314, Training Accuracy= 1.000\n",
      "Epoch: 370, Loss= 0.0280, Training Accuracy= 1.000\n",
      "Epoch: 380, Loss= 0.0253, Training Accuracy= 1.000\n",
      "Epoch: 390, Loss= 0.0230, Training Accuracy= 1.000\n",
      "Epoch: 400, Loss= 0.0210, Training Accuracy= 1.000\n",
      "Epoch: 410, Loss= 0.0193, Training Accuracy= 1.000\n",
      "Epoch: 420, Loss= 0.0179, Training Accuracy= 1.000\n",
      "Epoch: 430, Loss= 0.0166, Training Accuracy= 1.000\n",
      "Epoch: 440, Loss= 0.0155, Training Accuracy= 1.000\n",
      "Epoch: 450, Loss= 0.0145, Training Accuracy= 1.000\n",
      "Epoch: 460, Loss= 0.0137, Training Accuracy= 1.000\n",
      "Epoch: 470, Loss= 0.0129, Training Accuracy= 1.000\n",
      "Epoch: 480, Loss= 0.0122, Training Accuracy= 1.000\n",
      "Epoch: 490, Loss= 0.0115, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.005\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 500\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "mean of test_accuracies_10replications:  1.0\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.0\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb51cb2c950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
