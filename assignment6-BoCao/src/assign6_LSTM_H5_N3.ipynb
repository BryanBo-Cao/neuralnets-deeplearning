{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 5\n",
    "N = 3\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Minibatch Loss= 0.7021, Training Accuracy= 0.502\n",
      "Epoch: 10, Minibatch Loss= 0.6937, Training Accuracy= 0.750\n",
      "Epoch: 20, Minibatch Loss= 0.6934, Training Accuracy= 0.750\n",
      "Epoch: 30, Minibatch Loss= 0.6932, Training Accuracy= 0.626\n",
      "Epoch: 40, Minibatch Loss= 0.6932, Training Accuracy= 0.500\n",
      "Epoch: 50, Minibatch Loss= 0.6931, Training Accuracy= 0.626\n",
      "Epoch: 60, Minibatch Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 70, Minibatch Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 80, Minibatch Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 90, Minibatch Loss= 0.6929, Training Accuracy= 0.505\n",
      "Epoch: 100, Minibatch Loss= 0.6929, Training Accuracy= 0.629\n",
      "Epoch: 110, Minibatch Loss= 0.6929, Training Accuracy= 0.629\n",
      "Epoch: 120, Minibatch Loss= 0.6928, Training Accuracy= 0.629\n",
      "Epoch: 130, Minibatch Loss= 0.6928, Training Accuracy= 0.629\n",
      "Epoch: 140, Minibatch Loss= 0.6928, Training Accuracy= 0.629\n",
      "Epoch: 150, Minibatch Loss= 0.6927, Training Accuracy= 0.629\n",
      "Epoch: 160, Minibatch Loss= 0.6927, Training Accuracy= 0.629\n",
      "Epoch: 170, Minibatch Loss= 0.6927, Training Accuracy= 0.629\n",
      "Epoch: 180, Minibatch Loss= 0.6926, Training Accuracy= 0.629\n",
      "Epoch: 190, Minibatch Loss= 0.6926, Training Accuracy= 0.629\n",
      "Epoch: 200, Minibatch Loss= 0.6926, Training Accuracy= 0.629\n",
      "Epoch: 210, Minibatch Loss= 0.6925, Training Accuracy= 0.629\n",
      "Epoch: 220, Minibatch Loss= 0.6924, Training Accuracy= 0.629\n",
      "Epoch: 230, Minibatch Loss= 0.6924, Training Accuracy= 0.629\n",
      "Epoch: 240, Minibatch Loss= 0.6923, Training Accuracy= 0.629\n",
      "Epoch: 250, Minibatch Loss= 0.6922, Training Accuracy= 0.629\n",
      "Epoch: 260, Minibatch Loss= 0.6920, Training Accuracy= 0.629\n",
      "Epoch: 270, Minibatch Loss= 0.6919, Training Accuracy= 0.629\n",
      "Epoch: 280, Minibatch Loss= 0.6916, Training Accuracy= 0.629\n",
      "Epoch: 290, Minibatch Loss= 0.6913, Training Accuracy= 0.629\n",
      "Epoch: 300, Minibatch Loss= 0.6909, Training Accuracy= 0.629\n",
      "Epoch: 310, Minibatch Loss= 0.6903, Training Accuracy= 0.629\n",
      "Epoch: 320, Minibatch Loss= 0.6894, Training Accuracy= 0.629\n",
      "Epoch: 330, Minibatch Loss= 0.6878, Training Accuracy= 0.629\n",
      "Epoch: 340, Minibatch Loss= 0.6846, Training Accuracy= 0.629\n",
      "Epoch: 350, Minibatch Loss= 0.6773, Training Accuracy= 0.629\n",
      "Epoch: 360, Minibatch Loss= 0.6541, Training Accuracy= 0.749\n",
      "Epoch: 370, Minibatch Loss= 0.5727, Training Accuracy= 0.872\n",
      "Epoch: 380, Minibatch Loss= 0.4534, Training Accuracy= 0.872\n",
      "Epoch: 390, Minibatch Loss= 0.3451, Training Accuracy= 0.872\n",
      "Epoch: 400, Minibatch Loss= 0.1990, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.1037, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0666, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0483, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0375, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0305, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0256, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0220, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0192, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0170, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 1: \n",
      "Epoch: 0, Minibatch Loss= 0.6956, Training Accuracy= 0.500\n",
      "Epoch: 10, Minibatch Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 20, Minibatch Loss= 0.6935, Training Accuracy= 0.500\n",
      "Epoch: 30, Minibatch Loss= 0.6935, Training Accuracy= 0.500\n",
      "Epoch: 40, Minibatch Loss= 0.6934, Training Accuracy= 0.500\n",
      "Epoch: 50, Minibatch Loss= 0.6934, Training Accuracy= 0.500\n",
      "Epoch: 60, Minibatch Loss= 0.6933, Training Accuracy= 0.500\n",
      "Epoch: 70, Minibatch Loss= 0.6933, Training Accuracy= 0.500\n",
      "Epoch: 80, Minibatch Loss= 0.6932, Training Accuracy= 0.500\n",
      "Epoch: 90, Minibatch Loss= 0.6932, Training Accuracy= 0.500\n",
      "Epoch: 100, Minibatch Loss= 0.6931, Training Accuracy= 0.500\n",
      "Epoch: 110, Minibatch Loss= 0.6930, Training Accuracy= 0.500\n",
      "Epoch: 120, Minibatch Loss= 0.6929, Training Accuracy= 0.500\n",
      "Epoch: 130, Minibatch Loss= 0.6928, Training Accuracy= 0.500\n",
      "Epoch: 140, Minibatch Loss= 0.6926, Training Accuracy= 0.500\n",
      "Epoch: 150, Minibatch Loss= 0.6923, Training Accuracy= 0.500\n",
      "Epoch: 160, Minibatch Loss= 0.6920, Training Accuracy= 0.623\n",
      "Epoch: 170, Minibatch Loss= 0.6915, Training Accuracy= 0.623\n",
      "Epoch: 180, Minibatch Loss= 0.6907, Training Accuracy= 0.623\n",
      "Epoch: 190, Minibatch Loss= 0.6894, Training Accuracy= 0.753\n",
      "Epoch: 200, Minibatch Loss= 0.6867, Training Accuracy= 0.753\n",
      "Epoch: 210, Minibatch Loss= 0.6802, Training Accuracy= 0.753\n",
      "Epoch: 220, Minibatch Loss= 0.6582, Training Accuracy= 0.879\n",
      "Epoch: 230, Minibatch Loss= 0.5678, Training Accuracy= 0.879\n",
      "Epoch: 240, Minibatch Loss= 0.4423, Training Accuracy= 0.879\n",
      "Epoch: 250, Minibatch Loss= 0.3664, Training Accuracy= 0.879\n",
      "Epoch: 260, Minibatch Loss= 0.2816, Training Accuracy= 0.879\n",
      "Epoch: 270, Minibatch Loss= 0.1503, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0821, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0558, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0421, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0336, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0280, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0239, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0208, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0184, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0164, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0136, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0115, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0088, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0083, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0078, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0067, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 2: \n",
      "Epoch: 0, Minibatch Loss= 0.6962, Training Accuracy= 0.750\n",
      "Epoch: 10, Minibatch Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 20, Minibatch Loss= 0.6932, Training Accuracy= 0.628\n",
      "Epoch: 30, Minibatch Loss= 0.6932, Training Accuracy= 0.628\n",
      "Epoch: 40, Minibatch Loss= 0.6932, Training Accuracy= 0.628\n",
      "Epoch: 50, Minibatch Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 60, Minibatch Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 70, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 80, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 90, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 100, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 110, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 120, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 130, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 140, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 150, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 160, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 170, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 180, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 190, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 200, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 210, Minibatch Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 220, Minibatch Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 230, Minibatch Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 240, Minibatch Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 250, Minibatch Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 260, Minibatch Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 270, Minibatch Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 280, Minibatch Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 290, Minibatch Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 300, Minibatch Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 310, Minibatch Loss= 0.6929, Training Accuracy= 0.503\n",
      "Epoch: 320, Minibatch Loss= 0.6929, Training Accuracy= 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330, Minibatch Loss= 0.6929, Training Accuracy= 0.503\n",
      "Epoch: 340, Minibatch Loss= 0.6929, Training Accuracy= 0.503\n",
      "Epoch: 350, Minibatch Loss= 0.6929, Training Accuracy= 0.378\n",
      "Epoch: 360, Minibatch Loss= 0.6928, Training Accuracy= 0.378\n",
      "Epoch: 370, Minibatch Loss= 0.6928, Training Accuracy= 0.378\n",
      "Epoch: 380, Minibatch Loss= 0.6928, Training Accuracy= 0.378\n",
      "Epoch: 390, Minibatch Loss= 0.6927, Training Accuracy= 0.378\n",
      "Epoch: 400, Minibatch Loss= 0.6927, Training Accuracy= 0.378\n",
      "Epoch: 410, Minibatch Loss= 0.6926, Training Accuracy= 0.378\n",
      "Epoch: 420, Minibatch Loss= 0.6926, Training Accuracy= 0.378\n",
      "Epoch: 430, Minibatch Loss= 0.6925, Training Accuracy= 0.378\n",
      "Epoch: 440, Minibatch Loss= 0.6925, Training Accuracy= 0.378\n",
      "Epoch: 450, Minibatch Loss= 0.6924, Training Accuracy= 0.503\n",
      "Epoch: 460, Minibatch Loss= 0.6923, Training Accuracy= 0.503\n",
      "Epoch: 470, Minibatch Loss= 0.6921, Training Accuracy= 0.503\n",
      "Epoch: 480, Minibatch Loss= 0.6920, Training Accuracy= 0.503\n",
      "Epoch: 490, Minibatch Loss= 0.6918, Training Accuracy= 0.630\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.6273\n",
      "Replication: 3: \n",
      "Epoch: 0, Minibatch Loss= 0.7037, Training Accuracy= 0.621\n",
      "Epoch: 10, Minibatch Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 20, Minibatch Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 30, Minibatch Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 40, Minibatch Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 50, Minibatch Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 60, Minibatch Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 70, Minibatch Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 80, Minibatch Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 90, Minibatch Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 100, Minibatch Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 110, Minibatch Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 120, Minibatch Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 130, Minibatch Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 140, Minibatch Loss= 0.6925, Training Accuracy= 0.512\n",
      "Epoch: 150, Minibatch Loss= 0.6924, Training Accuracy= 0.512\n",
      "Epoch: 160, Minibatch Loss= 0.6923, Training Accuracy= 0.512\n",
      "Epoch: 170, Minibatch Loss= 0.6922, Training Accuracy= 0.512\n",
      "Epoch: 180, Minibatch Loss= 0.6921, Training Accuracy= 0.512\n",
      "Epoch: 190, Minibatch Loss= 0.6919, Training Accuracy= 0.512\n",
      "Epoch: 200, Minibatch Loss= 0.6916, Training Accuracy= 0.512\n",
      "Epoch: 210, Minibatch Loss= 0.6913, Training Accuracy= 0.638\n",
      "Epoch: 220, Minibatch Loss= 0.6906, Training Accuracy= 0.638\n",
      "Epoch: 230, Minibatch Loss= 0.6895, Training Accuracy= 0.638\n",
      "Epoch: 240, Minibatch Loss= 0.6871, Training Accuracy= 0.759\n",
      "Epoch: 250, Minibatch Loss= 0.6804, Training Accuracy= 0.759\n",
      "Epoch: 260, Minibatch Loss= 0.6548, Training Accuracy= 0.759\n",
      "Epoch: 270, Minibatch Loss= 0.5631, Training Accuracy= 0.876\n",
      "Epoch: 280, Minibatch Loss= 0.4554, Training Accuracy= 0.876\n",
      "Epoch: 290, Minibatch Loss= 0.3664, Training Accuracy= 0.876\n",
      "Epoch: 300, Minibatch Loss= 0.2654, Training Accuracy= 0.876\n",
      "Epoch: 310, Minibatch Loss= 0.1515, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0888, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0610, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0460, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0368, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0305, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0259, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0225, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0198, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0177, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0159, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0145, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0133, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0122, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0105, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0087, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 4: \n",
      "Epoch: 0, Minibatch Loss= 0.7048, Training Accuracy= 0.484\n",
      "Epoch: 10, Minibatch Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 20, Minibatch Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 30, Minibatch Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 40, Minibatch Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 50, Minibatch Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 60, Minibatch Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 70, Minibatch Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 80, Minibatch Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 90, Minibatch Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 100, Minibatch Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 110, Minibatch Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 120, Minibatch Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 130, Minibatch Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 140, Minibatch Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 150, Minibatch Loss= 0.6924, Training Accuracy= 0.516\n",
      "Epoch: 160, Minibatch Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 170, Minibatch Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 180, Minibatch Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 190, Minibatch Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 200, Minibatch Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 210, Minibatch Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 220, Minibatch Loss= 0.6921, Training Accuracy= 0.516\n",
      "Epoch: 230, Minibatch Loss= 0.6920, Training Accuracy= 0.516\n",
      "Epoch: 240, Minibatch Loss= 0.6919, Training Accuracy= 0.516\n",
      "Epoch: 250, Minibatch Loss= 0.6918, Training Accuracy= 0.516\n",
      "Epoch: 260, Minibatch Loss= 0.6917, Training Accuracy= 0.516\n",
      "Epoch: 270, Minibatch Loss= 0.6915, Training Accuracy= 0.516\n",
      "Epoch: 280, Minibatch Loss= 0.6913, Training Accuracy= 0.516\n",
      "Epoch: 290, Minibatch Loss= 0.6909, Training Accuracy= 0.516\n",
      "Epoch: 300, Minibatch Loss= 0.6904, Training Accuracy= 0.516\n",
      "Epoch: 310, Minibatch Loss= 0.6897, Training Accuracy= 0.516\n",
      "Epoch: 320, Minibatch Loss= 0.6884, Training Accuracy= 0.516\n",
      "Epoch: 330, Minibatch Loss= 0.6864, Training Accuracy= 0.516\n",
      "Epoch: 340, Minibatch Loss= 0.6826, Training Accuracy= 0.386\n",
      "Epoch: 350, Minibatch Loss= 0.6748, Training Accuracy= 0.628\n",
      "Epoch: 360, Minibatch Loss= 0.6570, Training Accuracy= 0.745\n",
      "Epoch: 370, Minibatch Loss= 0.6122, Training Accuracy= 0.745\n",
      "Epoch: 380, Minibatch Loss= 0.5175, Training Accuracy= 0.875\n",
      "Epoch: 390, Minibatch Loss= 0.3593, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.1945, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.1058, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0667, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0472, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0360, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0288, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0239, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0204, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0177, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0156, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 5: \n",
      "Epoch: 0, Minibatch Loss= 0.6966, Training Accuracy= 0.374\n",
      "Epoch: 10, Minibatch Loss= 0.6939, Training Accuracy= 0.755\n",
      "Epoch: 20, Minibatch Loss= 0.6936, Training Accuracy= 0.755\n",
      "Epoch: 30, Minibatch Loss= 0.6933, Training Accuracy= 0.755\n",
      "Epoch: 40, Minibatch Loss= 0.6932, Training Accuracy= 0.755\n",
      "Epoch: 50, Minibatch Loss= 0.6931, Training Accuracy= 0.755\n",
      "Epoch: 60, Minibatch Loss= 0.6931, Training Accuracy= 0.755\n",
      "Epoch: 70, Minibatch Loss= 0.6930, Training Accuracy= 0.755\n",
      "Epoch: 80, Minibatch Loss= 0.6930, Training Accuracy= 0.755\n",
      "Epoch: 90, Minibatch Loss= 0.6930, Training Accuracy= 0.755\n",
      "Epoch: 100, Minibatch Loss= 0.6930, Training Accuracy= 0.630\n",
      "Epoch: 110, Minibatch Loss= 0.6929, Training Accuracy= 0.630\n",
      "Epoch: 120, Minibatch Loss= 0.6929, Training Accuracy= 0.630\n",
      "Epoch: 130, Minibatch Loss= 0.6929, Training Accuracy= 0.630\n",
      "Epoch: 140, Minibatch Loss= 0.6929, Training Accuracy= 0.630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Minibatch Loss= 0.6929, Training Accuracy= 0.630\n",
      "Epoch: 160, Minibatch Loss= 0.6929, Training Accuracy= 0.630\n",
      "Epoch: 170, Minibatch Loss= 0.6928, Training Accuracy= 0.630\n",
      "Epoch: 180, Minibatch Loss= 0.6928, Training Accuracy= 0.630\n",
      "Epoch: 190, Minibatch Loss= 0.6928, Training Accuracy= 0.630\n",
      "Epoch: 200, Minibatch Loss= 0.6928, Training Accuracy= 0.630\n",
      "Epoch: 210, Minibatch Loss= 0.6927, Training Accuracy= 0.630\n",
      "Epoch: 220, Minibatch Loss= 0.6927, Training Accuracy= 0.630\n",
      "Epoch: 230, Minibatch Loss= 0.6926, Training Accuracy= 0.630\n",
      "Epoch: 240, Minibatch Loss= 0.6925, Training Accuracy= 0.630\n",
      "Epoch: 250, Minibatch Loss= 0.6925, Training Accuracy= 0.630\n",
      "Epoch: 260, Minibatch Loss= 0.6924, Training Accuracy= 0.630\n",
      "Epoch: 270, Minibatch Loss= 0.6922, Training Accuracy= 0.630\n",
      "Epoch: 280, Minibatch Loss= 0.6921, Training Accuracy= 0.630\n",
      "Epoch: 290, Minibatch Loss= 0.6919, Training Accuracy= 0.630\n",
      "Epoch: 300, Minibatch Loss= 0.6916, Training Accuracy= 0.630\n",
      "Epoch: 310, Minibatch Loss= 0.6912, Training Accuracy= 0.630\n",
      "Epoch: 320, Minibatch Loss= 0.6908, Training Accuracy= 0.755\n",
      "Epoch: 330, Minibatch Loss= 0.6901, Training Accuracy= 0.755\n",
      "Epoch: 340, Minibatch Loss= 0.6890, Training Accuracy= 0.755\n",
      "Epoch: 350, Minibatch Loss= 0.6873, Training Accuracy= 0.755\n",
      "Epoch: 360, Minibatch Loss= 0.6843, Training Accuracy= 0.755\n",
      "Epoch: 370, Minibatch Loss= 0.6785, Training Accuracy= 0.755\n",
      "Epoch: 380, Minibatch Loss= 0.6651, Training Accuracy= 0.755\n",
      "Epoch: 390, Minibatch Loss= 0.6296, Training Accuracy= 0.876\n",
      "Epoch: 400, Minibatch Loss= 0.5482, Training Accuracy= 0.876\n",
      "Epoch: 410, Minibatch Loss= 0.4441, Training Accuracy= 0.876\n",
      "Epoch: 420, Minibatch Loss= 0.3452, Training Accuracy= 0.876\n",
      "Epoch: 430, Minibatch Loss= 0.2444, Training Accuracy= 0.876\n",
      "Epoch: 440, Minibatch Loss= 0.1466, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0824, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0527, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0374, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0284, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0225, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 6: \n",
      "Epoch: 0, Minibatch Loss= 0.6951, Training Accuracy= 0.501\n",
      "Epoch: 10, Minibatch Loss= 0.6933, Training Accuracy= 0.751\n",
      "Epoch: 20, Minibatch Loss= 0.6933, Training Accuracy= 0.751\n",
      "Epoch: 30, Minibatch Loss= 0.6932, Training Accuracy= 0.751\n",
      "Epoch: 40, Minibatch Loss= 0.6932, Training Accuracy= 0.751\n",
      "Epoch: 50, Minibatch Loss= 0.6932, Training Accuracy= 0.751\n",
      "Epoch: 60, Minibatch Loss= 0.6931, Training Accuracy= 0.751\n",
      "Epoch: 70, Minibatch Loss= 0.6931, Training Accuracy= 0.751\n",
      "Epoch: 80, Minibatch Loss= 0.6931, Training Accuracy= 0.751\n",
      "Epoch: 90, Minibatch Loss= 0.6931, Training Accuracy= 0.751\n",
      "Epoch: 100, Minibatch Loss= 0.6931, Training Accuracy= 0.751\n",
      "Epoch: 110, Minibatch Loss= 0.6931, Training Accuracy= 0.751\n",
      "Epoch: 120, Minibatch Loss= 0.6931, Training Accuracy= 0.751\n",
      "Epoch: 130, Minibatch Loss= 0.6930, Training Accuracy= 0.751\n",
      "Epoch: 140, Minibatch Loss= 0.6930, Training Accuracy= 0.751\n",
      "Epoch: 150, Minibatch Loss= 0.6930, Training Accuracy= 0.751\n",
      "Epoch: 160, Minibatch Loss= 0.6930, Training Accuracy= 0.751\n",
      "Epoch: 170, Minibatch Loss= 0.6930, Training Accuracy= 0.751\n",
      "Epoch: 180, Minibatch Loss= 0.6929, Training Accuracy= 0.751\n",
      "Epoch: 190, Minibatch Loss= 0.6929, Training Accuracy= 0.751\n",
      "Epoch: 200, Minibatch Loss= 0.6929, Training Accuracy= 0.751\n",
      "Epoch: 210, Minibatch Loss= 0.6928, Training Accuracy= 0.751\n",
      "Epoch: 220, Minibatch Loss= 0.6927, Training Accuracy= 0.751\n",
      "Epoch: 230, Minibatch Loss= 0.6927, Training Accuracy= 0.751\n",
      "Epoch: 240, Minibatch Loss= 0.6926, Training Accuracy= 0.751\n",
      "Epoch: 250, Minibatch Loss= 0.6924, Training Accuracy= 0.751\n",
      "Epoch: 260, Minibatch Loss= 0.6922, Training Accuracy= 0.751\n",
      "Epoch: 270, Minibatch Loss= 0.6920, Training Accuracy= 0.751\n",
      "Epoch: 280, Minibatch Loss= 0.6915, Training Accuracy= 0.751\n",
      "Epoch: 290, Minibatch Loss= 0.6908, Training Accuracy= 0.751\n",
      "Epoch: 300, Minibatch Loss= 0.6895, Training Accuracy= 0.751\n",
      "Epoch: 310, Minibatch Loss= 0.6867, Training Accuracy= 0.751\n",
      "Epoch: 320, Minibatch Loss= 0.6797, Training Accuracy= 0.751\n",
      "Epoch: 330, Minibatch Loss= 0.6587, Training Accuracy= 0.751\n",
      "Epoch: 340, Minibatch Loss= 0.6111, Training Accuracy= 0.751\n",
      "Epoch: 350, Minibatch Loss= 0.5426, Training Accuracy= 0.751\n",
      "Epoch: 360, Minibatch Loss= 0.4546, Training Accuracy= 0.878\n",
      "Epoch: 370, Minibatch Loss= 0.3449, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.2299, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.1379, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0815, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0518, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0362, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0272, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0216, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0177, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0101, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 7: \n",
      "Epoch: 0, Minibatch Loss= 0.6957, Training Accuracy= 0.748\n",
      "Epoch: 10, Minibatch Loss= 0.6940, Training Accuracy= 0.618\n",
      "Epoch: 20, Minibatch Loss= 0.6934, Training Accuracy= 0.497\n",
      "Epoch: 30, Minibatch Loss= 0.6933, Training Accuracy= 0.501\n",
      "Epoch: 40, Minibatch Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 50, Minibatch Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 60, Minibatch Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 70, Minibatch Loss= 0.6932, Training Accuracy= 0.501\n",
      "Epoch: 80, Minibatch Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 90, Minibatch Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 100, Minibatch Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 110, Minibatch Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 120, Minibatch Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 130, Minibatch Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 140, Minibatch Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 150, Minibatch Loss= 0.6929, Training Accuracy= 0.501\n",
      "Epoch: 160, Minibatch Loss= 0.6929, Training Accuracy= 0.501\n",
      "Epoch: 170, Minibatch Loss= 0.6929, Training Accuracy= 0.501\n",
      "Epoch: 180, Minibatch Loss= 0.6928, Training Accuracy= 0.501\n",
      "Epoch: 190, Minibatch Loss= 0.6928, Training Accuracy= 0.501\n",
      "Epoch: 200, Minibatch Loss= 0.6927, Training Accuracy= 0.501\n",
      "Epoch: 210, Minibatch Loss= 0.6927, Training Accuracy= 0.501\n",
      "Epoch: 220, Minibatch Loss= 0.6926, Training Accuracy= 0.501\n",
      "Epoch: 230, Minibatch Loss= 0.6925, Training Accuracy= 0.501\n",
      "Epoch: 240, Minibatch Loss= 0.6924, Training Accuracy= 0.501\n",
      "Epoch: 250, Minibatch Loss= 0.6923, Training Accuracy= 0.501\n",
      "Epoch: 260, Minibatch Loss= 0.6921, Training Accuracy= 0.501\n",
      "Epoch: 270, Minibatch Loss= 0.6918, Training Accuracy= 0.501\n",
      "Epoch: 280, Minibatch Loss= 0.6914, Training Accuracy= 0.501\n",
      "Epoch: 290, Minibatch Loss= 0.6909, Training Accuracy= 0.501\n",
      "Epoch: 300, Minibatch Loss= 0.6898, Training Accuracy= 0.501\n",
      "Epoch: 310, Minibatch Loss= 0.6876, Training Accuracy= 0.624\n",
      "Epoch: 320, Minibatch Loss= 0.6817, Training Accuracy= 0.748\n",
      "Epoch: 330, Minibatch Loss= 0.6607, Training Accuracy= 0.748\n",
      "Epoch: 340, Minibatch Loss= 0.5962, Training Accuracy= 0.748\n",
      "Epoch: 350, Minibatch Loss= 0.5052, Training Accuracy= 0.875\n",
      "Epoch: 360, Minibatch Loss= 0.4114, Training Accuracy= 0.875\n",
      "Epoch: 370, Minibatch Loss= 0.2810, Training Accuracy= 0.875\n",
      "Epoch: 380, Minibatch Loss= 0.1434, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0845, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0580, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0434, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0343, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0281, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0236, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0202, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0176, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0155, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 480, Minibatch Loss= 0.0138, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 8: \n",
      "Epoch: 0, Minibatch Loss= 0.6939, Training Accuracy= 0.620\n",
      "Epoch: 10, Minibatch Loss= 0.6937, Training Accuracy= 0.373\n",
      "Epoch: 20, Minibatch Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 30, Minibatch Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 40, Minibatch Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 50, Minibatch Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 60, Minibatch Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 70, Minibatch Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 80, Minibatch Loss= 0.6928, Training Accuracy= 0.503\n",
      "Epoch: 90, Minibatch Loss= 0.6926, Training Accuracy= 0.503\n",
      "Epoch: 100, Minibatch Loss= 0.6923, Training Accuracy= 0.503\n",
      "Epoch: 110, Minibatch Loss= 0.6920, Training Accuracy= 0.633\n",
      "Epoch: 120, Minibatch Loss= 0.6914, Training Accuracy= 0.633\n",
      "Epoch: 130, Minibatch Loss= 0.6907, Training Accuracy= 0.633\n",
      "Epoch: 140, Minibatch Loss= 0.6896, Training Accuracy= 0.633\n",
      "Epoch: 150, Minibatch Loss= 0.6879, Training Accuracy= 0.503\n",
      "Epoch: 160, Minibatch Loss= 0.6849, Training Accuracy= 0.503\n",
      "Epoch: 170, Minibatch Loss= 0.6793, Training Accuracy= 0.750\n",
      "Epoch: 180, Minibatch Loss= 0.6687, Training Accuracy= 0.750\n",
      "Epoch: 190, Minibatch Loss= 0.6475, Training Accuracy= 0.750\n",
      "Epoch: 200, Minibatch Loss= 0.6068, Training Accuracy= 0.750\n",
      "Epoch: 210, Minibatch Loss= 0.5347, Training Accuracy= 0.750\n",
      "Epoch: 220, Minibatch Loss= 0.4097, Training Accuracy= 0.870\n",
      "Epoch: 230, Minibatch Loss= 0.2413, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.1326, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0827, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0580, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0439, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0351, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0290, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0246, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0213, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0188, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0168, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0151, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0137, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0087, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0078, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0070, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0066, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0063, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0061, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0058, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 9: \n",
      "Epoch: 0, Minibatch Loss= 0.6956, Training Accuracy= 0.378\n",
      "Epoch: 10, Minibatch Loss= 0.6934, Training Accuracy= 0.378\n",
      "Epoch: 20, Minibatch Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 30, Minibatch Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 40, Minibatch Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 50, Minibatch Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 60, Minibatch Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 70, Minibatch Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 80, Minibatch Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 90, Minibatch Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 100, Minibatch Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 110, Minibatch Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 120, Minibatch Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 130, Minibatch Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 140, Minibatch Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 150, Minibatch Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 160, Minibatch Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 170, Minibatch Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 180, Minibatch Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 190, Minibatch Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 200, Minibatch Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 210, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 220, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 230, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 240, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 250, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 260, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 270, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 280, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 290, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 300, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 310, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 320, Minibatch Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 330, Minibatch Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 340, Minibatch Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 350, Minibatch Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 360, Minibatch Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 370, Minibatch Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 380, Minibatch Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 390, Minibatch Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 400, Minibatch Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 410, Minibatch Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 420, Minibatch Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 430, Minibatch Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 440, Minibatch Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 450, Minibatch Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 460, Minibatch Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 470, Minibatch Loss= 0.6925, Training Accuracy= 0.510\n",
      "Epoch: 480, Minibatch Loss= 0.6925, Training Accuracy= 0.510\n",
      "Epoch: 490, Minibatch Loss= 0.6925, Training Accuracy= 0.510\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4955\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.02\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 500\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "minibatch_losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                minibatch_losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [1.0, 1.0, 0.62730002, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4955]\n",
      "mean of test_accuracies_10replications:  0.91228\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.00177898153663\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEoCAYAAACpaN3LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhXPOuaz+P/elTC2SpdOUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c28567850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minibatch_losses_1st_replication\n",
    "plt.plot(minibatch_losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, minibacth loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
