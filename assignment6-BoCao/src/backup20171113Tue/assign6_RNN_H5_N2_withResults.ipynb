{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 5\n",
    "N = 2\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Minibatch Loss= 0.7809, Training Accuracy= 0.508\n",
      "Epoch: 10, Minibatch Loss= 0.7210, Training Accuracy= 0.508\n",
      "Epoch: 20, Minibatch Loss= 0.7051, Training Accuracy= 0.508\n",
      "Epoch: 30, Minibatch Loss= 0.6980, Training Accuracy= 0.508\n",
      "Epoch: 40, Minibatch Loss= 0.6938, Training Accuracy= 0.756\n",
      "Epoch: 50, Minibatch Loss= 0.6905, Training Accuracy= 0.756\n",
      "Epoch: 60, Minibatch Loss= 0.6870, Training Accuracy= 0.756\n",
      "Epoch: 70, Minibatch Loss= 0.6818, Training Accuracy= 0.756\n",
      "Epoch: 80, Minibatch Loss= 0.6726, Training Accuracy= 0.756\n",
      "Epoch: 90, Minibatch Loss= 0.6546, Training Accuracy= 0.745\n",
      "Epoch: 100, Minibatch Loss= 0.6212, Training Accuracy= 0.745\n",
      "Epoch: 110, Minibatch Loss= 0.5677, Training Accuracy= 0.745\n",
      "Epoch: 120, Minibatch Loss= 0.4963, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.4138, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.3304, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.2559, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.1958, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.1502, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.1168, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0926, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0751, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0621, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0524, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0450, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0391, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0345, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0307, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0276, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0250, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0228, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0209, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0193, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0179, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0156, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0146, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0138, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0130, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0123, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0117, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0111, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0101, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0086, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0077, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 1: \n",
      "Epoch: 0, Minibatch Loss= 0.7239, Training Accuracy= 0.505\n",
      "Epoch: 10, Minibatch Loss= 0.6791, Training Accuracy= 0.499\n",
      "Epoch: 20, Minibatch Loss= 0.6416, Training Accuracy= 0.749\n",
      "Epoch: 30, Minibatch Loss= 0.5353, Training Accuracy= 1.000\n",
      "Epoch: 40, Minibatch Loss= 0.3337, Training Accuracy= 1.000\n",
      "Epoch: 50, Minibatch Loss= 0.2009, Training Accuracy= 1.000\n",
      "Epoch: 60, Minibatch Loss= 0.1356, Training Accuracy= 1.000\n",
      "Epoch: 70, Minibatch Loss= 0.1000, Training Accuracy= 1.000\n",
      "Epoch: 80, Minibatch Loss= 0.0783, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.0640, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.0538, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.0463, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.0405, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.0360, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.0322, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.0292, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.0266, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.0244, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0225, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0209, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0194, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0181, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0170, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0160, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0142, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0135, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0128, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0121, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0110, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0101, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0086, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0083, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0080, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0077, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0072, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0070, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0068, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0065, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0064, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0062, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0060, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0058, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0057, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 2: \n",
      "Epoch: 0, Minibatch Loss= 0.7188, Training Accuracy= 0.500\n",
      "Epoch: 10, Minibatch Loss= 0.5989, Training Accuracy= 1.000\n",
      "Epoch: 20, Minibatch Loss= 0.4711, Training Accuracy= 1.000\n",
      "Epoch: 30, Minibatch Loss= 0.3255, Training Accuracy= 1.000\n",
      "Epoch: 40, Minibatch Loss= 0.2189, Training Accuracy= 1.000\n",
      "Epoch: 50, Minibatch Loss= 0.1547, Training Accuracy= 1.000\n",
      "Epoch: 60, Minibatch Loss= 0.1160, Training Accuracy= 1.000\n",
      "Epoch: 70, Minibatch Loss= 0.0914, Training Accuracy= 1.000\n",
      "Epoch: 80, Minibatch Loss= 0.0747, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.0629, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.0541, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.0474, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.0421, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.0378, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.0343, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.0313, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.0288, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.0267, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0249, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0232, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0218, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0206, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0194, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0184, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0175, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0159, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0152, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0146, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0140, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0135, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0130, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330, Minibatch Loss= 0.0121, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0109, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0094, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0087, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0085, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0083, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0081, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0077, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 3: \n",
      "Epoch: 0, Minibatch Loss= 0.7695, Training Accuracy= 0.251\n",
      "Epoch: 10, Minibatch Loss= 0.6611, Training Accuracy= 0.756\n",
      "Epoch: 20, Minibatch Loss= 0.5936, Training Accuracy= 0.756\n",
      "Epoch: 30, Minibatch Loss= 0.4420, Training Accuracy= 1.000\n",
      "Epoch: 40, Minibatch Loss= 0.2954, Training Accuracy= 1.000\n",
      "Epoch: 50, Minibatch Loss= 0.2055, Training Accuracy= 1.000\n",
      "Epoch: 60, Minibatch Loss= 0.1522, Training Accuracy= 1.000\n",
      "Epoch: 70, Minibatch Loss= 0.1185, Training Accuracy= 1.000\n",
      "Epoch: 80, Minibatch Loss= 0.0959, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.0798, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.0680, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.0589, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.0518, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.0460, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.0413, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.0374, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.0340, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.0312, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0287, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0266, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0247, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0230, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0215, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0202, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0190, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0179, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0170, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0161, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0153, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0145, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0138, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0132, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0121, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0111, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0099, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0087, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0084, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0081, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0076, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0072, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0070, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 4: \n",
      "Epoch: 0, Minibatch Loss= 0.7676, Training Accuracy= 0.509\n",
      "Epoch: 10, Minibatch Loss= 0.6724, Training Accuracy= 0.509\n",
      "Epoch: 20, Minibatch Loss= 0.6397, Training Accuracy= 0.752\n",
      "Epoch: 30, Minibatch Loss= 0.5733, Training Accuracy= 1.000\n",
      "Epoch: 40, Minibatch Loss= 0.4504, Training Accuracy= 1.000\n",
      "Epoch: 50, Minibatch Loss= 0.3096, Training Accuracy= 1.000\n",
      "Epoch: 60, Minibatch Loss= 0.2105, Training Accuracy= 1.000\n",
      "Epoch: 70, Minibatch Loss= 0.1519, Training Accuracy= 1.000\n",
      "Epoch: 80, Minibatch Loss= 0.1167, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.0940, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.0783, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.0669, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.0583, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.0515, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.0461, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.0417, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.0380, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.0349, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0323, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0300, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0280, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0262, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0246, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0232, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0220, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0209, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0198, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0189, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0181, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0173, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0166, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0159, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0153, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0147, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0142, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0137, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0132, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0128, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0117, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0110, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0104, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0101, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0099, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0094, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 5: \n",
      "Epoch: 0, Minibatch Loss= 0.7323, Training Accuracy= 0.507\n",
      "Epoch: 10, Minibatch Loss= 0.6956, Training Accuracy= 0.507\n",
      "Epoch: 20, Minibatch Loss= 0.6892, Training Accuracy= 0.507\n",
      "Epoch: 30, Minibatch Loss= 0.6845, Training Accuracy= 0.507\n",
      "Epoch: 40, Minibatch Loss= 0.6756, Training Accuracy= 0.507\n",
      "Epoch: 50, Minibatch Loss= 0.6536, Training Accuracy= 0.507\n",
      "Epoch: 60, Minibatch Loss= 0.6012, Training Accuracy= 0.753\n",
      "Epoch: 70, Minibatch Loss= 0.5064, Training Accuracy= 0.753\n",
      "Epoch: 80, Minibatch Loss= 0.3913, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.2902, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.2168, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.1671, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.1332, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.1093, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.0920, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Minibatch Loss= 0.0789, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.0687, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.0606, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0541, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0487, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0441, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0402, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0369, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0340, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0315, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0292, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0272, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0255, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0239, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0224, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0211, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0200, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0189, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0179, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0170, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0162, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0155, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0148, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0141, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0135, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0130, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0115, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0111, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0097, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 6: \n",
      "Epoch: 0, Minibatch Loss= 0.7886, Training Accuracy= 0.490\n",
      "Epoch: 10, Minibatch Loss= 0.6770, Training Accuracy= 0.501\n",
      "Epoch: 20, Minibatch Loss= 0.6320, Training Accuracy= 0.756\n",
      "Epoch: 30, Minibatch Loss= 0.5257, Training Accuracy= 0.756\n",
      "Epoch: 40, Minibatch Loss= 0.3798, Training Accuracy= 1.000\n",
      "Epoch: 50, Minibatch Loss= 0.2657, Training Accuracy= 1.000\n",
      "Epoch: 60, Minibatch Loss= 0.1945, Training Accuracy= 1.000\n",
      "Epoch: 70, Minibatch Loss= 0.1495, Training Accuracy= 1.000\n",
      "Epoch: 80, Minibatch Loss= 0.1196, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.0987, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.0834, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.0719, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.0630, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.0559, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.0502, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.0455, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.0415, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.0381, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0353, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0328, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0306, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0287, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0269, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0254, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0241, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0228, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0217, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0207, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0198, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0189, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0181, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0174, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0161, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0155, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0145, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0140, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0136, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0127, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0117, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0114, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0111, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0105, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 7: \n",
      "Epoch: 0, Minibatch Loss= 0.9077, Training Accuracy= 0.495\n",
      "Epoch: 10, Minibatch Loss= 0.6612, Training Accuracy= 0.748\n",
      "Epoch: 20, Minibatch Loss= 0.6091, Training Accuracy= 0.748\n",
      "Epoch: 30, Minibatch Loss= 0.5001, Training Accuracy= 1.000\n",
      "Epoch: 40, Minibatch Loss= 0.3663, Training Accuracy= 1.000\n",
      "Epoch: 50, Minibatch Loss= 0.2605, Training Accuracy= 1.000\n",
      "Epoch: 60, Minibatch Loss= 0.1914, Training Accuracy= 1.000\n",
      "Epoch: 70, Minibatch Loss= 0.1471, Training Accuracy= 1.000\n",
      "Epoch: 80, Minibatch Loss= 0.1175, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.0966, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.0811, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.0694, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.0602, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.0529, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.0469, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.0420, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.0379, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.0344, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0314, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0289, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0267, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0247, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0230, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0215, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0201, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0189, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0179, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0169, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0160, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0152, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0144, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0138, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0132, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0121, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0111, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0099, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0087, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0084, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0077, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 480, Minibatch Loss= 0.0075, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0073, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 8: \n",
      "Epoch: 0, Minibatch Loss= 0.9515, Training Accuracy= 0.494\n",
      "Epoch: 10, Minibatch Loss= 0.7002, Training Accuracy= 0.493\n",
      "Epoch: 20, Minibatch Loss= 0.6928, Training Accuracy= 0.493\n",
      "Epoch: 30, Minibatch Loss= 0.6884, Training Accuracy= 0.493\n",
      "Epoch: 40, Minibatch Loss= 0.6836, Training Accuracy= 0.748\n",
      "Epoch: 50, Minibatch Loss= 0.6770, Training Accuracy= 0.748\n",
      "Epoch: 60, Minibatch Loss= 0.6663, Training Accuracy= 0.748\n",
      "Epoch: 70, Minibatch Loss= 0.6473, Training Accuracy= 0.748\n",
      "Epoch: 80, Minibatch Loss= 0.6107, Training Accuracy= 0.748\n",
      "Epoch: 90, Minibatch Loss= 0.5422, Training Accuracy= 0.748\n",
      "Epoch: 100, Minibatch Loss= 0.4365, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.3245, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.2384, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.1807, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.1423, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.1158, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.0967, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.0824, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0714, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0628, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0558, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0501, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0454, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0414, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0379, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0350, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0325, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0302, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0283, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0265, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0250, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0236, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0223, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0212, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0201, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0192, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0183, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0175, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0168, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0161, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0155, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0144, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0139, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0134, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0129, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0121, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0114, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 9: \n",
      "Epoch: 0, Minibatch Loss= 0.7957, Training Accuracy= 0.501\n",
      "Epoch: 10, Minibatch Loss= 0.6914, Training Accuracy= 0.503\n",
      "Epoch: 20, Minibatch Loss= 0.6776, Training Accuracy= 0.503\n",
      "Epoch: 30, Minibatch Loss= 0.6663, Training Accuracy= 0.503\n",
      "Epoch: 40, Minibatch Loss= 0.6521, Training Accuracy= 0.503\n",
      "Epoch: 50, Minibatch Loss= 0.6322, Training Accuracy= 0.503\n",
      "Epoch: 60, Minibatch Loss= 0.6063, Training Accuracy= 0.753\n",
      "Epoch: 70, Minibatch Loss= 0.5763, Training Accuracy= 0.753\n",
      "Epoch: 80, Minibatch Loss= 0.5442, Training Accuracy= 0.753\n",
      "Epoch: 90, Minibatch Loss= 0.5118, Training Accuracy= 0.753\n",
      "Epoch: 100, Minibatch Loss= 0.4806, Training Accuracy= 0.753\n",
      "Epoch: 110, Minibatch Loss= 0.4518, Training Accuracy= 0.753\n",
      "Epoch: 120, Minibatch Loss= 0.4249, Training Accuracy= 0.753\n",
      "Epoch: 130, Minibatch Loss= 0.3971, Training Accuracy= 0.753\n",
      "Epoch: 140, Minibatch Loss= 0.3647, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.3257, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.2825, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.2403, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.2015, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.1654, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.1330, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.1061, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0852, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0695, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0576, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0487, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0418, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0363, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0320, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0285, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0257, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0233, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0213, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0195, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0181, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0168, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0157, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0147, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0138, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0130, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0123, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0117, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0111, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0101, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0089, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0085, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.0035\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 500\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "minibatch_losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                minibatch_losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "mean of test_accuracies_10replications:  1.0\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.0\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEoCAYAAACpaN3LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzrlqebJwzjlXLU8WzjnnqvX/JXUCa0ov9y0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7facd30cd650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minibatch_losses_1st_replication\n",
    "plt.plot(minibatch_losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, minibacth loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
