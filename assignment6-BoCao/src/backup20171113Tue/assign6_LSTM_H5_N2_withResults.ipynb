{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 5\n",
    "N = 2\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Minibatch Loss= 0.6961, Training Accuracy= 0.505\n",
      "Epoch: 10, Minibatch Loss= 0.6855, Training Accuracy= 0.493\n",
      "Epoch: 20, Minibatch Loss= 0.6788, Training Accuracy= 0.493\n",
      "Epoch: 30, Minibatch Loss= 0.6693, Training Accuracy= 0.493\n",
      "Epoch: 40, Minibatch Loss= 0.6564, Training Accuracy= 0.493\n",
      "Epoch: 50, Minibatch Loss= 0.6380, Training Accuracy= 0.493\n",
      "Epoch: 60, Minibatch Loss= 0.6111, Training Accuracy= 0.745\n",
      "Epoch: 70, Minibatch Loss= 0.5730, Training Accuracy= 0.745\n",
      "Epoch: 80, Minibatch Loss= 0.5217, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.4574, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.3820, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.3019, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.2280, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.1703, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.1299, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.1026, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.0836, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.0701, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0600, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0523, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0462, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0412, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0372, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0338, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0310, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0285, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0264, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0246, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0230, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0215, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0203, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0191, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0181, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0172, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0163, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0156, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0149, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0142, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0136, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0121, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0116, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0112, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0105, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0101, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0095, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0092, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 1: \n",
      "Epoch: 0, Minibatch Loss= 2.4119, Training Accuracy= 0.503\n",
      "Epoch: 10, Minibatch Loss= 0.7000, Training Accuracy= 0.503\n",
      "Epoch: 20, Minibatch Loss= 0.6916, Training Accuracy= 0.500\n",
      "Epoch: 30, Minibatch Loss= 0.6901, Training Accuracy= 0.500\n",
      "Epoch: 40, Minibatch Loss= 0.6883, Training Accuracy= 0.500\n",
      "Epoch: 50, Minibatch Loss= 0.6861, Training Accuracy= 0.500\n",
      "Epoch: 60, Minibatch Loss= 0.6831, Training Accuracy= 0.755\n",
      "Epoch: 70, Minibatch Loss= 0.6789, Training Accuracy= 0.755\n",
      "Epoch: 80, Minibatch Loss= 0.6731, Training Accuracy= 0.755\n",
      "Epoch: 90, Minibatch Loss= 0.6643, Training Accuracy= 0.507\n",
      "Epoch: 100, Minibatch Loss= 0.6502, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.6261, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.5851, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.5198, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.4302, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.3334, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.2512, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.1913, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.1496, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.1203, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0993, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0837, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0718, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0625, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0551, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0491, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0441, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0399, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0364, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0334, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0308, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0285, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0265, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0248, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0232, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0218, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0206, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0194, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0184, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0175, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0166, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0158, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0151, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0145, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0139, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0133, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0128, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0123, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0118, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0114, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 2: \n",
      "Epoch: 0, Minibatch Loss= 1.0255, Training Accuracy= 0.501\n",
      "Epoch: 10, Minibatch Loss= 0.6967, Training Accuracy= 0.504\n",
      "Epoch: 20, Minibatch Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 30, Minibatch Loss= 0.6907, Training Accuracy= 0.504\n",
      "Epoch: 40, Minibatch Loss= 0.6882, Training Accuracy= 0.501\n",
      "Epoch: 50, Minibatch Loss= 0.6857, Training Accuracy= 0.501\n",
      "Epoch: 60, Minibatch Loss= 0.6828, Training Accuracy= 0.501\n",
      "Epoch: 70, Minibatch Loss= 0.6796, Training Accuracy= 0.501\n",
      "Epoch: 80, Minibatch Loss= 0.6757, Training Accuracy= 0.501\n",
      "Epoch: 90, Minibatch Loss= 0.6706, Training Accuracy= 0.501\n",
      "Epoch: 100, Minibatch Loss= 0.6640, Training Accuracy= 0.501\n",
      "Epoch: 110, Minibatch Loss= 0.6548, Training Accuracy= 0.501\n",
      "Epoch: 120, Minibatch Loss= 0.6415, Training Accuracy= 0.501\n",
      "Epoch: 130, Minibatch Loss= 0.6221, Training Accuracy= 0.501\n",
      "Epoch: 140, Minibatch Loss= 0.5941, Training Accuracy= 0.752\n",
      "Epoch: 150, Minibatch Loss= 0.5543, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.5002, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.4309, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.3502, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.2680, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.1975, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.1461, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.1116, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0885, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0725, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0610, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0524, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0458, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0406, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0363, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0328, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0299, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0274, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330, Minibatch Loss= 0.0253, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0235, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0219, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0204, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0192, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0181, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0171, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0162, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0153, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0146, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0139, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0133, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0127, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0122, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0117, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 3: \n",
      "Epoch: 0, Minibatch Loss= 1.2608, Training Accuracy= 0.503\n",
      "Epoch: 10, Minibatch Loss= 0.6913, Training Accuracy= 0.249\n",
      "Epoch: 20, Minibatch Loss= 0.6890, Training Accuracy= 0.500\n",
      "Epoch: 30, Minibatch Loss= 0.6864, Training Accuracy= 0.500\n",
      "Epoch: 40, Minibatch Loss= 0.6833, Training Accuracy= 0.500\n",
      "Epoch: 50, Minibatch Loss= 0.6794, Training Accuracy= 1.000\n",
      "Epoch: 60, Minibatch Loss= 0.6741, Training Accuracy= 1.000\n",
      "Epoch: 70, Minibatch Loss= 0.6665, Training Accuracy= 1.000\n",
      "Epoch: 80, Minibatch Loss= 0.6548, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.6354, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.6018, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.5449, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.4588, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.3547, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.2602, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.1910, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.1445, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.1132, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0916, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0760, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0644, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0556, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0486, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0431, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0385, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0348, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0316, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0289, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0266, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0246, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0229, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0214, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0200, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0188, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0177, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0158, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0143, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0136, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0130, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0119, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0114, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0110, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0102, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0098, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0095, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0091, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 4: \n",
      "Epoch: 0, Minibatch Loss= 0.7661, Training Accuracy= 0.504\n",
      "Epoch: 10, Minibatch Loss= 0.6894, Training Accuracy= 0.503\n",
      "Epoch: 20, Minibatch Loss= 0.6868, Training Accuracy= 0.503\n",
      "Epoch: 30, Minibatch Loss= 0.6838, Training Accuracy= 0.756\n",
      "Epoch: 40, Minibatch Loss= 0.6801, Training Accuracy= 0.756\n",
      "Epoch: 50, Minibatch Loss= 0.6750, Training Accuracy= 1.000\n",
      "Epoch: 60, Minibatch Loss= 0.6678, Training Accuracy= 1.000\n",
      "Epoch: 70, Minibatch Loss= 0.6568, Training Accuracy= 1.000\n",
      "Epoch: 80, Minibatch Loss= 0.6393, Training Accuracy= 1.000\n",
      "Epoch: 90, Minibatch Loss= 0.6104, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.5621, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.4864, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.3885, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.2928, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.2186, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.1667, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.1310, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.1060, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0880, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0746, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0643, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0563, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0499, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0447, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0403, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0367, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0336, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0310, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0287, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0267, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0249, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0234, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0220, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0208, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0196, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0186, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0177, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0169, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0161, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0154, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0147, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0141, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0136, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0126, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0121, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0117, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0109, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0106, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 5: \n",
      "Epoch: 0, Minibatch Loss= 0.7028, Training Accuracy= 0.246\n",
      "Epoch: 10, Minibatch Loss= 0.6965, Training Accuracy= 0.494\n",
      "Epoch: 20, Minibatch Loss= 0.6941, Training Accuracy= 0.494\n",
      "Epoch: 30, Minibatch Loss= 0.6923, Training Accuracy= 0.494\n",
      "Epoch: 40, Minibatch Loss= 0.6908, Training Accuracy= 0.494\n",
      "Epoch: 50, Minibatch Loss= 0.6892, Training Accuracy= 0.754\n",
      "Epoch: 60, Minibatch Loss= 0.6874, Training Accuracy= 1.000\n",
      "Epoch: 70, Minibatch Loss= 0.6852, Training Accuracy= 0.754\n",
      "Epoch: 80, Minibatch Loss= 0.6824, Training Accuracy= 0.754\n",
      "Epoch: 90, Minibatch Loss= 0.6786, Training Accuracy= 0.754\n",
      "Epoch: 100, Minibatch Loss= 0.6734, Training Accuracy= 0.754\n",
      "Epoch: 110, Minibatch Loss= 0.6658, Training Accuracy= 0.754\n",
      "Epoch: 120, Minibatch Loss= 0.6542, Training Accuracy= 0.754\n",
      "Epoch: 130, Minibatch Loss= 0.6358, Training Accuracy= 0.754\n",
      "Epoch: 140, Minibatch Loss= 0.6058, Training Accuracy= 0.754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, Minibatch Loss= 0.5577, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.4867, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.3982, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.3087, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.2335, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.1779, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.1390, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.1116, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0920, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0775, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0665, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0579, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0511, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0456, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0410, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0372, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0340, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0313, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0289, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0268, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0250, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0234, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0220, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0207, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0196, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0185, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0176, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0159, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0152, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0145, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0139, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0134, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0128, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0124, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 6: \n",
      "Epoch: 0, Minibatch Loss= 1.1718, Training Accuracy= 0.498\n",
      "Epoch: 10, Minibatch Loss= 0.6994, Training Accuracy= 0.498\n",
      "Epoch: 20, Minibatch Loss= 0.6913, Training Accuracy= 0.250\n",
      "Epoch: 30, Minibatch Loss= 0.6908, Training Accuracy= 0.498\n",
      "Epoch: 40, Minibatch Loss= 0.6904, Training Accuracy= 0.498\n",
      "Epoch: 50, Minibatch Loss= 0.6900, Training Accuracy= 0.498\n",
      "Epoch: 60, Minibatch Loss= 0.6896, Training Accuracy= 0.498\n",
      "Epoch: 70, Minibatch Loss= 0.6890, Training Accuracy= 0.498\n",
      "Epoch: 80, Minibatch Loss= 0.6885, Training Accuracy= 0.498\n",
      "Epoch: 90, Minibatch Loss= 0.6879, Training Accuracy= 0.498\n",
      "Epoch: 100, Minibatch Loss= 0.6871, Training Accuracy= 0.498\n",
      "Epoch: 110, Minibatch Loss= 0.6863, Training Accuracy= 0.498\n",
      "Epoch: 120, Minibatch Loss= 0.6854, Training Accuracy= 0.498\n",
      "Epoch: 130, Minibatch Loss= 0.6844, Training Accuracy= 0.498\n",
      "Epoch: 140, Minibatch Loss= 0.6832, Training Accuracy= 0.498\n",
      "Epoch: 150, Minibatch Loss= 0.6818, Training Accuracy= 0.498\n",
      "Epoch: 160, Minibatch Loss= 0.6801, Training Accuracy= 0.498\n",
      "Epoch: 170, Minibatch Loss= 0.6782, Training Accuracy= 0.752\n",
      "Epoch: 180, Minibatch Loss= 0.6759, Training Accuracy= 0.752\n",
      "Epoch: 190, Minibatch Loss= 0.6732, Training Accuracy= 0.752\n",
      "Epoch: 200, Minibatch Loss= 0.6699, Training Accuracy= 0.752\n",
      "Epoch: 210, Minibatch Loss= 0.6659, Training Accuracy= 0.752\n",
      "Epoch: 220, Minibatch Loss= 0.6612, Training Accuracy= 0.752\n",
      "Epoch: 230, Minibatch Loss= 0.6554, Training Accuracy= 0.752\n",
      "Epoch: 240, Minibatch Loss= 0.6484, Training Accuracy= 0.752\n",
      "Epoch: 250, Minibatch Loss= 0.6401, Training Accuracy= 0.752\n",
      "Epoch: 260, Minibatch Loss= 0.6302, Training Accuracy= 0.752\n",
      "Epoch: 270, Minibatch Loss= 0.6186, Training Accuracy= 0.752\n",
      "Epoch: 280, Minibatch Loss= 0.6053, Training Accuracy= 0.752\n",
      "Epoch: 290, Minibatch Loss= 0.5905, Training Accuracy= 0.752\n",
      "Epoch: 300, Minibatch Loss= 0.5743, Training Accuracy= 0.752\n",
      "Epoch: 310, Minibatch Loss= 0.5571, Training Accuracy= 0.752\n",
      "Epoch: 320, Minibatch Loss= 0.5395, Training Accuracy= 0.752\n",
      "Epoch: 330, Minibatch Loss= 0.5218, Training Accuracy= 0.752\n",
      "Epoch: 340, Minibatch Loss= 0.5044, Training Accuracy= 0.752\n",
      "Epoch: 350, Minibatch Loss= 0.4874, Training Accuracy= 0.752\n",
      "Epoch: 360, Minibatch Loss= 0.4711, Training Accuracy= 0.752\n",
      "Epoch: 370, Minibatch Loss= 0.4552, Training Accuracy= 0.752\n",
      "Epoch: 380, Minibatch Loss= 0.4397, Training Accuracy= 0.752\n",
      "Epoch: 390, Minibatch Loss= 0.4243, Training Accuracy= 0.752\n",
      "Epoch: 400, Minibatch Loss= 0.4085, Training Accuracy= 0.752\n",
      "Epoch: 410, Minibatch Loss= 0.3920, Training Accuracy= 0.752\n",
      "Epoch: 420, Minibatch Loss= 0.3742, Training Accuracy= 0.752\n",
      "Epoch: 430, Minibatch Loss= 0.3547, Training Accuracy= 0.752\n",
      "Epoch: 440, Minibatch Loss= 0.3331, Training Accuracy= 0.752\n",
      "Epoch: 450, Minibatch Loss= 0.3092, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.2831, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.2554, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.2271, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.1994, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 7: \n",
      "Epoch: 0, Minibatch Loss= 1.1023, Training Accuracy= 0.504\n",
      "Epoch: 10, Minibatch Loss= 0.6951, Training Accuracy= 0.507\n",
      "Epoch: 20, Minibatch Loss= 0.6943, Training Accuracy= 0.507\n",
      "Epoch: 30, Minibatch Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 40, Minibatch Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 50, Minibatch Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 60, Minibatch Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 70, Minibatch Loss= 0.6926, Training Accuracy= 0.507\n",
      "Epoch: 80, Minibatch Loss= 0.6924, Training Accuracy= 0.507\n",
      "Epoch: 90, Minibatch Loss= 0.6922, Training Accuracy= 0.507\n",
      "Epoch: 100, Minibatch Loss= 0.6920, Training Accuracy= 0.507\n",
      "Epoch: 110, Minibatch Loss= 0.6917, Training Accuracy= 0.507\n",
      "Epoch: 120, Minibatch Loss= 0.6915, Training Accuracy= 0.507\n",
      "Epoch: 130, Minibatch Loss= 0.6912, Training Accuracy= 0.507\n",
      "Epoch: 140, Minibatch Loss= 0.6909, Training Accuracy= 0.507\n",
      "Epoch: 150, Minibatch Loss= 0.6905, Training Accuracy= 0.507\n",
      "Epoch: 160, Minibatch Loss= 0.6901, Training Accuracy= 0.507\n",
      "Epoch: 170, Minibatch Loss= 0.6897, Training Accuracy= 0.507\n",
      "Epoch: 180, Minibatch Loss= 0.6891, Training Accuracy= 0.507\n",
      "Epoch: 190, Minibatch Loss= 0.6884, Training Accuracy= 0.507\n",
      "Epoch: 200, Minibatch Loss= 0.6876, Training Accuracy= 0.507\n",
      "Epoch: 210, Minibatch Loss= 0.6866, Training Accuracy= 0.507\n",
      "Epoch: 220, Minibatch Loss= 0.6853, Training Accuracy= 0.507\n",
      "Epoch: 230, Minibatch Loss= 0.6836, Training Accuracy= 0.507\n",
      "Epoch: 240, Minibatch Loss= 0.6814, Training Accuracy= 0.757\n",
      "Epoch: 250, Minibatch Loss= 0.6783, Training Accuracy= 0.757\n",
      "Epoch: 260, Minibatch Loss= 0.6740, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.6676, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.6577, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.6419, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.6151, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.5698, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.4990, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.4076, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.3150, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.2381, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.1810, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.1402, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.1110, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0898, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0740, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0621, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0529, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0457, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0399, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0352, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0314, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0282, Training Accuracy= 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 480, Minibatch Loss= 0.0255, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0232, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 8: \n",
      "Epoch: 0, Minibatch Loss= 0.8876, Training Accuracy= 0.486\n",
      "Epoch: 10, Minibatch Loss= 0.6827, Training Accuracy= 0.514\n",
      "Epoch: 20, Minibatch Loss= 0.6781, Training Accuracy= 0.754\n",
      "Epoch: 30, Minibatch Loss= 0.6719, Training Accuracy= 0.754\n",
      "Epoch: 40, Minibatch Loss= 0.6631, Training Accuracy= 0.754\n",
      "Epoch: 50, Minibatch Loss= 0.6500, Training Accuracy= 0.754\n",
      "Epoch: 60, Minibatch Loss= 0.6299, Training Accuracy= 0.754\n",
      "Epoch: 70, Minibatch Loss= 0.5977, Training Accuracy= 0.754\n",
      "Epoch: 80, Minibatch Loss= 0.5463, Training Accuracy= 0.754\n",
      "Epoch: 90, Minibatch Loss= 0.4682, Training Accuracy= 1.000\n",
      "Epoch: 100, Minibatch Loss= 0.3675, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.2679, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.1915, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.1401, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.1065, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.0840, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.0684, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.0571, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0486, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0421, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0370, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0329, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0295, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0267, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0244, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0224, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0207, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0192, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0178, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0167, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0157, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0147, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0139, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0132, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0119, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0113, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0108, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0099, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0095, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0091, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0088, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0085, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0082, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0079, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0076, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0074, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0071, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0069, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n",
      "Replication: 9: \n",
      "Epoch: 0, Minibatch Loss= 0.7249, Training Accuracy= 0.502\n",
      "Epoch: 10, Minibatch Loss= 0.6928, Training Accuracy= 0.502\n",
      "Epoch: 20, Minibatch Loss= 0.6912, Training Accuracy= 0.747\n",
      "Epoch: 30, Minibatch Loss= 0.6889, Training Accuracy= 0.747\n",
      "Epoch: 40, Minibatch Loss= 0.6847, Training Accuracy= 0.747\n",
      "Epoch: 50, Minibatch Loss= 0.6756, Training Accuracy= 0.747\n",
      "Epoch: 60, Minibatch Loss= 0.6571, Training Accuracy= 0.747\n",
      "Epoch: 70, Minibatch Loss= 0.6311, Training Accuracy= 0.747\n",
      "Epoch: 80, Minibatch Loss= 0.5979, Training Accuracy= 0.747\n",
      "Epoch: 90, Minibatch Loss= 0.5553, Training Accuracy= 0.747\n",
      "Epoch: 100, Minibatch Loss= 0.5035, Training Accuracy= 1.000\n",
      "Epoch: 110, Minibatch Loss= 0.4437, Training Accuracy= 1.000\n",
      "Epoch: 120, Minibatch Loss= 0.3760, Training Accuracy= 1.000\n",
      "Epoch: 130, Minibatch Loss= 0.3016, Training Accuracy= 1.000\n",
      "Epoch: 140, Minibatch Loss= 0.2285, Training Accuracy= 1.000\n",
      "Epoch: 150, Minibatch Loss= 0.1687, Training Accuracy= 1.000\n",
      "Epoch: 160, Minibatch Loss= 0.1264, Training Accuracy= 1.000\n",
      "Epoch: 170, Minibatch Loss= 0.0982, Training Accuracy= 1.000\n",
      "Epoch: 180, Minibatch Loss= 0.0792, Training Accuracy= 1.000\n",
      "Epoch: 190, Minibatch Loss= 0.0658, Training Accuracy= 1.000\n",
      "Epoch: 200, Minibatch Loss= 0.0559, Training Accuracy= 1.000\n",
      "Epoch: 210, Minibatch Loss= 0.0485, Training Accuracy= 1.000\n",
      "Epoch: 220, Minibatch Loss= 0.0427, Training Accuracy= 1.000\n",
      "Epoch: 230, Minibatch Loss= 0.0380, Training Accuracy= 1.000\n",
      "Epoch: 240, Minibatch Loss= 0.0342, Training Accuracy= 1.000\n",
      "Epoch: 250, Minibatch Loss= 0.0311, Training Accuracy= 1.000\n",
      "Epoch: 260, Minibatch Loss= 0.0285, Training Accuracy= 1.000\n",
      "Epoch: 270, Minibatch Loss= 0.0262, Training Accuracy= 1.000\n",
      "Epoch: 280, Minibatch Loss= 0.0243, Training Accuracy= 1.000\n",
      "Epoch: 290, Minibatch Loss= 0.0226, Training Accuracy= 1.000\n",
      "Epoch: 300, Minibatch Loss= 0.0211, Training Accuracy= 1.000\n",
      "Epoch: 310, Minibatch Loss= 0.0198, Training Accuracy= 1.000\n",
      "Epoch: 320, Minibatch Loss= 0.0186, Training Accuracy= 1.000\n",
      "Epoch: 330, Minibatch Loss= 0.0176, Training Accuracy= 1.000\n",
      "Epoch: 340, Minibatch Loss= 0.0166, Training Accuracy= 1.000\n",
      "Epoch: 350, Minibatch Loss= 0.0158, Training Accuracy= 1.000\n",
      "Epoch: 360, Minibatch Loss= 0.0150, Training Accuracy= 1.000\n",
      "Epoch: 370, Minibatch Loss= 0.0143, Training Accuracy= 1.000\n",
      "Epoch: 380, Minibatch Loss= 0.0136, Training Accuracy= 1.000\n",
      "Epoch: 390, Minibatch Loss= 0.0131, Training Accuracy= 1.000\n",
      "Epoch: 400, Minibatch Loss= 0.0125, Training Accuracy= 1.000\n",
      "Epoch: 410, Minibatch Loss= 0.0120, Training Accuracy= 1.000\n",
      "Epoch: 420, Minibatch Loss= 0.0115, Training Accuracy= 1.000\n",
      "Epoch: 430, Minibatch Loss= 0.0111, Training Accuracy= 1.000\n",
      "Epoch: 440, Minibatch Loss= 0.0107, Training Accuracy= 1.000\n",
      "Epoch: 450, Minibatch Loss= 0.0103, Training Accuracy= 1.000\n",
      "Epoch: 460, Minibatch Loss= 0.0100, Training Accuracy= 1.000\n",
      "Epoch: 470, Minibatch Loss= 0.0096, Training Accuracy= 1.000\n",
      "Epoch: 480, Minibatch Loss= 0.0093, Training Accuracy= 1.000\n",
      "Epoch: 490, Minibatch Loss= 0.0090, Training Accuracy= 1.000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.005\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 500\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "minibatch_losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                minibatch_losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Minibatch Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "mean of test_accuracies_10replications:  1.0\n",
      "standard deviation of test_accuracies_10replications_std_mean:  0.0\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEoCAYAAACpaN3LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XecVPX1//HXe3fpVYooVVDAjmUj\nCiZRjIrGbkxsMcYY/SYGRY0tJmo0MWosP2OM3WjUGDU2xB57QUMxoIgooghI70WWLef3x70Lszuz\ns3fL3Wnn+Xjcx8z9zL0zZy/snL2fKjPDOeecS6co0wE455zLfp4snHPO1cuThXPOuXp5snDOOVcv\nTxbOOefq5cnCOedcvTxZOOecq5cnC+ecc/Uqqe8ASSOBK4AB4fECzMwGxRuac865bKH6RnBL+gQ4\nF5gMVFaXm9myeENzzjmXLeq9swBWmdnzsUfinHMua0W5s7gGKAaeAMqqy81sSryhOeecyxZRksVr\nKYrNzEbFE5JzzrlsU2+ycM455+rtOiupl6R7JD0f7u8o6Wfxh+accy5bRBlncR/wItA73P8UGBtX\nQM4557JPlGTRw8weBaoAzKyChC60zjnn8l+UZLFOUnfAACTtDayKNSrnnHNZJco4i/OAccC2kt4B\negI/iDUq55xzWSVSbyhJJcBQgqk+ZppZedyBOeecyx5RekO1By4GxprZR8A2kg6LPTLnnHNZI0qb\nxd+BjcA+4f484A+xReSccy7rREkW25rZdUA5gJl9Q1Ad5ZxLIGk/SfMS9qdL2i+Gz3le0k+a+32d\nSydKstgoqR2be0NtS8IcUc7FTdKvJE2SVCbpvgac96Wk78UYWlpmtpOZvd6U95B0haQHa73vIWZ2\nf5OCc66BovSGuhx4Aegn6SFgJHBqnEE5V8vXBFWfBwPt4voQSSXhOCLnXC1p7ywkCfgEOIYgQTwM\nlDb1ryXnGsLMnjCzp4CkNVQk9ZA0XtJKScslvSWpSNIDQH/gGUlrJV2Y4tz9JM2TdJGkhQTtc0g6\nTNL/wvd8V9KuCed8KekSSR9LWiHp75Lapoo78c5GUrGk30j6XNIaSZMl9Qtfu1nSXEmrw/Jvh+Wj\ngd8APwp/hqlh+euSTg+fF0n6raQ5khZL+oekLuFr20gyST+R9JWkpZIubfy/hCtkaZOFBf1qnzKz\nZWb2rJmNN7OlLRSbc1GcT9DpoifQi+DL1czsx8BXwOFm1jFsd0tlK6AbwUqQZ0jaA7gXOBPoDtwB\njJPUJuGckwjucrYFhgC/jRDnecAJwKFAZ+A0YH342kRgtzCOfwKPSWprZi8AVwOPhD/DsBTve2q4\n7Q8MAjoCf611zL4EXd8PAC6TtEOEeJ2rIUqbxXuSvhV7JM41TjmwNTDAzMrN7C1r2FTKVcDlZlYW\ndt74OXCHmb1vZpVh20AZsHfCOX81s7lmthz4I0ESqM/pwG/NbKYFplavNmlmD4Z/kFWY2Q1AG4Iv\n9yhOAm40s9lmtha4BDg+HBtV7fdm9o2ZTQWmAqmSjnNpRUkW+wMTwtvnaZI+lDQt7sCci+jPwCzg\nJUmzJV3cwPOXmNmGhP0BwPlhFdRKSSuBfmyeSBNgbsLzObVeq0s/4PNUL0g6X9IMSavCz+sC9IgY\nf+8whsR4SgjusqotTHi+nuDuw7kGidLAfUjsUTjXSGa2hqAq6nxJOwGvSZpoZq8Q9uCr7y1q7c8F\n/mhmf0xzTr+E5/0JGuDrM5eg2uqjxMKwfeIigiqi6WZWJWkFm7un1/czfE2Q4BLjqQAWAX0jxOVc\nJFHuLNak2KL8cjjXLCSVhI3IxUCxpLbV1SxhY/R2YWeM1QQzIlfPiryIoB6/Ie4C/k/ScAU6SPq+\npE4Jx5wlqa+kbgRtJI9EeN+7gaskDQ7fd9dwgs5OBF/uS4ASSZcRtGlUW0Qwa0Jdv6sPA+dKGiip\nI5vbOLxXl2tWUZLFFIL/yJ8Cn4XPv5A0RdKecQbnXOi3wDcE086cHD6vblQeDPwHWAtMAP6W0Fvv\nT8Bvw+qkX0f5IDObRNBu8VdgBUEV16m1Dvsn8BIwO9yizGhwI/BoeN5q4B6CbsAvAs8T/H7NATZQ\ns5rrsfBxmaRU697fCzwAvAl8EZ4/JkI8zjVIlDW4bweeNLMXw/2DgNEE//FvNrPhsUfpXJaQ9CVw\nupn9J9OxONeSotxZlFYnCgAzewn4jpm9R9BrwznnXJ6LkiyWh4OWBoTbhcAKScWEq+elIunecJDQ\nR3W8flLYu2paOPDJu/M551yWilIN1YNgyo99w6K3gSsJVsvrb2az6jjvOwT1yP8ws51TvD4CmGFm\nKyQdAlzhVVrOOZedIi1+1Og3l7YBxqdKFrWO2wL4yMz6xBaMc865RotSDdUSfkbQI8Q551wWijIo\nL1aS9idIFvumOeYM4AyADh067Ln99tu3UHTOOZcfJk+evNTMejb2/Iwmi3A2z7uBQ6rnyUnFzO4E\n7gQoLS21SZMmtVCEzjmXHyTNqf+outWbLCT1JBiktE3i8WZ2WlM+WFJ/4Angx2b2aVPeyznnXLyi\n3Fk8DbxFMEq2sp5jN5H0MLAf0EPBUpOXA60AzOx24DKCKaD/FszUQIWZlTYkeOeccy0jSrJob2YX\nNfSNzSzttM1mdjrBtM3OOeeyXJTeUOMlHRp7JM4557JWlGRxDkHC+CZc9nGNpNVxB+accy571FsN\nZWad6jvGOedcfqszWUja3sw+CdckTmJmqaZLds45l4fS3VmcRzAQ7oYUrxkwKpaInHPOZZ06k4WZ\nnRE+7t9y4TjnnMtG2TI3lHPOuSzmycI551y9PFk455yrV73JQtJISR3C5ydLulHSgPhDc845ly2i\n3FncBqwPlz29EJgD/CPWqJxzzmWVKMmiwoLl9I4EbjazmwEfqOeccwUkykSCayRdApwMfEdSMeHs\nsc455wpDlDuLHwFlwM/MbCHQB/hzrFE555zLKpHuLAiqnyolDQG2Bx6ONyznnHPZJMqdxZtAG0l9\ngFeAnwL3xRmUc8657BIlWcjM1gPHALeY2dHATvGG5ZxzLptEShaS9gFOAp4Ny4rjC8k551y2iZIs\nxgKXAE+a2XRJg4DX4g3LOedcNomy+NEbwBuSOknqaGazgbPjD80551y2iDLdxy6SPgA+Aj6WNFmS\nt1k451wBiVINdQdwnpkNMLP+wPnAXfGG5ZxzLptESRYdzGxTG4WZvQ50iC0i55xzWSfKoLzZkn4H\nPBDunwx8EV9Izjnnsk2UO4vTgJ7AE8CT4fOfxhmUc8657BKlN9QKvPeTc84VtDqThaRnAKvrdTM7\nIpaInHPOZZ10dxbXt1gUBaq8HL75JtNROOdc/epMFuFgvEaTdC9wGLDYzHZO8bqAm4FDgfXAqWY2\npSmfmSumT4fzzoO33vJk4ZzLDVF6QzXWfcBfqXsJ1kOAweE2nGD51uExxpMVPv8c9tsPli6FEsrp\nhGcL51z81jTx/NiShZm9KWmbNIccCfwjXLL1PUldJW1tZgviiikbPPRQkChG8zwP8GN6sCzTITnn\nCoCaeH6UrrNx6QPMTdifF5YlkXSGpEmSJi1ZsqRFgovL228Hj2O4xROFcy5n1HtnEa6OdwEwIPF4\nMxvVxM9OlehS9r4yszuBOwFKS0vr7KGVC6rbKDol3BSupx0VsdYIOudc0yqionxDPQbcTjAfVGWT\nPq2meUC/hP2+wNfN+P5ZqTK8gsUJl/JAXuZdRmYoIudcYWhaRVSUZFFhZrc16VNSGwf8StK/CBq2\nV+V7ewVARUXwWELF5rLwn6F9eyj2ZaWcczFY08QW7nSD8rqFT5+R9EuCqT7Kql83s+Xp3ljSw8B+\nQA9J84DLgVbhubcDzxF0m51F0HW2IKYQSZcs3nwT9twzE1E55/KdmtjCne7OYjJBG0L1R1yQ8JoB\ng9K9sZmdUM/rBpwVIca8ki5ZlHizhXMuS6UblDcQQFJbM9uQ+JqktnEHlq9StVl4snDOZbsoXWff\njVjmIkh1Z1FJ0FDhycI5l63StVlsRTDuoZ2k3dlcHdUZaN8CseWldNVQ3rjtnMtW6f6WPRg4laBL\n6w1sThargd/EG1b+qq6G8jYL51wuSddmcT9wv6RjzezxFowpr3kDt3MuF0Vps9hTUtfqHUlbSPpD\njDHltepkkaqB26uhnHPZKkqyOMTMVlbvhCvnHRpfSPktVTWUN3A757JdlGRRLKlN9Y6kdkCbNMe7\nNLwayjmXi6J8PT0IvCLp7wSD8U4D7o81qjzmvaGcc7mo3mRhZtdJ+hA4gKBH1FVm9mLskeWpdG0W\nfmfhnMtWkb6ezOx54PmYYykI3nXWOZeL6m2zkLS3pImS1kraKKlS0uqWCC4fBXcWRitv4HbO5ZAo\nDdx/BU4APgPaAacDt8QZVL6qqgIzKKJqcxnCwn+GokyuW+icc2lErYaaJanYzCqBv0vyuaEawScR\ndM7lqihfUesltQb+J+k6YAHQId6w8pN3m3XO5aooFR8/Do/7FbCOYCnUY+MMKl+lm3HWu80657JZ\nlK6zc8I7i22AJ4CZZrYx7sDykfeEcs7lqnq/oiR9H7gd+JxgnMVASWeG3WldA/gYC+dcroryFXUD\nsL+ZzQKQtC3wLD7uosF89LZzLldFabNYXJ0oQrOBxTHFk9e8gds5l6vSrZR3TPh0uqTngEcJ5oY6\nDpjYArHlHZ9x1jmXq9J9RR2e8HwR8N3w+RJgi9giymN+Z+Gcy1XpVsr7aUsGUgh84SPnXK5KVw11\nYTjj7C0E1U81mNnZsUaWh7zrrHMuV6X7ipoRPk5qiUAKgVdDOedyVbpqqGfCR1/oqJn4CG7nXK6K\nMihvCPBrghHcm443s1HxhZWffCJB51yuivIV9RjBCO67IeFbLgJJo4GbgWLgbjO7ptbr/QmWaO0a\nHnOxmT3XkM/IJV4N5ZzLVVG+oirM7LaGvrGkYuBW4EBgHjBR0jgz+zjhsN8Cj5rZbZJ2BJ4juIPJ\nSz6C2zmXq6KM4H5G0i8lbS2pW/UW4by9gFlmNjucePBfwJG1jjGgc/i8C/B15MhzULo2C7+zcM5l\nsyhfUT8JHy9IKDNgUD3n9QHmJuzPA4bXOuYK4CVJYwjWyPhehHhylrdZOOdyVZQpygc28r2V6u1q\n7Z8A3GdmN0jaB3hA0s5mVpV4kKQzgDMA+vfv38hwMs/bLJxzuSrdoLxRZvZqwhxRNZjZE/W89zyC\nhZKq9SW5mulnwOjw/SZIagv0oNZEhWZ2J3AnQGlpadIAwVzhbRbOuVyV7u/Z7wKvUnOOqGpGsBBS\nOhOBwZIGAvOB44ETax3zFXAAcJ+kHYC2BHNP5SUfwe2cy1XpBuVdHj42ao4oM6uQ9CvgRYJusfea\n2XRJVwKTzGwccD5wl6RzCRLQqWaWs3cO9Uk1N5Q3cDvnckGUQXldgVNIHpRX79xQ4ZiJ52qVXZbw\n/GNgZPRwc5tXQznnclWUv2efA94DPgSq6jnWpeEN3M65XBXlK6qtmZ0XeyQ5qqICrrkGxo+HWbPS\nH7thQ/DoycI5l2uifEU9IOnnwHigrLrQzJbHFlWOMIOTToKnHi3jMq5kb96LdF7vhE5hXg3lnMsF\nUZLFRuDPwKVsHicRZVBe3vvoI3j0UTiBx7mUqxv1HuW0AvzOwjmX3aJ8RZ0HbGdmS+MOJte8/nrw\nuA1fNur8Sop4kqMBaNu2eWJyzrk4REkW04H1cQeSi9asCR4T2yAe5CT+TrTexjMZynz6AjCyYPqE\nOedyUZRkUQn8T9Jr1GyzKPhlVVP1bvqMwbzKAQ16n8GDYfTo5ozMOeeaV5Rk8VS4uVrSdYWNokMH\nGDUKbr8dunZt7uicc675RJlI0JdVrUO6Edm/+Q2ce27687t0gVat4orOOeeaj/fBaYJ0cz116QI9\nemQiKueca35RFj/KKjOXzWTR2kWZDgPw6Tucc4Uj55LF2rK1HPvosWys3JjpUFJWQ/mIbOdcPmrw\nV5qkq4FVwN1mtqz5Q6rfO3PfYdDNgzhi6BFs32N7hnYfypDuQ+jfpT/FRS33J70vk+qcKxSN+Ur7\nL7AtcBPBbLQZMX/NfG6bdFuNsjbFbdiu23YM7TGUnXvuTGnvUvbsvSe9O/WOJQZfn8I5Vyga/JVm\nZlnbjbassozpS6Yzfcl0npixeW2mrTpuRWnvUkb2G8l3B3yX0t6ltCpuejekdNVQ3mbhnMsn6ZZV\nvYXkNbM3yaVBeQvXLmT8p+MZ/+l4ADq06sCIfiM4aNuDOGzIYQztPhQp1ZLh6Xk1lHOuUKRr4J4E\nTCZY6nQP4LNw2w0S/pRuYc1xR7CufB0vz36ZC16+gB1u3YHBtwxm7AtjeWvOW1RZ9CU7fH0K51yh\nSLes6v0Akk4F9jez8nD/duClFokuhV177coDv3yAGUtnMGPJDD5d/ikzl85k5rKZrNywslHv+fmK\nz7n5/Zu5+f2bGdBlACftchIn73oyO/TcIe151W0W3hvKOZfvonyl9QY6AdXrV3QMyzJmh547BF/k\nCd/lZsbS9Uv5dNmnTF8ynclfT2bSgkl8uOhDyqvKI7/3nFVzuPrtq7n67asZ3mc45ww/hx/s+IOU\ndzQ+zsI5VyiiJItrgA/CiQQBvgtcEVtEjSSJnh160rNDT0b2Hwl7BuVlFWV8uPhD3v7qbd6Y8wZv\nznmT5d9EW7fp/fnvc+ITJ3LByxdw1rfO4qy9zqJzm86bXvc2C+dcoYgyN9TfJT0PDA+LLjazhfGG\n1XzalLShtHcppb1LGbv3WKqsig8Xfcjzs55n/KfjmTBvQr3tFPPXzOc3r/6G6ydczwUjLmDMXmPo\n0LqDt1k45wpGvSO4FXQT+h4wzMyeBlpL2iv2yGJSpCKGbTWMi/e9mLdPe5vFv17M/Ufdz+jtRlOk\n9Jdj+TfLueSVS9julu14aNpDVFQEncW8zcI5l++iTPfxN2Af4IRwfw1wa2wRtbDu7btzyrBTeP6k\n55l/3nxuOvgmhvUalvachWsXcvKTJ/Pe3ElA6moob7NwzuWTKMliuJmdBWwAMLMVQOtYo8qQrTpu\nxdi9x/LBmR/wxqlvcMwOx6S921i5Llgqz6uhnHP5LkqyKJdUTDhAT1JPIPpghBwkie8M+A6P//Bx\nPhvzGT/d7aepk0ZV2PPJq6Gcc3kuSrL4C/AksKWkPwJvA1fHGlUWGbTFIO498l5mnDWDo7c/uuaL\nYbLw3lDOuXxXb7Iws4eAC4E/AQuAo8zssbgDyzZDug/hiR89wQsnvcDgboODQgsTg4+zcM7lubTJ\nQlKRpI/M7BMzu9XM/mpmM6K+uaTRkmZKmiXp4jqO+aGkjyVNl/TPhv4ALe3g7Q5m2i+mMXb42LTV\nULdNvgWzOqfWcs65nJI2WZhZFTBVUv+GvnHYznErcAiwI3CCpB1rHTMYuAQYaWY7AWMb+jmZ0Lak\nLTeNvolBXYYAqe8s7v/wbv5v/P9RWZWxabScc67ZRKlZ3xqYLum/wLrqQjM7op7z9gJmmdlsAEn/\nAo4EPk445ufArWEPK8xscQNiz7i2RZ2A1G0WFFVw55Q7WVW2igePeZCSIm/EcM7lrijfYL9v5Hv3\nAeYm7M9j8yjwakMAJL0DFANXmNkLjfy8FpduIkEUlD0y/RFaF7fmvqPuq3fQn3POZaso03280cj3\nTrVARO1K/BJgMLAf0Bd4S9LOZlZj+lhJZwBnAPTv3+Aasdikm+6Dos1lD0x7gM5tOnPLIbc0at0M\n55zLtDj/1J0H9EvY7wt8neKYp82s3My+AGYSJI8azOxOMys1s9KePXvGFnBDpZtIMDFZANw68VYu\ne+2ylgrNOeeaVZzJYiIwWNJASa2B44FxtY55CtgfQFIPgmqp2THG1KzSLatKUXLD9h/e+gMPTnuw\nJUJzzrlmFVuyMLMK4FfAi8AM4FEzmy7pSknVjeMvAsskfQy8BlxgZsviiqm5VbdZpKqGuuQ7F6Y8\n5/Rxp/PevPdij80555qT6hsLIGkkwfoVAwjaGASYmQ2KPboUSktLbdKkSZn46CQ9e8LSpbCU7nQP\n14bqzlKW053Fi+GGqRdz7TvXJp3Xq0MvJv58Iv269Et6zTnn4iBpspmVNvb8KHcW9wA3AvsC3wJK\nw8eCV9/iR3864E8cv/PxSectWreIH/77h5RXRl/BzznnMilKslhlZs+b2WIzW1a9xR5ZDkjXZlFc\nHExIeO8R91LaOzmZvzfvPX732u9aJE7nnGuqKMniNUl/lrSPpD2qt9gjywFRVspr16odTx//NL07\nJS9bfu071/LCrJwZVuKcK2CR1rMgqHq6Grgh3K6PM6hckaqBO9Wss7079eax4x6jWMmzC57y5Cks\nWLMg1jidc66posw6u3+KbVRLBJftgjsLoyShGqquKcpH9BvBH0b9Iek9lqxfwpnjz/RJB51zWa3O\nZCHp5PDxvFRby4WYnaqqwAyKEtaBqqQICy9pUYore+HICzlo24OSyp/59Bkff+Gcy2rp7iw6hI+d\n6tgKWpT2itqKVMQ/jvoHW3bYMum1s184m6/X1B7g7pxz2aHOuaHM7I7wsbETCea1VJMIRlklr1fH\nXtz+/ds55tFjapSv3LCSM8efybjjx/n8Uc65rJNz06AuWgTXXw/XXANPPZW5OBpzZ1Ht6B2O5oSd\nT0gqH//peJ6Y8USzxeicc80l55LFvHlwwQVwySXw8MOZiyNdsoiypOoth9xCrw69ksrPeeEc1pSt\naZYYnXOuueT0ijzvvQdvvpmZz14ZTqLe0Gqoat3bd+dv3/8bxz56bI3y+Wvm8/s3fs/1B3nvZOdc\n9ogyN9TVwHXVa0xI2gI438x+2wLxJemgHawPT/NZsG5Sxm3FAhYQDLhbwFb0ZgG9esHChfWfa2Yc\n/vDhPPvZszXKi1XMB2d+wC69dokjZOdcAWrq3FBRksUHZrZ7rbIpZpaRUdylkl3KUZzDzZzKfXRh\nVSbC2KQTaziDuwCYS1/6M5c+fYLqsihmr5jNTn/biQ0VG2qUf2fAd3j9J697Y7dzrlk0NVlEqYYq\nltTGzMrCD2wHtGnsBzaHEip4mBMYybuZDCNJdTVUlDaLaoO2GMSl3740aZ6oN+e8yVOfPMXROxzd\nnCE651yjRGngfhB4RdLPJJ0GvAzcH29Y6ZVQwR5MyWQIKb3DSAB69GjYeReMuIAh3ZOr1S54+QI2\nVm5sjtCcc65JoqzBfZ2kacD3CNayuMrMXow9sjRKqKjRC+kirtm8Ql2GLKUHjxM0Vh94YMPObVPS\nhhsOuoHDHz68RvnnKz7n1v/eyrn7nNtcYTrnXKNEabMYCCwwsw3hfjugl5l9GX94yUol+zP78V3e\noIgg9mIqqKIBdT8x2mkneOUV6JXcKzYtM+PABw7klS9eqVG+RdstmHX2LLq169aMUTrnCk1LtFk8\nBoxI2K8MyzK2AFIryjclCoCqsDbt29/OVETQqRPsvz+cemrDq6EgWPvihoNuYPc7dscSfrYVG1Zw\n9VtXe1da51xGRUkWJWa2qeLczDZKah1jTPVqy+aeQ+XhSq/FxZkbc9Fchm01jNN2P417PrinRvmt\nE2/l3L3PpU/nPhmKzDlX6KI0cC+RdET1jqQjgaXxhVS/xGQRdYqNXHHV/lfRvlX7GmUbKjbwhzeT\npzd3zrmWEiVZ/B/wG0lfSZoLXAScGW9Y6eVzsti609aM2WtMUvndH9zN7BWzMxCRc85FW/zoczPb\nG9gR2NHMRpjZrPhDq1sbyjY9z7dkAcG6F53bdK5RVlFVwe/f8AmAnXOZEWkiQUnfB34JnCvpMkmX\nxRtWevmeLLq168av9/l1UvkDUx/g4yUfZyAi51yhqzdZSLod+BEwhmCcxXHAgJjjSiuxGqohk/fl\nkrF7j6VH+5rdqgzj8tcvz1BEzrlCFuXOYoSZnQKsCBdC2gfoF29Y6eVzm0W1Tm06cfHIi5PK//3x\nv5m+eHoGInLOFbIoyeKb8HG9pN5AOTAwvpDq15ryTc/zNVkA/PJbv6R3p95J5X96+08ZiMY5V8ii\nJIvxkroCfwamAF8CGVx2qKZ8ThbtWrXjopEXJZU//NHDzFqe0T4GzrkCE6U31FVmttLMHidoq9je\nzCI1cEsaLWmmpFmSkutUNh/3A0kmqcFD0RuyOl0uOn2P09myw5Y1yqqsimveviZDETnnClGDllU1\nszIzi7SAhKRi4FbgEIJutydI2jHFcZ2As4H3GxJLtXy+swBo36o95+9zflL5/VPv56tVX2UgIudc\nIYpzDe69gFlmNjucLuRfwJEpjrsKuA7YkOK1euVrb6hEvyj9BVu03aJGWUVVBde9c12GInLOFZo4\nk0UfYG7C/rywbBNJuwP9zGx8Yz8k3+8sIOgZdc7wc5LK755yNwvXRli/1TnnmijKOItXopSlOjVF\n2abpVCUVATcByXUsyZ93hqRJkibVfq0QkgXAmOFj6NS6U42yssoyrn/XZ6N1zsWvzmQhqa2kbkAP\nSVtI6hZu2wDJ/TmTzaPmeIy+wNcJ+52AnYHXJX0J7A2MS9XIbWZ3mllpqrnYCyVZdGvXjbO+dVZS\n+e2TbmfZ+mUZiMg5V0jS3VmcCUwGtg8fq7enCRqu6zMRGCxpYDil+fHAuOoXzWyVmfUws23MbBvg\nPeAIM0u6e0inUJIFwLn7nEu7knY1ytaVr+PWiVH+OZxzrvHqTBZmdrOZDQR+bWaDzGxguA0zs7/W\n98ZmVgH8CngRmAE8ambTJV2ZOOV5UxVSstiyw5b8fI+fJ5X/5f2/sG7jugxE5JwrFFEauBeG3VuR\n9FtJT0jaI8qbm9lzZjbEzLY1sz+GZZeZ2bgUx+7X0LsKKIzeUInOH3E+JUU1f9hl3yzj7il3Zygi\n51whiJIsfmdmayTtCxwM3A/cFm9Y0RXSnQVA/y79OXnXk5PKr59wPRsrN6Y4wznnmi5KsqgMH78P\n3GZmTwMZXVY1Ub6P4E7lwhEXolqdzeatnsdD0x7KUETOuXwXJVnMl3QH8EPgOUltIp7XIgrtzgJg\nh547cNT2RyWVX/vOtVRZVQYics7luyhf+j8kaKQebWYrgW7ABbFG1QCFmCwALt43eaqtmctm8tQn\nT2UgGudcvosykeB6YDGwb1jx9KjXAAAVgklEQVRUAXwWZ1ANUajJYq8+ezFq4Kik8j+9/SfMLMUZ\nzjnXeFFGcF8OXARcEha1Ah6MM6iGKNRkAXDJvpcklU36ehKvfvFqBqJxzuWzKNVQRwNHAOsAzOxr\ngtHXWaHQus4mOmDgAZT2Tp7V3RdHcs41tyjJYqMF9RoGIKlDvCE1TCHfWUhKufTqK1+8wsT5EzMQ\nkXMuX0VJFo+GvaG6Svo58B8ga0aAFXKyADh6h6MZ2n1oUvk17/jiSM655hOlgft64N/A48BQ4DIz\n+0vcgUVV6MmiSEUpl159csaTfLL0kwxE5JzLR1EauK81s5fN7AIz+7WZvSzp2pYILopCTxYAJ+16\nEn07961RZhjXvpM1/0zOuRwXpRrqwBRlhzR3II1ViCO4a2td3Drl0qsPTnuQuavmpjjDOecaJt16\nFr+Q9CEwVNK0hO0LYFrLhZheIfeGSvTzPX5Ot3bdapRVVFVww4QbMhSRcy6fpLuz+CdwOMEaFIcn\nbHuaWfJMdhni1VCBDq07cPZeZyeV3zXlLpauX5qBiJxz+STdeharzOxLMzvBzOYkbMtbMsD6eLLY\nbMzwMXRoVbNn8/ry9dzy/i0Zisg5ly+yZkLAxvJksVm3dt04c88zk8pv+e8trClbk4GInHP5wpNF\nnjl3n3NpVdSqRtmKDSu4c/KdGYrIOZcPPFnkmb6d+3LKsFOSym9870bKKsoyEJFzLh80OFlI+o+k\n5yUdFkdADeXJItmFI5MXR/p6zdc8MO2BDEXknMt1jbmzOAX4LTCgmWNpFO86m2xI9yEcu+OxSeXX\nvXMdlVWVKc5wzrn0IiULSe0kDYVg1lkzm2xmt8YbWjR+Z5FaqunLP1v+GU/MeCID0Tjncl2U6T4O\nB/4HvBDu7yZpXNyBReXJIrU9tt6Dg7Y9KKn8j2/90Zdedc41WJQ7iyuAvYCVAGb2P2Cb+EJqmPW0\nB2DLLTMcSBZKNX351EVTeeSjRzIQjXMul0VJFhVmtir2SCKqbqMA+JTBjOcwWrWC/fbLXEzZar9t\n9mPvvnsnlV/66qXeM8o51yBRksVHkk4EiiUNlnQL8G7McdVpKrvSnzn0Zw7b8wlr1IW77oJOWbN2\nX/aQxNWjrk4q/2LlF9wx+Y4MROScy1UKFsFLc4DUHrgUOAgQ8CJwlZltiD+8ZD17ltree08CYKed\n4IgjYMSITESSOw596FCen/V8jbIe7Xvw+dmf07lN5wxF5ZxrSZImm1nyOsxRz68vWWSb0tJSmzRp\nUqbDyClTF05l9zt2x6j5b33RyIu45nu+op5zhaCpySJKb6jXJL1ae2vsB7qWN2yrYZy8a/JEwTdO\nuJGZS2dmICLnXK6J0mbxa+CCcPsdQTfaSH/aSxotaaakWZKSuuZIOk/Sx+E6Ga9IyoqBfvnoqv2v\nok1xmxpl5VXljHl+DLl2d+mca3lR1uCenLC9Y2bnAcPrO09SMXArwap6OwInSNqx1mEfAKVmtivB\nOt/XNfgncJEM6Dog5VrdL89+mcdnPJ6BiJxzuSRKNVS3hK2HpIOBrSK8917ALDObbWYbgX8BRyYe\nYGavmdn6cPc9oC8uNhfvezHbdN0mqfzcF8/1Kcydc2lFqYaaTFDtNBmYAJwP/CzCeX2AxAWg54Vl\ndfkZ8HyqFySdIWmSpElLliyJ8NEulXat2vGX0X9JKp+3eh7nv5S8hrdzzlWLUg010MwGhY+Dzewg\nM3s7wnsrRVnKynFJJwOlwJ/riOFOMys1s9KePXtG+GhXl8OHHs5hQ5InDL5ryl08++mzGYjIOZcL\n6pxRSdIx6U40s/pmpJsH9EvY7wt8neJzvkcwjuO7ZubDilvAX0b/hde+eI115etqlJ/+zOl89IuP\n6N6+e4Yic85lq3R3Foen2aKsZTERGCxpoKTWwPFAjQkIJe0O3AEcYWaLGx6+a4yBWwzkhoNuSCpf\nuHYhZ44/03tHOeeSxDooT9KhwP8DioF7zeyPkq4EJpnZOEn/AXYBFoSnfGVmR6R7Tx+U1zzMjEP/\neSgvzHoh6bWbDr6JsXuPzUBUzrm4xD6CW1J34HJgX4I2h7eBK81sWWM/tCk8WTSf+avns/NtO7Ny\nw8oa5SVFJbz2k9fYt/++GYrMOdfcYh/BTdDldQlwLPCD8LnPcZ0H+nTuw52H3ZlUXlFVwTGPHMPs\nFbMzEJVzLhtFSRbdzOwqM/si3P4AdI07MNcyjtvpOM4Zfk5S+ZL1Szj0oUNZ/s3yDETlnMs2UZLF\na5KOl1QUbj8EvI9lHrnuwOsY0S956t6Zy2Zy6EOHsrpsdQaics5lkzqThaQ1klYDZwL/BMrC7V/A\nuS0TnmsJrYtb8+/j/k3/Lv2TXnt//vsc/ODBnjCcK3B1Jgsz62RmncPHIjNrFW5FZuaLIOSZrTtt\nzbMnPptyfYv35r3H6AdHJzWEO+cKR5RqKFcgdt5yZ5780ZNJs9MCTJg3gRH3jODLlV+2fGDOuYzz\nZOFqGDVwFE8f/3TKhDFj6QyG3z2c9+e9n4HInHOZ5MnCJTl4u4N56vinaF3cOum1xesW8+2/f5ub\n37vZR3o7V0AiJQtJxZJ6S+pfvcUdmMus0duN5pkTnqFT605Jr5VXlTP2xbEc8a8jWLzOZ2lxrhBE\nWc9iDLAIeJmgy+yzwPiY43JZ4KBtD+Lt096mb+fUy4yM/3Q82/91e/7+wd/9LsO5PBflzuIcYKiZ\n7WRmu4TbrnEH5rLDrr125f3T32fPrfdM+fqKDSs4bdxp7H///kz+enILR+ecaylRksVcYFXcgbjs\n1btTb9457R3O3uvsOo95Y84blN5VyomPn8hnyz5rweiccy0hykSC9wBDCaqfNq03YWY3xhtaaj6R\nYGY9/cnTnDbutLTTgBSpiGN3OJaLRl7Enr1T35E451pWS0wk+BVBe0VroFPC5grQkdsfyYyzZnDi\nLifWeUyVVfHYx49Relcp+9+/Pw9/+DAbKja0YJTOueYW63oWcfA7i+zx4qwXGfP8GD5bXn+1U7d2\n3fjxrj/m+J2PZ3if4UipVt11zsUltvUsJP0/Mxsr6RlSrJ1d3yJFcfFkkV3KK8u5a8pdXPH6FSxZ\nvyTSOf069+O4HY/j2B2PZa8+e1FSVOfqvs65ZhJnstjTzCZL+m6q183sjcZ+aFN4sshOa8rWcMfk\nO7hxwo0sWLug/hNCXdt25YCBB3Dwtgdz0LYHMaDrgBijdK5wxb5SXrbxZJHdyirKeGDaA9ww4QY+\nWfpJg88f2HUgI/qNYES/EezTdx926bWL33k41ww8WbisZGa89dVb3DXlLh6b/hhllWX1n5RCh1Yd\nKO1dyrBewxi21TCG9RrGjj13pF2rds0csXP5zZOFy3rLv1nOkzOe5LGPH+M/s/9DpVU26f2KVcyQ\n7kPYecudGdp9KEO6D2Fw98EM6T6Ebu26NVPUzuUXTxYupyxbv4ynPnmKZz59hle/eJU1G9c06/t3\nb9edId2HMHCLgfTv3J/+XYJtQNcB9O/SP+V6Hc4VgtiThaSXgePMbGW4vwXwLzM7uLEf2hSeLPJH\neWU5E+ZN4KXPX+LFz19kyoIpVFlVrJ/ZpU0X+nfpT+9Ovdmq41Zs1XErenXotel59da1bVfv3uvy\nSkskiw/MbPf6ylqKJ4v8tXbjWv47/7+8O/dd3p37LhPmTcjY6nyti1vTs31PurXrRvf23YPHdjUf\nE1/r1q4bXdp0oX2r9p5kXFZqarKI0s2kSlJ/M/sq/MABpBh34VxTdWzdkVEDRzFq4CggGAn+2bLP\nmLpoKlMXTg0eF01l3up5sceysXIj89fMZ/6a+Q06r0hFdG7TOfXWuuZ+pzad6Ni6Ix1adaB9q/Z0\naB0+tupQ43lxUXFMP6Vz0UW5sxgN3AlUj6v4DnCGmb0Yc2wp+Z2FW/7Ncj5c9CGfLvs02JYHj58v\n/5zyqvJMh9fsWhe3Tkog1cmlXUk72pa0pU1JG9oWt6VtSdvN+yUJ+8U19+s6pk1JG1oVtaJ1cWta\nFbeiVVErv1PKEy3SwC2pB7A3IGCCmS1t7Ac2lScLV5eKqgq+WvUVs5bP4qtVX23a5qyaw1ervmLu\nqrl5mUziVlJUEiSPhCTS4P2izcknMREVFxVTUlSStBUruTzVsU05rvrYYhVTpKK8T4qxV0NJOhp4\n1czGh/tdJR1lZk819kOdi0NJUQmDthjEoC0GpXy9yqpYtHYRc1fPZeHahSxau4iFaxcG27rN+wvW\nLmB9+foWjj57VVRVUFFVkekwYidEcVGQOIpUtCmJVJfFtR/pnHreq0hFCG16nmprqihtFpeb2ZPV\nO2a2UtLlQL3JIqzCuhkoBu42s2tqvd4G+AewJ7AM+JGZfRk9fOeiK1IRW3famq07bV3vsWs3rmXZ\n+mUs/2Y5y74JH2vvJ5Sv2LCCNWVr+Kbimxb4SVwcDCuIpNhYUZJFqpQU5Y6kGLgVOBCYB0yUNM7M\nPk447GfACjPbTtLxwLXAjyLE5FysOrbuSMfWHRs8V1V5ZTlrNq5hddlqVm1Yxeqy1XVvG1ezbuM6\n1pWvY335etZtDB9r7Zv3J3FZIEqymCTpRoIvfgPGAFHWz9wLmGVmswEk/Qs4EkhMFkcCV4TP/w38\nVZIs10YKOhdqVdxqU1fa5mBmbKjYkDKhrNu4jrLKMjZUbNi0lVXU2q/9ej3Hb6zcSHlVefBYWe5t\nPG6TKMliDPA74BGCBu6XgLMinNeHYEnWavOA4XUdY2YVklYB3YGMNaA7l00k0a5Vu4zNhWVmlFeV\nU15ZnpRImrK/sXIjlVWVm9pDKqoqqLSa+3WWVSWXNeTY2sdVWVXsg0HzQb3JwszWARc34r1TdS2o\nfccQ5RgknQGcEe6WSfqoEfHkox54Yq3m12Izvxab+bXYbGhTTo7S9tATuBDYCWhbXW5mo+o5dR7Q\nL2G/L/B1HcfMk1QCdAGSFnc2szsJxnogaVJTun/lE78Wm/m12MyvxWZ+LTaT1KQxB1H6Uz0EfAIM\nBH4PfAlMjHDeRGCwpIGSWgPHA+NqHTMO+En4/AcEXXS9vcI557JMlGTR3czuAcrN7A0zO41ggF5a\nZlYB/Ap4EZgBPGpm0yVdKal6SdZ7gO6SZgHn0bjqLuecczGL0sBd3R1igaTvE1Ql9Y3y5mb2HPBc\nrbLLEp5vAI6LFuomdzbw+Hzm12Izvxab+bXYzK/FZk26FlHmhjoMeIugbeEWoDPwezOrXaXknHMu\nT+Xc4kfOOedaXtMnDGlBkkZLmilplqS8b9+QdK+kxYldhSV1k/SypM/Cxy3Cckn6S3htpknaI3OR\nNy9J/SS9JmmGpOmSzgnLC/FatJX0X0lTw2vx+7B8oKT3w2vxSNipBEltwv1Z4evbZDL+OEgqlvSB\npOr56wryWkj6UtKHkv5X3fOpOX9HciZZJEwfcgiwI3CCpB0zG1Xs7gNG1yq7GHjFzAYDr7C5U8Ah\nwOBwOwO4rYVibAkVwPlmtgNB54qzwn/7QrwWZcAoMxsG7AaMlrQ3wVQ5N4XXYgXBVDqQMKUOcFN4\nXL45h6ATTbVCvhb7m9luCd2Fm+93xMxyYgP2AV5M2L8EuCTTcbXAz70N8FHC/kxg6/D51sDM8Pkd\nwAmpjsu3DXiaYM6xgr4WQHtgCsHMCEuBkrB80+8KQW/EfcLnJeFxynTszXgN+oZfgqOA8QQDfQv1\nWnwJ9KhV1my/I1EG5bUBjg2/tDYdb2ZX1nduM4syfUgh6GVmCwDMbIGkLcPyVNenD7CgheOLVVh1\nsDvwPgV6LcK77MnAdgR3258DKy3org6bf17I/yl1/h/BoOFO4X53CvdaGPCSJAPusGAwc7P9jkTp\nOvs0sIrgP2dZw+NvNpGmBilgeX99JHUEHgfGmtlq1b1YTV5fCzOrBHaT1BV4Etgh1WHhY95ei7Cn\n5mIzmyxpv+riFIfm/bUIjTSzr8OE8LKkT9Ic2+BrESVZ9DWz2vXmmRBl+pBCsEjS1uFfCVsDi8Py\nvL4+kloRJIqHzOyJsLggr0U1C9aWeZ2gHaerpJLwL+rEnzfSlDo5aiRwhKRDCaYi6kxwp1GI1wIz\n+zp8XCzpSYKZv5vtdyRKA/e7knZpeOjNLsr0IYUgcYqUnxDc+VWXnxL2ctgbWFV9+5nrFNxC3APM\nMLMbE14qxGvRM7yjQFI74HsEjbuvEUyZA8nXIi+n1DGzS8ysr5ltQ/B98KqZnUQBXgtJHSR1qn4O\nHAR8RHP+jkRoNPkY2EjQADIN+BCYlqEGnEOBTwnqaC/NdINSC/y8DxPUIZYT/CXwM4I61leAz8LH\nbuGxYnP99YdAaabjb8brsC/BLfI04H/hdmiBXotdgQ/Ca/ERcFlYPgj4LzALeAxoE5a3Dfdnha8P\nyvTPENN12Q8YX6jXIvyZp4bb9Orvx+b8HYkygjvlUmFmNiftic455/JGnW0Wkjqb2WpgTQvG45xz\nLgvVeWchabyZHSbpC4IqgMTWczOzQS0RoHPOuczzuaGcc87VK0rXWcL5RAZTc6W8N+MKyjnnXHaJ\nMoL7dIK5V/oS9ELZG5hAMLzeOedcAYgyzuIc4FvAHDPbn2CqhSWxRuVcnpK0X/XsqM7lkijJYoMF\nK9ohqY2ZfQIMjTcs55xz2SRKspgXjhh9imC+kafJw6kTnEsk6eRw3Yj/SbojXDNhraQbJE2R9Iqk\nnuGxu0l6L1wX4MmENQO2k/QfBWtPTJG0bfj2HSX9W9Inkh5SmkmunMsW9SYLMzvazFaa2RXA7wim\nXTgq7sCcyxRJOwA/IpiYbTegEjgJ6ABMMbM9gDeAy8NT/gFcZGa7EoyGrS5/CLjVgrUnRrB5Rs/d\ngbEE67IMIpjjyLmslraBW1IRwdQeOwOY2RstEpVzmXUAsCcwMfyjvx3BBGxVwCPhMQ8CT0jqAnRN\n+N24H3gsnKenj5k9CZBQlQvwXzObF+7/j2D6/7fj/7Gca7y0dxZmVgVMldS/heJxLhsIuN+CFcd2\nM7Oh4Z11bekGKaWrWkqc6r+SiF3YncukKG0WWwPTwzracdVb3IE5l0GvAD+oXigmXMd4AMHvS/Vs\npicCb5vZKmCFpG+H5T8G3ginypkn6ajwPdpIat+iP4VzzSjKXzS/jz0K57KImX0s6bcEq44VEcz6\nexawDthJ0mSCBcF+FJ7yE+D2MBnMBn4alv8YuEPSleF7HNeCP4ZzzSrKrLPXmtlF9ZU5l+8krTWz\njpmOw7lMiFINdWCKskOaOxDnnHPZK90U5b8AfgkMkjQt4aVOwDtxB+ZctvG7ClfI0k1R3gXYAvgT\ncHHCS2vMLG/WrXXOOVc/n6LcOedcvaK0WTjnnCtwniycc87Vy5OFc865enmycM45Vy9PFs455+r1\n/wF985lD+0BAMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f45534b1690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minibatch_losses_1st_replication\n",
    "plt.plot(minibatch_losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, minibacth loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
