{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 10\n",
    "N = 50\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 1.1311, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.8453, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.7990, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.7826, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.7747, Training Accuracy= 0.498\n",
      "Epoch: 50, Loss= 0.7706, Training Accuracy= 0.498\n",
      "Epoch: 60, Loss= 0.7682, Training Accuracy= 0.498\n",
      "Epoch: 70, Loss= 0.7665, Training Accuracy= 0.498\n",
      "Epoch: 80, Loss= 0.7650, Training Accuracy= 0.498\n",
      "Epoch: 90, Loss= 0.7633, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.7610, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.7579, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.7542, Training Accuracy= 0.498\n",
      "Epoch: 130, Loss= 0.7508, Training Accuracy= 0.498\n",
      "Epoch: 140, Loss= 0.7479, Training Accuracy= 0.498\n",
      "Epoch: 150, Loss= 0.7460, Training Accuracy= 0.498\n",
      "Epoch: 160, Loss= 0.7445, Training Accuracy= 0.498\n",
      "Epoch: 170, Loss= 0.7433, Training Accuracy= 0.498\n",
      "Epoch: 180, Loss= 0.7421, Training Accuracy= 0.498\n",
      "Epoch: 190, Loss= 0.7409, Training Accuracy= 0.498\n",
      "Epoch: 200, Loss= 0.7406, Training Accuracy= 0.498\n",
      "Epoch: 210, Loss= 0.7415, Training Accuracy= 0.498\n",
      "Epoch: 220, Loss= 0.7421, Training Accuracy= 0.498\n",
      "Epoch: 230, Loss= 0.7392, Training Accuracy= 0.498\n",
      "Epoch: 240, Loss= 0.7335, Training Accuracy= 0.498\n",
      "Epoch: 250, Loss= 0.7300, Training Accuracy= 0.498\n",
      "Epoch: 260, Loss= 0.7320, Training Accuracy= 0.499\n",
      "Epoch: 270, Loss= 0.7323, Training Accuracy= 0.501\n",
      "Epoch: 280, Loss= 0.7293, Training Accuracy= 0.501\n",
      "Epoch: 290, Loss= 0.7284, Training Accuracy= 0.501\n",
      "Epoch: 300, Loss= 0.8162, Training Accuracy= 0.498\n",
      "Epoch: 310, Loss= 0.7719, Training Accuracy= 0.498\n",
      "Epoch: 320, Loss= 0.8732, Training Accuracy= 0.498\n",
      "Epoch: 330, Loss= 0.8684, Training Accuracy= 0.498\n",
      "Epoch: 340, Loss= 0.8541, Training Accuracy= 0.498\n",
      "Epoch: 350, Loss= 0.8479, Training Accuracy= 0.498\n",
      "Epoch: 360, Loss= 0.8468, Training Accuracy= 0.498\n",
      "Epoch: 370, Loss= 0.8464, Training Accuracy= 0.498\n",
      "Epoch: 380, Loss= 0.8464, Training Accuracy= 0.496\n",
      "Epoch: 390, Loss= 0.8469, Training Accuracy= 0.498\n",
      "Epoch: 400, Loss= 0.8471, Training Accuracy= 0.498\n",
      "Epoch: 410, Loss= 0.8468, Training Accuracy= 0.498\n",
      "Epoch: 420, Loss= 0.8459, Training Accuracy= 0.498\n",
      "Epoch: 430, Loss= 0.9587, Training Accuracy= 0.498\n",
      "Epoch: 440, Loss= 0.9590, Training Accuracy= 0.498\n",
      "Epoch: 450, Loss= 0.9594, Training Accuracy= 0.498\n",
      "Epoch: 460, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 470, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 480, Loss= 0.9597, Training Accuracy= 0.498\n",
      "Epoch: 490, Loss= 0.9597, Training Accuracy= 0.498\n",
      "Epoch: 500, Loss= 0.9597, Training Accuracy= 0.498\n",
      "Epoch: 510, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 520, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 550, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 570, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 580, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 590, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 600, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 610, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 620, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 630, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 640, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 650, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 660, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 670, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 680, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 690, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 700, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 710, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 720, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 730, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 740, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 750, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 760, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 770, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 780, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 790, Loss= 0.9596, Training Accuracy= 0.498\n",
      "Epoch: 800, Loss= 0.9595, Training Accuracy= 0.498\n",
      "Epoch: 810, Loss= 0.9595, Training Accuracy= 0.498\n",
      "Epoch: 820, Loss= 0.9595, Training Accuracy= 0.498\n",
      "Epoch: 830, Loss= 0.9595, Training Accuracy= 0.498\n",
      "Epoch: 840, Loss= 0.9595, Training Accuracy= 0.498\n",
      "Epoch: 850, Loss= 0.9595, Training Accuracy= 0.498\n",
      "Epoch: 860, Loss= 0.9595, Training Accuracy= 0.498\n",
      "Epoch: 870, Loss= 0.9595, Training Accuracy= 0.498\n",
      "Epoch: 880, Loss= 0.9595, Training Accuracy= 0.498\n",
      "Epoch: 890, Loss= 0.9595, Training Accuracy= 0.498\n",
      "Epoch: 900, Loss= 0.9595, Training Accuracy= 0.498\n",
      "Epoch: 910, Loss= 0.9594, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.9594, Training Accuracy= 0.498\n",
      "Epoch: 930, Loss= 0.9594, Training Accuracy= 0.498\n",
      "Epoch: 940, Loss= 0.9594, Training Accuracy= 0.498\n",
      "Epoch: 950, Loss= 0.9594, Training Accuracy= 0.498\n",
      "Epoch: 960, Loss= 0.9594, Training Accuracy= 0.498\n",
      "Epoch: 970, Loss= 0.9594, Training Accuracy= 0.498\n",
      "Epoch: 980, Loss= 0.9594, Training Accuracy= 0.498\n",
      "Epoch: 990, Loss= 0.9594, Training Accuracy= 0.498\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5074\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7377, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 0.7377, Training Accuracy= 0.496\n",
      "Epoch: 20, Loss= 0.7377, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.7377, Training Accuracy= 0.496\n",
      "Epoch: 40, Loss= 0.7378, Training Accuracy= 0.496\n",
      "Epoch: 50, Loss= 0.7378, Training Accuracy= 0.496\n",
      "Epoch: 60, Loss= 0.7378, Training Accuracy= 0.496\n",
      "Epoch: 70, Loss= 0.7378, Training Accuracy= 0.496\n",
      "Epoch: 80, Loss= 0.7378, Training Accuracy= 0.496\n",
      "Epoch: 90, Loss= 0.7378, Training Accuracy= 0.496\n",
      "Epoch: 100, Loss= 0.7379, Training Accuracy= 0.496\n",
      "Epoch: 110, Loss= 0.7380, Training Accuracy= 0.496\n",
      "Epoch: 120, Loss= 0.7382, Training Accuracy= 0.496\n",
      "Epoch: 130, Loss= 0.7390, Training Accuracy= 0.496\n",
      "Epoch: 140, Loss= 0.7401, Training Accuracy= 0.496\n",
      "Epoch: 150, Loss= 0.7422, Training Accuracy= 0.496\n",
      "Epoch: 160, Loss= 0.7443, Training Accuracy= 0.496\n",
      "Epoch: 170, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 180, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 190, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 200, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 210, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 220, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 230, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 240, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 250, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 260, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 270, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 280, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 290, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 300, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 310, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 320, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 330, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 340, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 350, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 360, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 370, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 380, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 390, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 400, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 410, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 420, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 430, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 440, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 450, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 460, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 470, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 480, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 490, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 500, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 510, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 520, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 530, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 540, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 550, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 560, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 570, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 580, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 590, Loss= 0.7440, Training Accuracy= 0.496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 610, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 620, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 630, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 640, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 650, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 660, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 670, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 680, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 690, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 700, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 710, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 720, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 730, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 740, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 750, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 760, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 770, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 780, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 790, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 800, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 810, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 820, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 830, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 840, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 850, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 860, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 870, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 880, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 890, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 900, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 910, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 920, Loss= 0.7441, Training Accuracy= 0.496\n",
      "Epoch: 930, Loss= 0.7441, Training Accuracy= 0.496\n",
      "Epoch: 940, Loss= 0.7441, Training Accuracy= 0.496\n",
      "Epoch: 950, Loss= 0.7441, Training Accuracy= 0.496\n",
      "Epoch: 960, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 970, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 980, Loss= 0.7440, Training Accuracy= 0.496\n",
      "Epoch: 990, Loss= 0.7441, Training Accuracy= 0.496\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5109\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.7207, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.7009, Training Accuracy= 0.498\n",
      "Epoch: 20, Loss= 0.6983, Training Accuracy= 0.498\n",
      "Epoch: 30, Loss= 0.6961, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.6949, Training Accuracy= 0.508\n",
      "Epoch: 50, Loss= 0.6942, Training Accuracy= 0.513\n",
      "Epoch: 60, Loss= 0.6938, Training Accuracy= 0.513\n",
      "Epoch: 70, Loss= 0.6954, Training Accuracy= 0.508\n",
      "Epoch: 80, Loss= 0.6955, Training Accuracy= 0.509\n",
      "Epoch: 90, Loss= 0.6963, Training Accuracy= 0.509\n",
      "Epoch: 100, Loss= 0.7123, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.7011, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.6975, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.6976, Training Accuracy= 0.498\n",
      "Epoch: 140, Loss= 0.7021, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.6952, Training Accuracy= 0.501\n",
      "Epoch: 160, Loss= 0.6999, Training Accuracy= 0.501\n",
      "Epoch: 170, Loss= 0.6998, Training Accuracy= 0.499\n",
      "Epoch: 180, Loss= 0.6981, Training Accuracy= 0.505\n",
      "Epoch: 190, Loss= 0.6965, Training Accuracy= 0.500\n",
      "Epoch: 200, Loss= 0.6956, Training Accuracy= 0.505\n",
      "Epoch: 210, Loss= 0.6953, Training Accuracy= 0.506\n",
      "Epoch: 220, Loss= 0.6947, Training Accuracy= 0.510\n",
      "Epoch: 230, Loss= 0.7057, Training Accuracy= 0.501\n",
      "Epoch: 240, Loss= 0.7048, Training Accuracy= 0.498\n",
      "Epoch: 250, Loss= 0.7124, Training Accuracy= 0.498\n",
      "Epoch: 260, Loss= 0.7046, Training Accuracy= 0.498\n",
      "Epoch: 270, Loss= 0.7064, Training Accuracy= 0.498\n",
      "Epoch: 280, Loss= 0.7026, Training Accuracy= 0.498\n",
      "Epoch: 290, Loss= 0.7078, Training Accuracy= 0.498\n",
      "Epoch: 300, Loss= 0.7053, Training Accuracy= 0.498\n",
      "Epoch: 310, Loss= 0.7053, Training Accuracy= 0.498\n",
      "Epoch: 320, Loss= 0.7036, Training Accuracy= 0.498\n",
      "Epoch: 330, Loss= 0.7011, Training Accuracy= 0.498\n",
      "Epoch: 340, Loss= 0.7030, Training Accuracy= 0.498\n",
      "Epoch: 350, Loss= 0.7006, Training Accuracy= 0.498\n",
      "Epoch: 360, Loss= 0.6972, Training Accuracy= 0.503\n",
      "Epoch: 370, Loss= 0.7060, Training Accuracy= 0.498\n",
      "Epoch: 380, Loss= 0.7077, Training Accuracy= 0.498\n",
      "Epoch: 390, Loss= 0.7062, Training Accuracy= 0.498\n",
      "Epoch: 400, Loss= 0.7024, Training Accuracy= 0.503\n",
      "Epoch: 410, Loss= 0.6997, Training Accuracy= 0.509\n",
      "Epoch: 420, Loss= 0.6977, Training Accuracy= 0.515\n",
      "Epoch: 430, Loss= 0.7112, Training Accuracy= 0.497\n",
      "Epoch: 440, Loss= 0.7026, Training Accuracy= 0.497\n",
      "Epoch: 450, Loss= 0.7147, Training Accuracy= 0.497\n",
      "Epoch: 460, Loss= 0.7081, Training Accuracy= 0.499\n",
      "Epoch: 470, Loss= 0.7327, Training Accuracy= 0.498\n",
      "Epoch: 480, Loss= 0.7303, Training Accuracy= 0.498\n",
      "Epoch: 490, Loss= 0.7168, Training Accuracy= 0.498\n",
      "Epoch: 500, Loss= 0.7158, Training Accuracy= 0.498\n",
      "Epoch: 510, Loss= 0.7949, Training Accuracy= 0.498\n",
      "Epoch: 520, Loss= 0.7261, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.7174, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.7197, Training Accuracy= 0.498\n",
      "Epoch: 550, Loss= 0.7153, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.7712, Training Accuracy= 0.498\n",
      "Epoch: 570, Loss= 0.7522, Training Accuracy= 0.498\n",
      "Epoch: 580, Loss= 0.8147, Training Accuracy= 0.498\n",
      "Epoch: 590, Loss= 0.8138, Training Accuracy= 0.498\n",
      "Epoch: 600, Loss= 0.8067, Training Accuracy= 0.498\n",
      "Epoch: 610, Loss= 0.7861, Training Accuracy= 0.498\n",
      "Epoch: 620, Loss= 0.7700, Training Accuracy= 0.498\n",
      "Epoch: 630, Loss= 0.7590, Training Accuracy= 0.498\n",
      "Epoch: 640, Loss= 0.7329, Training Accuracy= 0.498\n",
      "Epoch: 650, Loss= 0.7069, Training Accuracy= 0.498\n",
      "Epoch: 660, Loss= 0.7167, Training Accuracy= 0.504\n",
      "Epoch: 670, Loss= 0.7150, Training Accuracy= 0.506\n",
      "Epoch: 680, Loss= 0.7130, Training Accuracy= 0.506\n",
      "Epoch: 690, Loss= 0.7023, Training Accuracy= 0.498\n",
      "Epoch: 700, Loss= 0.7008, Training Accuracy= 0.498\n",
      "Epoch: 710, Loss= 0.7026, Training Accuracy= 0.498\n",
      "Epoch: 720, Loss= 0.7004, Training Accuracy= 0.498\n",
      "Epoch: 730, Loss= 0.7064, Training Accuracy= 0.501\n",
      "Epoch: 740, Loss= 0.7069, Training Accuracy= 0.500\n",
      "Epoch: 750, Loss= 0.7077, Training Accuracy= 0.500\n",
      "Epoch: 760, Loss= 0.7095, Training Accuracy= 0.498\n",
      "Epoch: 770, Loss= 0.7179, Training Accuracy= 0.499\n",
      "Epoch: 780, Loss= 0.7176, Training Accuracy= 0.498\n",
      "Epoch: 790, Loss= 0.7134, Training Accuracy= 0.498\n",
      "Epoch: 800, Loss= 0.7203, Training Accuracy= 0.499\n",
      "Epoch: 810, Loss= 0.7196, Training Accuracy= 0.498\n",
      "Epoch: 820, Loss= 0.7629, Training Accuracy= 0.498\n",
      "Epoch: 830, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 840, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 850, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 860, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 870, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 880, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 890, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 900, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 910, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 930, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 940, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 950, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 960, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 970, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 980, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Epoch: 990, Loss= 0.8161, Training Accuracy= 0.498\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4959\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.8135, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.7717, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.7570, Training Accuracy= 0.502\n",
      "Epoch: 30, Loss= 0.7436, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.7386, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.7364, Training Accuracy= 0.502\n",
      "Epoch: 60, Loss= 0.7351, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.7338, Training Accuracy= 0.502\n",
      "Epoch: 80, Loss= 0.7313, Training Accuracy= 0.502\n",
      "Epoch: 90, Loss= 0.7420, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.7276, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.7241, Training Accuracy= 0.502\n",
      "Epoch: 120, Loss= 0.7209, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.7156, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.7131, Training Accuracy= 0.502\n",
      "Epoch: 150, Loss= 0.7122, Training Accuracy= 0.502\n",
      "Epoch: 160, Loss= 0.7119, Training Accuracy= 0.502\n",
      "Epoch: 170, Loss= 0.7104, Training Accuracy= 0.502\n",
      "Epoch: 180, Loss= 0.7085, Training Accuracy= 0.502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.7088, Training Accuracy= 0.502\n",
      "Epoch: 200, Loss= 0.7078, Training Accuracy= 0.502\n",
      "Epoch: 210, Loss= 0.7061, Training Accuracy= 0.502\n",
      "Epoch: 220, Loss= 0.7069, Training Accuracy= 0.502\n",
      "Epoch: 230, Loss= 0.7294, Training Accuracy= 0.502\n",
      "Epoch: 240, Loss= 0.7318, Training Accuracy= 0.502\n",
      "Epoch: 250, Loss= 0.7289, Training Accuracy= 0.502\n",
      "Epoch: 260, Loss= 0.7110, Training Accuracy= 0.502\n",
      "Epoch: 270, Loss= 0.7085, Training Accuracy= 0.502\n",
      "Epoch: 280, Loss= 0.8146, Training Accuracy= 0.502\n",
      "Epoch: 290, Loss= 0.8161, Training Accuracy= 0.502\n",
      "Epoch: 300, Loss= 0.8161, Training Accuracy= 0.502\n",
      "Epoch: 310, Loss= 0.8161, Training Accuracy= 0.502\n",
      "Epoch: 320, Loss= 0.8161, Training Accuracy= 0.502\n",
      "Epoch: 330, Loss= 0.8160, Training Accuracy= 0.502\n",
      "Epoch: 340, Loss= 0.8161, Training Accuracy= 0.502\n",
      "Epoch: 350, Loss= 0.8161, Training Accuracy= 0.502\n",
      "Epoch: 360, Loss= 0.8160, Training Accuracy= 0.502\n",
      "Epoch: 370, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 380, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 390, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 400, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 410, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 420, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 430, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 440, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 450, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 460, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 470, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.8163, Training Accuracy= 0.502\n",
      "Epoch: 490, Loss= 0.8162, Training Accuracy= 0.502\n",
      "Epoch: 500, Loss= 0.8162, Training Accuracy= 0.502\n",
      "Epoch: 510, Loss= 0.8162, Training Accuracy= 0.502\n",
      "Epoch: 520, Loss= 0.8162, Training Accuracy= 0.502\n",
      "Epoch: 530, Loss= 0.8162, Training Accuracy= 0.502\n",
      "Epoch: 540, Loss= 0.8161, Training Accuracy= 0.502\n",
      "Epoch: 550, Loss= 0.8161, Training Accuracy= 0.502\n",
      "Epoch: 560, Loss= 0.8160, Training Accuracy= 0.502\n",
      "Epoch: 570, Loss= 0.8160, Training Accuracy= 0.502\n",
      "Epoch: 580, Loss= 0.8158, Training Accuracy= 0.502\n",
      "Epoch: 590, Loss= 0.8156, Training Accuracy= 0.502\n",
      "Epoch: 600, Loss= 0.8153, Training Accuracy= 0.502\n",
      "Epoch: 610, Loss= 0.8143, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 0.8086, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 0.7840, Training Accuracy= 0.502\n",
      "Epoch: 640, Loss= 0.7838, Training Accuracy= 0.502\n",
      "Epoch: 650, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 660, Loss= 0.7838, Training Accuracy= 0.502\n",
      "Epoch: 670, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 680, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 690, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 700, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 710, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 740, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 750, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 760, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 770, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 780, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 790, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 800, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 810, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 820, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 830, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 840, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 850, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 860, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 870, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 880, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 890, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 900, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 910, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 920, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 930, Loss= 0.7837, Training Accuracy= 0.502\n",
      "Epoch: 940, Loss= 0.7838, Training Accuracy= 0.502\n",
      "Epoch: 950, Loss= 0.7838, Training Accuracy= 0.502\n",
      "Epoch: 960, Loss= 0.7838, Training Accuracy= 0.502\n",
      "Epoch: 970, Loss= 0.7838, Training Accuracy= 0.502\n",
      "Epoch: 980, Loss= 0.7838, Training Accuracy= 0.502\n",
      "Epoch: 990, Loss= 0.7838, Training Accuracy= 0.502\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5059\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.7767, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.7583, Training Accuracy= 0.495\n",
      "Epoch: 20, Loss= 0.7570, Training Accuracy= 0.495\n",
      "Epoch: 30, Loss= 0.7560, Training Accuracy= 0.495\n",
      "Epoch: 40, Loss= 0.7536, Training Accuracy= 0.495\n",
      "Epoch: 50, Loss= 0.7471, Training Accuracy= 0.495\n",
      "Epoch: 60, Loss= 0.7430, Training Accuracy= 0.495\n",
      "Epoch: 70, Loss= 0.7425, Training Accuracy= 0.495\n",
      "Epoch: 80, Loss= 0.7422, Training Accuracy= 0.495\n",
      "Epoch: 90, Loss= 0.7418, Training Accuracy= 0.495\n",
      "Epoch: 100, Loss= 0.7414, Training Accuracy= 0.495\n",
      "Epoch: 110, Loss= 0.7409, Training Accuracy= 0.495\n",
      "Epoch: 120, Loss= 0.7402, Training Accuracy= 0.495\n",
      "Epoch: 130, Loss= 0.7391, Training Accuracy= 0.495\n",
      "Epoch: 140, Loss= 0.7368, Training Accuracy= 0.495\n",
      "Epoch: 150, Loss= 0.7306, Training Accuracy= 0.495\n",
      "Epoch: 160, Loss= 0.7261, Training Accuracy= 0.495\n",
      "Epoch: 170, Loss= 0.7222, Training Accuracy= 0.495\n",
      "Epoch: 180, Loss= 0.7553, Training Accuracy= 0.495\n",
      "Epoch: 190, Loss= 0.7502, Training Accuracy= 0.495\n",
      "Epoch: 200, Loss= 0.7458, Training Accuracy= 0.495\n",
      "Epoch: 210, Loss= 0.7442, Training Accuracy= 0.495\n",
      "Epoch: 220, Loss= 0.7436, Training Accuracy= 0.495\n",
      "Epoch: 230, Loss= 0.7430, Training Accuracy= 0.495\n",
      "Epoch: 240, Loss= 0.7418, Training Accuracy= 0.495\n",
      "Epoch: 250, Loss= 0.7420, Training Accuracy= 0.495\n",
      "Epoch: 260, Loss= 0.7417, Training Accuracy= 0.495\n",
      "Epoch: 270, Loss= 0.7414, Training Accuracy= 0.495\n",
      "Epoch: 280, Loss= 0.7411, Training Accuracy= 0.495\n",
      "Epoch: 290, Loss= 0.7409, Training Accuracy= 0.495\n",
      "Epoch: 300, Loss= 0.7408, Training Accuracy= 0.495\n",
      "Epoch: 310, Loss= 0.7443, Training Accuracy= 0.495\n",
      "Epoch: 320, Loss= 0.7408, Training Accuracy= 0.495\n",
      "Epoch: 330, Loss= 0.7389, Training Accuracy= 0.495\n",
      "Epoch: 340, Loss= 0.7378, Training Accuracy= 0.495\n",
      "Epoch: 350, Loss= 0.7344, Training Accuracy= 0.495\n",
      "Epoch: 360, Loss= 0.7242, Training Accuracy= 0.495\n",
      "Epoch: 370, Loss= 0.7393, Training Accuracy= 0.495\n",
      "Epoch: 380, Loss= 0.7762, Training Accuracy= 0.495\n",
      "Epoch: 390, Loss= 0.7619, Training Accuracy= 0.495\n",
      "Epoch: 400, Loss= 0.7520, Training Accuracy= 0.495\n",
      "Epoch: 410, Loss= 0.7461, Training Accuracy= 0.495\n",
      "Epoch: 420, Loss= 0.7420, Training Accuracy= 0.495\n",
      "Epoch: 430, Loss= 0.7365, Training Accuracy= 0.495\n",
      "Epoch: 440, Loss= 0.7269, Training Accuracy= 0.495\n",
      "Epoch: 450, Loss= 0.7159, Training Accuracy= 0.495\n",
      "Epoch: 460, Loss= 0.7171, Training Accuracy= 0.495\n",
      "Epoch: 470, Loss= 0.7165, Training Accuracy= 0.495\n",
      "Epoch: 480, Loss= 0.7155, Training Accuracy= 0.495\n",
      "Epoch: 490, Loss= 0.7168, Training Accuracy= 0.495\n",
      "Epoch: 500, Loss= 0.7166, Training Accuracy= 0.494\n",
      "Epoch: 510, Loss= 0.7857, Training Accuracy= 0.495\n",
      "Epoch: 520, Loss= 0.7851, Training Accuracy= 0.495\n",
      "Epoch: 530, Loss= 0.7848, Training Accuracy= 0.495\n",
      "Epoch: 540, Loss= 0.7847, Training Accuracy= 0.495\n",
      "Epoch: 550, Loss= 0.7845, Training Accuracy= 0.495\n",
      "Epoch: 560, Loss= 0.7844, Training Accuracy= 0.495\n",
      "Epoch: 570, Loss= 0.7843, Training Accuracy= 0.495\n",
      "Epoch: 580, Loss= 0.7842, Training Accuracy= 0.495\n",
      "Epoch: 590, Loss= 0.7840, Training Accuracy= 0.495\n",
      "Epoch: 600, Loss= 0.7839, Training Accuracy= 0.495\n",
      "Epoch: 610, Loss= 0.7837, Training Accuracy= 0.495\n",
      "Epoch: 620, Loss= 0.7835, Training Accuracy= 0.495\n",
      "Epoch: 630, Loss= 0.7832, Training Accuracy= 0.495\n",
      "Epoch: 640, Loss= 0.7828, Training Accuracy= 0.495\n",
      "Epoch: 650, Loss= 0.7822, Training Accuracy= 0.495\n",
      "Epoch: 660, Loss= 0.7807, Training Accuracy= 0.495\n",
      "Epoch: 670, Loss= 0.7756, Training Accuracy= 0.495\n",
      "Epoch: 680, Loss= 0.7726, Training Accuracy= 0.495\n",
      "Epoch: 690, Loss= 0.7730, Training Accuracy= 0.495\n",
      "Epoch: 700, Loss= 0.7733, Training Accuracy= 0.495\n",
      "Epoch: 710, Loss= 0.7734, Training Accuracy= 0.495\n",
      "Epoch: 720, Loss= 0.7733, Training Accuracy= 0.495\n",
      "Epoch: 730, Loss= 0.7733, Training Accuracy= 0.495\n",
      "Epoch: 740, Loss= 0.7731, Training Accuracy= 0.495\n",
      "Epoch: 750, Loss= 0.7727, Training Accuracy= 0.495\n",
      "Epoch: 760, Loss= 0.7720, Training Accuracy= 0.495\n",
      "Epoch: 770, Loss= 0.7702, Training Accuracy= 0.495\n",
      "Epoch: 780, Loss= 0.7662, Training Accuracy= 0.495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.7599, Training Accuracy= 0.495\n",
      "Epoch: 800, Loss= 0.7584, Training Accuracy= 0.495\n",
      "Epoch: 810, Loss= 0.7573, Training Accuracy= 0.495\n",
      "Epoch: 820, Loss= 0.7566, Training Accuracy= 0.495\n",
      "Epoch: 830, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 840, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 850, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 860, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 870, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 880, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 890, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 900, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 910, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 920, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 930, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 940, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 950, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 960, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 970, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 980, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Epoch: 990, Loss= 0.7909, Training Accuracy= 0.495\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4913\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.8033, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.7894, Training Accuracy= 0.505\n",
      "Epoch: 20, Loss= 0.7848, Training Accuracy= 0.505\n",
      "Epoch: 30, Loss= 0.7848, Training Accuracy= 0.502\n",
      "Epoch: 40, Loss= 0.7875, Training Accuracy= 0.502\n",
      "Epoch: 50, Loss= 0.7812, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.7243, Training Accuracy= 0.502\n",
      "Epoch: 70, Loss= 0.7228, Training Accuracy= 0.501\n",
      "Epoch: 80, Loss= 0.7228, Training Accuracy= 0.501\n",
      "Epoch: 90, Loss= 0.7215, Training Accuracy= 0.501\n",
      "Epoch: 100, Loss= 0.7326, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.7326, Training Accuracy= 0.502\n",
      "Epoch: 120, Loss= 0.7325, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.7318, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.7308, Training Accuracy= 0.502\n",
      "Epoch: 150, Loss= 0.7295, Training Accuracy= 0.502\n",
      "Epoch: 160, Loss= 0.7593, Training Accuracy= 0.504\n",
      "Epoch: 170, Loss= 0.7184, Training Accuracy= 0.501\n",
      "Epoch: 180, Loss= 0.6997, Training Accuracy= 0.499\n",
      "Epoch: 190, Loss= 0.6994, Training Accuracy= 0.499\n",
      "Epoch: 200, Loss= 0.6991, Training Accuracy= 0.499\n",
      "Epoch: 210, Loss= 0.6990, Training Accuracy= 0.499\n",
      "Epoch: 220, Loss= 0.6990, Training Accuracy= 0.499\n",
      "Epoch: 230, Loss= 0.6990, Training Accuracy= 0.499\n",
      "Epoch: 240, Loss= 0.6989, Training Accuracy= 0.504\n",
      "Epoch: 250, Loss= 0.6990, Training Accuracy= 0.507\n",
      "Epoch: 260, Loss= 0.6993, Training Accuracy= 0.507\n",
      "Epoch: 270, Loss= 0.6996, Training Accuracy= 0.508\n",
      "Epoch: 280, Loss= 0.6991, Training Accuracy= 0.504\n",
      "Epoch: 290, Loss= 0.6992, Training Accuracy= 0.502\n",
      "Epoch: 300, Loss= 0.7001, Training Accuracy= 0.509\n",
      "Epoch: 310, Loss= 0.6998, Training Accuracy= 0.506\n",
      "Epoch: 320, Loss= 0.7278, Training Accuracy= 0.502\n",
      "Epoch: 330, Loss= 0.7278, Training Accuracy= 0.502\n",
      "Epoch: 340, Loss= 0.7281, Training Accuracy= 0.502\n",
      "Epoch: 350, Loss= 0.7266, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.7286, Training Accuracy= 0.502\n",
      "Epoch: 370, Loss= 0.7282, Training Accuracy= 0.502\n",
      "Epoch: 380, Loss= 0.7280, Training Accuracy= 0.502\n",
      "Epoch: 390, Loss= 0.7278, Training Accuracy= 0.502\n",
      "Epoch: 400, Loss= 0.7273, Training Accuracy= 0.502\n",
      "Epoch: 410, Loss= 0.7252, Training Accuracy= 0.503\n",
      "Epoch: 420, Loss= 0.7501, Training Accuracy= 0.502\n",
      "Epoch: 430, Loss= 0.7492, Training Accuracy= 0.502\n",
      "Epoch: 440, Loss= 0.7486, Training Accuracy= 0.502\n",
      "Epoch: 450, Loss= 0.7479, Training Accuracy= 0.502\n",
      "Epoch: 460, Loss= 0.7459, Training Accuracy= 0.502\n",
      "Epoch: 470, Loss= 0.7452, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.7445, Training Accuracy= 0.502\n",
      "Epoch: 490, Loss= 0.7435, Training Accuracy= 0.502\n",
      "Epoch: 500, Loss= 0.7515, Training Accuracy= 0.502\n",
      "Epoch: 510, Loss= 0.7528, Training Accuracy= 0.502\n",
      "Epoch: 520, Loss= 0.7462, Training Accuracy= 0.502\n",
      "Epoch: 530, Loss= 0.7463, Training Accuracy= 0.502\n",
      "Epoch: 540, Loss= 0.7517, Training Accuracy= 0.502\n",
      "Epoch: 550, Loss= 0.7397, Training Accuracy= 0.502\n",
      "Epoch: 560, Loss= 0.7841, Training Accuracy= 0.502\n",
      "Epoch: 570, Loss= 0.7650, Training Accuracy= 0.502\n",
      "Epoch: 580, Loss= 0.7589, Training Accuracy= 0.502\n",
      "Epoch: 590, Loss= 0.7531, Training Accuracy= 0.502\n",
      "Epoch: 600, Loss= 0.7467, Training Accuracy= 0.502\n",
      "Epoch: 610, Loss= 0.7443, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 0.7432, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 0.7474, Training Accuracy= 0.502\n",
      "Epoch: 640, Loss= 0.7482, Training Accuracy= 0.502\n",
      "Epoch: 650, Loss= 0.7483, Training Accuracy= 0.502\n",
      "Epoch: 660, Loss= 0.7482, Training Accuracy= 0.502\n",
      "Epoch: 670, Loss= 0.7479, Training Accuracy= 0.502\n",
      "Epoch: 680, Loss= 0.7473, Training Accuracy= 0.502\n",
      "Epoch: 690, Loss= 0.7461, Training Accuracy= 0.502\n",
      "Epoch: 700, Loss= 0.7447, Training Accuracy= 0.502\n",
      "Epoch: 710, Loss= 0.7424, Training Accuracy= 0.502\n",
      "Epoch: 720, Loss= 0.7420, Training Accuracy= 0.502\n",
      "Epoch: 730, Loss= 0.7387, Training Accuracy= 0.502\n",
      "Epoch: 740, Loss= 0.7374, Training Accuracy= 0.502\n",
      "Epoch: 750, Loss= 0.7370, Training Accuracy= 0.502\n",
      "Epoch: 760, Loss= 0.7367, Training Accuracy= 0.502\n",
      "Epoch: 770, Loss= 0.7128, Training Accuracy= 0.502\n",
      "Epoch: 780, Loss= 0.7084, Training Accuracy= 0.502\n",
      "Epoch: 790, Loss= 0.7103, Training Accuracy= 0.502\n",
      "Epoch: 800, Loss= 0.7117, Training Accuracy= 0.502\n",
      "Epoch: 810, Loss= 0.7184, Training Accuracy= 0.502\n",
      "Epoch: 820, Loss= 0.7197, Training Accuracy= 0.502\n",
      "Epoch: 830, Loss= 0.7197, Training Accuracy= 0.502\n",
      "Epoch: 840, Loss= 0.7193, Training Accuracy= 0.502\n",
      "Epoch: 850, Loss= 0.7195, Training Accuracy= 0.502\n",
      "Epoch: 860, Loss= 0.7195, Training Accuracy= 0.502\n",
      "Epoch: 870, Loss= 0.7195, Training Accuracy= 0.502\n",
      "Epoch: 880, Loss= 0.7195, Training Accuracy= 0.502\n",
      "Epoch: 890, Loss= 0.7195, Training Accuracy= 0.502\n",
      "Epoch: 900, Loss= 0.7197, Training Accuracy= 0.502\n",
      "Epoch: 910, Loss= 0.7197, Training Accuracy= 0.502\n",
      "Epoch: 920, Loss= 0.7197, Training Accuracy= 0.502\n",
      "Epoch: 930, Loss= 0.7196, Training Accuracy= 0.502\n",
      "Epoch: 940, Loss= 0.7196, Training Accuracy= 0.502\n",
      "Epoch: 950, Loss= 0.7195, Training Accuracy= 0.502\n",
      "Epoch: 960, Loss= 0.7192, Training Accuracy= 0.502\n",
      "Epoch: 970, Loss= 0.7193, Training Accuracy= 0.502\n",
      "Epoch: 980, Loss= 0.7193, Training Accuracy= 0.502\n",
      "Epoch: 990, Loss= 0.7192, Training Accuracy= 0.502\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5034\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.7780, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.7294, Training Accuracy= 0.495\n",
      "Epoch: 20, Loss= 0.7190, Training Accuracy= 0.495\n",
      "Epoch: 30, Loss= 0.7130, Training Accuracy= 0.495\n",
      "Epoch: 40, Loss= 0.7101, Training Accuracy= 0.494\n",
      "Epoch: 50, Loss= 0.7086, Training Accuracy= 0.495\n",
      "Epoch: 60, Loss= 0.7075, Training Accuracy= 0.494\n",
      "Epoch: 70, Loss= 0.7071, Training Accuracy= 0.495\n",
      "Epoch: 80, Loss= 0.7106, Training Accuracy= 0.494\n",
      "Epoch: 90, Loss= 0.7076, Training Accuracy= 0.495\n",
      "Epoch: 100, Loss= 0.7061, Training Accuracy= 0.497\n",
      "Epoch: 110, Loss= 0.7058, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.7077, Training Accuracy= 0.496\n",
      "Epoch: 130, Loss= 0.7076, Training Accuracy= 0.499\n",
      "Epoch: 140, Loss= 0.7231, Training Accuracy= 0.495\n",
      "Epoch: 150, Loss= 0.7042, Training Accuracy= 0.502\n",
      "Epoch: 160, Loss= 0.7038, Training Accuracy= 0.499\n",
      "Epoch: 170, Loss= 0.7016, Training Accuracy= 0.506\n",
      "Epoch: 180, Loss= 0.7023, Training Accuracy= 0.509\n",
      "Epoch: 190, Loss= 0.7072, Training Accuracy= 0.500\n",
      "Epoch: 200, Loss= 0.7087, Training Accuracy= 0.497\n",
      "Epoch: 210, Loss= 0.8001, Training Accuracy= 0.495\n",
      "Epoch: 220, Loss= 0.7687, Training Accuracy= 0.495\n",
      "Epoch: 230, Loss= 0.7601, Training Accuracy= 0.495\n",
      "Epoch: 240, Loss= 0.7491, Training Accuracy= 0.495\n",
      "Epoch: 250, Loss= 0.7440, Training Accuracy= 0.495\n",
      "Epoch: 260, Loss= 0.7421, Training Accuracy= 0.495\n",
      "Epoch: 270, Loss= 0.7205, Training Accuracy= 0.495\n",
      "Epoch: 280, Loss= 0.7178, Training Accuracy= 0.494\n",
      "Epoch: 290, Loss= 0.8541, Training Accuracy= 0.495\n",
      "Epoch: 300, Loss= 0.8539, Training Accuracy= 0.495\n",
      "Epoch: 310, Loss= 0.8534, Training Accuracy= 0.495\n",
      "Epoch: 320, Loss= 0.8436, Training Accuracy= 0.495\n",
      "Epoch: 330, Loss= 0.8431, Training Accuracy= 0.495\n",
      "Epoch: 340, Loss= 0.8429, Training Accuracy= 0.495\n",
      "Epoch: 350, Loss= 0.8423, Training Accuracy= 0.495\n",
      "Epoch: 360, Loss= 0.8400, Training Accuracy= 0.495\n",
      "Epoch: 370, Loss= 0.8277, Training Accuracy= 0.495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.8257, Training Accuracy= 0.495\n",
      "Epoch: 390, Loss= 0.8243, Training Accuracy= 0.495\n",
      "Epoch: 400, Loss= 0.8233, Training Accuracy= 0.495\n",
      "Epoch: 410, Loss= 0.8224, Training Accuracy= 0.495\n",
      "Epoch: 420, Loss= 0.8216, Training Accuracy= 0.495\n",
      "Epoch: 430, Loss= 0.8208, Training Accuracy= 0.495\n",
      "Epoch: 440, Loss= 0.8199, Training Accuracy= 0.495\n",
      "Epoch: 450, Loss= 0.8701, Training Accuracy= 0.495\n",
      "Epoch: 460, Loss= 0.8715, Training Accuracy= 0.495\n",
      "Epoch: 470, Loss= 0.8631, Training Accuracy= 0.495\n",
      "Epoch: 480, Loss= 0.8333, Training Accuracy= 0.495\n",
      "Epoch: 490, Loss= 0.8328, Training Accuracy= 0.495\n",
      "Epoch: 500, Loss= 0.8338, Training Accuracy= 0.495\n",
      "Epoch: 510, Loss= 0.8357, Training Accuracy= 0.495\n",
      "Epoch: 520, Loss= 0.8360, Training Accuracy= 0.495\n",
      "Epoch: 530, Loss= 0.8361, Training Accuracy= 0.495\n",
      "Epoch: 540, Loss= 0.8363, Training Accuracy= 0.495\n",
      "Epoch: 550, Loss= 0.8363, Training Accuracy= 0.495\n",
      "Epoch: 560, Loss= 0.8363, Training Accuracy= 0.495\n",
      "Epoch: 570, Loss= 0.8364, Training Accuracy= 0.495\n",
      "Epoch: 580, Loss= 0.8361, Training Accuracy= 0.495\n",
      "Epoch: 590, Loss= 0.8360, Training Accuracy= 0.495\n",
      "Epoch: 600, Loss= 0.8360, Training Accuracy= 0.495\n",
      "Epoch: 610, Loss= 0.8360, Training Accuracy= 0.495\n",
      "Epoch: 620, Loss= 0.8359, Training Accuracy= 0.495\n",
      "Epoch: 630, Loss= 0.8347, Training Accuracy= 0.495\n",
      "Epoch: 640, Loss= 0.8345, Training Accuracy= 0.495\n",
      "Epoch: 650, Loss= 0.8344, Training Accuracy= 0.495\n",
      "Epoch: 660, Loss= 0.8343, Training Accuracy= 0.495\n",
      "Epoch: 670, Loss= 0.8343, Training Accuracy= 0.495\n",
      "Epoch: 680, Loss= 0.8343, Training Accuracy= 0.495\n",
      "Epoch: 690, Loss= 0.8341, Training Accuracy= 0.495\n",
      "Epoch: 700, Loss= 0.8341, Training Accuracy= 0.495\n",
      "Epoch: 710, Loss= 0.8341, Training Accuracy= 0.495\n",
      "Epoch: 720, Loss= 0.8345, Training Accuracy= 0.495\n",
      "Epoch: 730, Loss= 0.8340, Training Accuracy= 0.495\n",
      "Epoch: 740, Loss= 0.8340, Training Accuracy= 0.495\n",
      "Epoch: 750, Loss= 0.8347, Training Accuracy= 0.495\n",
      "Epoch: 760, Loss= 0.8339, Training Accuracy= 0.495\n",
      "Epoch: 770, Loss= 0.8339, Training Accuracy= 0.495\n",
      "Epoch: 780, Loss= 0.8340, Training Accuracy= 0.495\n",
      "Epoch: 790, Loss= 0.8340, Training Accuracy= 0.495\n",
      "Epoch: 800, Loss= 0.8340, Training Accuracy= 0.495\n",
      "Epoch: 810, Loss= 0.8341, Training Accuracy= 0.495\n",
      "Epoch: 820, Loss= 0.8341, Training Accuracy= 0.495\n",
      "Epoch: 830, Loss= 0.8342, Training Accuracy= 0.495\n",
      "Epoch: 840, Loss= 0.8343, Training Accuracy= 0.495\n",
      "Epoch: 850, Loss= 0.8344, Training Accuracy= 0.495\n",
      "Epoch: 860, Loss= 0.8345, Training Accuracy= 0.495\n",
      "Epoch: 870, Loss= 0.8346, Training Accuracy= 0.495\n",
      "Epoch: 880, Loss= 0.8348, Training Accuracy= 0.495\n",
      "Epoch: 890, Loss= 0.8349, Training Accuracy= 0.495\n",
      "Epoch: 900, Loss= 0.8350, Training Accuracy= 0.495\n",
      "Epoch: 910, Loss= 0.8352, Training Accuracy= 0.495\n",
      "Epoch: 920, Loss= 0.8354, Training Accuracy= 0.495\n",
      "Epoch: 930, Loss= 0.8356, Training Accuracy= 0.495\n",
      "Epoch: 940, Loss= 0.8358, Training Accuracy= 0.495\n",
      "Epoch: 950, Loss= 0.8360, Training Accuracy= 0.495\n",
      "Epoch: 960, Loss= 0.8362, Training Accuracy= 0.495\n",
      "Epoch: 970, Loss= 0.8364, Training Accuracy= 0.495\n",
      "Epoch: 980, Loss= 0.8366, Training Accuracy= 0.495\n",
      "Epoch: 990, Loss= 0.8368, Training Accuracy= 0.495\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5009\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.9053, Training Accuracy= 0.499\n",
      "Epoch: 10, Loss= 0.7716, Training Accuracy= 0.499\n",
      "Epoch: 20, Loss= 0.7427, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.7299, Training Accuracy= 0.499\n",
      "Epoch: 40, Loss= 0.7228, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.7186, Training Accuracy= 0.499\n",
      "Epoch: 60, Loss= 0.7157, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.7129, Training Accuracy= 0.499\n",
      "Epoch: 80, Loss= 0.7089, Training Accuracy= 0.501\n",
      "Epoch: 90, Loss= 0.7044, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.7026, Training Accuracy= 0.505\n",
      "Epoch: 110, Loss= 0.7018, Training Accuracy= 0.507\n",
      "Epoch: 120, Loss= 0.8063, Training Accuracy= 0.499\n",
      "Epoch: 130, Loss= 0.7584, Training Accuracy= 0.499\n",
      "Epoch: 140, Loss= 0.7498, Training Accuracy= 0.500\n",
      "Epoch: 150, Loss= 0.9336, Training Accuracy= 0.499\n",
      "Epoch: 160, Loss= 0.8847, Training Accuracy= 0.499\n",
      "Epoch: 170, Loss= 0.8253, Training Accuracy= 0.499\n",
      "Epoch: 180, Loss= 0.7785, Training Accuracy= 0.500\n",
      "Epoch: 190, Loss= 0.8769, Training Accuracy= 0.499\n",
      "Epoch: 200, Loss= 0.8485, Training Accuracy= 0.499\n",
      "Epoch: 210, Loss= 0.7951, Training Accuracy= 0.500\n",
      "Epoch: 220, Loss= 0.8910, Training Accuracy= 0.499\n",
      "Epoch: 230, Loss= 0.8428, Training Accuracy= 0.499\n",
      "Epoch: 240, Loss= 0.8624, Training Accuracy= 0.499\n",
      "Epoch: 250, Loss= 0.8476, Training Accuracy= 0.499\n",
      "Epoch: 260, Loss= 0.8494, Training Accuracy= 0.499\n",
      "Epoch: 270, Loss= 0.7960, Training Accuracy= 0.499\n",
      "Epoch: 280, Loss= 0.7892, Training Accuracy= 0.499\n",
      "Epoch: 290, Loss= 0.7874, Training Accuracy= 0.499\n",
      "Epoch: 300, Loss= 0.7693, Training Accuracy= 0.499\n",
      "Epoch: 310, Loss= 0.7470, Training Accuracy= 0.499\n",
      "Epoch: 320, Loss= 0.7617, Training Accuracy= 0.499\n",
      "Epoch: 330, Loss= 0.7600, Training Accuracy= 0.499\n",
      "Epoch: 340, Loss= 0.7486, Training Accuracy= 0.499\n",
      "Epoch: 350, Loss= 0.7420, Training Accuracy= 0.499\n",
      "Epoch: 360, Loss= 0.7412, Training Accuracy= 0.499\n",
      "Epoch: 370, Loss= 0.7356, Training Accuracy= 0.499\n",
      "Epoch: 380, Loss= 0.8340, Training Accuracy= 0.499\n",
      "Epoch: 390, Loss= 0.7743, Training Accuracy= 0.499\n",
      "Epoch: 400, Loss= 0.7646, Training Accuracy= 0.499\n",
      "Epoch: 410, Loss= 0.7515, Training Accuracy= 0.499\n",
      "Epoch: 420, Loss= 0.9719, Training Accuracy= 0.499\n",
      "Epoch: 430, Loss= 0.9475, Training Accuracy= 0.499\n",
      "Epoch: 440, Loss= 0.9459, Training Accuracy= 0.499\n",
      "Epoch: 450, Loss= 0.9426, Training Accuracy= 0.499\n",
      "Epoch: 460, Loss= 0.9261, Training Accuracy= 0.499\n",
      "Epoch: 470, Loss= 0.9011, Training Accuracy= 0.499\n",
      "Epoch: 480, Loss= 0.9004, Training Accuracy= 0.499\n",
      "Epoch: 490, Loss= 0.9001, Training Accuracy= 0.499\n",
      "Epoch: 500, Loss= 0.8998, Training Accuracy= 0.499\n",
      "Epoch: 510, Loss= 0.8993, Training Accuracy= 0.499\n",
      "Epoch: 520, Loss= 0.8983, Training Accuracy= 0.499\n",
      "Epoch: 530, Loss= 0.8956, Training Accuracy= 0.499\n",
      "Epoch: 540, Loss= 0.8759, Training Accuracy= 0.497\n",
      "Epoch: 550, Loss= 0.7851, Training Accuracy= 0.501\n",
      "Epoch: 560, Loss= 0.7812, Training Accuracy= 0.499\n",
      "Epoch: 570, Loss= 0.7804, Training Accuracy= 0.499\n",
      "Epoch: 580, Loss= 0.7801, Training Accuracy= 0.499\n",
      "Epoch: 590, Loss= 0.7806, Training Accuracy= 0.499\n",
      "Epoch: 600, Loss= 0.7822, Training Accuracy= 0.499\n",
      "Epoch: 610, Loss= 0.7858, Training Accuracy= 0.499\n",
      "Epoch: 620, Loss= 0.7899, Training Accuracy= 0.499\n",
      "Epoch: 630, Loss= 0.7915, Training Accuracy= 0.499\n",
      "Epoch: 640, Loss= 0.7912, Training Accuracy= 0.499\n",
      "Epoch: 650, Loss= 0.7902, Training Accuracy= 0.499\n",
      "Epoch: 660, Loss= 0.7891, Training Accuracy= 0.499\n",
      "Epoch: 670, Loss= 0.7881, Training Accuracy= 0.499\n",
      "Epoch: 680, Loss= 0.7872, Training Accuracy= 0.499\n",
      "Epoch: 690, Loss= 0.7864, Training Accuracy= 0.499\n",
      "Epoch: 700, Loss= 0.7857, Training Accuracy= 0.499\n",
      "Epoch: 710, Loss= 0.7851, Training Accuracy= 0.499\n",
      "Epoch: 720, Loss= 0.7846, Training Accuracy= 0.499\n",
      "Epoch: 730, Loss= 0.7842, Training Accuracy= 0.499\n",
      "Epoch: 740, Loss= 0.7838, Training Accuracy= 0.499\n",
      "Epoch: 750, Loss= 0.7835, Training Accuracy= 0.499\n",
      "Epoch: 760, Loss= 0.7832, Training Accuracy= 0.499\n",
      "Epoch: 770, Loss= 0.7829, Training Accuracy= 0.499\n",
      "Epoch: 780, Loss= 0.7826, Training Accuracy= 0.499\n",
      "Epoch: 790, Loss= 0.7824, Training Accuracy= 0.499\n",
      "Epoch: 800, Loss= 0.7821, Training Accuracy= 0.499\n",
      "Epoch: 810, Loss= 0.7819, Training Accuracy= 0.499\n",
      "Epoch: 820, Loss= 0.7817, Training Accuracy= 0.499\n",
      "Epoch: 830, Loss= 0.7815, Training Accuracy= 0.499\n",
      "Epoch: 840, Loss= 0.7814, Training Accuracy= 0.499\n",
      "Epoch: 850, Loss= 0.7812, Training Accuracy= 0.499\n",
      "Epoch: 860, Loss= 0.7810, Training Accuracy= 0.499\n",
      "Epoch: 870, Loss= 0.7809, Training Accuracy= 0.499\n",
      "Epoch: 880, Loss= 0.7807, Training Accuracy= 0.499\n",
      "Epoch: 890, Loss= 0.7806, Training Accuracy= 0.499\n",
      "Epoch: 900, Loss= 0.7804, Training Accuracy= 0.499\n",
      "Epoch: 910, Loss= 0.7803, Training Accuracy= 0.499\n",
      "Epoch: 920, Loss= 0.7801, Training Accuracy= 0.499\n",
      "Epoch: 930, Loss= 0.7799, Training Accuracy= 0.499\n",
      "Epoch: 940, Loss= 0.7795, Training Accuracy= 0.499\n",
      "Epoch: 950, Loss= 0.7790, Training Accuracy= 0.499\n",
      "Epoch: 960, Loss= 0.7779, Training Accuracy= 0.499\n",
      "Epoch: 970, Loss= 0.7754, Training Accuracy= 0.499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.7719, Training Accuracy= 0.499\n",
      "Epoch: 990, Loss= 0.7678, Training Accuracy= 0.499\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5074\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 10, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 20, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 30, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 40, Loss= 0.6922, Training Accuracy= 0.519\n",
      "Epoch: 50, Loss= 0.6921, Training Accuracy= 0.520\n",
      "Epoch: 60, Loss= 0.6918, Training Accuracy= 0.521\n",
      "Epoch: 70, Loss= 0.6913, Training Accuracy= 0.526\n",
      "Epoch: 80, Loss= 0.6909, Training Accuracy= 0.527\n",
      "Epoch: 90, Loss= 0.6909, Training Accuracy= 0.525\n",
      "Epoch: 100, Loss= 0.6912, Training Accuracy= 0.529\n",
      "Epoch: 110, Loss= 0.6910, Training Accuracy= 0.531\n",
      "Epoch: 120, Loss= 0.6908, Training Accuracy= 0.525\n",
      "Epoch: 130, Loss= 0.6907, Training Accuracy= 0.531\n",
      "Epoch: 140, Loss= 0.6903, Training Accuracy= 0.534\n",
      "Epoch: 150, Loss= 0.6928, Training Accuracy= 0.521\n",
      "Epoch: 160, Loss= 0.6911, Training Accuracy= 0.530\n",
      "Epoch: 170, Loss= 0.6916, Training Accuracy= 0.531\n",
      "Epoch: 180, Loss= 0.6911, Training Accuracy= 0.529\n",
      "Epoch: 190, Loss= 0.6923, Training Accuracy= 0.527\n",
      "Epoch: 200, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 220, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 230, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 240, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 250, Loss= 0.6935, Training Accuracy= 0.510\n",
      "Epoch: 260, Loss= 0.6936, Training Accuracy= 0.510\n",
      "Epoch: 270, Loss= 0.6938, Training Accuracy= 0.518\n",
      "Epoch: 280, Loss= 0.6936, Training Accuracy= 0.517\n",
      "Epoch: 290, Loss= 0.6938, Training Accuracy= 0.519\n",
      "Epoch: 300, Loss= 0.6942, Training Accuracy= 0.517\n",
      "Epoch: 310, Loss= 0.6943, Training Accuracy= 0.513\n",
      "Epoch: 320, Loss= 0.6935, Training Accuracy= 0.513\n",
      "Epoch: 330, Loss= 0.6940, Training Accuracy= 0.511\n",
      "Epoch: 340, Loss= 0.6988, Training Accuracy= 0.512\n",
      "Epoch: 350, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 360, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 390, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 400, Loss= 0.6978, Training Accuracy= 0.506\n",
      "Epoch: 410, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 420, Loss= 0.6950, Training Accuracy= 0.508\n",
      "Epoch: 430, Loss= 0.6944, Training Accuracy= 0.508\n",
      "Epoch: 440, Loss= 0.6951, Training Accuracy= 0.508\n",
      "Epoch: 450, Loss= 0.6949, Training Accuracy= 0.508\n",
      "Epoch: 460, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 470, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 500, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 520, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 530, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 540, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 550, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 560, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 570, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 580, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 590, Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 600, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 610, Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 620, Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 630, Loss= 0.6926, Training Accuracy= 0.508\n",
      "Epoch: 640, Loss= 0.6926, Training Accuracy= 0.510\n",
      "Epoch: 650, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 660, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 670, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 680, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 690, Loss= 0.6927, Training Accuracy= 0.517\n",
      "Epoch: 700, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 710, Loss= 0.6929, Training Accuracy= 0.517\n",
      "Epoch: 720, Loss= 0.6932, Training Accuracy= 0.517\n",
      "Epoch: 730, Loss= 0.6935, Training Accuracy= 0.517\n",
      "Epoch: 740, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 750, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 770, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 780, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 790, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 800, Loss= 0.6928, Training Accuracy= 0.521\n",
      "Epoch: 810, Loss= 0.6928, Training Accuracy= 0.520\n",
      "Epoch: 820, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 830, Loss= 0.6935, Training Accuracy= 0.520\n",
      "Epoch: 840, Loss= 0.6923, Training Accuracy= 0.523\n",
      "Epoch: 850, Loss= 0.6922, Training Accuracy= 0.521\n",
      "Epoch: 860, Loss= 0.6925, Training Accuracy= 0.524\n",
      "Epoch: 870, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 880, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 890, Loss= 0.6925, Training Accuracy= 0.522\n",
      "Epoch: 900, Loss= 0.6928, Training Accuracy= 0.517\n",
      "Epoch: 910, Loss= 0.6926, Training Accuracy= 0.520\n",
      "Epoch: 920, Loss= 0.6926, Training Accuracy= 0.522\n",
      "Epoch: 930, Loss= 0.6940, Training Accuracy= 0.521\n",
      "Epoch: 940, Loss= 0.6919, Training Accuracy= 0.524\n",
      "Epoch: 950, Loss= 0.6946, Training Accuracy= 0.511\n",
      "Epoch: 960, Loss= 0.6960, Training Accuracy= 0.513\n",
      "Epoch: 970, Loss= 0.6942, Training Accuracy= 0.507\n",
      "Epoch: 980, Loss= 0.6944, Training Accuracy= 0.514\n",
      "Epoch: 990, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5086\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7032, Training Accuracy= 0.502\n",
      "Epoch: 10, Loss= 0.6957, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6948, Training Accuracy= 0.505\n",
      "Epoch: 30, Loss= 0.6946, Training Accuracy= 0.504\n",
      "Epoch: 40, Loss= 0.6945, Training Accuracy= 0.505\n",
      "Epoch: 50, Loss= 0.6943, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6942, Training Accuracy= 0.505\n",
      "Epoch: 70, Loss= 0.6940, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6939, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6938, Training Accuracy= 0.511\n",
      "Epoch: 100, Loss= 0.6936, Training Accuracy= 0.510\n",
      "Epoch: 110, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 120, Loss= 0.6934, Training Accuracy= 0.514\n",
      "Epoch: 130, Loss= 0.6934, Training Accuracy= 0.513\n",
      "Epoch: 140, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 150, Loss= 0.6953, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6948, Training Accuracy= 0.509\n",
      "Epoch: 170, Loss= 0.6935, Training Accuracy= 0.509\n",
      "Epoch: 180, Loss= 0.7007, Training Accuracy= 0.496\n",
      "Epoch: 190, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 200, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 210, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 220, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 230, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 240, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 250, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 260, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 270, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 280, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 290, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 300, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 310, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 320, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 330, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 340, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 350, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 360, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 370, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 380, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 390, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 400, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 410, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 420, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 430, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 440, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 450, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 460, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 470, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 480, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 490, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 500, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 510, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 520, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 530, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 540, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 550, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 560, Loss= 0.7002, Training Accuracy= 0.496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 580, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 590, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 600, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 610, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 620, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 630, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 640, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 650, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 660, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 670, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 680, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 690, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 700, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 710, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 720, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 730, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 740, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 750, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 760, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 770, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 780, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 790, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 800, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 810, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 820, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 830, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 840, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 850, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 860, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 870, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 880, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 890, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 900, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 910, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 920, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 930, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 940, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 950, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 960, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 970, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 980, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Epoch: 990, Loss= 0.7002, Training Accuracy= 0.496\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5115\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.3\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.50739998, 0.51090002, 0.49590001, 0.50590003, 0.49129999, 0.50340003, 0.50089997, 0.50739998, 0.5086, 0.5115]\n",
      "mean of test_accuracies_10replications:  0.50432\n",
      "standard deviation of test_accuracies_10replications_std_mean:  6.2257242389e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmcXFWZ//HPtzvdnaSTkB1CFhIw\ngBBAsAUUlE0HyChxF2RTwkRHZVCYn8LIiPs44qio4ICCuCAIDmJkMWpYFUES9rCGsCRkJfvWSS/P\n7497O6neb3W6uqq7v+/Xq15d99S59z5Vqe4n555zz1FEYGZmllVZsQMwM7PexYnDzMzy4sRhZmZ5\nceIwM7O8OHGYmVlenDjMzCwvThxm3UTSsZKW5GwvkHRsAc5zp6Szu/u4Zlk5cVjJk/QZSfMkbZN0\nXR77vSzpnQUMrUMRcWBE3LMrx5D0ZUm/anHckyPi57sUnNkuGFDsAMwyWAp8HTgRGFSok0gaEBH1\nhTq+WV/hFoeVvIi4JSJuBVa3fE3SaEm3SVonaY2k+yWVSfolMAn4g6RNkj7fxr7HSloi6QuSlgM/\nS8vfLemx9JgPSDo4Z5+XJV0s6WlJayX9TNLAtuLObfFIKpf0H5JelLRR0nxJE9PXLpe0WNKGtPzt\naflJwH8AH0nfw+Np+T2Szk2fl0m6RNIrklZK+oWk3dLXJksKSWdLelXS65K+2PV/CbOEE4f1dhcC\nS4AxwO4kf2gjIs4EXgXeExFDIuLb7ey/BzAS2AuYJekw4FrgE8Ao4CpgtqSqnH1OJ2n97APsC1yS\nIc4LgNOA6cAw4BxgS/raw8Cb0jh+DdwsaWBE/BH4JvCb9D0c0sZxP5Y+jgP2BoYAP2pR52hgP+AE\n4EuS3pghXrN2OXFYb1cHjAP2ioi6iLg/8puArRG4NCK2RcRW4F+AqyLioYhoSPsStgFH5uzzo4hY\nHBFrgG+QJITOnAtcEhHPReLxiFgNEBG/iojVEVEfEf8DVJH8oc/idOC7EbEoIjYBFwOnSsq9DP2V\niNgaEY8DjwNtJSCzzJw4rLe7DFgI/EnSIkkX5bn/qoiozdneC7gwvUy1TtI6YCKwZ06dxTnPX2nx\nWnsmAi+29YKkCyU9I2l9er7dgNEZ498zjSE3ngEkra8my3OebyFplZh1mROH9WoRsTEiLoyIvYH3\nABdIOqHp5SyHaLG9GPhGRAzPeQyOiBty6kzMeT6JpPO+M4tJLm01k/ZnfAH4MDAiIoYD6wFlfA9L\nSZJdbjz1wIoMMZl1iROHlTxJA9IO6HKgXNLApksxaUf2GyQJ2AA0pA9I/njunefpfgJ8UtIRSlRL\n+mdJQ3PqfFrSBEkjSfpUfpPhuD8FviZpanrcgyWNAoaS/KFfBQyQ9CWSPpAmK4DJktr7Xb0B+Jyk\nKZKGsLNPxKPDrGCcOKw3uATYClwEnJE+b+qQngr8BdgE/B24Mufeif8CLkkvOf17lhNFxDySfo4f\nAWtJLoN9rEW1XwN/Ahalj69nOPR3gZvS/TYA15AMLZ4D3Ak8T3KZqZbml8JuTn+ulvRIG8e9Fvgl\ncB/wUrr/eRniMesyeSEns+wkvQycGxF/KXYsZsXiFoeZmeWl08Qh6ShJf5b0fDpq5SVJizLsd216\nQ9JT7bx+uqQn0scDkjxE0MysF+j0UpWkZ4HPAfPZ2elI0xj0DvZ7B8l1519ExLQ2Xn8b8ExErJV0\nMvDliDgi/7dgZmY9KctcVesj4s58DxwR90ma3MHrD+RsPghMyPccZmbW87IkjrslXQbcQnIHLQAR\n0dYIj66aSTKypE2SZgGzAKqrq9+8//77d+Opzcz6vvnz578eEWO641hZEkfT5aOanLIAju+OACQd\nR5I4jm6vTkRcDVwNUFNTE/PmzeuOU5uZ9RuSXum8VjadJo6IOK67TtZSOuvoT4GTO+szMTOz0pBl\nVNXukq6RdGe6fYCkmbt6YkmTSC5/nRkRz+/q8czMrGdkuY/jOpK7W5smcnse+GxnO0m6geRO3v3S\nNQ9mSvqkpE+mVb5EMm31lenaB77+ZGbWC2Tp4xgdETdJuhggIuolNXS2U0R0ONV0RJxLMtW0mZn1\nIllaHJvTydgCQNKRJLN3mplZP5SlxXEBMBvYR9LfSFZa+2BBozIzs5KVZVTVI5KOIVmRTMBzEVFX\n8MjMzKwkZRlVNZhkOuvPRsRTJGsDvLvgkZmZWUnK0sfxM2A78NZ0ewnZ1h8oiBfXvsiMG2fwrb9+\nq1ghmJn1a1n6OPaJiI9IOg0gIramq60Vxbqt65j93GwGlGUJ3czMuluWFsd2SYPYOapqH3LmrCqW\nugZ3s5iZFUOW/7ZfCvwRmCjpeuAoWi+l2ePqGp04zMyKocPEkV6SehZ4P3Akyaiq8yPi9R6IrUP1\njfXFDsHMrF/qMHFEREi6NSLeDNzeQzFl4ktVZmbFkaWP40FJbyl4JHlyi8PMrDiy9HEcB3winct9\nM8nlqoiIgwsaWSfcx2FmVhxZEsfJBY+iC3ypysysOLIkjo0Zy3qUL1WZmRVHlj6OR4BVJOtwvJA+\nf0nSI5LeXMjgOuJLVWZmxZElcfwRmB4RoyNiFMmlq5uATwFXFjK4jrjFYWZWHFkSR01EzGnaiIg/\nAe+IiAeBqoJF1gn3cZiZFUeWPo41kr4A3JhufwRYK6kcaCxYZJ3wpSozs+LI0uL4KDABuDV9TEzL\nyoEPFy60jvlSlZlZcWRZyOl14Lx2Xl7YveFk50tVZmbFkaXFUZLc4jAzK45emzjcx2FmVhy9N3H4\nUpWZWVF02schaQzwL8Dk3PoRcU7hwupcQzQQERRxMUIzs34py3Dc3wP3A38BGgobTn7qG+upKK8o\ndhhmZv1KlsQxOCK+UPBIumB7w3YnDjOzHpalj+M2SdMLHkkXbK3fWuwQzMz6nSyJ43yS5LFV0gZJ\nGyVt6GwnSddKWinpqXZel6QfSFoo6QlJh+Ub/NY6Jw4zs57WaeKIiKERURYRgyJiWLo9LMOxrwNO\n6uD1k4Gp6WMW8OMsAefaUrcl313MzGwXtdvHIWn/iHi2vZZARDzS0YEj4j5JkzuoMgP4RUQEyfK0\nwyWNi4hlGeIGnDjMzIqho87xC0haAv/TxmsBHL+L5x4PLM7ZXpKWOXGYmZWwdhNHRMxKfx5XoHO3\ndQNGtFlRmkWSxGDcznJ3jpuZ9bxi3jm+hGSm3SYTgKVtVYyIqyOiJiJqcsvd4jAz63nFTByzgbPS\n0VVHAuvz6d8AJw4zs2LIcgNgl0i6ATgWGC1pCXApUAEQEf8L3AFMJ5mafQvw8XzPsXHbxu4K1/qo\nu1+6m+ufvJ6x1WM57/DzGDd0XOc7mVmHssxVdRTwWERslnQGcBhweUS80tF+EXFaJ68H8Ol8gm1p\nbe3aXdnd+rj7XrmPf/rVP+2Ygv+aR6/h/CPOZ7eq3WiIBhoaG3b8rG+sb1WW+7O+sZ6Gxq7NuJPv\nfGpqs/vP5/A5du0c3SlLi+PHwCGSDgE+D1wD/AI4ppCBZbF2qxOHte9XT/yq2botKzev5It3fbGI\nEZn1DVn6OOrT1sEMkpbG5cDQwoaVjVsc1pHlm5YXOwSzPilLi2OjpIuBM4B3SCon7asotjVb1xQ7\nBCthL617qdghmPVJWVocHwG2ATMjYjnJTXqXFTSqjNzisI7sOXTPYodg1idlanGQXKJqkLQvsD9w\nQ2HDysZ9HNaRttalP2DMARyz1zGUq5zysnLKVc6AsgE7nrf1c0DZgB3P8+3EjLbvaW2/fuRX3+fw\nObKe46IvX5T3edqTJXHcB7xd0ghgLjCPpBVyerdF0UVucVhH2kocV06/kmMmF31ch1mPu4juSxxZ\nLlUpIrYA7wd+GBHvAw7stgh2gfs4rCNtJY4BZQW7dcms38iUOCS9laSFcXtaVl64kLJbX7u+y2Pr\nre9z4jArjCyJ47PAxcDvImKBpL2BuwsbVjZB8PqW14sdhpUoJw6zwuj0tygi7gXulTRU0pCIWAT8\nW+FDy+a1ja+x+5Ddix2GlSAnDrPC6LTFIekgSY8CTwFPS5ovqST6OABe2/BasUOwEuXEYVYYWS5V\nXQVcEBF7RcQk4ELgJ4UNK7slG5YUOwRrQ0TQGI1FjaGtxFFeVhLdc2a9WpbEUR0RO/o0IuIeoLpg\nEeXptY1ucZSStVvXMuPGGQz8xkAOuOIA7n/l/qLF4haHWWFkSRyLJP2npMnp4xKgZOZycOIoLV+7\n72vMfm422xu289zq5zjr1rPYVr+tKLE4cZgVRpbEcQ4wBrgF+F36PO+1MwrllXUdzu5uPezGp25s\ntv3yupe5/YXb26ldWE4cZoXRaeKIiLUR8W8RcVhEHBoR50dEydyyvXDNwmKHYDmWbWq9iOMHbvoA\n1z12XY+3PJw4zAqj3d8iSX+A9idEiYhTChJRnhZvWMzWuq0MqhhU7FCsAx///cf5j7n/wWXvuoyP\nHvTRHlmExonDrDA6+i36To9FkYeK8grqqGtW9uLaF5k2dlqRIuq/ttZtpa6xjmFVwzLVX7ZpGWf8\n7gzuXHgnPz3lpwwcMLCg8bU1q4ATh9mua/dSVUTc29GjJ4PM1dYfm+dXP1+ESPqv+sZ6LphzAYO/\nOZhR3x7FJXddktf+1z95Pcf//HiWblxaoAgTbnGYFUav+y0aOGAgG9nYrOzJFU/y/je+v0gR9T9z\nF83lew9+D0j+OH/j/m/wnn3fwxETjsh8jL8v+TsHXnkgpx90Om/Z8y2MHzae4QOHM2jAIAYOGMig\nikEIEQQRsePn61teZ23tWsYMHsOgikGs2ryK2vpahlYNpbK8koqyCirKK6gqr2Jz3eZW53XiMNt1\nve63qK2+jEeXP1qESPqvHz38o1Zln7rjUzx07kN5HWdd7TquePiK7gorEycOs12XZThuSRk8YHCr\nMieOnnXPy/e0Kntk2SMdjpo6bNxhVJQVf8XhcvnOcbNdlXfikPRNSV+QNKoQAXVmUMUgytQ87FfX\nv8ryTcuLEU6/NHHYxDbLtzW0nzhmHjqTuWfNZY8hexQqrE5Vllf2yGgus76uKy2OfwD1wPe6OZZM\nylTGQWMPalXe1v+CrTDa++NfW1/b7j4VZRW8fa+38/gnH+fcQ88tSuvj+CnH9/g5zfqivC/4RsSt\nhQgkH8dOPpbHVzzerOyul+7i1GmnFimi/qW94bcdXaqqKE8SxdjqsfzklJ/wrXd+i7teuouHXnuI\nxRsWs3TjUjZv30xtfS1b67dSW19LRCAJoR0thYhg+MDh1NbX0hiNjBw0kurKajZv30xdYx11DXXU\nNdZRW1/L+tr1OzrIp42dxvdP/H43fxJm/VNHNwD+kI5vACzamhzHTj6Wyx+6vFnZ7Odm8+N//rFn\nP+0B7SWOjlocLTulRw0exYcO/BAfOvBD3RpbS9sbttMYjQW/Z8SsP+noUtU8YD4wEDgMeCF9vAko\n6nqtx085nqryqmZlKzavYO5Lc4sUUf/S3sik1VtX571PoVWWVzppmHWzjm4A/HlE/ByYChwXET+M\niB8CJ5Akj6IZVjWMk95wUqvy7zxQkje79zlt3VgH8NLa9idNFu6UNusrsnSO7wkMzdkekpZ1StJJ\nkp6TtFDSRW28PknS3ZIelfSEpOnZwoazDjmrVdmfF/2Z+165L+shrIsaou0G55wX57S7T7R/1dPM\nepksieNbwKOSrpN0HfAI8M3OdpJUDlwBnAwcAJwm6YAW1S4BboqIQ4FTgSuzBj5jvxlMHTm1VfnM\n2TNZX7s+62GsC9prcdz/avEWbTKznpNlWvWfAUeQrMXxO+Ct6SWszhwOLIyIRRGxHbgRmNHy8EBT\nT+tuQObJi8rLyvn8UZ9vVb5wzULe+5v3OnkUUFuTB0JyP017fKnKrO/oNHEoGQf5TuCQiPg9UCnp\n8AzHHg8sztlekpbl+jJwhqQlwB3Aee3EMEvSPEnzVq1ataP842/6OEdOOLJV/Xtevoe3Xfs25i2d\nlyFMy1d7l6rMrH/IcqnqSuCtwGnp9kaSS1Cdaeu/mC0vdJ8GXBcRE4DpwC8ltYopIq6OiJqIqBkz\nZsyO8vKycn4242cMqRzS6kRPr3qaw39yOB+46QP8ZdFfqGuoa1XHuqa9S1UdcR+HWd+RZYzkERFx\nmKRHIVkRUFJlhv2WALlzU0yg9aWomcBJ6XH/LmkgMBpYmeH4AOw/en9u+uBNvOeG97T6n3AQ3PLM\nLdzyzC2MHDSSd+z1Dg7f83AO2v0g9hmxD1NGTPFQzS5o71KVmfUPWRJHXdrRHQCSxgCNGfZ7GJgq\naQrwGknn90db1HmVZHjvdZLeSHLPyCrydPLUk5l92mw+fPOH25xKG2DN1jXc+uyt3Prszhvfhdhj\nyB7sPmR3xlaPZffqnT/3GLIHewzZg3FDxzFuyDhGDhrpeY5SXWlxmFnfkSVx/ICkU3yspG8AHyQZ\nDdWhiKiX9BlgDlAOXBsRCyR9FZgXEbOBC4GfSPocSWL6WER06ZrG9KnTeWDmA5z5uzN5YsUTmfYJ\ngmWblrW5TnZLleWV7DFkDyYMm8C+o/Zlv1H7se+ofdl/9P5MHTl1x5Qa/UFX+jj22m2vAkRiZsWg\nLH+nJe1P0jIQMDcinil0YO2pqamJefPa7/Sua6jjyoev5LIHLuO1ja/1SEwVZRXsP3p/Dhx7INPG\nTGPa2OQxZcSUVjP59gXHXHdMXvfLTNptEi+f/7JbbGZFJGl+RNR0x7E6bHGkHdVPRMQ04NnuOGGh\nVZRXcP6R5/Ovb/lXbnnmFv7vmf/jjhfuYEvdloKds66xjidXPsmTK59sVj64YjBvHP3GHYmk6TF+\n6Phe/Uc03z6O2z96e69+v2bWXIeJIyIaJT0uaVJEtD9IvwRVlldy6rRTOXXaqWyr38Zjyx/jodce\n4rHlj/Hi2hd5cc2LBW+RbKnbwvxl85m/bH6z8oqyCsZUj2Fs9VjGDB7DmOoxjB40mhGDRjB84PB2\nH8OqhrXbgokIGqKB7Q3b2Va/je0N29lt4G55d/7f8/I9LNu4jBP2PoGx1WPbPE9dY+sRavd+7F4+\nc8dnWiXPB855gGljp+UVg5mVtix9HOOABZL+AezoeY6IUwoWVTerGlDFEROOaLUmdm19LSs2rWDF\n5hWs3LySFZuSn8s3LWf55uUs25j0fyzbuKzdTveuqGusY+nGpSzdmPl+RyDpzB9WNYyK8ood63DX\nNdSxvWE72xu2txryKsTk4ZM5atJRvG//93HiPidSXVnd7vE/ffunuXJecvP+yEEj+ePpf2Ty8MmM\nqR7D8k3LOfvWs5m7aG6bfRwDBwzk0U88ylfu/QrfvP+bBMGlx1za5n02Zta7ddrHIemYtsoj4t6C\nRNSJzvo4CmXjto0s3biUF9e+yHOvP8fzq5/n2dXP8vSqp1m5OfPo4aKqKq/ihL1P4OiJR1OzZw2H\njTuMUYNHUVtfyy8f/yWzbpvV5n5HTzqacUPGcfPTN7d77Pmz5nPYuMOAZCrzcpV7inuzEtKdfRyZ\nOsdLSbESR0dWbl7JgpULWLBqAU+tfGrHY/22/jPtyWOfeIxD9jik2GGYWTt6rHPcshlbPZaxU8Zy\n3JTjdpRFBK9tfI0FK9NksipJJs+seqZbL3uVimKtt2FmPc+/7QUiiQnDJjBh2AROfMOJzV7bvH0z\nq7asYtXmVazcvJJVW1axZusa1tWua/extnYtm7Zv6vCc5SqnsrySyvJKysvKWbN1TSHfYvNz+7KU\nWb/hxFEE1ZXVVFdWM3n45Lz2q2+sZ33tehqiYcc63BVlFc2SRa5N2zdx3yv3ccszt/CH5/9Q0L6Y\ncjlxmPUXnSYOSUeRzGK7V1pfQETE3oUNzVoaUDaAUYNHZa4/pHII06dOZ/rU6TRGI/OWzuPBJQ8y\nf9l85i2dx/Orn98xfciQyiEcPeloFqxcwOINizs5cnPDqoYxabdJee1jZr1XlhbHNcDnSNYf9+x2\nvVSZyjh8/OEcPn7njPh1DXWs3rqaIZVDKFc5gyoGtdqvtr6WG5+6kSdXPMltL9zG86ufb1Xn4qMv\npmpAVatyM+ubsgzHfSgijuiwUg8qxVFV/dFrG17jnpfv4Q0j39Dq/hgzKz09ParqbkmXAbcA25oK\nI+KR7gjAeqfxw8Zz+sGnFzsMMyuCTOtxpD9zM1UAx3d/OGZmVuo6TRwRcVxndczMrP9oN3FIOiMi\nfiXpgrZej4jvFi4sMzMrVR21OJpmwxvaE4GYmVnv0G7iiIir0p9f6blwzMys1PW95enMzKygnDjM\nzCwvThxmZpaXThOHpG9KGp6zPULS1wsblpmZlaosLY6TI2Jd00ZErAWmFy4kMzMrZVkSR7mkHTPY\nSRoEeEY7M7N+KsuUI78C5kr6GclUI+cAPy9oVGZmVrKyTDnybUlPAO8kWYvjaxExp+CRmZlZScqy\nkNMU4J6I+GO6PUjS5Ih4udDBmZlZ6cnSx3Ez0Jiz3ZCWmZlZP5QlcQyIiO1NG+nzyiwHl3SSpOck\nLZR0UTt1PizpaUkLJP06W9hmZlYsWTrHV0k6JSJmA0iaAbze2U6SyoErgHcBS4CHJc2OiKdz6kwF\nLgaOioi1ksZ25U2YmVnPyZI4PglcL+lHJJ3ji4GzMux3OLAwIhYBSLoRmAE8nVPnX4Ar0ntDiIiV\necRuZmZFkGVU1YvAkZKGkKxRvjHjsceTJJkmS9i5mmCTfQEk/Q0oB77c1AmfS9IsYBbApEmTMp7e\nzMwKIUuLA0n/DBwIDJQEQER8tbPd2iiLNs4/FTgWmADcL2la7p3q6bmuBq4GqKmpaXkMMzPrQVnm\nqvpf4CPAeSTJ4EPAXhmOvQSYmLM9AVjaRp3fR0RdRLwEPEeSSMzMrERlGVX1tog4C1ibLur0Vpon\nhPY8DEyVNEVSJXAqMLtFnVuB4wAkjSa5dLUoa/BmZtbzsiSOrenPLZL2BOqAKZ3tFBH1wGeAOcAz\nwE0RsUDSVyWdklabA6yW9DRwN/D/ImJ1vm/CzMx6TpY+jtvSadUvAx4h6af4SZaDR8QdwB0tyr6U\n8zyAC9KHmZn1AllGVX0tffp/km4DBkbE+sKGZWZmpSrTqKomEbEN2FagWMzMrBfw0rFmZpYXJw4z\nM8tLlvs45mYpMzOz/qHdPg5JA4HBwGhJI9h5J/gwYM8eiM3MzEpQR53jnwA+S5Ik5rMzcWwgmfXW\nzMz6oXYTR0RcDlwu6byI+GEPxmRmZiUsy3Dc5ZKGRsRGSZcAhwFfj4hHChyb7YKIzp83NkJtLWzY\nAFu3wtChUFEBd90Fjz8OAwdCVRUsXgx/+AO8+lI9DZQDopx6qtnM0dN348orYa82Zi9raAAJygo4\nBKO2NjlPdXXhzmFmzWVJHP8ZETdLOho4EfgO8GNaT5HeI9Y+8hK/HXTGjnl2o8UkvI252wGRzuZL\n7Ky782+nmk3XG6GcXXOf52znnLfZcXL/OHewb8t4o4MYWr2XDvZt67X2tnOfl9HIcNYxhE1so4pq\nNrOa3dmPzTRSRjkNTGYb72Y9R/AQ26jiYd7C23iA4axn1R2juXPyyQAMqgoaGyEag7LGetbEcH7J\nmTw78iiGDUsSSFMiae95W2WVlTBpEmzcCGvXwurVyfMtW5LnO95LWfPECDAi1nAh/8MkXtlR1ta0\nza1kqJTpOLt4kG45R287TzcE05v+jXvyPN1F0fI3rWUF6dGIOFTSfwFPRsSvm8p6JsTmaqSYV4wT\nW5dsopq9WcQqchd3TFJe06OMxmbb7ZWJYCC1rGB3IuNI8u9zPufzg8K8ObNeRDA/Imq641hZWhyv\nSboKeCfw35Kq8P0fltEQNrOS3WmgLE0Iu76cSh0DWMUYXmM8/+BwhrGBAdSzncpWdU+i1bpgZraL\nsrQ4BgMnkbQ2XpA0DjgoIv7UEwG2NEp7x8kka0ipxR+hjrZdt/3XBlLLYiYyiK1sZRBD2MRmqhnK\nRrZRxW6sZyl7sppRlNPAILZSRwVVbKOazUDzNsQ1zKSSOkrNTH7KNqqKHYZZUVzPmT3X4oiILZJW\nAkcDLwD16c+iWMNIrueMYp3eMjiav/KJZMHGDjUiGilrdUGqZVnTNkA1m6mgPu+Y7uZYrmVm3vuZ\n9R1ndtuRsrQ4LgVqgP0iYt90TY6bI+KobosiD1JNgHs5SlkF23knf0EE93IM26hqlgR2rRcvdnTm\nH8YjTOZlGiljPbu1ak012cQQ5nAimxmyC+c16+3Uo30c7wMOJVmLg4hYKmlod5zceo7U9vOBA6G+\nHrZv775z1VHJnUzvvgM2I9YxgnWMYEmmhSjNrLtlSRzbIyIkBYCkoo6YP/hgmDMned7eH0M/77r6\n+p3DYZvu8xg0aOdxN2xIkk1lJWzeDK+/nrwekTwqK5NHRUWy/8qVSXljY+ufWcoaGmDFip3DbiXY\nZx+YOBGGDEnuPamqSuo1DeHtSCcN7G6r05vO05ti9Xm6fow9u3GiqCyJ46Z0VNVwSf8CnAP8tPtC\nyE9FBeyxR7HO3vcNyPlGlJXB4MHNXx82bOfz6urOb7xr68bAQijkTYZm1lyWzvHvSHoXyRxV+wFf\niog/FzwyMzMrSZ0mDkn/HRFfAP7cRpmZmfUzWRr472qj7OTuDsTMzHqHjtbj+FfgU8Dekp7IeWko\n8LdCB2ZmZqWpo0tVvwbuBP4LuCinfGNErCloVGZmVrI6Wo9jPbAeOK3nwjEzs1LnQYxmZpYXJw4z\nM8uLE4eZmeUl78Qh6S+S7pT07gx1T5L0nKSFki7qoN4HJYWkbpmAy8zMCifLlCMtnQWMA47sqJKk\ncuAKkvtAlgAPS5odEU+3qDcU+DfgoS7EYmZmPSxTi0PSIEn7QTI7bkTMj4grOtntcGBhRCyKiO3A\njcCMNup9Dfg2UJtH3GZmViSdJg5J7wEeg2QNTklvkjQ7w7HHA4tztpekZbnHPhSYGBG3dRLDLEnz\nJM1btWpVhlObmVmhZGlxfJmk9bAOICIeAyZn2K+tCa53TP4rqQz4HnBhZweKiKsjoiYiasaMGZPh\n1GZmVihZEkd9ejNgvpZAs5V2JgBLc7aHAtOAeyS9TNJnMtsd5GZmpS1L4nhK0keBcklTJf0QeCDD\nfg8DUyVNkVQJnArsuMQVEevWV3HcAAAMs0lEQVQjYnRETI6IycCDwCkR4XVhzcxKWJbEcR5wILAN\nuIFkXY7PdrZTRNQDnwHmAM8AN0XEAklflXRK10M2M7NiUmRZl7CE1NTUxLx5bpSYmeVD0vyI6Jau\ngCwLOd1NTqd2k4g4vjsCMDOz3iXLDYD/nvN8IPABoL4w4ZiZWanLsub4/BZFf5N0b4HiMTOzEpfl\nUtXInM0y4M3AHgWLyMzMSlqWS1XzSfo4RHKJ6iVgZiGDMjOz0pXlUtWUngjEzMx6h3YTh6T3d7Rj\nRNzS/eGYmVmp66jF8Z4OXgvAicPMrB9qN3FExMd7MhAzM+sdskyrPkrSDyQ9Imm+pMsljeqJ4MzM\nrPRkmavqRmAVyY1/H0yf/6aQQZmZWenKMhx3ZER8LWf765LeW6iAzMystGVpcdwt6VRJZenjw8Dt\nhQ7MzMxKU0fDcTey88a/C4Bfpi+VA5uASwsenZmZlZyORlUN7clAzMysd8hyqcrMzGwHJw4zM8uL\nE4eZmeUly3BcJJUDu+fWj4hXCxWUmZmVrizrcZxHMoJqBdCYFgdwcAHjMjOzEpWlxXE+sF9ErC50\nMGZmVvqy9HEsBtYXOhAzM+sdsrQ4FgH3SLod2NZUGBHfLVhUZmZWsrIkjlfTR2X6MDOzfizL0rFf\n6YlAzMysd+horqrvR8RnJf2BZBRVMxFxSkEjMzOzktRRi6NpUsPv9EQgZmbWO3Q0yeH89Oe9XT24\npJOAy0lm1P1pRHyrxesXAOcC9SQLRJ0TEa909XxmZlZ4BZtyJL3b/ArgZOAA4DRJB7So9ihQExEH\nA78Fvl2oeMzMrHsUcq6qw4GFEbEoIraTLEE7I7dCRNwdEVvSzQeBCQWMx8zMukEhE8d4kpsHmyxJ\ny9ozE7izgPGYmVk36DRxSPqzpOE52yMkzclwbLVR1mp0VnrMM4Aa4LJ2Xp8laZ6keatWrcpwajMz\nK5QsLY7REbGuaSMi1gJjM+y3BJiYsz0BWNqykqR3Al8ETomIbS1fT895dUTURETNmDFjMpzazMwK\nJUviaJQ0qWlD0l6003Jo4WFgqqQpkiqBU4HZuRUkHQpcRZI0VmYP28zMiiXLlCNfBP4qqWlY7juA\nWZ3tFBH1kj4DzCEZjnttRCyQ9FVgXkTMJrk0NQS4WRLAq76x0MystCmi88aDpNHAkST9Fn+PiNcL\nHVh7ampqYt68ecU6vZlZryRpfkTUdMexsnSOvw+oi4jbIuIPQL2k93bHyc3MrPfJ0sdxaUTsWI8j\n7Si/tHAhmZlZKcuSONqqk2mtcjMz63uyJI55kr4raR9Je0v6HjC/0IGZmVlpypI4zgO2A78BbgZq\ngU8XMigzMytdWRZy2gxc1AOxmJlZL9Bp4pA0Bvg8cCAwsKk8Io4vYFxmZlaislyquh54FpgCfAV4\nmeSucDMz64eyJI5REXENyb0c90bEOSQ3A5qZWT+UZVhtXfpzmaR/Jpmo0OtmmJn1U1kSx9cl7QZc\nCPwQGAZ8rqBRmZlZycoyquq29Ol64LjChmNmZqWukCsAmplZH+TEYWZmeXHiMDOzvGS5AbAK+AAw\nObd+RHy1cGGZmVmpyjKq6vckHePzgTbXBDczs/4jS+KYEBEnFTwSMzPrFbL0cTwg6aCCR2JmZr1C\nlhbH0cDHJL1EcqlKQETEwQWNzMzMSlKWxHFywaMwM7Neo93EIWlYRGwANvZgPGZmVuI6anH8Gng3\nyWiqILlE1SSAvQsYl5mZlah2E0dEvDv9OaXnwjEzs1KXpY8DSSOAqTRfAfC+QgVlZmalK8ud4+cC\n55OswfEYySJOfwe8dKyZWT+U5T6O84G3AK9ExHHAocCqgkZlZmYlK0viqI2IWkjmrYqIZ4H9ChuW\nmZmVqiyJY4mk4cCtwJ8l/Z5k+dhOSTpJ0nOSFkq6qI3XqyT9Jn39IUmT8wnezMx6XpYVAN+XPv2y\npLuB3YA/drafpHLgCuBdwBLgYUmzI+LpnGozgbUR8QZJpwL/DXwkz/dgZmY9qMMWh6QySU81bUfE\nvRExOyK2Zzj24cDCiFiU1r8RmNGizgzg5+nz3wInSBJmZlayOmxxRESjpMclTYqIV/M89nhgcc72\nEuCI9upERL2k9cAo4PXcSpJmAbPSzW25yayfG02Lz6of82exkz+LnfxZ7NRtfdNZ7uMYByyQ9A9g\nc1NhRJzSyX5ttRyiC3WIiKuBqwEkzYuImk7O3S/4s9jJn8VO/ix28mexk6R53XWsLInjK1089hJg\nYs72BFp3qjfVWSJpAEn/yZouns/MzHpAllFV09O+jR0PYHqG/R4GpkqaIqkSOBWY3aLObODs9PkH\ngbsiolWLw8zMSkeWxPGuNso6nWo9IuqBzwBzgGeAmyJigaSvSmq6zHUNMErSQuACoNWQ3TZcnaFO\nf+HPYid/Fjv5s9jJn8VO3fZZqL3/4Ev6V+BTJLPgvpjz0lDgbxFxRncFYWZmvUdHiWM3YATwXzRv\nCWyMCPdDmJn1U+0mDjMzs7Zk6eMoGZ1NYdKXSJoo6W5Jz0haIOn8tHykpD9LeiH9OSItl6QfpJ/N\nE5IOK+476H6SyiU9Kum2dHtKOlXNC+nUNZVpeZ+eykbScEm/lfRs+v14a3/9Xkj6XPr78ZSkGyQN\n7E/fC0nXSlqZe29bV74Lks5O678g6ey2zpWr1ySOnClMTgYOAE6TdEBxoyqoeuDCiHgjyVT2n07f\n70XA3IiYCsxl52XEk0nWTJlKcrPkj3s+5II7n2SgRZP/Br6XfhZrSaawgZypbIDvpfX6ksuBP0bE\n/sAhJJ9Jv/teSBoP/BtQExHTgHKS0Zv96XtxHXBSi7K8vguSRgKXktygfThwaVOyaVdE9IoH8FZg\nTs72xcDFxY6rB9//70lGuD0HjEvLxgHPpc+vAk7Lqb+jXl94kNwHNJdkHZjbSG4efR0Y0PL7QTKS\n763p8wFpPRX7PXTT5zAMeKnl++mP3wt2zjwxMv13vg04sb99L4DJwFNd/S4ApwFX5ZQ3q9fWo9e0\nOGh7CpPxRYqlR6VN6kOBh4DdI2IZQPpzbFqtr38+3wc+DzSm26OAdZEM+4bm77fZVDZA01Q2fcHe\nJOvh/Cy9bPdTSdX0w+9FRLwGfAd4FVhG8u88n/75vciV73ch7+9Ib0ocmaYn6WskDQH+D/hsRGzo\nqGobZX3i85H0bmBlRMzPLW6jamR4rbcbABwG/DgiDiWZBqij/r4++1mkl1NmAFOAPYFq2r7HrD98\nL7Jo7/3n/bn0psSRZQqTPkVSBUnSuD4ibkmLV0gal74+DliZlvflz+co4BRJL5PMsnw8SQtkeDpV\nDTR/vzs+iz44lc0SYElEPJRu/5YkkfTH78U7gZciYlVE1AG3AG+jf34vcuX7Xcj7O9KbEkeWKUz6\nDEkiubP+mYj4bs5LudO0nE3S99FUflY6cuJIYH1Tc7W3i4iLI2JCREwm+Xe/KyJOB+4mmaoGWn8W\nfXIqm4hYDiyW1DTT6QnA0/TD7wXJJaojJQ1Of1+aPot+971oId/vwhzgnySNSFtx/5SWta/YHTt5\ndgJNB54nuZP9i8WOp8Dv9WiS5uITwGPpYzrJNdm5wAvpz5FpfZGMOnsReJJkpEnR30cBPpdjgdvS\n53sD/wAWAjcDVWn5wHR7Yfr63sWOu5s/gzcB89Lvxq0kN+r2y+8FySSszwJPAb8EqvrT9wK4gaR/\np46k5TCzK98F4Jz0c1kIfLyz8/oGQDMzy0tvulRlZmYlwInDzMzy4sRhZmZ5ceIwM7O8OHGYmVle\nnDjMepCkY5tm9zXrrZw4zMwsL04cZm2QdIakf0h6TNJV6VogmyT9j6RHJM2VNCat+yZJD6ZrHPwu\nZ/2DN0j6i6TH0332SQ8/JGc9jevTu57Neg0nDrMWJL0R+AhwVES8CWgATieZRO+RiDgMuJdkDQOA\nXwBfiIiDSe7IbSq/HrgiIg4hmUOpaaqPQ4HPkqwrszfJXFxmvcaAzquY9TsnAG8GHk4bA4NIJopr\nBH6T1vkVcIuk3YDhEXFvWv5z4GZJQ4HxEfE7gIioBUiP94+IWJJuP0aynsJfC/+2zLqHE4dZawJ+\nHhEXNyuU/rNFvY7m6+no8tO2nOcN+PfQehlfqjJrbS7wQUljYccaznuR/L40zbr6UeCvEbEeWCvp\n7Wn5mcC9kaydskTSe9NjVEka3KPvwqxA/D8dsxYi4mlJlwB/klRGMvPop0kWTTpQ0nyS1eM+ku5y\nNvC/aWJYBHw8LT8TuErSV9NjfKgH34ZZwXh2XLOMJG2KiCHFjsOs2HypyszM8uIWh5mZ5cUtDjMz\ny4sTh5mZ5cWJw8zM8uLEYWZmeXHiMDOzvPx/F+JgMMBJLzoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f258acc5d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
