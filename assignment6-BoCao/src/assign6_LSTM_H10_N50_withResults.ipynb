{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 10\n",
    "N = 50\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.6977, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 0.6953, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.6948, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.6946, Training Accuracy= 0.496\n",
      "Epoch: 40, Loss= 0.6945, Training Accuracy= 0.496\n",
      "Epoch: 50, Loss= 0.6945, Training Accuracy= 0.496\n",
      "Epoch: 60, Loss= 0.6945, Training Accuracy= 0.496\n",
      "Epoch: 70, Loss= 0.6944, Training Accuracy= 0.496\n",
      "Epoch: 80, Loss= 0.6944, Training Accuracy= 0.496\n",
      "Epoch: 90, Loss= 0.6944, Training Accuracy= 0.496\n",
      "Epoch: 100, Loss= 0.6943, Training Accuracy= 0.496\n",
      "Epoch: 110, Loss= 0.6943, Training Accuracy= 0.496\n",
      "Epoch: 120, Loss= 0.6943, Training Accuracy= 0.496\n",
      "Epoch: 130, Loss= 0.6943, Training Accuracy= 0.496\n",
      "Epoch: 140, Loss= 0.6943, Training Accuracy= 0.496\n",
      "Epoch: 150, Loss= 0.6943, Training Accuracy= 0.496\n",
      "Epoch: 160, Loss= 0.6942, Training Accuracy= 0.496\n",
      "Epoch: 170, Loss= 0.6942, Training Accuracy= 0.496\n",
      "Epoch: 180, Loss= 0.6942, Training Accuracy= 0.496\n",
      "Epoch: 190, Loss= 0.6942, Training Accuracy= 0.496\n",
      "Epoch: 200, Loss= 0.6942, Training Accuracy= 0.496\n",
      "Epoch: 210, Loss= 0.6942, Training Accuracy= 0.496\n",
      "Epoch: 220, Loss= 0.6942, Training Accuracy= 0.496\n",
      "Epoch: 230, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 240, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 250, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 260, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 270, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 280, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 290, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 300, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 310, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 320, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 330, Loss= 0.6941, Training Accuracy= 0.496\n",
      "Epoch: 340, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 350, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 360, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 370, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 380, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 390, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 400, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 410, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 420, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 430, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 440, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 450, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 460, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 470, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 480, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 490, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 500, Loss= 0.6940, Training Accuracy= 0.496\n",
      "Epoch: 510, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 520, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 530, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 540, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 550, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 560, Loss= 0.6939, Training Accuracy= 0.495\n",
      "Epoch: 570, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 580, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 590, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 600, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 610, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 620, Loss= 0.6939, Training Accuracy= 0.496\n",
      "Epoch: 630, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 640, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 650, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 660, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 670, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 680, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 690, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 700, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 710, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 720, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 730, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 740, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 750, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 760, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 770, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 780, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 790, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 800, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 810, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 820, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 830, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 840, Loss= 0.6938, Training Accuracy= 0.497\n",
      "Epoch: 850, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 860, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 870, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 880, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 890, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 900, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 910, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 920, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 930, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 940, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 950, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 960, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 970, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 980, Loss= 0.6938, Training Accuracy= 0.498\n",
      "Epoch: 990, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5006\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 10, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 170, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 180, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 190, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 200, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 210, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 220, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 240, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 250, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 260, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 270, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 290, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 300, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 310, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 320, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 330, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 340, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 350, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 360, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 370, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 380, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 390, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 400, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 410, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 420, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 430, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 440, Loss= 0.6931, Training Accuracy= 0.505\n",
      "Epoch: 450, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 460, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 470, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 480, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 490, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 500, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 510, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 520, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 530, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 540, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 550, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 560, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 570, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 580, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 590, Loss= 0.6931, Training Accuracy= 0.507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 600, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 610, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 620, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 630, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 680, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 700, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 710, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 730, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 740, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 750, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 770, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 780, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 790, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 800, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 810, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 820, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 830, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 840, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 850, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 860, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 870, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 880, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 890, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 900, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 910, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 920, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 930, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 940, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 950, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 960, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 970, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 990, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4973\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.6989, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.6964, Training Accuracy= 0.501\n",
      "Epoch: 20, Loss= 0.6955, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.6951, Training Accuracy= 0.501\n",
      "Epoch: 40, Loss= 0.6948, Training Accuracy= 0.501\n",
      "Epoch: 50, Loss= 0.6947, Training Accuracy= 0.501\n",
      "Epoch: 60, Loss= 0.6945, Training Accuracy= 0.501\n",
      "Epoch: 70, Loss= 0.6944, Training Accuracy= 0.501\n",
      "Epoch: 80, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 90, Loss= 0.6943, Training Accuracy= 0.501\n",
      "Epoch: 100, Loss= 0.6942, Training Accuracy= 0.501\n",
      "Epoch: 110, Loss= 0.6942, Training Accuracy= 0.501\n",
      "Epoch: 120, Loss= 0.6941, Training Accuracy= 0.501\n",
      "Epoch: 130, Loss= 0.6941, Training Accuracy= 0.501\n",
      "Epoch: 140, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 160, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 170, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 180, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 190, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 200, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 210, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 220, Loss= 0.6939, Training Accuracy= 0.501\n",
      "Epoch: 230, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 240, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 250, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 260, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 270, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 280, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 290, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 300, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 310, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 320, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 330, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 340, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 350, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 360, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 370, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 380, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 390, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 400, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 410, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 420, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 430, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 450, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 460, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 470, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 490, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 500, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 510, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 520, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 530, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 540, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 550, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 560, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 570, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 580, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 590, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 600, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 610, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 620, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 630, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 640, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 650, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 660, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 670, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 680, Loss= 0.6936, Training Accuracy= 0.507\n",
      "Epoch: 690, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 700, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 710, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 720, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 730, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 740, Loss= 0.6935, Training Accuracy= 0.505\n",
      "Epoch: 750, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 760, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 770, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 780, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 790, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 800, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 810, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 820, Loss= 0.6934, Training Accuracy= 0.509\n",
      "Epoch: 830, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 840, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 850, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 860, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 870, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 880, Loss= 0.6932, Training Accuracy= 0.515\n",
      "Epoch: 890, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 900, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 910, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 920, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 930, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 940, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 950, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 960, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 970, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 990, Loss= 0.6928, Training Accuracy= 0.516\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4954\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 10, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 20, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 30, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 40, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 50, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 60, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 70, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 80, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 90, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 100, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 110, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 120, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 130, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 140, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 150, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 160, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 200, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 210, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 220, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 230, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 240, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 250, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 260, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 270, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 280, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 290, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 320, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 330, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 340, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 350, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 360, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 400, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 420, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 430, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 470, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 500, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 510, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 520, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 530, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 540, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 550, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 560, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 570, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 580, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 590, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 600, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 610, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 620, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 630, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.511\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 680, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 690, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 700, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 710, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 720, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 730, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 740, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 750, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 760, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 770, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 780, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 790, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 800, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 810, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 820, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 830, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 840, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 850, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 860, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 870, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 880, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 890, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 900, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 910, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 920, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 930, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 940, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 950, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 960, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 970, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 980, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 990, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4957\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.6971, Training Accuracy= 0.508\n",
      "Epoch: 10, Loss= 0.6943, Training Accuracy= 0.508\n",
      "Epoch: 20, Loss= 0.6940, Training Accuracy= 0.508\n",
      "Epoch: 30, Loss= 0.6938, Training Accuracy= 0.508\n",
      "Epoch: 40, Loss= 0.6937, Training Accuracy= 0.508\n",
      "Epoch: 50, Loss= 0.6936, Training Accuracy= 0.508\n",
      "Epoch: 60, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.6935, Training Accuracy= 0.508\n",
      "Epoch: 80, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 90, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 100, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 110, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 120, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 130, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 140, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 150, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 160, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 170, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 180, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 190, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 210, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 220, Loss= 0.6932, Training Accuracy= 0.508\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 240, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 250, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 260, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 270, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 290, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 310, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 320, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 330, Loss= 0.6930, Training Accuracy= 0.510\n",
      "Epoch: 340, Loss= 0.6930, Training Accuracy= 0.509\n",
      "Epoch: 350, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 360, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 370, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 380, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 390, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 400, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 410, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 430, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 440, Loss= 0.6928, Training Accuracy= 0.508\n",
      "Epoch: 450, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 460, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 470, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 480, Loss= 0.6927, Training Accuracy= 0.509\n",
      "Epoch: 490, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 500, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 520, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 530, Loss= 0.6927, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 550, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 560, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 570, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 580, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 590, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 600, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 610, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 620, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 630, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 640, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 650, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 660, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 670, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 680, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 690, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 700, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 710, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 720, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 730, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 740, Loss= 0.6925, Training Accuracy= 0.516\n",
      "Epoch: 750, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 760, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 770, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 780, Loss= 0.6925, Training Accuracy= 0.515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 790, Loss= 0.6925, Training Accuracy= 0.514\n",
      "Epoch: 800, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 810, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 820, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 830, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 840, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 850, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 860, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 870, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 880, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 890, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 900, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 910, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 920, Loss= 0.6923, Training Accuracy= 0.513\n",
      "Epoch: 930, Loss= 0.6923, Training Accuracy= 0.513\n",
      "Epoch: 940, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 950, Loss= 0.6923, Training Accuracy= 0.512\n",
      "Epoch: 960, Loss= 0.6923, Training Accuracy= 0.512\n",
      "Epoch: 970, Loss= 0.6923, Training Accuracy= 0.511\n",
      "Epoch: 980, Loss= 0.6923, Training Accuracy= 0.512\n",
      "Epoch: 990, Loss= 0.6923, Training Accuracy= 0.511\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4976\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.6953, Training Accuracy= 0.495\n",
      "Epoch: 10, Loss= 0.6933, Training Accuracy= 0.502\n",
      "Epoch: 20, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 30, Loss= 0.6931, Training Accuracy= 0.506\n",
      "Epoch: 40, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 50, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 70, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 80, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 90, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 100, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 110, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 120, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 130, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 140, Loss= 0.6930, Training Accuracy= 0.508\n",
      "Epoch: 150, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 160, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 170, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 180, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 200, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 210, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 220, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 230, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 240, Loss= 0.6929, Training Accuracy= 0.507\n",
      "Epoch: 250, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 260, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 270, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 280, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 290, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 300, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 310, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 320, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 330, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 340, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 350, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 360, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 370, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 380, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 390, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 400, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 410, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 420, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 430, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 440, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 450, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 460, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 470, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 480, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 490, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 500, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 510, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 520, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 530, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 540, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 550, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 560, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 570, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 580, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 590, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 600, Loss= 0.6928, Training Accuracy= 0.510\n",
      "Epoch: 610, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 620, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 630, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 640, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 650, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 660, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 670, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 680, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 690, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 700, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 710, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 720, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 730, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 740, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 750, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 760, Loss= 0.6927, Training Accuracy= 0.515\n",
      "Epoch: 770, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 780, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 790, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 800, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 810, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 820, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 830, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 840, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 850, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 860, Loss= 0.6925, Training Accuracy= 0.515\n",
      "Epoch: 870, Loss= 0.6924, Training Accuracy= 0.513\n",
      "Epoch: 880, Loss= 0.6924, Training Accuracy= 0.514\n",
      "Epoch: 890, Loss= 0.6924, Training Accuracy= 0.515\n",
      "Epoch: 900, Loss= 0.6923, Training Accuracy= 0.516\n",
      "Epoch: 910, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 920, Loss= 0.6923, Training Accuracy= 0.514\n",
      "Epoch: 930, Loss= 0.6922, Training Accuracy= 0.513\n",
      "Epoch: 940, Loss= 0.6922, Training Accuracy= 0.515\n",
      "Epoch: 950, Loss= 0.6922, Training Accuracy= 0.515\n",
      "Epoch: 960, Loss= 0.6921, Training Accuracy= 0.512\n",
      "Epoch: 970, Loss= 0.6921, Training Accuracy= 0.514\n",
      "Epoch: 980, Loss= 0.6921, Training Accuracy= 0.514\n",
      "Epoch: 990, Loss= 0.6921, Training Accuracy= 0.514\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 10, Loss= 0.6934, Training Accuracy= 0.495\n",
      "Epoch: 20, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 30, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 40, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 50, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 60, Loss= 0.6933, Training Accuracy= 0.505\n",
      "Epoch: 70, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 80, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 90, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 100, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 110, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 120, Loss= 0.6932, Training Accuracy= 0.506\n",
      "Epoch: 130, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 140, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 150, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 160, Loss= 0.6932, Training Accuracy= 0.507\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 190, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 200, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 210, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 240, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 250, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 260, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 270, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 290, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 300, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 310, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 320, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 330, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 340, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 350, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 360, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 370, Loss= 0.6931, Training Accuracy= 0.508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380, Loss= 0.6931, Training Accuracy= 0.508\n",
      "Epoch: 390, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 400, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 410, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 420, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 430, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 440, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 450, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 460, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 470, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 480, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 490, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 500, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 510, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 520, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 530, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 540, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 550, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 560, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 570, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 580, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 590, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 600, Loss= 0.6931, Training Accuracy= 0.507\n",
      "Epoch: 610, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 620, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 630, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 680, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 700, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 710, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 730, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 740, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 750, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 770, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 780, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 790, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 800, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 810, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 820, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 830, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 840, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 850, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 860, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 870, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 880, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 890, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 900, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 910, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 920, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 930, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 940, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 950, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 960, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 970, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 980, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 990, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5081\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.6980, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.6960, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.6955, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.6953, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6950, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6949, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6947, Training Accuracy= 0.503\n",
      "Epoch: 70, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 80, Loss= 0.6945, Training Accuracy= 0.503\n",
      "Epoch: 90, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 100, Loss= 0.6944, Training Accuracy= 0.503\n",
      "Epoch: 110, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 140, Loss= 0.6942, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 180, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 190, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 200, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 210, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 220, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 230, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 260, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 270, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 280, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 290, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 300, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 310, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 320, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 330, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 340, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 350, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 370, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 380, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 400, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 410, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 420, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 430, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 440, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 450, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 460, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 480, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 490, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 500, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 510, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 520, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 530, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 540, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 550, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 560, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 570, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 580, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 590, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 600, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 610, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 620, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 630, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 640, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 650, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 660, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 670, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 680, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 690, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 700, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 710, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 720, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 730, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 740, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 750, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 760, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 770, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 780, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 790, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 800, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 810, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 820, Loss= 0.6935, Training Accuracy= 0.503\n",
      "Epoch: 830, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 840, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 850, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 860, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 870, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 880, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 890, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 900, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 910, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 920, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 930, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 940, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 950, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 960, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 970, Loss= 0.6934, Training Accuracy= 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 980, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Epoch: 990, Loss= 0.6934, Training Accuracy= 0.503\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5028\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.6955, Training Accuracy= 0.507\n",
      "Epoch: 10, Loss= 0.6935, Training Accuracy= 0.500\n",
      "Epoch: 20, Loss= 0.6933, Training Accuracy= 0.499\n",
      "Epoch: 30, Loss= 0.6932, Training Accuracy= 0.498\n",
      "Epoch: 40, Loss= 0.6932, Training Accuracy= 0.499\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.500\n",
      "Epoch: 60, Loss= 0.6932, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 80, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 90, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 110, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 120, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 130, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 140, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 160, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 180, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 190, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 200, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 210, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 220, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 230, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 240, Loss= 0.6931, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 260, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 270, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 280, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 290, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 300, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 310, Loss= 0.6931, Training Accuracy= 0.500\n",
      "Epoch: 320, Loss= 0.6931, Training Accuracy= 0.500\n",
      "Epoch: 330, Loss= 0.6931, Training Accuracy= 0.501\n",
      "Epoch: 340, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 350, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 360, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 370, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 380, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 390, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 400, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 420, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 430, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 440, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 450, Loss= 0.6930, Training Accuracy= 0.501\n",
      "Epoch: 460, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 470, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 480, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 490, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 500, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 510, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 520, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 530, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 540, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 550, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 560, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 570, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 580, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 590, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 600, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 610, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 620, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 640, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 650, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 660, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 670, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 680, Loss= 0.6930, Training Accuracy= 0.503\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 700, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 710, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 720, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 730, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 740, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 750, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 760, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 770, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 780, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 790, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 810, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 820, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 830, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 840, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 850, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 860, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 870, Loss= 0.6930, Training Accuracy= 0.505\n",
      "Epoch: 880, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 890, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 900, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 910, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 920, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 930, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 940, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 950, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 960, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 970, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 980, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Epoch: 990, Loss= 0.6930, Training Accuracy= 0.507\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5073\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7005, Training Accuracy= 0.512\n",
      "Epoch: 10, Loss= 0.6986, Training Accuracy= 0.512\n",
      "Epoch: 20, Loss= 0.6976, Training Accuracy= 0.512\n",
      "Epoch: 30, Loss= 0.6969, Training Accuracy= 0.512\n",
      "Epoch: 40, Loss= 0.6964, Training Accuracy= 0.512\n",
      "Epoch: 50, Loss= 0.6960, Training Accuracy= 0.512\n",
      "Epoch: 60, Loss= 0.6957, Training Accuracy= 0.512\n",
      "Epoch: 70, Loss= 0.6955, Training Accuracy= 0.512\n",
      "Epoch: 80, Loss= 0.6953, Training Accuracy= 0.512\n",
      "Epoch: 90, Loss= 0.6951, Training Accuracy= 0.512\n",
      "Epoch: 100, Loss= 0.6950, Training Accuracy= 0.512\n",
      "Epoch: 110, Loss= 0.6948, Training Accuracy= 0.512\n",
      "Epoch: 120, Loss= 0.6947, Training Accuracy= 0.512\n",
      "Epoch: 130, Loss= 0.6946, Training Accuracy= 0.512\n",
      "Epoch: 140, Loss= 0.6945, Training Accuracy= 0.512\n",
      "Epoch: 150, Loss= 0.6945, Training Accuracy= 0.512\n",
      "Epoch: 160, Loss= 0.6944, Training Accuracy= 0.512\n",
      "Epoch: 170, Loss= 0.6943, Training Accuracy= 0.512\n",
      "Epoch: 180, Loss= 0.6943, Training Accuracy= 0.512\n",
      "Epoch: 190, Loss= 0.6942, Training Accuracy= 0.512\n",
      "Epoch: 200, Loss= 0.6941, Training Accuracy= 0.512\n",
      "Epoch: 210, Loss= 0.6941, Training Accuracy= 0.512\n",
      "Epoch: 220, Loss= 0.6941, Training Accuracy= 0.512\n",
      "Epoch: 230, Loss= 0.6940, Training Accuracy= 0.512\n",
      "Epoch: 240, Loss= 0.6940, Training Accuracy= 0.512\n",
      "Epoch: 250, Loss= 0.6939, Training Accuracy= 0.512\n",
      "Epoch: 260, Loss= 0.6939, Training Accuracy= 0.512\n",
      "Epoch: 270, Loss= 0.6939, Training Accuracy= 0.512\n",
      "Epoch: 280, Loss= 0.6938, Training Accuracy= 0.512\n",
      "Epoch: 290, Loss= 0.6938, Training Accuracy= 0.512\n",
      "Epoch: 300, Loss= 0.6938, Training Accuracy= 0.512\n",
      "Epoch: 310, Loss= 0.6938, Training Accuracy= 0.512\n",
      "Epoch: 320, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 330, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 340, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 350, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 360, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 370, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 380, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 390, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 400, Loss= 0.6936, Training Accuracy= 0.512\n",
      "Epoch: 410, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 420, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 430, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 440, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 450, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 460, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 470, Loss= 0.6935, Training Accuracy= 0.512\n",
      "Epoch: 480, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 490, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 500, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 510, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 520, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 530, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 540, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 550, Loss= 0.6934, Training Accuracy= 0.512\n",
      "Epoch: 560, Loss= 0.6934, Training Accuracy= 0.512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 570, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 580, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 590, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 600, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 610, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 620, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 630, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 640, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 650, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 660, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 670, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 680, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 690, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 700, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 710, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 720, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 730, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 740, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 750, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 760, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 770, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 780, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 790, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 800, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 810, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 820, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 830, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 840, Loss= 0.6932, Training Accuracy= 0.511\n",
      "Epoch: 850, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 860, Loss= 0.6932, Training Accuracy= 0.512\n",
      "Epoch: 870, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 880, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 890, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 900, Loss= 0.6931, Training Accuracy= 0.511\n",
      "Epoch: 910, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 920, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 930, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 940, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 950, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 960, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 970, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 980, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 990, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4981\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.06\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 1000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.50059998, 0.4973, 0.49540001, 0.4957, 0.49759999, 0.5, 0.50809997, 0.50279999, 0.50730002, 0.49810001]\n",
      "mean of test_accuracies_10replications:  0.50029\n",
      "standard deviation of test_accuracies_10replications_std_mean:  4.27280552685e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERHRkSSOiIjoSBJHRER0JIkjIiI68v8BJ+28cVO79NcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7e3dc02510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
