{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    http://www.deeplearning.net/tutorial/lstm.html#lstm\n",
    "    https://github.com/llSourcell/LSTM_Networks/blob/master/LSTM%20Demo.ipynb\n",
    "    https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    Recurrent Neural Network.\n",
    "    A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "    This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "    Links:\n",
    "    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\n",
    "    Author: Aymeric Damien\n",
    "    Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "\n",
    "H = 25\n",
    "N = 50\n",
    "\n",
    "#Reference: Denis\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    return np.expand_dims(sequences, axis=2), y\n",
    "\n",
    "#Reference: Modified from Denis by Bo Cao\n",
    "def generate_parity_sequences(N, count):\n",
    "    \"\"\"\n",
    "    Generate :count: sequences of length :N:.\n",
    "    If odd # of 1's -> output 1\n",
    "    else -> output 0\n",
    "    \"\"\"\n",
    "    xor = lambda x: 1 if (x % 2 == 1) else 0\n",
    "    sequences = np.random.choice([0, 1], size=[count, N], replace=True)\n",
    "    counts = np.count_nonzero(sequences == 1, axis=1)\n",
    "    # xor each sequence, expand dimensions by 1 to match sequences shape\n",
    "    y = np.expand_dims(np.array([xor(x) for x in counts]), axis=1)\n",
    "\n",
    "    # In case if you wanted to have the answer just appended at the end of the sequence:\n",
    "    #     # append the answer at the end of each sequence\n",
    "    #     seq_plus_y = np.concatenate([sequences, y], axis=1)\n",
    "    #     print(sequences.shape, y.shape, seq_plus_y.shape)\n",
    "    #     return seq_plus_y\n",
    "    \n",
    "    new_y = []\n",
    "    for i in range(len(y)):\n",
    "        new_yy = []\n",
    "        if y[i] == 0:\n",
    "            new_yy.append(0)\n",
    "            new_yy.append(1)\n",
    "        else:\n",
    "            new_yy.append(1)\n",
    "            new_yy.append(0)\n",
    "        new_y.append(new_yy)\n",
    "\n",
    "    return np.expand_dims(sequences, axis=2), new_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication: 0: \n",
      "Epoch: 0, Loss= 0.7185, Training Accuracy= 0.512\n",
      "Epoch: 10, Loss= 0.7003, Training Accuracy= 0.517\n",
      "Epoch: 20, Loss= 0.6972, Training Accuracy= 0.520\n",
      "Epoch: 30, Loss= 0.6961, Training Accuracy= 0.521\n",
      "Epoch: 40, Loss= 0.6957, Training Accuracy= 0.525\n",
      "Epoch: 50, Loss= 0.6953, Training Accuracy= 0.522\n",
      "Epoch: 60, Loss= 0.6948, Training Accuracy= 0.523\n",
      "Epoch: 70, Loss= 0.6949, Training Accuracy= 0.527\n",
      "Epoch: 80, Loss= 0.6944, Training Accuracy= 0.529\n",
      "Epoch: 90, Loss= 0.6944, Training Accuracy= 0.530\n",
      "Epoch: 100, Loss= 0.6945, Training Accuracy= 0.534\n",
      "Epoch: 110, Loss= 0.6943, Training Accuracy= 0.535\n",
      "Epoch: 120, Loss= 0.6933, Training Accuracy= 0.536\n",
      "Epoch: 130, Loss= 0.6930, Training Accuracy= 0.538\n",
      "Epoch: 140, Loss= 0.6938, Training Accuracy= 0.538\n",
      "Epoch: 150, Loss= 0.6932, Training Accuracy= 0.537\n",
      "Epoch: 160, Loss= 0.6943, Training Accuracy= 0.536\n",
      "Epoch: 170, Loss= 0.6980, Training Accuracy= 0.528\n",
      "Epoch: 180, Loss= 0.7025, Training Accuracy= 0.520\n",
      "Epoch: 190, Loss= 0.7024, Training Accuracy= 0.525\n",
      "Epoch: 200, Loss= 0.7074, Training Accuracy= 0.513\n",
      "Epoch: 210, Loss= 0.7077, Training Accuracy= 0.519\n",
      "Epoch: 220, Loss= 0.7077, Training Accuracy= 0.519\n",
      "Epoch: 230, Loss= 0.6892, Training Accuracy= 0.549\n",
      "Epoch: 240, Loss= 0.6902, Training Accuracy= 0.548\n",
      "Epoch: 250, Loss= 0.6959, Training Accuracy= 0.543\n",
      "Epoch: 260, Loss= 0.7009, Training Accuracy= 0.531\n",
      "Epoch: 270, Loss= 0.6975, Training Accuracy= 0.543\n",
      "Epoch: 280, Loss= 0.6993, Training Accuracy= 0.541\n",
      "Epoch: 290, Loss= 0.6894, Training Accuracy= 0.551\n",
      "Epoch: 300, Loss= 0.7022, Training Accuracy= 0.530\n",
      "Epoch: 310, Loss= 0.6834, Training Accuracy= 0.558\n",
      "Epoch: 320, Loss= 0.6975, Training Accuracy= 0.535\n",
      "Epoch: 330, Loss= 0.6813, Training Accuracy= 0.563\n",
      "Epoch: 340, Loss= 0.6918, Training Accuracy= 0.546\n",
      "Epoch: 350, Loss= 0.6917, Training Accuracy= 0.545\n",
      "Epoch: 360, Loss= 0.6962, Training Accuracy= 0.521\n",
      "Epoch: 370, Loss= 0.7051, Training Accuracy= 0.531\n",
      "Epoch: 380, Loss= 0.6920, Training Accuracy= 0.542\n",
      "Epoch: 390, Loss= 0.6887, Training Accuracy= 0.562\n",
      "Epoch: 400, Loss= 0.6988, Training Accuracy= 0.533\n",
      "Epoch: 410, Loss= 0.6956, Training Accuracy= 0.515\n",
      "Epoch: 420, Loss= 0.6919, Training Accuracy= 0.526\n",
      "Epoch: 430, Loss= 0.6906, Training Accuracy= 0.534\n",
      "Epoch: 440, Loss= 0.6897, Training Accuracy= 0.538\n",
      "Epoch: 450, Loss= 0.6888, Training Accuracy= 0.534\n",
      "Epoch: 460, Loss= 0.6881, Training Accuracy= 0.537\n",
      "Epoch: 470, Loss= 0.6888, Training Accuracy= 0.543\n",
      "Epoch: 480, Loss= 0.6881, Training Accuracy= 0.547\n",
      "Epoch: 490, Loss= 0.6880, Training Accuracy= 0.549\n",
      "Epoch: 500, Loss= 0.6854, Training Accuracy= 0.555\n",
      "Epoch: 510, Loss= 0.6872, Training Accuracy= 0.553\n",
      "Epoch: 520, Loss= 0.6944, Training Accuracy= 0.541\n",
      "Epoch: 530, Loss= 0.6915, Training Accuracy= 0.549\n",
      "Epoch: 540, Loss= 0.6923, Training Accuracy= 0.544\n",
      "Epoch: 550, Loss= 0.6936, Training Accuracy= 0.548\n",
      "Epoch: 560, Loss= 0.6842, Training Accuracy= 0.560\n",
      "Epoch: 570, Loss= 0.6979, Training Accuracy= 0.543\n",
      "Epoch: 580, Loss= 0.6838, Training Accuracy= 0.565\n",
      "Epoch: 590, Loss= 0.6796, Training Accuracy= 0.569\n",
      "Epoch: 600, Loss= 0.6846, Training Accuracy= 0.551\n",
      "Epoch: 610, Loss= 0.6823, Training Accuracy= 0.560\n",
      "Epoch: 620, Loss= 0.6881, Training Accuracy= 0.558\n",
      "Epoch: 630, Loss= 0.6801, Training Accuracy= 0.565\n",
      "Epoch: 640, Loss= 0.6740, Training Accuracy= 0.580\n",
      "Epoch: 650, Loss= 0.6739, Training Accuracy= 0.580\n",
      "Epoch: 660, Loss= 0.6920, Training Accuracy= 0.557\n",
      "Epoch: 670, Loss= 0.6876, Training Accuracy= 0.562\n",
      "Epoch: 680, Loss= 0.6847, Training Accuracy= 0.567\n",
      "Epoch: 690, Loss= 0.6930, Training Accuracy= 0.555\n",
      "Epoch: 700, Loss= 0.6921, Training Accuracy= 0.550\n",
      "Epoch: 710, Loss= 0.6767, Training Accuracy= 0.576\n",
      "Epoch: 720, Loss= 0.6771, Training Accuracy= 0.574\n",
      "Epoch: 730, Loss= 0.6679, Training Accuracy= 0.589\n",
      "Epoch: 740, Loss= 0.6753, Training Accuracy= 0.574\n",
      "Epoch: 750, Loss= 0.6712, Training Accuracy= 0.586\n",
      "Epoch: 760, Loss= 0.6797, Training Accuracy= 0.572\n",
      "Epoch: 770, Loss= 0.6795, Training Accuracy= 0.575\n",
      "Epoch: 780, Loss= 0.6745, Training Accuracy= 0.584\n",
      "Epoch: 790, Loss= 0.6656, Training Accuracy= 0.598\n",
      "Epoch: 800, Loss= 0.6707, Training Accuracy= 0.591\n",
      "Epoch: 810, Loss= 0.6693, Training Accuracy= 0.591\n",
      "Epoch: 820, Loss= 0.6678, Training Accuracy= 0.597\n",
      "Epoch: 830, Loss= 0.6788, Training Accuracy= 0.577\n",
      "Epoch: 840, Loss= 0.6943, Training Accuracy= 0.516\n",
      "Epoch: 850, Loss= 0.6945, Training Accuracy= 0.515\n",
      "Epoch: 860, Loss= 0.6940, Training Accuracy= 0.512\n",
      "Epoch: 870, Loss= 0.6940, Training Accuracy= 0.513\n",
      "Epoch: 880, Loss= 0.6950, Training Accuracy= 0.512\n",
      "Epoch: 890, Loss= 0.6935, Training Accuracy= 0.514\n",
      "Epoch: 900, Loss= 0.6935, Training Accuracy= 0.514\n",
      "Epoch: 910, Loss= 0.6925, Training Accuracy= 0.519\n",
      "Epoch: 920, Loss= 0.6936, Training Accuracy= 0.514\n",
      "Epoch: 930, Loss= 0.6926, Training Accuracy= 0.522\n",
      "Epoch: 940, Loss= 0.6908, Training Accuracy= 0.527\n",
      "Epoch: 950, Loss= 0.6908, Training Accuracy= 0.530\n",
      "Epoch: 960, Loss= 0.6937, Training Accuracy= 0.515\n",
      "Epoch: 970, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 980, Loss= 0.6940, Training Accuracy= 0.514\n",
      "Epoch: 990, Loss= 0.6944, Training Accuracy= 0.517\n",
      "Epoch: 1000, Loss= 0.6923, Training Accuracy= 0.518\n",
      "Epoch: 1010, Loss= 0.6928, Training Accuracy= 0.520\n",
      "Epoch: 1020, Loss= 0.6918, Training Accuracy= 0.523\n",
      "Epoch: 1030, Loss= 0.6912, Training Accuracy= 0.525\n",
      "Epoch: 1040, Loss= 0.6918, Training Accuracy= 0.523\n",
      "Epoch: 1050, Loss= 0.6906, Training Accuracy= 0.528\n",
      "Epoch: 1060, Loss= 0.6931, Training Accuracy= 0.518\n",
      "Epoch: 1070, Loss= 0.6902, Training Accuracy= 0.530\n",
      "Epoch: 1080, Loss= 0.6922, Training Accuracy= 0.520\n",
      "Epoch: 1090, Loss= 0.6914, Training Accuracy= 0.523\n",
      "Epoch: 1100, Loss= 0.6918, Training Accuracy= 0.523\n",
      "Epoch: 1110, Loss= 0.6919, Training Accuracy= 0.524\n",
      "Epoch: 1120, Loss= 0.6921, Training Accuracy= 0.524\n",
      "Epoch: 1130, Loss= 0.6897, Training Accuracy= 0.533\n",
      "Epoch: 1140, Loss= 0.6896, Training Accuracy= 0.531\n",
      "Epoch: 1150, Loss= 0.6943, Training Accuracy= 0.516\n",
      "Epoch: 1160, Loss= 0.6885, Training Accuracy= 0.534\n",
      "Epoch: 1170, Loss= 0.6910, Training Accuracy= 0.532\n",
      "Epoch: 1180, Loss= 0.6877, Training Accuracy= 0.540\n",
      "Epoch: 1190, Loss= 0.6876, Training Accuracy= 0.545\n",
      "Epoch: 1200, Loss= 0.6879, Training Accuracy= 0.538\n",
      "Epoch: 1210, Loss= 0.6930, Training Accuracy= 0.524\n",
      "Epoch: 1220, Loss= 0.6860, Training Accuracy= 0.545\n",
      "Epoch: 1230, Loss= 0.6871, Training Accuracy= 0.547\n",
      "Epoch: 1240, Loss= 0.6910, Training Accuracy= 0.525\n",
      "Epoch: 1250, Loss= 0.6872, Training Accuracy= 0.543\n",
      "Epoch: 1260, Loss= 0.6889, Training Accuracy= 0.542\n",
      "Epoch: 1270, Loss= 0.6883, Training Accuracy= 0.538\n",
      "Epoch: 1280, Loss= 0.6855, Training Accuracy= 0.552\n",
      "Epoch: 1290, Loss= 0.6856, Training Accuracy= 0.548\n",
      "Epoch: 1300, Loss= 0.6866, Training Accuracy= 0.548\n",
      "Epoch: 1310, Loss= 0.6877, Training Accuracy= 0.544\n",
      "Epoch: 1320, Loss= 0.6919, Training Accuracy= 0.525\n",
      "Epoch: 1330, Loss= 0.6866, Training Accuracy= 0.546\n",
      "Epoch: 1340, Loss= 0.6950, Training Accuracy= 0.519\n",
      "Epoch: 1350, Loss= 0.6851, Training Accuracy= 0.549\n",
      "Epoch: 1360, Loss= 0.6851, Training Accuracy= 0.546\n",
      "Epoch: 1370, Loss= 0.6915, Training Accuracy= 0.533\n",
      "Epoch: 1380, Loss= 0.6828, Training Accuracy= 0.559\n",
      "Epoch: 1390, Loss= 0.6831, Training Accuracy= 0.554\n",
      "Epoch: 1400, Loss= 0.6871, Training Accuracy= 0.542\n",
      "Epoch: 1410, Loss= 0.6857, Training Accuracy= 0.552\n",
      "Epoch: 1420, Loss= 0.6953, Training Accuracy= 0.514\n",
      "Epoch: 1430, Loss= 0.6928, Training Accuracy= 0.517\n",
      "Epoch: 1440, Loss= 0.6922, Training Accuracy= 0.521\n",
      "Epoch: 1450, Loss= 0.6934, Training Accuracy= 0.519\n",
      "Epoch: 1460, Loss= 0.6950, Training Accuracy= 0.510\n",
      "Epoch: 1470, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 1480, Loss= 0.6925, Training Accuracy= 0.517\n",
      "Epoch: 1490, Loss= 0.6922, Training Accuracy= 0.522\n",
      "Epoch: 1500, Loss= 0.6918, Training Accuracy= 0.525\n",
      "Epoch: 1510, Loss= 0.6915, Training Accuracy= 0.525\n",
      "Epoch: 1520, Loss= 0.6910, Training Accuracy= 0.526\n",
      "Epoch: 1530, Loss= 0.6903, Training Accuracy= 0.531\n",
      "Epoch: 1540, Loss= 0.6896, Training Accuracy= 0.533\n",
      "Epoch: 1550, Loss= 0.6892, Training Accuracy= 0.534\n",
      "Epoch: 1560, Loss= 0.6888, Training Accuracy= 0.538\n",
      "Epoch: 1570, Loss= 0.6880, Training Accuracy= 0.542\n",
      "Epoch: 1580, Loss= 0.6883, Training Accuracy= 0.538\n",
      "Epoch: 1590, Loss= 0.6885, Training Accuracy= 0.539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1600, Loss= 0.6871, Training Accuracy= 0.542\n",
      "Epoch: 1610, Loss= 0.6872, Training Accuracy= 0.544\n",
      "Epoch: 1620, Loss= 0.6866, Training Accuracy= 0.543\n",
      "Epoch: 1630, Loss= 0.6860, Training Accuracy= 0.545\n",
      "Epoch: 1640, Loss= 0.7058, Training Accuracy= 0.504\n",
      "Epoch: 1650, Loss= 0.6959, Training Accuracy= 0.507\n",
      "Epoch: 1660, Loss= 0.6934, Training Accuracy= 0.513\n",
      "Epoch: 1670, Loss= 0.6934, Training Accuracy= 0.513\n",
      "Epoch: 1680, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 1690, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 1700, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 1710, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 1720, Loss= 0.6931, Training Accuracy= 0.515\n",
      "Epoch: 1730, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 1740, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 1750, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 1760, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 1770, Loss= 0.6924, Training Accuracy= 0.517\n",
      "Epoch: 1780, Loss= 0.6923, Training Accuracy= 0.519\n",
      "Epoch: 1790, Loss= 0.6922, Training Accuracy= 0.520\n",
      "Epoch: 1800, Loss= 0.6920, Training Accuracy= 0.521\n",
      "Epoch: 1810, Loss= 0.6919, Training Accuracy= 0.522\n",
      "Epoch: 1820, Loss= 0.6917, Training Accuracy= 0.526\n",
      "Epoch: 1830, Loss= 0.6915, Training Accuracy= 0.526\n",
      "Epoch: 1840, Loss= 0.6913, Training Accuracy= 0.526\n",
      "Epoch: 1850, Loss= 0.6911, Training Accuracy= 0.528\n",
      "Epoch: 1860, Loss= 0.6908, Training Accuracy= 0.528\n",
      "Epoch: 1870, Loss= 0.6905, Training Accuracy= 0.528\n",
      "Epoch: 1880, Loss= 0.6902, Training Accuracy= 0.528\n",
      "Epoch: 1890, Loss= 0.6900, Training Accuracy= 0.528\n",
      "Epoch: 1900, Loss= 0.6900, Training Accuracy= 0.526\n",
      "Epoch: 1910, Loss= 0.6898, Training Accuracy= 0.530\n",
      "Epoch: 1920, Loss= 0.6902, Training Accuracy= 0.531\n",
      "Epoch: 1930, Loss= 0.6895, Training Accuracy= 0.535\n",
      "Epoch: 1940, Loss= 0.6891, Training Accuracy= 0.538\n",
      "Epoch: 1950, Loss= 0.6887, Training Accuracy= 0.539\n",
      "Epoch: 1960, Loss= 0.6889, Training Accuracy= 0.537\n",
      "Epoch: 1970, Loss= 0.7010, Training Accuracy= 0.506\n",
      "Epoch: 1980, Loss= 0.6919, Training Accuracy= 0.522\n",
      "Epoch: 1990, Loss= 0.6889, Training Accuracy= 0.532\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4991\n",
      "Replication: 1: \n",
      "Epoch: 0, Loss= 0.7000, Training Accuracy= 0.506\n",
      "Epoch: 10, Loss= 0.6937, Training Accuracy= 0.512\n",
      "Epoch: 20, Loss= 0.6936, Training Accuracy= 0.513\n",
      "Epoch: 30, Loss= 0.6935, Training Accuracy= 0.514\n",
      "Epoch: 40, Loss= 0.6933, Training Accuracy= 0.513\n",
      "Epoch: 50, Loss= 0.6931, Training Accuracy= 0.514\n",
      "Epoch: 60, Loss= 0.6928, Training Accuracy= 0.513\n",
      "Epoch: 70, Loss= 0.6926, Training Accuracy= 0.514\n",
      "Epoch: 80, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 90, Loss= 0.6921, Training Accuracy= 0.519\n",
      "Epoch: 100, Loss= 0.6919, Training Accuracy= 0.520\n",
      "Epoch: 110, Loss= 0.6917, Training Accuracy= 0.521\n",
      "Epoch: 120, Loss= 0.6915, Training Accuracy= 0.526\n",
      "Epoch: 130, Loss= 0.6913, Training Accuracy= 0.527\n",
      "Epoch: 140, Loss= 0.6912, Training Accuracy= 0.526\n",
      "Epoch: 150, Loss= 0.6910, Training Accuracy= 0.529\n",
      "Epoch: 160, Loss= 0.6908, Training Accuracy= 0.529\n",
      "Epoch: 170, Loss= 0.6906, Training Accuracy= 0.528\n",
      "Epoch: 180, Loss= 0.6904, Training Accuracy= 0.530\n",
      "Epoch: 190, Loss= 0.6902, Training Accuracy= 0.531\n",
      "Epoch: 200, Loss= 0.6899, Training Accuracy= 0.531\n",
      "Epoch: 210, Loss= 0.6897, Training Accuracy= 0.529\n",
      "Epoch: 220, Loss= 0.6895, Training Accuracy= 0.532\n",
      "Epoch: 230, Loss= 0.6893, Training Accuracy= 0.533\n",
      "Epoch: 240, Loss= 0.6891, Training Accuracy= 0.538\n",
      "Epoch: 250, Loss= 0.6891, Training Accuracy= 0.543\n",
      "Epoch: 260, Loss= 0.6890, Training Accuracy= 0.540\n",
      "Epoch: 270, Loss= 0.6887, Training Accuracy= 0.541\n",
      "Epoch: 280, Loss= 0.6878, Training Accuracy= 0.545\n",
      "Epoch: 290, Loss= 0.6873, Training Accuracy= 0.549\n",
      "Epoch: 300, Loss= 0.6883, Training Accuracy= 0.550\n",
      "Epoch: 310, Loss= 0.6881, Training Accuracy= 0.551\n",
      "Epoch: 320, Loss= 0.6879, Training Accuracy= 0.552\n",
      "Epoch: 330, Loss= 0.6885, Training Accuracy= 0.552\n",
      "Epoch: 340, Loss= 0.6956, Training Accuracy= 0.543\n",
      "Epoch: 350, Loss= 0.6912, Training Accuracy= 0.546\n",
      "Epoch: 360, Loss= 0.6861, Training Accuracy= 0.555\n",
      "Epoch: 370, Loss= 0.6861, Training Accuracy= 0.555\n",
      "Epoch: 380, Loss= 0.6882, Training Accuracy= 0.558\n",
      "Epoch: 390, Loss= 0.6832, Training Accuracy= 0.561\n",
      "Epoch: 400, Loss= 0.6850, Training Accuracy= 0.558\n",
      "Epoch: 410, Loss= 0.6840, Training Accuracy= 0.557\n",
      "Epoch: 420, Loss= 0.6837, Training Accuracy= 0.558\n",
      "Epoch: 430, Loss= 0.6916, Training Accuracy= 0.545\n",
      "Epoch: 440, Loss= 0.6838, Training Accuracy= 0.558\n",
      "Epoch: 450, Loss= 0.6953, Training Accuracy= 0.554\n",
      "Epoch: 460, Loss= 0.6842, Training Accuracy= 0.561\n",
      "Epoch: 470, Loss= 0.6768, Training Accuracy= 0.571\n",
      "Epoch: 480, Loss= 0.6800, Training Accuracy= 0.566\n",
      "Epoch: 490, Loss= 0.6918, Training Accuracy= 0.551\n",
      "Epoch: 500, Loss= 0.6769, Training Accuracy= 0.578\n",
      "Epoch: 510, Loss= 0.6832, Training Accuracy= 0.563\n",
      "Epoch: 520, Loss= 0.6742, Training Accuracy= 0.574\n",
      "Epoch: 530, Loss= 0.6953, Training Accuracy= 0.506\n",
      "Epoch: 540, Loss= 0.6927, Training Accuracy= 0.516\n",
      "Epoch: 550, Loss= 0.6922, Training Accuracy= 0.516\n",
      "Epoch: 560, Loss= 0.6918, Training Accuracy= 0.519\n",
      "Epoch: 570, Loss= 0.6910, Training Accuracy= 0.521\n",
      "Epoch: 580, Loss= 0.6944, Training Accuracy= 0.522\n",
      "Epoch: 590, Loss= 0.6918, Training Accuracy= 0.530\n",
      "Epoch: 600, Loss= 0.7009, Training Accuracy= 0.518\n",
      "Epoch: 610, Loss= 0.6864, Training Accuracy= 0.544\n",
      "Epoch: 620, Loss= 0.6839, Training Accuracy= 0.545\n",
      "Epoch: 630, Loss= 0.6952, Training Accuracy= 0.525\n",
      "Epoch: 640, Loss= 0.6873, Training Accuracy= 0.544\n",
      "Epoch: 650, Loss= 0.6910, Training Accuracy= 0.524\n",
      "Epoch: 660, Loss= 0.6887, Training Accuracy= 0.540\n",
      "Epoch: 670, Loss= 0.6808, Training Accuracy= 0.558\n",
      "Epoch: 680, Loss= 0.6985, Training Accuracy= 0.514\n",
      "Epoch: 690, Loss= 0.6827, Training Accuracy= 0.553\n",
      "Epoch: 700, Loss= 0.6884, Training Accuracy= 0.546\n",
      "Epoch: 710, Loss= 0.6869, Training Accuracy= 0.540\n",
      "Epoch: 720, Loss= 0.6814, Training Accuracy= 0.560\n",
      "Epoch: 730, Loss= 0.6855, Training Accuracy= 0.547\n",
      "Epoch: 740, Loss= 0.6797, Training Accuracy= 0.563\n",
      "Epoch: 750, Loss= 0.7066, Training Accuracy= 0.517\n",
      "Epoch: 760, Loss= 0.6795, Training Accuracy= 0.561\n",
      "Epoch: 770, Loss= 0.6856, Training Accuracy= 0.550\n",
      "Epoch: 780, Loss= 0.6918, Training Accuracy= 0.522\n",
      "Epoch: 790, Loss= 0.6814, Training Accuracy= 0.555\n",
      "Epoch: 800, Loss= 0.6860, Training Accuracy= 0.551\n",
      "Epoch: 810, Loss= 0.6787, Training Accuracy= 0.562\n",
      "Epoch: 820, Loss= 0.6879, Training Accuracy= 0.546\n",
      "Epoch: 830, Loss= 0.7115, Training Accuracy= 0.524\n",
      "Epoch: 840, Loss= 0.6794, Training Accuracy= 0.560\n",
      "Epoch: 850, Loss= 0.6936, Training Accuracy= 0.530\n",
      "Epoch: 860, Loss= 0.6812, Training Accuracy= 0.560\n",
      "Epoch: 870, Loss= 0.6801, Training Accuracy= 0.560\n",
      "Epoch: 880, Loss= 0.7022, Training Accuracy= 0.531\n",
      "Epoch: 890, Loss= 0.6751, Training Accuracy= 0.575\n",
      "Epoch: 900, Loss= 0.6790, Training Accuracy= 0.562\n",
      "Epoch: 910, Loss= 0.6759, Training Accuracy= 0.572\n",
      "Epoch: 920, Loss= 0.6740, Training Accuracy= 0.579\n",
      "Epoch: 930, Loss= 0.6819, Training Accuracy= 0.559\n",
      "Epoch: 940, Loss= 0.6750, Training Accuracy= 0.573\n",
      "Epoch: 950, Loss= 0.6973, Training Accuracy= 0.535\n",
      "Epoch: 960, Loss= 0.6960, Training Accuracy= 0.499\n",
      "Epoch: 970, Loss= 0.6941, Training Accuracy= 0.500\n",
      "Epoch: 980, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 990, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 1000, Loss= 0.6940, Training Accuracy= 0.499\n",
      "Epoch: 1010, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 1020, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 1030, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 1040, Loss= 0.6939, Training Accuracy= 0.500\n",
      "Epoch: 1050, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 1060, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 1070, Loss= 0.6938, Training Accuracy= 0.500\n",
      "Epoch: 1080, Loss= 0.6938, Training Accuracy= 0.500\n",
      "Epoch: 1090, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 1100, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 1110, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 1120, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 1130, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 1140, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 1150, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 1160, Loss= 0.6966, Training Accuracy= 0.499\n",
      "Epoch: 1170, Loss= 0.6965, Training Accuracy= 0.499\n",
      "Epoch: 1180, Loss= 0.6965, Training Accuracy= 0.499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1190, Loss= 0.6965, Training Accuracy= 0.499\n",
      "Epoch: 1200, Loss= 0.6938, Training Accuracy= 0.500\n",
      "Epoch: 1210, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 1220, Loss= 0.6938, Training Accuracy= 0.501\n",
      "Epoch: 1230, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 1240, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 1250, Loss= 0.6938, Training Accuracy= 0.502\n",
      "Epoch: 1260, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 1270, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 1280, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 1290, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 1300, Loss= 0.6937, Training Accuracy= 0.504\n",
      "Epoch: 1310, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 1320, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Epoch: 1330, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 1340, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 1350, Loss= 0.6936, Training Accuracy= 0.503\n",
      "Epoch: 1360, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 1370, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 1380, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 1390, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 1400, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 1410, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 1420, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 1430, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 1440, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 1450, Loss= 0.6933, Training Accuracy= 0.508\n",
      "Epoch: 1460, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 1470, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 1480, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 1490, Loss= 0.6933, Training Accuracy= 0.510\n",
      "Epoch: 1500, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 1510, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 1520, Loss= 0.6932, Training Accuracy= 0.510\n",
      "Epoch: 1530, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 1540, Loss= 0.6931, Training Accuracy= 0.510\n",
      "Epoch: 1550, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 1560, Loss= 0.6931, Training Accuracy= 0.512\n",
      "Epoch: 1570, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 1580, Loss= 0.6930, Training Accuracy= 0.512\n",
      "Epoch: 1590, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 1600, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 1610, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 1620, Loss= 0.6930, Training Accuracy= 0.513\n",
      "Epoch: 1630, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 1640, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 1650, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 1660, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 1670, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 1680, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 1690, Loss= 0.6929, Training Accuracy= 0.516\n",
      "Epoch: 1700, Loss= 0.6929, Training Accuracy= 0.515\n",
      "Epoch: 1710, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 1720, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 1730, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 1740, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 1750, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 1760, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 1770, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 1780, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 1790, Loss= 0.6928, Training Accuracy= 0.514\n",
      "Epoch: 1800, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 1810, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 1820, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 1830, Loss= 0.6927, Training Accuracy= 0.514\n",
      "Epoch: 1840, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 1850, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 1860, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 1870, Loss= 0.6926, Training Accuracy= 0.513\n",
      "Epoch: 1880, Loss= 0.6926, Training Accuracy= 0.512\n",
      "Epoch: 1890, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 1900, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 1910, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 1920, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 1930, Loss= 0.6926, Training Accuracy= 0.511\n",
      "Epoch: 1940, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 1950, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 1960, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 1970, Loss= 0.6925, Training Accuracy= 0.511\n",
      "Epoch: 1980, Loss= 0.6925, Training Accuracy= 0.510\n",
      "Epoch: 1990, Loss= 0.6925, Training Accuracy= 0.510\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4916\n",
      "Replication: 2: \n",
      "Epoch: 0, Loss= 0.9092, Training Accuracy= 0.497\n",
      "Epoch: 10, Loss= 0.7638, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.7399, Training Accuracy= 0.497\n",
      "Epoch: 30, Loss= 0.7293, Training Accuracy= 0.497\n",
      "Epoch: 40, Loss= 0.7232, Training Accuracy= 0.497\n",
      "Epoch: 50, Loss= 0.7193, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.7165, Training Accuracy= 0.497\n",
      "Epoch: 70, Loss= 0.7144, Training Accuracy= 0.497\n",
      "Epoch: 80, Loss= 0.7128, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.7115, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.7104, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.7093, Training Accuracy= 0.498\n",
      "Epoch: 120, Loss= 0.7082, Training Accuracy= 0.500\n",
      "Epoch: 130, Loss= 0.7071, Training Accuracy= 0.499\n",
      "Epoch: 140, Loss= 0.7063, Training Accuracy= 0.501\n",
      "Epoch: 150, Loss= 0.7056, Training Accuracy= 0.502\n",
      "Epoch: 160, Loss= 0.7050, Training Accuracy= 0.504\n",
      "Epoch: 170, Loss= 0.7045, Training Accuracy= 0.504\n",
      "Epoch: 180, Loss= 0.7040, Training Accuracy= 0.506\n",
      "Epoch: 190, Loss= 0.7035, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.7029, Training Accuracy= 0.509\n",
      "Epoch: 210, Loss= 0.7022, Training Accuracy= 0.511\n",
      "Epoch: 220, Loss= 0.7015, Training Accuracy= 0.511\n",
      "Epoch: 230, Loss= 0.7010, Training Accuracy= 0.512\n",
      "Epoch: 240, Loss= 0.7009, Training Accuracy= 0.515\n",
      "Epoch: 250, Loss= 0.7011, Training Accuracy= 0.515\n",
      "Epoch: 260, Loss= 0.7014, Training Accuracy= 0.517\n",
      "Epoch: 270, Loss= 0.7015, Training Accuracy= 0.521\n",
      "Epoch: 280, Loss= 0.7004, Training Accuracy= 0.524\n",
      "Epoch: 290, Loss= 0.6997, Training Accuracy= 0.524\n",
      "Epoch: 300, Loss= 0.6989, Training Accuracy= 0.528\n",
      "Epoch: 310, Loss= 0.6978, Training Accuracy= 0.530\n",
      "Epoch: 320, Loss= 0.6972, Training Accuracy= 0.531\n",
      "Epoch: 330, Loss= 0.6968, Training Accuracy= 0.535\n",
      "Epoch: 340, Loss= 0.6968, Training Accuracy= 0.539\n",
      "Epoch: 350, Loss= 0.6962, Training Accuracy= 0.541\n",
      "Epoch: 360, Loss= 0.6966, Training Accuracy= 0.543\n",
      "Epoch: 370, Loss= 0.7014, Training Accuracy= 0.535\n",
      "Epoch: 380, Loss= 0.6967, Training Accuracy= 0.541\n",
      "Epoch: 390, Loss= 0.7046, Training Accuracy= 0.534\n",
      "Epoch: 400, Loss= 0.6988, Training Accuracy= 0.537\n",
      "Epoch: 410, Loss= 0.7005, Training Accuracy= 0.538\n",
      "Epoch: 420, Loss= 0.6941, Training Accuracy= 0.543\n",
      "Epoch: 430, Loss= 0.6976, Training Accuracy= 0.540\n",
      "Epoch: 440, Loss= 0.6915, Training Accuracy= 0.545\n",
      "Epoch: 450, Loss= 0.6954, Training Accuracy= 0.544\n",
      "Epoch: 460, Loss= 0.6889, Training Accuracy= 0.551\n",
      "Epoch: 470, Loss= 0.6879, Training Accuracy= 0.554\n",
      "Epoch: 480, Loss= 0.6871, Training Accuracy= 0.563\n",
      "Epoch: 490, Loss= 0.6972, Training Accuracy= 0.546\n",
      "Epoch: 500, Loss= 0.6819, Training Accuracy= 0.567\n",
      "Epoch: 510, Loss= 0.6890, Training Accuracy= 0.561\n",
      "Epoch: 520, Loss= 0.6918, Training Accuracy= 0.556\n",
      "Epoch: 530, Loss= 0.6866, Training Accuracy= 0.567\n",
      "Epoch: 540, Loss= 0.6836, Training Accuracy= 0.564\n",
      "Epoch: 550, Loss= 0.6986, Training Accuracy= 0.548\n",
      "Epoch: 560, Loss= 0.6788, Training Accuracy= 0.574\n",
      "Epoch: 570, Loss= 0.6970, Training Accuracy= 0.551\n",
      "Epoch: 580, Loss= 0.6873, Training Accuracy= 0.561\n",
      "Epoch: 590, Loss= 0.6741, Training Accuracy= 0.586\n",
      "Epoch: 600, Loss= 0.6746, Training Accuracy= 0.582\n",
      "Epoch: 610, Loss= 0.6712, Training Accuracy= 0.588\n",
      "Epoch: 620, Loss= 0.6733, Training Accuracy= 0.584\n",
      "Epoch: 630, Loss= 0.6703, Training Accuracy= 0.590\n",
      "Epoch: 640, Loss= 0.6735, Training Accuracy= 0.580\n",
      "Epoch: 650, Loss= 0.6690, Training Accuracy= 0.588\n",
      "Epoch: 660, Loss= 0.6752, Training Accuracy= 0.576\n",
      "Epoch: 670, Loss= 0.6740, Training Accuracy= 0.582\n",
      "Epoch: 680, Loss= 0.6861, Training Accuracy= 0.562\n",
      "Epoch: 690, Loss= 0.7044, Training Accuracy= 0.524\n",
      "Epoch: 700, Loss= 0.7086, Training Accuracy= 0.507\n",
      "Epoch: 710, Loss= 0.7034, Training Accuracy= 0.510\n",
      "Epoch: 720, Loss= 0.7019, Training Accuracy= 0.508\n",
      "Epoch: 730, Loss= 0.7003, Training Accuracy= 0.511\n",
      "Epoch: 740, Loss= 0.6968, Training Accuracy= 0.516\n",
      "Epoch: 750, Loss= 0.6978, Training Accuracy= 0.510\n",
      "Epoch: 760, Loss= 0.6952, Training Accuracy= 0.521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 770, Loss= 0.6922, Training Accuracy= 0.534\n",
      "Epoch: 780, Loss= 0.6935, Training Accuracy= 0.527\n",
      "Epoch: 790, Loss= 0.6906, Training Accuracy= 0.538\n",
      "Epoch: 800, Loss= 0.6897, Training Accuracy= 0.536\n",
      "Epoch: 810, Loss= 0.7091, Training Accuracy= 0.498\n",
      "Epoch: 820, Loss= 0.7054, Training Accuracy= 0.497\n",
      "Epoch: 830, Loss= 0.7048, Training Accuracy= 0.497\n",
      "Epoch: 840, Loss= 0.7043, Training Accuracy= 0.497\n",
      "Epoch: 850, Loss= 0.7039, Training Accuracy= 0.497\n",
      "Epoch: 860, Loss= 0.7035, Training Accuracy= 0.497\n",
      "Epoch: 870, Loss= 0.7031, Training Accuracy= 0.497\n",
      "Epoch: 880, Loss= 0.7028, Training Accuracy= 0.497\n",
      "Epoch: 890, Loss= 0.7025, Training Accuracy= 0.497\n",
      "Epoch: 900, Loss= 0.7023, Training Accuracy= 0.498\n",
      "Epoch: 910, Loss= 0.7021, Training Accuracy= 0.498\n",
      "Epoch: 920, Loss= 0.7020, Training Accuracy= 0.500\n",
      "Epoch: 930, Loss= 0.7017, Training Accuracy= 0.500\n",
      "Epoch: 940, Loss= 0.7015, Training Accuracy= 0.501\n",
      "Epoch: 950, Loss= 0.7013, Training Accuracy= 0.502\n",
      "Epoch: 960, Loss= 0.7011, Training Accuracy= 0.503\n",
      "Epoch: 970, Loss= 0.7009, Training Accuracy= 0.504\n",
      "Epoch: 980, Loss= 0.7007, Training Accuracy= 0.504\n",
      "Epoch: 990, Loss= 0.7005, Training Accuracy= 0.505\n",
      "Epoch: 1000, Loss= 0.7003, Training Accuracy= 0.507\n",
      "Epoch: 1010, Loss= 0.7001, Training Accuracy= 0.507\n",
      "Epoch: 1020, Loss= 0.6999, Training Accuracy= 0.508\n",
      "Epoch: 1030, Loss= 0.6997, Training Accuracy= 0.510\n",
      "Epoch: 1040, Loss= 0.6996, Training Accuracy= 0.512\n",
      "Epoch: 1050, Loss= 0.6993, Training Accuracy= 0.514\n",
      "Epoch: 1060, Loss= 0.6988, Training Accuracy= 0.515\n",
      "Epoch: 1070, Loss= 0.6980, Training Accuracy= 0.517\n",
      "Epoch: 1080, Loss= 0.6973, Training Accuracy= 0.520\n",
      "Epoch: 1090, Loss= 0.6967, Training Accuracy= 0.521\n",
      "Epoch: 1100, Loss= 0.6962, Training Accuracy= 0.523\n",
      "Epoch: 1110, Loss= 0.6957, Training Accuracy= 0.524\n",
      "Epoch: 1120, Loss= 0.6953, Training Accuracy= 0.524\n",
      "Epoch: 1130, Loss= 0.6953, Training Accuracy= 0.525\n",
      "Epoch: 1140, Loss= 0.6951, Training Accuracy= 0.523\n",
      "Epoch: 1150, Loss= 0.6958, Training Accuracy= 0.524\n",
      "Epoch: 1160, Loss= 0.6944, Training Accuracy= 0.529\n",
      "Epoch: 1170, Loss= 0.6943, Training Accuracy= 0.528\n",
      "Epoch: 1180, Loss= 0.6943, Training Accuracy= 0.530\n",
      "Epoch: 1190, Loss= 0.6922, Training Accuracy= 0.538\n",
      "Epoch: 1200, Loss= 0.6951, Training Accuracy= 0.526\n",
      "Epoch: 1210, Loss= 0.6891, Training Accuracy= 0.540\n",
      "Epoch: 1220, Loss= 0.6890, Training Accuracy= 0.539\n",
      "Epoch: 1230, Loss= 0.6869, Training Accuracy= 0.546\n",
      "Epoch: 1240, Loss= 0.6913, Training Accuracy= 0.542\n",
      "Epoch: 1250, Loss= 0.6869, Training Accuracy= 0.547\n",
      "Epoch: 1260, Loss= 0.6860, Training Accuracy= 0.550\n",
      "Epoch: 1270, Loss= 0.6867, Training Accuracy= 0.552\n",
      "Epoch: 1280, Loss= 0.6860, Training Accuracy= 0.553\n",
      "Epoch: 1290, Loss= 0.6896, Training Accuracy= 0.549\n",
      "Epoch: 1300, Loss= 0.6930, Training Accuracy= 0.539\n",
      "Epoch: 1310, Loss= 0.6989, Training Accuracy= 0.525\n",
      "Epoch: 1320, Loss= 0.7008, Training Accuracy= 0.532\n",
      "Epoch: 1330, Loss= 0.6874, Training Accuracy= 0.552\n",
      "Epoch: 1340, Loss= 0.6857, Training Accuracy= 0.556\n",
      "Epoch: 1350, Loss= 0.6806, Training Accuracy= 0.564\n",
      "Epoch: 1360, Loss= 0.6942, Training Accuracy= 0.527\n",
      "Epoch: 1370, Loss= 0.6913, Training Accuracy= 0.531\n",
      "Epoch: 1380, Loss= 0.6891, Training Accuracy= 0.536\n",
      "Epoch: 1390, Loss= 0.6890, Training Accuracy= 0.537\n",
      "Epoch: 1400, Loss= 0.6879, Training Accuracy= 0.539\n",
      "Epoch: 1410, Loss= 0.6872, Training Accuracy= 0.543\n",
      "Epoch: 1420, Loss= 0.6870, Training Accuracy= 0.542\n",
      "Epoch: 1430, Loss= 0.7074, Training Accuracy= 0.497\n",
      "Epoch: 1440, Loss= 0.7064, Training Accuracy= 0.497\n",
      "Epoch: 1450, Loss= 0.7059, Training Accuracy= 0.497\n",
      "Epoch: 1460, Loss= 0.7054, Training Accuracy= 0.498\n",
      "Epoch: 1470, Loss= 0.7048, Training Accuracy= 0.499\n",
      "Epoch: 1480, Loss= 0.7041, Training Accuracy= 0.499\n",
      "Epoch: 1490, Loss= 0.7035, Training Accuracy= 0.500\n",
      "Epoch: 1500, Loss= 0.7029, Training Accuracy= 0.501\n",
      "Epoch: 1510, Loss= 0.7024, Training Accuracy= 0.501\n",
      "Epoch: 1520, Loss= 0.7018, Training Accuracy= 0.500\n",
      "Epoch: 1530, Loss= 0.7013, Training Accuracy= 0.501\n",
      "Epoch: 1540, Loss= 0.7009, Training Accuracy= 0.502\n",
      "Epoch: 1550, Loss= 0.7004, Training Accuracy= 0.502\n",
      "Epoch: 1560, Loss= 0.7000, Training Accuracy= 0.505\n",
      "Epoch: 1570, Loss= 0.6995, Training Accuracy= 0.506\n",
      "Epoch: 1580, Loss= 0.6990, Training Accuracy= 0.506\n",
      "Epoch: 1590, Loss= 0.6986, Training Accuracy= 0.508\n",
      "Epoch: 1600, Loss= 0.6981, Training Accuracy= 0.509\n",
      "Epoch: 1610, Loss= 0.6977, Training Accuracy= 0.510\n",
      "Epoch: 1620, Loss= 0.6973, Training Accuracy= 0.511\n",
      "Epoch: 1630, Loss= 0.6970, Training Accuracy= 0.512\n",
      "Epoch: 1640, Loss= 0.6967, Training Accuracy= 0.513\n",
      "Epoch: 1650, Loss= 0.6965, Training Accuracy= 0.513\n",
      "Epoch: 1660, Loss= 0.6963, Training Accuracy= 0.514\n",
      "Epoch: 1670, Loss= 0.6960, Training Accuracy= 0.516\n",
      "Epoch: 1680, Loss= 0.6957, Training Accuracy= 0.517\n",
      "Epoch: 1690, Loss= 0.6953, Training Accuracy= 0.519\n",
      "Epoch: 1700, Loss= 0.6951, Training Accuracy= 0.521\n",
      "Epoch: 1710, Loss= 0.6949, Training Accuracy= 0.522\n",
      "Epoch: 1720, Loss= 0.6947, Training Accuracy= 0.524\n",
      "Epoch: 1730, Loss= 0.6944, Training Accuracy= 0.524\n",
      "Epoch: 1740, Loss= 0.6941, Training Accuracy= 0.523\n",
      "Epoch: 1750, Loss= 0.6940, Training Accuracy= 0.522\n",
      "Epoch: 1760, Loss= 0.6939, Training Accuracy= 0.523\n",
      "Epoch: 1770, Loss= 0.6935, Training Accuracy= 0.525\n",
      "Epoch: 1780, Loss= 0.6926, Training Accuracy= 0.530\n",
      "Epoch: 1790, Loss= 0.6998, Training Accuracy= 0.511\n",
      "Epoch: 1800, Loss= 0.6925, Training Accuracy= 0.527\n",
      "Epoch: 1810, Loss= 0.6912, Training Accuracy= 0.534\n",
      "Epoch: 1820, Loss= 0.6896, Training Accuracy= 0.536\n",
      "Epoch: 1830, Loss= 0.6917, Training Accuracy= 0.535\n",
      "Epoch: 1840, Loss= 0.7013, Training Accuracy= 0.505\n",
      "Epoch: 1850, Loss= 0.6929, Training Accuracy= 0.529\n",
      "Epoch: 1860, Loss= 0.6891, Training Accuracy= 0.534\n",
      "Epoch: 1870, Loss= 0.6905, Training Accuracy= 0.531\n",
      "Epoch: 1880, Loss= 0.6897, Training Accuracy= 0.531\n",
      "Epoch: 1890, Loss= 0.7038, Training Accuracy= 0.498\n",
      "Epoch: 1900, Loss= 0.7028, Training Accuracy= 0.498\n",
      "Epoch: 1910, Loss= 0.7006, Training Accuracy= 0.504\n",
      "Epoch: 1920, Loss= 0.6991, Training Accuracy= 0.507\n",
      "Epoch: 1930, Loss= 0.6983, Training Accuracy= 0.508\n",
      "Epoch: 1940, Loss= 0.6969, Training Accuracy= 0.512\n",
      "Epoch: 1950, Loss= 0.6919, Training Accuracy= 0.528\n",
      "Epoch: 1960, Loss= 0.6955, Training Accuracy= 0.524\n",
      "Epoch: 1970, Loss= 0.6875, Training Accuracy= 0.539\n",
      "Epoch: 1980, Loss= 0.6874, Training Accuracy= 0.539\n",
      "Epoch: 1990, Loss= 0.6871, Training Accuracy= 0.544\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4976\n",
      "Replication: 3: \n",
      "Epoch: 0, Loss= 0.7328, Training Accuracy= 0.498\n",
      "Epoch: 10, Loss= 0.7119, Training Accuracy= 0.497\n",
      "Epoch: 20, Loss= 0.7060, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.7028, Training Accuracy= 0.495\n",
      "Epoch: 40, Loss= 0.7010, Training Accuracy= 0.495\n",
      "Epoch: 50, Loss= 0.6999, Training Accuracy= 0.497\n",
      "Epoch: 60, Loss= 0.6988, Training Accuracy= 0.500\n",
      "Epoch: 70, Loss= 0.6982, Training Accuracy= 0.506\n",
      "Epoch: 80, Loss= 0.7003, Training Accuracy= 0.513\n",
      "Epoch: 90, Loss= 0.7003, Training Accuracy= 0.512\n",
      "Epoch: 100, Loss= 0.7003, Training Accuracy= 0.515\n",
      "Epoch: 110, Loss= 0.7003, Training Accuracy= 0.517\n",
      "Epoch: 120, Loss= 0.7003, Training Accuracy= 0.519\n",
      "Epoch: 130, Loss= 0.7001, Training Accuracy= 0.521\n",
      "Epoch: 140, Loss= 0.6998, Training Accuracy= 0.521\n",
      "Epoch: 150, Loss= 0.6992, Training Accuracy= 0.521\n",
      "Epoch: 160, Loss= 0.6987, Training Accuracy= 0.524\n",
      "Epoch: 170, Loss= 0.6987, Training Accuracy= 0.526\n",
      "Epoch: 180, Loss= 0.6996, Training Accuracy= 0.526\n",
      "Epoch: 190, Loss= 0.7022, Training Accuracy= 0.522\n",
      "Epoch: 200, Loss= 0.7081, Training Accuracy= 0.522\n",
      "Epoch: 210, Loss= 0.7141, Training Accuracy= 0.521\n",
      "Epoch: 220, Loss= 0.7152, Training Accuracy= 0.525\n",
      "Epoch: 230, Loss= 0.7122, Training Accuracy= 0.529\n",
      "Epoch: 240, Loss= 0.7085, Training Accuracy= 0.529\n",
      "Epoch: 250, Loss= 0.7073, Training Accuracy= 0.531\n",
      "Epoch: 260, Loss= 0.7054, Training Accuracy= 0.538\n",
      "Epoch: 270, Loss= 0.7016, Training Accuracy= 0.540\n",
      "Epoch: 280, Loss= 0.7012, Training Accuracy= 0.543\n",
      "Epoch: 290, Loss= 0.7034, Training Accuracy= 0.540\n",
      "Epoch: 300, Loss= 0.7048, Training Accuracy= 0.543\n",
      "Epoch: 310, Loss= 0.7149, Training Accuracy= 0.540\n",
      "Epoch: 320, Loss= 0.7095, Training Accuracy= 0.546\n",
      "Epoch: 330, Loss= 0.7044, Training Accuracy= 0.548\n",
      "Epoch: 340, Loss= 0.6974, Training Accuracy= 0.557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 350, Loss= 0.6966, Training Accuracy= 0.556\n",
      "Epoch: 360, Loss= 0.6964, Training Accuracy= 0.555\n",
      "Epoch: 370, Loss= 0.6865, Training Accuracy= 0.562\n",
      "Epoch: 380, Loss= 0.6973, Training Accuracy= 0.558\n",
      "Epoch: 390, Loss= 0.7028, Training Accuracy= 0.549\n",
      "Epoch: 400, Loss= 0.7097, Training Accuracy= 0.547\n",
      "Epoch: 410, Loss= 0.7069, Training Accuracy= 0.553\n",
      "Epoch: 420, Loss= 0.6986, Training Accuracy= 0.560\n",
      "Epoch: 430, Loss= 0.6991, Training Accuracy= 0.553\n",
      "Epoch: 440, Loss= 0.6948, Training Accuracy= 0.561\n",
      "Epoch: 450, Loss= 0.6801, Training Accuracy= 0.572\n",
      "Epoch: 460, Loss= 0.7181, Training Accuracy= 0.498\n",
      "Epoch: 470, Loss= 0.7085, Training Accuracy= 0.498\n",
      "Epoch: 480, Loss= 0.7068, Training Accuracy= 0.498\n",
      "Epoch: 490, Loss= 0.7057, Training Accuracy= 0.498\n",
      "Epoch: 500, Loss= 0.7049, Training Accuracy= 0.498\n",
      "Epoch: 510, Loss= 0.7041, Training Accuracy= 0.498\n",
      "Epoch: 520, Loss= 0.7033, Training Accuracy= 0.498\n",
      "Epoch: 530, Loss= 0.7025, Training Accuracy= 0.498\n",
      "Epoch: 540, Loss= 0.7019, Training Accuracy= 0.498\n",
      "Epoch: 550, Loss= 0.7015, Training Accuracy= 0.498\n",
      "Epoch: 560, Loss= 0.7010, Training Accuracy= 0.498\n",
      "Epoch: 570, Loss= 0.7006, Training Accuracy= 0.498\n",
      "Epoch: 580, Loss= 0.7002, Training Accuracy= 0.499\n",
      "Epoch: 590, Loss= 0.6998, Training Accuracy= 0.499\n",
      "Epoch: 600, Loss= 0.6994, Training Accuracy= 0.500\n",
      "Epoch: 610, Loss= 0.6990, Training Accuracy= 0.500\n",
      "Epoch: 620, Loss= 0.6986, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 0.6981, Training Accuracy= 0.504\n",
      "Epoch: 640, Loss= 0.6977, Training Accuracy= 0.505\n",
      "Epoch: 650, Loss= 0.6973, Training Accuracy= 0.505\n",
      "Epoch: 660, Loss= 0.6970, Training Accuracy= 0.506\n",
      "Epoch: 670, Loss= 0.6967, Training Accuracy= 0.507\n",
      "Epoch: 680, Loss= 0.6963, Training Accuracy= 0.508\n",
      "Epoch: 690, Loss= 0.6959, Training Accuracy= 0.509\n",
      "Epoch: 700, Loss= 0.6955, Training Accuracy= 0.511\n",
      "Epoch: 710, Loss= 0.6951, Training Accuracy= 0.514\n",
      "Epoch: 720, Loss= 0.6947, Training Accuracy= 0.518\n",
      "Epoch: 730, Loss= 0.6943, Training Accuracy= 0.521\n",
      "Epoch: 740, Loss= 0.6939, Training Accuracy= 0.526\n",
      "Epoch: 750, Loss= 0.6932, Training Accuracy= 0.529\n",
      "Epoch: 760, Loss= 0.6922, Training Accuracy= 0.530\n",
      "Epoch: 770, Loss= 0.6910, Training Accuracy= 0.533\n",
      "Epoch: 780, Loss= 0.6895, Training Accuracy= 0.538\n",
      "Epoch: 790, Loss= 0.6883, Training Accuracy= 0.540\n",
      "Epoch: 800, Loss= 0.6875, Training Accuracy= 0.545\n",
      "Epoch: 810, Loss= 0.6854, Training Accuracy= 0.552\n",
      "Epoch: 820, Loss= 0.6860, Training Accuracy= 0.551\n",
      "Epoch: 830, Loss= 0.6838, Training Accuracy= 0.557\n",
      "Epoch: 840, Loss= 0.6860, Training Accuracy= 0.556\n",
      "Epoch: 850, Loss= 0.6862, Training Accuracy= 0.549\n",
      "Epoch: 860, Loss= 0.6855, Training Accuracy= 0.551\n",
      "Epoch: 870, Loss= 0.6856, Training Accuracy= 0.554\n",
      "Epoch: 880, Loss= 0.6811, Training Accuracy= 0.565\n",
      "Epoch: 890, Loss= 0.6813, Training Accuracy= 0.566\n",
      "Epoch: 900, Loss= 0.6785, Training Accuracy= 0.572\n",
      "Epoch: 910, Loss= 0.6821, Training Accuracy= 0.562\n",
      "Epoch: 920, Loss= 0.6811, Training Accuracy= 0.561\n",
      "Epoch: 930, Loss= 0.6802, Training Accuracy= 0.563\n",
      "Epoch: 940, Loss= 0.6794, Training Accuracy= 0.567\n",
      "Epoch: 950, Loss= 0.6803, Training Accuracy= 0.566\n",
      "Epoch: 960, Loss= 0.6808, Training Accuracy= 0.569\n",
      "Epoch: 970, Loss= 0.6894, Training Accuracy= 0.543\n",
      "Epoch: 980, Loss= 0.6792, Training Accuracy= 0.566\n",
      "Epoch: 990, Loss= 0.7189, Training Accuracy= 0.502\n",
      "Epoch: 1000, Loss= 0.7021, Training Accuracy= 0.498\n",
      "Epoch: 1010, Loss= 0.7021, Training Accuracy= 0.498\n",
      "Epoch: 1020, Loss= 0.7022, Training Accuracy= 0.498\n",
      "Epoch: 1030, Loss= 0.7023, Training Accuracy= 0.498\n",
      "Epoch: 1040, Loss= 0.7023, Training Accuracy= 0.498\n",
      "Epoch: 1050, Loss= 0.7024, Training Accuracy= 0.498\n",
      "Epoch: 1060, Loss= 0.7023, Training Accuracy= 0.498\n",
      "Epoch: 1070, Loss= 0.7023, Training Accuracy= 0.498\n",
      "Epoch: 1080, Loss= 0.7023, Training Accuracy= 0.498\n",
      "Epoch: 1090, Loss= 0.7022, Training Accuracy= 0.498\n",
      "Epoch: 1100, Loss= 0.7022, Training Accuracy= 0.498\n",
      "Epoch: 1110, Loss= 0.7021, Training Accuracy= 0.498\n",
      "Epoch: 1120, Loss= 0.7021, Training Accuracy= 0.498\n",
      "Epoch: 1130, Loss= 0.7021, Training Accuracy= 0.498\n",
      "Epoch: 1140, Loss= 0.7020, Training Accuracy= 0.498\n",
      "Epoch: 1150, Loss= 0.7019, Training Accuracy= 0.498\n",
      "Epoch: 1160, Loss= 0.7018, Training Accuracy= 0.498\n",
      "Epoch: 1170, Loss= 0.7017, Training Accuracy= 0.498\n",
      "Epoch: 1180, Loss= 0.7015, Training Accuracy= 0.498\n",
      "Epoch: 1190, Loss= 0.7014, Training Accuracy= 0.498\n",
      "Epoch: 1200, Loss= 0.7012, Training Accuracy= 0.498\n",
      "Epoch: 1210, Loss= 0.7010, Training Accuracy= 0.498\n",
      "Epoch: 1220, Loss= 0.7009, Training Accuracy= 0.498\n",
      "Epoch: 1230, Loss= 0.7007, Training Accuracy= 0.498\n",
      "Epoch: 1240, Loss= 0.7006, Training Accuracy= 0.497\n",
      "Epoch: 1250, Loss= 0.7004, Training Accuracy= 0.499\n",
      "Epoch: 1260, Loss= 0.7002, Training Accuracy= 0.500\n",
      "Epoch: 1270, Loss= 0.7001, Training Accuracy= 0.500\n",
      "Epoch: 1280, Loss= 0.7000, Training Accuracy= 0.501\n",
      "Epoch: 1290, Loss= 0.6996, Training Accuracy= 0.503\n",
      "Epoch: 1300, Loss= 0.6999, Training Accuracy= 0.505\n",
      "Epoch: 1310, Loss= 0.6995, Training Accuracy= 0.504\n",
      "Epoch: 1320, Loss= 0.7001, Training Accuracy= 0.503\n",
      "Epoch: 1330, Loss= 0.7002, Training Accuracy= 0.508\n",
      "Epoch: 1340, Loss= 0.7011, Training Accuracy= 0.508\n",
      "Epoch: 1350, Loss= 0.7021, Training Accuracy= 0.504\n",
      "Epoch: 1360, Loss= 0.7004, Training Accuracy= 0.504\n",
      "Epoch: 1370, Loss= 0.7026, Training Accuracy= 0.514\n",
      "Epoch: 1380, Loss= 0.7029, Training Accuracy= 0.511\n",
      "Epoch: 1390, Loss= 0.7016, Training Accuracy= 0.512\n",
      "Epoch: 1400, Loss= 0.7072, Training Accuracy= 0.502\n",
      "Epoch: 1410, Loss= 0.7017, Training Accuracy= 0.498\n",
      "Epoch: 1420, Loss= 0.7012, Training Accuracy= 0.498\n",
      "Epoch: 1430, Loss= 0.7010, Training Accuracy= 0.498\n",
      "Epoch: 1440, Loss= 0.7008, Training Accuracy= 0.498\n",
      "Epoch: 1450, Loss= 0.7007, Training Accuracy= 0.498\n",
      "Epoch: 1460, Loss= 0.7007, Training Accuracy= 0.498\n",
      "Epoch: 1470, Loss= 0.7006, Training Accuracy= 0.499\n",
      "Epoch: 1480, Loss= 0.7005, Training Accuracy= 0.500\n",
      "Epoch: 1490, Loss= 0.7002, Training Accuracy= 0.500\n",
      "Epoch: 1500, Loss= 0.6998, Training Accuracy= 0.500\n",
      "Epoch: 1510, Loss= 0.6996, Training Accuracy= 0.500\n",
      "Epoch: 1520, Loss= 0.7000, Training Accuracy= 0.501\n",
      "Epoch: 1530, Loss= 0.6997, Training Accuracy= 0.501\n",
      "Epoch: 1540, Loss= 0.7015, Training Accuracy= 0.494\n",
      "Epoch: 1550, Loss= 0.7036, Training Accuracy= 0.498\n",
      "Epoch: 1560, Loss= 0.7023, Training Accuracy= 0.498\n",
      "Epoch: 1570, Loss= 0.7020, Training Accuracy= 0.498\n",
      "Epoch: 1580, Loss= 0.7013, Training Accuracy= 0.498\n",
      "Epoch: 1590, Loss= 0.7001, Training Accuracy= 0.498\n",
      "Epoch: 1600, Loss= 0.6998, Training Accuracy= 0.497\n",
      "Epoch: 1610, Loss= 0.6997, Training Accuracy= 0.497\n",
      "Epoch: 1620, Loss= 0.7027, Training Accuracy= 0.498\n",
      "Epoch: 1630, Loss= 0.7021, Training Accuracy= 0.498\n",
      "Epoch: 1640, Loss= 0.7017, Training Accuracy= 0.498\n",
      "Epoch: 1650, Loss= 0.7013, Training Accuracy= 0.502\n",
      "Epoch: 1660, Loss= 0.7007, Training Accuracy= 0.501\n",
      "Epoch: 1670, Loss= 0.7005, Training Accuracy= 0.501\n",
      "Epoch: 1680, Loss= 0.7002, Training Accuracy= 0.501\n",
      "Epoch: 1690, Loss= 0.7000, Training Accuracy= 0.503\n",
      "Epoch: 1700, Loss= 0.7011, Training Accuracy= 0.499\n",
      "Epoch: 1710, Loss= 0.7006, Training Accuracy= 0.500\n",
      "Epoch: 1720, Loss= 0.7005, Training Accuracy= 0.503\n",
      "Epoch: 1730, Loss= 0.7001, Training Accuracy= 0.503\n",
      "Epoch: 1740, Loss= 0.6997, Training Accuracy= 0.506\n",
      "Epoch: 1750, Loss= 0.6991, Training Accuracy= 0.507\n",
      "Epoch: 1760, Loss= 0.6992, Training Accuracy= 0.507\n",
      "Epoch: 1770, Loss= 0.6986, Training Accuracy= 0.508\n",
      "Epoch: 1780, Loss= 0.6986, Training Accuracy= 0.509\n",
      "Epoch: 1790, Loss= 0.6984, Training Accuracy= 0.507\n",
      "Epoch: 1800, Loss= 0.7031, Training Accuracy= 0.498\n",
      "Epoch: 1810, Loss= 0.7020, Training Accuracy= 0.498\n",
      "Epoch: 1820, Loss= 0.7019, Training Accuracy= 0.498\n",
      "Epoch: 1830, Loss= 0.7019, Training Accuracy= 0.498\n",
      "Epoch: 1840, Loss= 0.7019, Training Accuracy= 0.498\n",
      "Epoch: 1850, Loss= 0.7019, Training Accuracy= 0.498\n",
      "Epoch: 1860, Loss= 0.7020, Training Accuracy= 0.498\n",
      "Epoch: 1870, Loss= 0.7022, Training Accuracy= 0.498\n",
      "Epoch: 1880, Loss= 0.7022, Training Accuracy= 0.497\n",
      "Epoch: 1890, Loss= 0.7022, Training Accuracy= 0.498\n",
      "Epoch: 1900, Loss= 0.7021, Training Accuracy= 0.498\n",
      "Epoch: 1910, Loss= 0.7019, Training Accuracy= 0.498\n",
      "Epoch: 1920, Loss= 0.7018, Training Accuracy= 0.498\n",
      "Epoch: 1930, Loss= 0.7016, Training Accuracy= 0.498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1940, Loss= 0.7015, Training Accuracy= 0.498\n",
      "Epoch: 1950, Loss= 0.7015, Training Accuracy= 0.498\n",
      "Epoch: 1960, Loss= 0.7016, Training Accuracy= 0.498\n",
      "Epoch: 1970, Loss= 0.7015, Training Accuracy= 0.498\n",
      "Epoch: 1980, Loss= 0.7015, Training Accuracy= 0.498\n",
      "Epoch: 1990, Loss= 0.7014, Training Accuracy= 0.498\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4949\n",
      "Replication: 4: \n",
      "Epoch: 0, Loss= 0.8153, Training Accuracy= 0.508\n",
      "Epoch: 10, Loss= 0.7294, Training Accuracy= 0.509\n",
      "Epoch: 20, Loss= 0.7182, Training Accuracy= 0.509\n",
      "Epoch: 30, Loss= 0.7128, Training Accuracy= 0.509\n",
      "Epoch: 40, Loss= 0.7096, Training Accuracy= 0.509\n",
      "Epoch: 50, Loss= 0.7074, Training Accuracy= 0.509\n",
      "Epoch: 60, Loss= 0.7058, Training Accuracy= 0.508\n",
      "Epoch: 70, Loss= 0.7045, Training Accuracy= 0.508\n",
      "Epoch: 80, Loss= 0.7036, Training Accuracy= 0.509\n",
      "Epoch: 90, Loss= 0.7027, Training Accuracy= 0.509\n",
      "Epoch: 100, Loss= 0.7019, Training Accuracy= 0.510\n",
      "Epoch: 110, Loss= 0.7011, Training Accuracy= 0.510\n",
      "Epoch: 120, Loss= 0.6999, Training Accuracy= 0.511\n",
      "Epoch: 130, Loss= 0.6998, Training Accuracy= 0.513\n",
      "Epoch: 140, Loss= 0.6996, Training Accuracy= 0.514\n",
      "Epoch: 150, Loss= 0.6993, Training Accuracy= 0.513\n",
      "Epoch: 160, Loss= 0.6998, Training Accuracy= 0.515\n",
      "Epoch: 170, Loss= 0.6997, Training Accuracy= 0.514\n",
      "Epoch: 180, Loss= 0.6980, Training Accuracy= 0.518\n",
      "Epoch: 190, Loss= 0.6965, Training Accuracy= 0.522\n",
      "Epoch: 200, Loss= 0.6971, Training Accuracy= 0.523\n",
      "Epoch: 210, Loss= 0.6977, Training Accuracy= 0.523\n",
      "Epoch: 220, Loss= 0.6979, Training Accuracy= 0.523\n",
      "Epoch: 230, Loss= 0.6986, Training Accuracy= 0.526\n",
      "Epoch: 240, Loss= 0.6988, Training Accuracy= 0.526\n",
      "Epoch: 250, Loss= 0.6990, Training Accuracy= 0.525\n",
      "Epoch: 260, Loss= 0.6989, Training Accuracy= 0.524\n",
      "Epoch: 270, Loss= 0.6989, Training Accuracy= 0.526\n",
      "Epoch: 280, Loss= 0.6991, Training Accuracy= 0.532\n",
      "Epoch: 290, Loss= 0.6955, Training Accuracy= 0.536\n",
      "Epoch: 300, Loss= 0.6953, Training Accuracy= 0.537\n",
      "Epoch: 310, Loss= 0.6961, Training Accuracy= 0.540\n",
      "Epoch: 320, Loss= 0.6968, Training Accuracy= 0.529\n",
      "Epoch: 330, Loss= 0.6950, Training Accuracy= 0.537\n",
      "Epoch: 340, Loss= 0.6941, Training Accuracy= 0.543\n",
      "Epoch: 350, Loss= 0.6922, Training Accuracy= 0.546\n",
      "Epoch: 360, Loss= 0.6912, Training Accuracy= 0.551\n",
      "Epoch: 370, Loss= 0.6936, Training Accuracy= 0.545\n",
      "Epoch: 380, Loss= 0.6939, Training Accuracy= 0.544\n",
      "Epoch: 390, Loss= 0.6916, Training Accuracy= 0.546\n",
      "Epoch: 400, Loss= 0.6880, Training Accuracy= 0.555\n",
      "Epoch: 410, Loss= 0.6869, Training Accuracy= 0.557\n",
      "Epoch: 420, Loss= 0.6844, Training Accuracy= 0.560\n",
      "Epoch: 430, Loss= 0.6864, Training Accuracy= 0.561\n",
      "Epoch: 440, Loss= 0.7027, Training Accuracy= 0.539\n",
      "Epoch: 450, Loss= 0.6922, Training Accuracy= 0.553\n",
      "Epoch: 460, Loss= 0.7101, Training Accuracy= 0.524\n",
      "Epoch: 470, Loss= 0.6942, Training Accuracy= 0.550\n",
      "Epoch: 480, Loss= 0.6948, Training Accuracy= 0.547\n",
      "Epoch: 490, Loss= 0.7013, Training Accuracy= 0.534\n",
      "Epoch: 500, Loss= 0.6983, Training Accuracy= 0.546\n",
      "Epoch: 510, Loss= 0.6936, Training Accuracy= 0.549\n",
      "Epoch: 520, Loss= 0.7105, Training Accuracy= 0.528\n",
      "Epoch: 530, Loss= 0.6917, Training Accuracy= 0.556\n",
      "Epoch: 540, Loss= 0.6958, Training Accuracy= 0.512\n",
      "Epoch: 550, Loss= 0.7487, Training Accuracy= 0.508\n",
      "Epoch: 560, Loss= 0.7382, Training Accuracy= 0.508\n",
      "Epoch: 570, Loss= 0.7316, Training Accuracy= 0.508\n",
      "Epoch: 580, Loss= 0.7251, Training Accuracy= 0.508\n",
      "Epoch: 590, Loss= 0.7197, Training Accuracy= 0.508\n",
      "Epoch: 600, Loss= 0.7153, Training Accuracy= 0.510\n",
      "Epoch: 610, Loss= 0.7116, Training Accuracy= 0.512\n",
      "Epoch: 620, Loss= 0.7104, Training Accuracy= 0.516\n",
      "Epoch: 630, Loss= 0.7093, Training Accuracy= 0.517\n",
      "Epoch: 640, Loss= 0.7066, Training Accuracy= 0.520\n",
      "Epoch: 650, Loss= 0.7004, Training Accuracy= 0.527\n",
      "Epoch: 660, Loss= 0.6995, Training Accuracy= 0.527\n",
      "Epoch: 670, Loss= 0.6932, Training Accuracy= 0.540\n",
      "Epoch: 680, Loss= 0.6909, Training Accuracy= 0.543\n",
      "Epoch: 690, Loss= 0.6891, Training Accuracy= 0.541\n",
      "Epoch: 700, Loss= 0.6863, Training Accuracy= 0.552\n",
      "Epoch: 710, Loss= 0.6891, Training Accuracy= 0.545\n",
      "Epoch: 720, Loss= 0.6845, Training Accuracy= 0.555\n",
      "Epoch: 730, Loss= 0.6848, Training Accuracy= 0.558\n",
      "Epoch: 740, Loss= 0.6991, Training Accuracy= 0.551\n",
      "Epoch: 750, Loss= 0.6840, Training Accuracy= 0.559\n",
      "Epoch: 760, Loss= 0.6896, Training Accuracy= 0.558\n",
      "Epoch: 770, Loss= 0.6879, Training Accuracy= 0.554\n",
      "Epoch: 780, Loss= 0.7228, Training Accuracy= 0.508\n",
      "Epoch: 790, Loss= 0.7159, Training Accuracy= 0.508\n",
      "Epoch: 800, Loss= 0.7135, Training Accuracy= 0.508\n",
      "Epoch: 810, Loss= 0.7119, Training Accuracy= 0.508\n",
      "Epoch: 820, Loss= 0.7107, Training Accuracy= 0.508\n",
      "Epoch: 830, Loss= 0.7098, Training Accuracy= 0.508\n",
      "Epoch: 840, Loss= 0.7090, Training Accuracy= 0.508\n",
      "Epoch: 850, Loss= 0.7083, Training Accuracy= 0.508\n",
      "Epoch: 860, Loss= 0.7077, Training Accuracy= 0.508\n",
      "Epoch: 870, Loss= 0.7071, Training Accuracy= 0.508\n",
      "Epoch: 880, Loss= 0.7066, Training Accuracy= 0.508\n",
      "Epoch: 890, Loss= 0.7062, Training Accuracy= 0.508\n",
      "Epoch: 900, Loss= 0.7058, Training Accuracy= 0.508\n",
      "Epoch: 910, Loss= 0.7054, Training Accuracy= 0.508\n",
      "Epoch: 920, Loss= 0.7050, Training Accuracy= 0.508\n",
      "Epoch: 930, Loss= 0.7047, Training Accuracy= 0.508\n",
      "Epoch: 940, Loss= 0.7044, Training Accuracy= 0.508\n",
      "Epoch: 950, Loss= 0.7041, Training Accuracy= 0.508\n",
      "Epoch: 960, Loss= 0.7038, Training Accuracy= 0.508\n",
      "Epoch: 970, Loss= 0.7035, Training Accuracy= 0.508\n",
      "Epoch: 980, Loss= 0.7032, Training Accuracy= 0.508\n",
      "Epoch: 990, Loss= 0.7030, Training Accuracy= 0.508\n",
      "Epoch: 1000, Loss= 0.7028, Training Accuracy= 0.508\n",
      "Epoch: 1010, Loss= 0.7025, Training Accuracy= 0.508\n",
      "Epoch: 1020, Loss= 0.7023, Training Accuracy= 0.508\n",
      "Epoch: 1030, Loss= 0.7021, Training Accuracy= 0.508\n",
      "Epoch: 1040, Loss= 0.7018, Training Accuracy= 0.508\n",
      "Epoch: 1050, Loss= 0.7015, Training Accuracy= 0.508\n",
      "Epoch: 1060, Loss= 0.7013, Training Accuracy= 0.508\n",
      "Epoch: 1070, Loss= 0.7011, Training Accuracy= 0.508\n",
      "Epoch: 1080, Loss= 0.7009, Training Accuracy= 0.508\n",
      "Epoch: 1090, Loss= 0.7007, Training Accuracy= 0.508\n",
      "Epoch: 1100, Loss= 0.7006, Training Accuracy= 0.508\n",
      "Epoch: 1110, Loss= 0.7108, Training Accuracy= 0.507\n",
      "Epoch: 1120, Loss= 0.7042, Training Accuracy= 0.508\n",
      "Epoch: 1130, Loss= 0.7039, Training Accuracy= 0.508\n",
      "Epoch: 1140, Loss= 0.7035, Training Accuracy= 0.508\n",
      "Epoch: 1150, Loss= 0.7030, Training Accuracy= 0.508\n",
      "Epoch: 1160, Loss= 0.7027, Training Accuracy= 0.508\n",
      "Epoch: 1170, Loss= 0.7024, Training Accuracy= 0.508\n",
      "Epoch: 1180, Loss= 0.7021, Training Accuracy= 0.508\n",
      "Epoch: 1190, Loss= 0.7020, Training Accuracy= 0.508\n",
      "Epoch: 1200, Loss= 0.7019, Training Accuracy= 0.508\n",
      "Epoch: 1210, Loss= 0.7018, Training Accuracy= 0.508\n",
      "Epoch: 1220, Loss= 0.7017, Training Accuracy= 0.508\n",
      "Epoch: 1230, Loss= 0.7016, Training Accuracy= 0.508\n",
      "Epoch: 1240, Loss= 0.7015, Training Accuracy= 0.508\n",
      "Epoch: 1250, Loss= 0.7014, Training Accuracy= 0.508\n",
      "Epoch: 1260, Loss= 0.7013, Training Accuracy= 0.508\n",
      "Epoch: 1270, Loss= 0.7012, Training Accuracy= 0.508\n",
      "Epoch: 1280, Loss= 0.7011, Training Accuracy= 0.508\n",
      "Epoch: 1290, Loss= 0.7010, Training Accuracy= 0.508\n",
      "Epoch: 1300, Loss= 0.7009, Training Accuracy= 0.508\n",
      "Epoch: 1310, Loss= 0.7008, Training Accuracy= 0.508\n",
      "Epoch: 1320, Loss= 0.7006, Training Accuracy= 0.509\n",
      "Epoch: 1330, Loss= 0.7005, Training Accuracy= 0.509\n",
      "Epoch: 1340, Loss= 0.7004, Training Accuracy= 0.509\n",
      "Epoch: 1350, Loss= 0.7003, Training Accuracy= 0.510\n",
      "Epoch: 1360, Loss= 0.7001, Training Accuracy= 0.510\n",
      "Epoch: 1370, Loss= 0.6999, Training Accuracy= 0.511\n",
      "Epoch: 1380, Loss= 0.6996, Training Accuracy= 0.511\n",
      "Epoch: 1390, Loss= 0.6993, Training Accuracy= 0.513\n",
      "Epoch: 1400, Loss= 0.6989, Training Accuracy= 0.514\n",
      "Epoch: 1410, Loss= 0.6985, Training Accuracy= 0.516\n",
      "Epoch: 1420, Loss= 0.6981, Training Accuracy= 0.516\n",
      "Epoch: 1430, Loss= 0.6978, Training Accuracy= 0.516\n",
      "Epoch: 1440, Loss= 0.6975, Training Accuracy= 0.518\n",
      "Epoch: 1450, Loss= 0.6973, Training Accuracy= 0.519\n",
      "Epoch: 1460, Loss= 0.6970, Training Accuracy= 0.519\n",
      "Epoch: 1470, Loss= 0.6968, Training Accuracy= 0.519\n",
      "Epoch: 1480, Loss= 0.6964, Training Accuracy= 0.521\n",
      "Epoch: 1490, Loss= 0.6960, Training Accuracy= 0.521\n",
      "Epoch: 1500, Loss= 0.6955, Training Accuracy= 0.521\n",
      "Epoch: 1510, Loss= 0.6949, Training Accuracy= 0.520\n",
      "Epoch: 1520, Loss= 0.6949, Training Accuracy= 0.522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1530, Loss= 0.6945, Training Accuracy= 0.522\n",
      "Epoch: 1540, Loss= 0.6945, Training Accuracy= 0.523\n",
      "Epoch: 1550, Loss= 0.6943, Training Accuracy= 0.524\n",
      "Epoch: 1560, Loss= 0.6938, Training Accuracy= 0.525\n",
      "Epoch: 1570, Loss= 0.6938, Training Accuracy= 0.528\n",
      "Epoch: 1580, Loss= 0.6936, Training Accuracy= 0.527\n",
      "Epoch: 1590, Loss= 0.6933, Training Accuracy= 0.533\n",
      "Epoch: 1600, Loss= 0.6932, Training Accuracy= 0.529\n",
      "Epoch: 1610, Loss= 0.6934, Training Accuracy= 0.531\n",
      "Epoch: 1620, Loss= 0.6928, Training Accuracy= 0.531\n",
      "Epoch: 1630, Loss= 0.6932, Training Accuracy= 0.530\n",
      "Epoch: 1640, Loss= 0.6950, Training Accuracy= 0.528\n",
      "Epoch: 1650, Loss= 0.6920, Training Accuracy= 0.534\n",
      "Epoch: 1660, Loss= 0.6924, Training Accuracy= 0.532\n",
      "Epoch: 1670, Loss= 0.6921, Training Accuracy= 0.529\n",
      "Epoch: 1680, Loss= 0.6909, Training Accuracy= 0.535\n",
      "Epoch: 1690, Loss= 0.7106, Training Accuracy= 0.508\n",
      "Epoch: 1700, Loss= 0.7081, Training Accuracy= 0.508\n",
      "Epoch: 1710, Loss= 0.7077, Training Accuracy= 0.508\n",
      "Epoch: 1720, Loss= 0.7076, Training Accuracy= 0.508\n",
      "Epoch: 1730, Loss= 0.7076, Training Accuracy= 0.508\n",
      "Epoch: 1740, Loss= 0.7075, Training Accuracy= 0.508\n",
      "Epoch: 1750, Loss= 0.7075, Training Accuracy= 0.508\n",
      "Epoch: 1760, Loss= 0.7075, Training Accuracy= 0.508\n",
      "Epoch: 1770, Loss= 0.7075, Training Accuracy= 0.508\n",
      "Epoch: 1780, Loss= 0.7075, Training Accuracy= 0.508\n",
      "Epoch: 1790, Loss= 0.7075, Training Accuracy= 0.508\n",
      "Epoch: 1800, Loss= 0.7075, Training Accuracy= 0.508\n",
      "Epoch: 1810, Loss= 0.7074, Training Accuracy= 0.508\n",
      "Epoch: 1820, Loss= 0.7073, Training Accuracy= 0.508\n",
      "Epoch: 1830, Loss= 0.7072, Training Accuracy= 0.508\n",
      "Epoch: 1840, Loss= 0.7071, Training Accuracy= 0.508\n",
      "Epoch: 1850, Loss= 0.7070, Training Accuracy= 0.508\n",
      "Epoch: 1860, Loss= 0.7068, Training Accuracy= 0.508\n",
      "Epoch: 1870, Loss= 0.7066, Training Accuracy= 0.508\n",
      "Epoch: 1880, Loss= 0.7064, Training Accuracy= 0.508\n",
      "Epoch: 1890, Loss= 0.7062, Training Accuracy= 0.508\n",
      "Epoch: 1900, Loss= 0.7060, Training Accuracy= 0.509\n",
      "Epoch: 1910, Loss= 0.7058, Training Accuracy= 0.509\n",
      "Epoch: 1920, Loss= 0.7055, Training Accuracy= 0.509\n",
      "Epoch: 1930, Loss= 0.7052, Training Accuracy= 0.510\n",
      "Epoch: 1940, Loss= 0.7049, Training Accuracy= 0.510\n",
      "Epoch: 1950, Loss= 0.7046, Training Accuracy= 0.510\n",
      "Epoch: 1960, Loss= 0.7043, Training Accuracy= 0.511\n",
      "Epoch: 1970, Loss= 0.7040, Training Accuracy= 0.512\n",
      "Epoch: 1980, Loss= 0.7036, Training Accuracy= 0.513\n",
      "Epoch: 1990, Loss= 0.7033, Training Accuracy= 0.514\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4966\n",
      "Replication: 5: \n",
      "Epoch: 0, Loss= 0.7139, Training Accuracy= 0.496\n",
      "Epoch: 10, Loss= 0.7253, Training Accuracy= 0.496\n",
      "Epoch: 20, Loss= 0.7164, Training Accuracy= 0.496\n",
      "Epoch: 30, Loss= 0.7117, Training Accuracy= 0.495\n",
      "Epoch: 40, Loss= 0.7087, Training Accuracy= 0.495\n",
      "Epoch: 50, Loss= 0.7063, Training Accuracy= 0.494\n",
      "Epoch: 60, Loss= 0.7043, Training Accuracy= 0.494\n",
      "Epoch: 70, Loss= 0.7029, Training Accuracy= 0.495\n",
      "Epoch: 80, Loss= 0.7020, Training Accuracy= 0.497\n",
      "Epoch: 90, Loss= 0.7015, Training Accuracy= 0.498\n",
      "Epoch: 100, Loss= 0.7011, Training Accuracy= 0.498\n",
      "Epoch: 110, Loss= 0.7008, Training Accuracy= 0.500\n",
      "Epoch: 120, Loss= 0.7006, Training Accuracy= 0.502\n",
      "Epoch: 130, Loss= 0.7004, Training Accuracy= 0.505\n",
      "Epoch: 140, Loss= 0.7002, Training Accuracy= 0.507\n",
      "Epoch: 150, Loss= 0.7000, Training Accuracy= 0.507\n",
      "Epoch: 160, Loss= 0.6997, Training Accuracy= 0.508\n",
      "Epoch: 170, Loss= 0.6995, Training Accuracy= 0.509\n",
      "Epoch: 180, Loss= 0.6993, Training Accuracy= 0.510\n",
      "Epoch: 190, Loss= 0.6991, Training Accuracy= 0.511\n",
      "Epoch: 200, Loss= 0.6990, Training Accuracy= 0.512\n",
      "Epoch: 210, Loss= 0.6988, Training Accuracy= 0.513\n",
      "Epoch: 220, Loss= 0.6986, Training Accuracy= 0.515\n",
      "Epoch: 230, Loss= 0.6983, Training Accuracy= 0.516\n",
      "Epoch: 240, Loss= 0.6975, Training Accuracy= 0.517\n",
      "Epoch: 250, Loss= 0.6976, Training Accuracy= 0.521\n",
      "Epoch: 260, Loss= 0.6979, Training Accuracy= 0.522\n",
      "Epoch: 270, Loss= 0.6980, Training Accuracy= 0.523\n",
      "Epoch: 280, Loss= 0.6981, Training Accuracy= 0.522\n",
      "Epoch: 290, Loss= 0.6981, Training Accuracy= 0.525\n",
      "Epoch: 300, Loss= 0.6984, Training Accuracy= 0.527\n",
      "Epoch: 310, Loss= 0.6990, Training Accuracy= 0.529\n",
      "Epoch: 320, Loss= 0.6990, Training Accuracy= 0.530\n",
      "Epoch: 330, Loss= 0.7001, Training Accuracy= 0.535\n",
      "Epoch: 340, Loss= 0.7016, Training Accuracy= 0.538\n",
      "Epoch: 350, Loss= 0.7028, Training Accuracy= 0.538\n",
      "Epoch: 360, Loss= 0.7066, Training Accuracy= 0.532\n",
      "Epoch: 370, Loss= 0.7041, Training Accuracy= 0.536\n",
      "Epoch: 380, Loss= 0.7056, Training Accuracy= 0.537\n",
      "Epoch: 390, Loss= 0.7041, Training Accuracy= 0.538\n",
      "Epoch: 400, Loss= 0.7154, Training Accuracy= 0.512\n",
      "Epoch: 410, Loss= 0.7044, Training Accuracy= 0.531\n",
      "Epoch: 420, Loss= 0.7047, Training Accuracy= 0.530\n",
      "Epoch: 430, Loss= 0.6993, Training Accuracy= 0.539\n",
      "Epoch: 440, Loss= 0.7055, Training Accuracy= 0.536\n",
      "Epoch: 450, Loss= 0.7011, Training Accuracy= 0.540\n",
      "Epoch: 460, Loss= 0.7107, Training Accuracy= 0.535\n",
      "Epoch: 470, Loss= 0.7085, Training Accuracy= 0.536\n",
      "Epoch: 480, Loss= 0.7058, Training Accuracy= 0.538\n",
      "Epoch: 490, Loss= 0.7177, Training Accuracy= 0.538\n",
      "Epoch: 500, Loss= 0.6955, Training Accuracy= 0.552\n",
      "Epoch: 510, Loss= 0.7001, Training Accuracy= 0.541\n",
      "Epoch: 520, Loss= 0.6961, Training Accuracy= 0.552\n",
      "Epoch: 530, Loss= 0.6957, Training Accuracy= 0.549\n",
      "Epoch: 540, Loss= 0.7015, Training Accuracy= 0.548\n",
      "Epoch: 550, Loss= 0.7040, Training Accuracy= 0.551\n",
      "Epoch: 560, Loss= 0.7025, Training Accuracy= 0.544\n",
      "Epoch: 570, Loss= 0.6982, Training Accuracy= 0.550\n",
      "Epoch: 580, Loss= 0.6983, Training Accuracy= 0.525\n",
      "Epoch: 590, Loss= 0.7052, Training Accuracy= 0.537\n",
      "Epoch: 600, Loss= 0.6920, Training Accuracy= 0.553\n",
      "Epoch: 610, Loss= 0.6880, Training Accuracy= 0.557\n",
      "Epoch: 620, Loss= 0.6823, Training Accuracy= 0.568\n",
      "Epoch: 630, Loss= 0.6912, Training Accuracy= 0.557\n",
      "Epoch: 640, Loss= 0.6833, Training Accuracy= 0.567\n",
      "Epoch: 650, Loss= 0.6833, Training Accuracy= 0.565\n",
      "Epoch: 660, Loss= 0.6829, Training Accuracy= 0.570\n",
      "Epoch: 670, Loss= 0.6825, Training Accuracy= 0.569\n",
      "Epoch: 680, Loss= 0.6838, Training Accuracy= 0.567\n",
      "Epoch: 690, Loss= 0.6810, Training Accuracy= 0.573\n",
      "Epoch: 700, Loss= 0.6842, Training Accuracy= 0.565\n",
      "Epoch: 710, Loss= 0.6801, Training Accuracy= 0.571\n",
      "Epoch: 720, Loss= 0.6768, Training Accuracy= 0.574\n",
      "Epoch: 730, Loss= 0.6979, Training Accuracy= 0.541\n",
      "Epoch: 740, Loss= 0.6784, Training Accuracy= 0.573\n",
      "Epoch: 750, Loss= 0.6878, Training Accuracy= 0.564\n",
      "Epoch: 760, Loss= 0.6784, Training Accuracy= 0.578\n",
      "Epoch: 770, Loss= 0.6740, Training Accuracy= 0.586\n",
      "Epoch: 780, Loss= 0.6821, Training Accuracy= 0.571\n",
      "Epoch: 790, Loss= 0.6733, Training Accuracy= 0.579\n",
      "Epoch: 800, Loss= 0.6802, Training Accuracy= 0.573\n",
      "Epoch: 810, Loss= 0.6840, Training Accuracy= 0.573\n",
      "Epoch: 820, Loss= 0.6661, Training Accuracy= 0.590\n",
      "Epoch: 830, Loss= 0.6712, Training Accuracy= 0.587\n",
      "Epoch: 840, Loss= 0.6702, Training Accuracy= 0.587\n",
      "Epoch: 850, Loss= 0.6771, Training Accuracy= 0.573\n",
      "Epoch: 860, Loss= 0.6730, Training Accuracy= 0.584\n",
      "Epoch: 870, Loss= 0.6720, Training Accuracy= 0.582\n",
      "Epoch: 880, Loss= 0.6766, Training Accuracy= 0.577\n",
      "Epoch: 890, Loss= 0.6751, Training Accuracy= 0.576\n",
      "Epoch: 900, Loss= 0.6700, Training Accuracy= 0.584\n",
      "Epoch: 910, Loss= 0.7375, Training Accuracy= 0.514\n",
      "Epoch: 920, Loss= 0.6749, Training Accuracy= 0.580\n",
      "Epoch: 930, Loss= 0.6706, Training Accuracy= 0.582\n",
      "Epoch: 940, Loss= 0.6687, Training Accuracy= 0.590\n",
      "Epoch: 950, Loss= 0.6781, Training Accuracy= 0.573\n",
      "Epoch: 960, Loss= 0.6726, Training Accuracy= 0.580\n",
      "Epoch: 970, Loss= 0.6744, Training Accuracy= 0.578\n",
      "Epoch: 980, Loss= 0.6662, Training Accuracy= 0.591\n",
      "Epoch: 990, Loss= 0.6697, Training Accuracy= 0.586\n",
      "Epoch: 1000, Loss= 0.6702, Training Accuracy= 0.583\n",
      "Epoch: 1010, Loss= 0.6686, Training Accuracy= 0.591\n",
      "Epoch: 1020, Loss= 0.6695, Training Accuracy= 0.583\n",
      "Epoch: 1030, Loss= 0.6650, Training Accuracy= 0.587\n",
      "Epoch: 1040, Loss= 0.6629, Training Accuracy= 0.593\n",
      "Epoch: 1050, Loss= 0.6639, Training Accuracy= 0.594\n",
      "Epoch: 1060, Loss= 0.6658, Training Accuracy= 0.594\n",
      "Epoch: 1070, Loss= 0.6668, Training Accuracy= 0.590\n",
      "Epoch: 1080, Loss= 0.6995, Training Accuracy= 0.498\n",
      "Epoch: 1090, Loss= 0.6965, Training Accuracy= 0.516\n",
      "Epoch: 1100, Loss= 0.6933, Training Accuracy= 0.532\n",
      "Epoch: 1110, Loss= 0.6898, Training Accuracy= 0.538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1120, Loss= 0.6879, Training Accuracy= 0.550\n",
      "Epoch: 1130, Loss= 0.6858, Training Accuracy= 0.551\n",
      "Epoch: 1140, Loss= 0.6820, Training Accuracy= 0.559\n",
      "Epoch: 1150, Loss= 0.6790, Training Accuracy= 0.567\n",
      "Epoch: 1160, Loss= 0.6928, Training Accuracy= 0.551\n",
      "Epoch: 1170, Loss= 0.6750, Training Accuracy= 0.567\n",
      "Epoch: 1180, Loss= 0.6753, Training Accuracy= 0.571\n",
      "Epoch: 1190, Loss= 0.6945, Training Accuracy= 0.551\n",
      "Epoch: 1200, Loss= 0.6728, Training Accuracy= 0.579\n",
      "Epoch: 1210, Loss= 0.6710, Training Accuracy= 0.579\n",
      "Epoch: 1220, Loss= 0.6769, Training Accuracy= 0.568\n",
      "Epoch: 1230, Loss= 0.6744, Training Accuracy= 0.574\n",
      "Epoch: 1240, Loss= 0.6661, Training Accuracy= 0.592\n",
      "Epoch: 1250, Loss= 0.6702, Training Accuracy= 0.584\n",
      "Epoch: 1260, Loss= 0.6779, Training Accuracy= 0.568\n",
      "Epoch: 1270, Loss= 0.6662, Training Accuracy= 0.589\n",
      "Epoch: 1280, Loss= 0.6699, Training Accuracy= 0.583\n",
      "Epoch: 1290, Loss= 0.6822, Training Accuracy= 0.560\n",
      "Epoch: 1300, Loss= 0.6765, Training Accuracy= 0.572\n",
      "Epoch: 1310, Loss= 0.6671, Training Accuracy= 0.592\n",
      "Epoch: 1320, Loss= 0.6848, Training Accuracy= 0.557\n",
      "Epoch: 1330, Loss= 0.7072, Training Accuracy= 0.540\n",
      "Epoch: 1340, Loss= 0.6625, Training Accuracy= 0.598\n",
      "Epoch: 1350, Loss= 0.6589, Training Accuracy= 0.599\n",
      "Epoch: 1360, Loss= 0.6573, Training Accuracy= 0.605\n",
      "Epoch: 1370, Loss= 0.6670, Training Accuracy= 0.594\n",
      "Epoch: 1380, Loss= 0.7015, Training Accuracy= 0.504\n",
      "Epoch: 1390, Loss= 0.6941, Training Accuracy= 0.518\n",
      "Epoch: 1400, Loss= 0.6928, Training Accuracy= 0.523\n",
      "Epoch: 1410, Loss= 0.6910, Training Accuracy= 0.523\n",
      "Epoch: 1420, Loss= 0.6898, Training Accuracy= 0.534\n",
      "Epoch: 1430, Loss= 0.6894, Training Accuracy= 0.538\n",
      "Epoch: 1440, Loss= 0.6927, Training Accuracy= 0.536\n",
      "Epoch: 1450, Loss= 0.6880, Training Accuracy= 0.539\n",
      "Epoch: 1460, Loss= 0.6888, Training Accuracy= 0.543\n",
      "Epoch: 1470, Loss= 0.6871, Training Accuracy= 0.545\n",
      "Epoch: 1480, Loss= 0.6851, Training Accuracy= 0.553\n",
      "Epoch: 1490, Loss= 0.6846, Training Accuracy= 0.554\n",
      "Epoch: 1500, Loss= 0.6869, Training Accuracy= 0.549\n",
      "Epoch: 1510, Loss= 0.6863, Training Accuracy= 0.550\n",
      "Epoch: 1520, Loss= 0.7013, Training Accuracy= 0.508\n",
      "Epoch: 1530, Loss= 0.6873, Training Accuracy= 0.546\n",
      "Epoch: 1540, Loss= 0.6806, Training Accuracy= 0.562\n",
      "Epoch: 1550, Loss= 0.6947, Training Accuracy= 0.521\n",
      "Epoch: 1560, Loss= 0.6846, Training Accuracy= 0.550\n",
      "Epoch: 1570, Loss= 0.6790, Training Accuracy= 0.564\n",
      "Epoch: 1580, Loss= 0.6837, Training Accuracy= 0.560\n",
      "Epoch: 1590, Loss= 0.6827, Training Accuracy= 0.560\n",
      "Epoch: 1600, Loss= 0.6772, Training Accuracy= 0.569\n",
      "Epoch: 1610, Loss= 0.6893, Training Accuracy= 0.543\n",
      "Epoch: 1620, Loss= 0.6850, Training Accuracy= 0.554\n",
      "Epoch: 1630, Loss= 0.6996, Training Accuracy= 0.496\n",
      "Epoch: 1640, Loss= 0.6994, Training Accuracy= 0.496\n",
      "Epoch: 1650, Loss= 0.6985, Training Accuracy= 0.496\n",
      "Epoch: 1660, Loss= 0.6975, Training Accuracy= 0.495\n",
      "Epoch: 1670, Loss= 0.6956, Training Accuracy= 0.510\n",
      "Epoch: 1680, Loss= 0.6936, Training Accuracy= 0.520\n",
      "Epoch: 1690, Loss= 0.6922, Training Accuracy= 0.525\n",
      "Epoch: 1700, Loss= 0.6891, Training Accuracy= 0.537\n",
      "Epoch: 1710, Loss= 0.6875, Training Accuracy= 0.542\n",
      "Epoch: 1720, Loss= 0.6843, Training Accuracy= 0.552\n",
      "Epoch: 1730, Loss= 0.6829, Training Accuracy= 0.558\n",
      "Epoch: 1740, Loss= 0.6834, Training Accuracy= 0.554\n",
      "Epoch: 1750, Loss= 0.6828, Training Accuracy= 0.555\n",
      "Epoch: 1760, Loss= 0.6932, Training Accuracy= 0.537\n",
      "Epoch: 1770, Loss= 0.6900, Training Accuracy= 0.547\n",
      "Epoch: 1780, Loss= 0.6860, Training Accuracy= 0.546\n",
      "Epoch: 1790, Loss= 0.6874, Training Accuracy= 0.535\n",
      "Epoch: 1800, Loss= 0.6814, Training Accuracy= 0.558\n",
      "Epoch: 1810, Loss= 0.7197, Training Accuracy= 0.511\n",
      "Epoch: 1820, Loss= 0.6949, Training Accuracy= 0.516\n",
      "Epoch: 1830, Loss= 0.6926, Training Accuracy= 0.523\n",
      "Epoch: 1840, Loss= 0.6905, Training Accuracy= 0.530\n",
      "Epoch: 1850, Loss= 0.6901, Training Accuracy= 0.530\n",
      "Epoch: 1860, Loss= 0.6988, Training Accuracy= 0.508\n",
      "Epoch: 1870, Loss= 0.6956, Training Accuracy= 0.515\n",
      "Epoch: 1880, Loss= 0.6954, Training Accuracy= 0.516\n",
      "Epoch: 1890, Loss= 0.6946, Training Accuracy= 0.518\n",
      "Epoch: 1900, Loss= 0.6951, Training Accuracy= 0.520\n",
      "Epoch: 1910, Loss= 0.6937, Training Accuracy= 0.523\n",
      "Epoch: 1920, Loss= 0.6988, Training Accuracy= 0.507\n",
      "Epoch: 1930, Loss= 0.6948, Training Accuracy= 0.517\n",
      "Epoch: 1940, Loss= 0.6923, Training Accuracy= 0.528\n",
      "Epoch: 1950, Loss= 0.6919, Training Accuracy= 0.529\n",
      "Epoch: 1960, Loss= 0.6927, Training Accuracy= 0.529\n",
      "Epoch: 1970, Loss= 0.6911, Training Accuracy= 0.533\n",
      "Epoch: 1980, Loss= 0.6925, Training Accuracy= 0.527\n",
      "Epoch: 1990, Loss= 0.6895, Training Accuracy= 0.533\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5037\n",
      "Replication: 6: \n",
      "Epoch: 0, Loss= 0.6987, Training Accuracy= 0.508\n",
      "Epoch: 10, Loss= 0.6985, Training Accuracy= 0.514\n",
      "Epoch: 20, Loss= 0.6974, Training Accuracy= 0.515\n",
      "Epoch: 30, Loss= 0.6964, Training Accuracy= 0.520\n",
      "Epoch: 40, Loss= 0.6957, Training Accuracy= 0.521\n",
      "Epoch: 50, Loss= 0.6953, Training Accuracy= 0.522\n",
      "Epoch: 60, Loss= 0.6950, Training Accuracy= 0.523\n",
      "Epoch: 70, Loss= 0.6948, Training Accuracy= 0.524\n",
      "Epoch: 80, Loss= 0.6948, Training Accuracy= 0.523\n",
      "Epoch: 90, Loss= 0.6949, Training Accuracy= 0.525\n",
      "Epoch: 100, Loss= 0.6949, Training Accuracy= 0.528\n",
      "Epoch: 110, Loss= 0.6949, Training Accuracy= 0.529\n",
      "Epoch: 120, Loss= 0.6949, Training Accuracy= 0.527\n",
      "Epoch: 130, Loss= 0.6945, Training Accuracy= 0.531\n",
      "Epoch: 140, Loss= 0.6938, Training Accuracy= 0.534\n",
      "Epoch: 150, Loss= 0.6936, Training Accuracy= 0.533\n",
      "Epoch: 160, Loss= 0.6958, Training Accuracy= 0.533\n",
      "Epoch: 170, Loss= 0.6982, Training Accuracy= 0.535\n",
      "Epoch: 180, Loss= 0.6952, Training Accuracy= 0.534\n",
      "Epoch: 190, Loss= 0.6951, Training Accuracy= 0.537\n",
      "Epoch: 200, Loss= 0.6947, Training Accuracy= 0.546\n",
      "Epoch: 210, Loss= 0.6940, Training Accuracy= 0.545\n",
      "Epoch: 220, Loss= 0.6936, Training Accuracy= 0.548\n",
      "Epoch: 230, Loss= 0.6923, Training Accuracy= 0.555\n",
      "Epoch: 240, Loss= 0.6922, Training Accuracy= 0.553\n",
      "Epoch: 250, Loss= 0.6898, Training Accuracy= 0.554\n",
      "Epoch: 260, Loss= 0.6910, Training Accuracy= 0.555\n",
      "Epoch: 270, Loss= 0.6925, Training Accuracy= 0.557\n",
      "Epoch: 280, Loss= 0.6923, Training Accuracy= 0.557\n",
      "Epoch: 290, Loss= 0.6892, Training Accuracy= 0.556\n",
      "Epoch: 300, Loss= 0.6930, Training Accuracy= 0.557\n",
      "Epoch: 310, Loss= 0.6962, Training Accuracy= 0.554\n",
      "Epoch: 320, Loss= 0.7062, Training Accuracy= 0.505\n",
      "Epoch: 330, Loss= 0.6960, Training Accuracy= 0.516\n",
      "Epoch: 340, Loss= 0.6917, Training Accuracy= 0.531\n",
      "Epoch: 350, Loss= 0.6875, Training Accuracy= 0.547\n",
      "Epoch: 360, Loss= 0.6851, Training Accuracy= 0.555\n",
      "Epoch: 370, Loss= 0.6832, Training Accuracy= 0.560\n",
      "Epoch: 380, Loss= 0.6821, Training Accuracy= 0.565\n",
      "Epoch: 390, Loss= 0.6796, Training Accuracy= 0.570\n",
      "Epoch: 400, Loss= 0.6811, Training Accuracy= 0.569\n",
      "Epoch: 410, Loss= 0.6819, Training Accuracy= 0.563\n",
      "Epoch: 420, Loss= 0.6806, Training Accuracy= 0.564\n",
      "Epoch: 430, Loss= 0.6801, Training Accuracy= 0.570\n",
      "Epoch: 440, Loss= 0.7056, Training Accuracy= 0.533\n",
      "Epoch: 450, Loss= 0.6824, Training Accuracy= 0.569\n",
      "Epoch: 460, Loss= 0.6988, Training Accuracy= 0.543\n",
      "Epoch: 470, Loss= 0.6830, Training Accuracy= 0.566\n",
      "Epoch: 480, Loss= 0.6743, Training Accuracy= 0.574\n",
      "Epoch: 490, Loss= 0.7011, Training Accuracy= 0.542\n",
      "Epoch: 500, Loss= 0.6698, Training Accuracy= 0.584\n",
      "Epoch: 510, Loss= 0.6789, Training Accuracy= 0.575\n",
      "Epoch: 520, Loss= 0.6876, Training Accuracy= 0.556\n",
      "Epoch: 530, Loss= 0.6704, Training Accuracy= 0.583\n",
      "Epoch: 540, Loss= 0.6761, Training Accuracy= 0.576\n",
      "Epoch: 550, Loss= 0.6853, Training Accuracy= 0.572\n",
      "Epoch: 560, Loss= 0.6688, Training Accuracy= 0.590\n",
      "Epoch: 570, Loss= 0.6650, Training Accuracy= 0.599\n",
      "Epoch: 580, Loss= 0.6718, Training Accuracy= 0.584\n",
      "Epoch: 590, Loss= 0.6690, Training Accuracy= 0.590\n",
      "Epoch: 600, Loss= 0.6680, Training Accuracy= 0.593\n",
      "Epoch: 610, Loss= 0.6646, Training Accuracy= 0.595\n",
      "Epoch: 620, Loss= 0.6733, Training Accuracy= 0.582\n",
      "Epoch: 630, Loss= 0.6738, Training Accuracy= 0.581\n",
      "Epoch: 640, Loss= 0.6724, Training Accuracy= 0.586\n",
      "Epoch: 650, Loss= 0.6742, Training Accuracy= 0.574\n",
      "Epoch: 660, Loss= 0.6737, Training Accuracy= 0.581\n",
      "Epoch: 670, Loss= 0.6736, Training Accuracy= 0.584\n",
      "Epoch: 680, Loss= 0.6627, Training Accuracy= 0.599\n",
      "Epoch: 690, Loss= 0.6843, Training Accuracy= 0.569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 700, Loss= 0.6735, Training Accuracy= 0.583\n",
      "Epoch: 710, Loss= 0.6742, Training Accuracy= 0.577\n",
      "Epoch: 720, Loss= 0.6816, Training Accuracy= 0.570\n",
      "Epoch: 730, Loss= 0.6730, Training Accuracy= 0.582\n",
      "Epoch: 740, Loss= 0.6630, Training Accuracy= 0.597\n",
      "Epoch: 750, Loss= 0.6654, Training Accuracy= 0.594\n",
      "Epoch: 760, Loss= 0.6925, Training Accuracy= 0.556\n",
      "Epoch: 770, Loss= 0.6682, Training Accuracy= 0.589\n",
      "Epoch: 780, Loss= 0.6736, Training Accuracy= 0.579\n",
      "Epoch: 790, Loss= 0.6656, Training Accuracy= 0.592\n",
      "Epoch: 800, Loss= 0.6615, Training Accuracy= 0.599\n",
      "Epoch: 810, Loss= 0.7180, Training Accuracy= 0.516\n",
      "Epoch: 820, Loss= 0.6962, Training Accuracy= 0.505\n",
      "Epoch: 830, Loss= 0.6949, Training Accuracy= 0.500\n",
      "Epoch: 840, Loss= 0.6942, Training Accuracy= 0.507\n",
      "Epoch: 850, Loss= 0.6941, Training Accuracy= 0.505\n",
      "Epoch: 860, Loss= 0.6940, Training Accuracy= 0.507\n",
      "Epoch: 870, Loss= 0.6939, Training Accuracy= 0.507\n",
      "Epoch: 880, Loss= 0.6939, Training Accuracy= 0.507\n",
      "Epoch: 890, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 900, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 910, Loss= 0.6937, Training Accuracy= 0.509\n",
      "Epoch: 920, Loss= 0.6936, Training Accuracy= 0.509\n",
      "Epoch: 930, Loss= 0.6936, Training Accuracy= 0.510\n",
      "Epoch: 940, Loss= 0.6935, Training Accuracy= 0.511\n",
      "Epoch: 950, Loss= 0.6934, Training Accuracy= 0.510\n",
      "Epoch: 960, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 970, Loss= 0.6933, Training Accuracy= 0.509\n",
      "Epoch: 980, Loss= 0.6933, Training Accuracy= 0.511\n",
      "Epoch: 990, Loss= 0.6933, Training Accuracy= 0.512\n",
      "Epoch: 1000, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Epoch: 1010, Loss= 0.6932, Training Accuracy= 0.514\n",
      "Epoch: 1020, Loss= 0.6932, Training Accuracy= 0.515\n",
      "Epoch: 1030, Loss= 0.6932, Training Accuracy= 0.515\n",
      "Epoch: 1040, Loss= 0.6932, Training Accuracy= 0.515\n",
      "Epoch: 1050, Loss= 0.6932, Training Accuracy= 0.515\n",
      "Epoch: 1060, Loss= 0.6932, Training Accuracy= 0.516\n",
      "Epoch: 1070, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 1080, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 1090, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 1100, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 1110, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 1120, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 1130, Loss= 0.6931, Training Accuracy= 0.517\n",
      "Epoch: 1140, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 1150, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 1160, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 1170, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 1180, Loss= 0.6931, Training Accuracy= 0.516\n",
      "Epoch: 1190, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 1200, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 1210, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 1220, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 1230, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 1240, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 1250, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 1260, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 1270, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 1280, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 1290, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 1300, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 1310, Loss= 0.6930, Training Accuracy= 0.516\n",
      "Epoch: 1320, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 1330, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 1340, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 1350, Loss= 0.6930, Training Accuracy= 0.515\n",
      "Epoch: 1360, Loss= 0.6930, Training Accuracy= 0.514\n",
      "Epoch: 1370, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 1380, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 1390, Loss= 0.6929, Training Accuracy= 0.513\n",
      "Epoch: 1400, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 1410, Loss= 0.6929, Training Accuracy= 0.514\n",
      "Epoch: 1420, Loss= 0.6928, Training Accuracy= 0.515\n",
      "Epoch: 1430, Loss= 0.6928, Training Accuracy= 0.516\n",
      "Epoch: 1440, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 1450, Loss= 0.6926, Training Accuracy= 0.518\n",
      "Epoch: 1460, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 1470, Loss= 0.6927, Training Accuracy= 0.518\n",
      "Epoch: 1480, Loss= 0.6925, Training Accuracy= 0.518\n",
      "Epoch: 1490, Loss= 0.6923, Training Accuracy= 0.522\n",
      "Epoch: 1500, Loss= 0.6920, Training Accuracy= 0.522\n",
      "Epoch: 1510, Loss= 0.6918, Training Accuracy= 0.521\n",
      "Epoch: 1520, Loss= 0.6917, Training Accuracy= 0.522\n",
      "Epoch: 1530, Loss= 0.6916, Training Accuracy= 0.523\n",
      "Epoch: 1540, Loss= 0.6915, Training Accuracy= 0.525\n",
      "Epoch: 1550, Loss= 0.6914, Training Accuracy= 0.525\n",
      "Epoch: 1560, Loss= 0.6913, Training Accuracy= 0.526\n",
      "Epoch: 1570, Loss= 0.6912, Training Accuracy= 0.526\n",
      "Epoch: 1580, Loss= 0.6911, Training Accuracy= 0.525\n",
      "Epoch: 1590, Loss= 0.6910, Training Accuracy= 0.526\n",
      "Epoch: 1600, Loss= 0.6908, Training Accuracy= 0.528\n",
      "Epoch: 1610, Loss= 0.6907, Training Accuracy= 0.528\n",
      "Epoch: 1620, Loss= 0.6905, Training Accuracy= 0.528\n",
      "Epoch: 1630, Loss= 0.6903, Training Accuracy= 0.531\n",
      "Epoch: 1640, Loss= 0.6975, Training Accuracy= 0.505\n",
      "Epoch: 1650, Loss= 0.6956, Training Accuracy= 0.506\n",
      "Epoch: 1660, Loss= 0.6962, Training Accuracy= 0.507\n",
      "Epoch: 1670, Loss= 0.6952, Training Accuracy= 0.503\n",
      "Epoch: 1680, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 1690, Loss= 0.6942, Training Accuracy= 0.504\n",
      "Epoch: 1700, Loss= 0.6934, Training Accuracy= 0.511\n",
      "Epoch: 1710, Loss= 0.6983, Training Accuracy= 0.502\n",
      "Epoch: 1720, Loss= 0.6963, Training Accuracy= 0.504\n",
      "Epoch: 1730, Loss= 0.6945, Training Accuracy= 0.508\n",
      "Epoch: 1740, Loss= 0.6954, Training Accuracy= 0.510\n",
      "Epoch: 1750, Loss= 0.6941, Training Accuracy= 0.502\n",
      "Epoch: 1760, Loss= 0.6946, Training Accuracy= 0.508\n",
      "Epoch: 1770, Loss= 0.6947, Training Accuracy= 0.508\n",
      "Epoch: 1780, Loss= 0.6950, Training Accuracy= 0.508\n",
      "Epoch: 1790, Loss= 0.6935, Training Accuracy= 0.511\n",
      "Epoch: 1800, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 1810, Loss= 0.6942, Training Accuracy= 0.505\n",
      "Epoch: 1820, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 1830, Loss= 0.6942, Training Accuracy= 0.506\n",
      "Epoch: 1840, Loss= 0.6949, Training Accuracy= 0.505\n",
      "Epoch: 1850, Loss= 0.6942, Training Accuracy= 0.506\n",
      "Epoch: 1860, Loss= 0.6939, Training Accuracy= 0.506\n",
      "Epoch: 1870, Loss= 0.6945, Training Accuracy= 0.506\n",
      "Epoch: 1880, Loss= 0.6935, Training Accuracy= 0.510\n",
      "Epoch: 1890, Loss= 0.6943, Training Accuracy= 0.506\n",
      "Epoch: 1900, Loss= 0.6940, Training Accuracy= 0.504\n",
      "Epoch: 1910, Loss= 0.6946, Training Accuracy= 0.503\n",
      "Epoch: 1920, Loss= 0.6976, Training Accuracy= 0.504\n",
      "Epoch: 1930, Loss= 0.6957, Training Accuracy= 0.505\n",
      "Epoch: 1940, Loss= 0.6944, Training Accuracy= 0.505\n",
      "Epoch: 1950, Loss= 0.6940, Training Accuracy= 0.507\n",
      "Epoch: 1960, Loss= 0.6941, Training Accuracy= 0.510\n",
      "Epoch: 1970, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 1980, Loss= 0.6940, Training Accuracy= 0.514\n",
      "Epoch: 1990, Loss= 0.6936, Training Accuracy= 0.504\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4927\n",
      "Replication: 7: \n",
      "Epoch: 0, Loss= 0.7980, Training Accuracy= 0.503\n",
      "Epoch: 10, Loss= 0.7117, Training Accuracy= 0.503\n",
      "Epoch: 20, Loss= 0.7035, Training Accuracy= 0.503\n",
      "Epoch: 30, Loss= 0.7001, Training Accuracy= 0.503\n",
      "Epoch: 40, Loss= 0.6983, Training Accuracy= 0.503\n",
      "Epoch: 50, Loss= 0.6973, Training Accuracy= 0.503\n",
      "Epoch: 60, Loss= 0.6966, Training Accuracy= 0.504\n",
      "Epoch: 70, Loss= 0.6962, Training Accuracy= 0.506\n",
      "Epoch: 80, Loss= 0.6958, Training Accuracy= 0.501\n",
      "Epoch: 90, Loss= 0.6956, Training Accuracy= 0.502\n",
      "Epoch: 100, Loss= 0.6954, Training Accuracy= 0.506\n",
      "Epoch: 110, Loss= 0.6952, Training Accuracy= 0.505\n",
      "Epoch: 120, Loss= 0.6950, Training Accuracy= 0.499\n",
      "Epoch: 130, Loss= 0.6949, Training Accuracy= 0.501\n",
      "Epoch: 140, Loss= 0.6948, Training Accuracy= 0.503\n",
      "Epoch: 150, Loss= 0.6947, Training Accuracy= 0.502\n",
      "Epoch: 160, Loss= 0.6947, Training Accuracy= 0.498\n",
      "Epoch: 170, Loss= 0.6946, Training Accuracy= 0.498\n",
      "Epoch: 180, Loss= 0.6945, Training Accuracy= 0.501\n",
      "Epoch: 190, Loss= 0.6945, Training Accuracy= 0.508\n",
      "Epoch: 200, Loss= 0.6944, Training Accuracy= 0.500\n",
      "Epoch: 210, Loss= 0.6944, Training Accuracy= 0.507\n",
      "Epoch: 220, Loss= 0.6943, Training Accuracy= 0.508\n",
      "Epoch: 230, Loss= 0.6943, Training Accuracy= 0.499\n",
      "Epoch: 240, Loss= 0.6943, Training Accuracy= 0.503\n",
      "Epoch: 250, Loss= 0.6942, Training Accuracy= 0.500\n",
      "Epoch: 260, Loss= 0.6942, Training Accuracy= 0.506\n",
      "Epoch: 270, Loss= 0.6942, Training Accuracy= 0.503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 280, Loss= 0.6941, Training Accuracy= 0.503\n",
      "Epoch: 290, Loss= 0.6941, Training Accuracy= 0.497\n",
      "Epoch: 300, Loss= 0.6941, Training Accuracy= 0.506\n",
      "Epoch: 310, Loss= 0.6941, Training Accuracy= 0.504\n",
      "Epoch: 320, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 330, Loss= 0.6940, Training Accuracy= 0.506\n",
      "Epoch: 340, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 350, Loss= 0.6940, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.6940, Training Accuracy= 0.501\n",
      "Epoch: 370, Loss= 0.6939, Training Accuracy= 0.499\n",
      "Epoch: 380, Loss= 0.6939, Training Accuracy= 0.503\n",
      "Epoch: 390, Loss= 0.6939, Training Accuracy= 0.498\n",
      "Epoch: 400, Loss= 0.6939, Training Accuracy= 0.499\n",
      "Epoch: 410, Loss= 0.6939, Training Accuracy= 0.497\n",
      "Epoch: 420, Loss= 0.6938, Training Accuracy= 0.506\n",
      "Epoch: 430, Loss= 0.6938, Training Accuracy= 0.511\n",
      "Epoch: 440, Loss= 0.6938, Training Accuracy= 0.499\n",
      "Epoch: 450, Loss= 0.6938, Training Accuracy= 0.509\n",
      "Epoch: 460, Loss= 0.6938, Training Accuracy= 0.503\n",
      "Epoch: 470, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 480, Loss= 0.6937, Training Accuracy= 0.506\n",
      "Epoch: 490, Loss= 0.6937, Training Accuracy= 0.503\n",
      "Epoch: 500, Loss= 0.6937, Training Accuracy= 0.502\n",
      "Epoch: 510, Loss= 0.6937, Training Accuracy= 0.501\n",
      "Epoch: 520, Loss= 0.6936, Training Accuracy= 0.499\n",
      "Epoch: 530, Loss= 0.6936, Training Accuracy= 0.508\n",
      "Epoch: 540, Loss= 0.6936, Training Accuracy= 0.502\n",
      "Epoch: 550, Loss= 0.6936, Training Accuracy= 0.506\n",
      "Epoch: 560, Loss= 0.6936, Training Accuracy= 0.498\n",
      "Epoch: 570, Loss= 0.6936, Training Accuracy= 0.500\n",
      "Epoch: 580, Loss= 0.6935, Training Accuracy= 0.498\n",
      "Epoch: 590, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 600, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 610, Loss= 0.6935, Training Accuracy= 0.507\n",
      "Epoch: 620, Loss= 0.6935, Training Accuracy= 0.502\n",
      "Epoch: 630, Loss= 0.6935, Training Accuracy= 0.501\n",
      "Epoch: 640, Loss= 0.6935, Training Accuracy= 0.498\n",
      "Epoch: 650, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 660, Loss= 0.6934, Training Accuracy= 0.500\n",
      "Epoch: 670, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 680, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 690, Loss= 0.6934, Training Accuracy= 0.504\n",
      "Epoch: 700, Loss= 0.6934, Training Accuracy= 0.498\n",
      "Epoch: 710, Loss= 0.6934, Training Accuracy= 0.501\n",
      "Epoch: 720, Loss= 0.6934, Training Accuracy= 0.508\n",
      "Epoch: 730, Loss= 0.6934, Training Accuracy= 0.505\n",
      "Epoch: 740, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 750, Loss= 0.6933, Training Accuracy= 0.502\n",
      "Epoch: 760, Loss= 0.6933, Training Accuracy= 0.507\n",
      "Epoch: 770, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 780, Loss= 0.6933, Training Accuracy= 0.504\n",
      "Epoch: 790, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 800, Loss= 0.6933, Training Accuracy= 0.496\n",
      "Epoch: 810, Loss= 0.6933, Training Accuracy= 0.503\n",
      "Epoch: 820, Loss= 0.6933, Training Accuracy= 0.506\n",
      "Epoch: 830, Loss= 0.6932, Training Accuracy= 0.509\n",
      "Epoch: 840, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 850, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 860, Loss= 0.6932, Training Accuracy= 0.505\n",
      "Epoch: 870, Loss= 0.6932, Training Accuracy= 0.503\n",
      "Epoch: 880, Loss= 0.6932, Training Accuracy= 0.499\n",
      "Epoch: 890, Loss= 0.6932, Training Accuracy= 0.504\n",
      "Epoch: 900, Loss= 0.6932, Training Accuracy= 0.502\n",
      "Epoch: 910, Loss= 0.6931, Training Accuracy= 0.500\n",
      "Epoch: 920, Loss= 0.6931, Training Accuracy= 0.500\n",
      "Epoch: 930, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 940, Loss= 0.6931, Training Accuracy= 0.502\n",
      "Epoch: 950, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 960, Loss= 0.6931, Training Accuracy= 0.494\n",
      "Epoch: 970, Loss= 0.6931, Training Accuracy= 0.504\n",
      "Epoch: 980, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 990, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 1000, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 1010, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 1020, Loss= 0.6930, Training Accuracy= 0.504\n",
      "Epoch: 1030, Loss= 0.6930, Training Accuracy= 0.506\n",
      "Epoch: 1040, Loss= 0.6930, Training Accuracy= 0.502\n",
      "Epoch: 1050, Loss= 0.6929, Training Accuracy= 0.512\n",
      "Epoch: 1060, Loss= 0.6929, Training Accuracy= 0.511\n",
      "Epoch: 1070, Loss= 0.6929, Training Accuracy= 0.509\n",
      "Epoch: 1080, Loss= 0.6929, Training Accuracy= 0.508\n",
      "Epoch: 1090, Loss= 0.6929, Training Accuracy= 0.506\n",
      "Epoch: 1100, Loss= 0.6929, Training Accuracy= 0.510\n",
      "Epoch: 1110, Loss= 0.6928, Training Accuracy= 0.506\n",
      "Epoch: 1120, Loss= 0.6928, Training Accuracy= 0.511\n",
      "Epoch: 1130, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 1140, Loss= 0.6928, Training Accuracy= 0.509\n",
      "Epoch: 1150, Loss= 0.6928, Training Accuracy= 0.507\n",
      "Epoch: 1160, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 1170, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 1180, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 1190, Loss= 0.6927, Training Accuracy= 0.508\n",
      "Epoch: 1200, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 1210, Loss= 0.6927, Training Accuracy= 0.506\n",
      "Epoch: 1220, Loss= 0.6927, Training Accuracy= 0.504\n",
      "Epoch: 1230, Loss= 0.6927, Training Accuracy= 0.511\n",
      "Epoch: 1240, Loss= 0.6927, Training Accuracy= 0.507\n",
      "Epoch: 1250, Loss= 0.6927, Training Accuracy= 0.501\n",
      "Epoch: 1260, Loss= 0.6927, Training Accuracy= 0.502\n",
      "Epoch: 1270, Loss= 0.6927, Training Accuracy= 0.513\n",
      "Epoch: 1280, Loss= 0.6927, Training Accuracy= 0.512\n",
      "Epoch: 1290, Loss= 0.6927, Training Accuracy= 0.501\n",
      "Epoch: 1300, Loss= 0.6927, Training Accuracy= 0.502\n",
      "Epoch: 1310, Loss= 0.6926, Training Accuracy= 0.505\n",
      "Epoch: 1320, Loss= 0.6926, Training Accuracy= 0.517\n",
      "Epoch: 1330, Loss= 0.6925, Training Accuracy= 0.506\n",
      "Epoch: 1340, Loss= 0.6925, Training Accuracy= 0.506\n",
      "Epoch: 1350, Loss= 0.6924, Training Accuracy= 0.505\n",
      "Epoch: 1360, Loss= 0.6924, Training Accuracy= 0.509\n",
      "Epoch: 1370, Loss= 0.6923, Training Accuracy= 0.509\n",
      "Epoch: 1380, Loss= 0.6923, Training Accuracy= 0.507\n",
      "Epoch: 1390, Loss= 0.6922, Training Accuracy= 0.506\n",
      "Epoch: 1400, Loss= 0.6922, Training Accuracy= 0.503\n",
      "Epoch: 1410, Loss= 0.6921, Training Accuracy= 0.505\n",
      "Epoch: 1420, Loss= 0.6921, Training Accuracy= 0.508\n",
      "Epoch: 1430, Loss= 0.6920, Training Accuracy= 0.507\n",
      "Epoch: 1440, Loss= 0.6919, Training Accuracy= 0.511\n",
      "Epoch: 1450, Loss= 0.6919, Training Accuracy= 0.508\n",
      "Epoch: 1460, Loss= 0.6918, Training Accuracy= 0.513\n",
      "Epoch: 1470, Loss= 0.6918, Training Accuracy= 0.503\n",
      "Epoch: 1480, Loss= 0.6917, Training Accuracy= 0.509\n",
      "Epoch: 1490, Loss= 0.6916, Training Accuracy= 0.505\n",
      "Epoch: 1500, Loss= 0.6915, Training Accuracy= 0.515\n",
      "Epoch: 1510, Loss= 0.6915, Training Accuracy= 0.511\n",
      "Epoch: 1520, Loss= 0.6914, Training Accuracy= 0.510\n",
      "Epoch: 1530, Loss= 0.6913, Training Accuracy= 0.515\n",
      "Epoch: 1540, Loss= 0.6912, Training Accuracy= 0.510\n",
      "Epoch: 1550, Loss= 0.6910, Training Accuracy= 0.517\n",
      "Epoch: 1560, Loss= 0.6909, Training Accuracy= 0.512\n",
      "Epoch: 1570, Loss= 0.6908, Training Accuracy= 0.519\n",
      "Epoch: 1580, Loss= 0.6906, Training Accuracy= 0.520\n",
      "Epoch: 1590, Loss= 0.6904, Training Accuracy= 0.522\n",
      "Epoch: 1600, Loss= 0.6903, Training Accuracy= 0.511\n",
      "Epoch: 1610, Loss= 0.6901, Training Accuracy= 0.519\n",
      "Epoch: 1620, Loss= 0.6899, Training Accuracy= 0.514\n",
      "Epoch: 1630, Loss= 0.6897, Training Accuracy= 0.518\n",
      "Epoch: 1640, Loss= 0.6895, Training Accuracy= 0.519\n",
      "Epoch: 1650, Loss= 0.6892, Training Accuracy= 0.515\n",
      "Epoch: 1660, Loss= 0.6890, Training Accuracy= 0.520\n",
      "Epoch: 1670, Loss= 0.6888, Training Accuracy= 0.518\n",
      "Epoch: 1680, Loss= 0.6885, Training Accuracy= 0.519\n",
      "Epoch: 1690, Loss= 0.6883, Training Accuracy= 0.522\n",
      "Epoch: 1700, Loss= 0.6881, Training Accuracy= 0.520\n",
      "Epoch: 1710, Loss= 0.6878, Training Accuracy= 0.524\n",
      "Epoch: 1720, Loss= 0.6876, Training Accuracy= 0.529\n",
      "Epoch: 1730, Loss= 0.6874, Training Accuracy= 0.527\n",
      "Epoch: 1740, Loss= 0.6871, Training Accuracy= 0.523\n",
      "Epoch: 1750, Loss= 0.6869, Training Accuracy= 0.528\n",
      "Epoch: 1760, Loss= 0.6867, Training Accuracy= 0.530\n",
      "Epoch: 1770, Loss= 0.6866, Training Accuracy= 0.526\n",
      "Epoch: 1780, Loss= 0.6864, Training Accuracy= 0.525\n",
      "Epoch: 1790, Loss= 0.6862, Training Accuracy= 0.528\n",
      "Epoch: 1800, Loss= 0.6861, Training Accuracy= 0.528\n",
      "Epoch: 1810, Loss= 0.6859, Training Accuracy= 0.533\n",
      "Epoch: 1820, Loss= 0.6857, Training Accuracy= 0.535\n",
      "Epoch: 1830, Loss= 0.6856, Training Accuracy= 0.534\n",
      "Epoch: 1840, Loss= 0.6854, Training Accuracy= 0.531\n",
      "Epoch: 1850, Loss= 0.6853, Training Accuracy= 0.536\n",
      "Epoch: 1860, Loss= 0.6851, Training Accuracy= 0.536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1870, Loss= 0.6855, Training Accuracy= 0.537\n",
      "Epoch: 1880, Loss= 0.6849, Training Accuracy= 0.536\n",
      "Epoch: 1890, Loss= 0.6853, Training Accuracy= 0.540\n",
      "Epoch: 1900, Loss= 0.6860, Training Accuracy= 0.541\n",
      "Epoch: 1910, Loss= 0.6854, Training Accuracy= 0.537\n",
      "Epoch: 1920, Loss= 0.6876, Training Accuracy= 0.535\n",
      "Epoch: 1930, Loss= 0.6849, Training Accuracy= 0.541\n",
      "Epoch: 1940, Loss= 0.6844, Training Accuracy= 0.537\n",
      "Epoch: 1950, Loss= 0.6860, Training Accuracy= 0.549\n",
      "Epoch: 1960, Loss= 0.6839, Training Accuracy= 0.546\n",
      "Epoch: 1970, Loss= 0.6893, Training Accuracy= 0.530\n",
      "Epoch: 1980, Loss= 0.6833, Training Accuracy= 0.550\n",
      "Epoch: 1990, Loss= 0.6886, Training Accuracy= 0.532\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5001\n",
      "Replication: 8: \n",
      "Epoch: 0, Loss= 0.7572, Training Accuracy= 0.506\n",
      "Epoch: 10, Loss= 0.7005, Training Accuracy= 0.507\n",
      "Epoch: 20, Loss= 0.6958, Training Accuracy= 0.509\n",
      "Epoch: 30, Loss= 0.6944, Training Accuracy= 0.513\n",
      "Epoch: 40, Loss= 0.6937, Training Accuracy= 0.516\n",
      "Epoch: 50, Loss= 0.6932, Training Accuracy= 0.517\n",
      "Epoch: 60, Loss= 0.6929, Training Accuracy= 0.518\n",
      "Epoch: 70, Loss= 0.6925, Training Accuracy= 0.520\n",
      "Epoch: 80, Loss= 0.6919, Training Accuracy= 0.525\n",
      "Epoch: 90, Loss= 0.6912, Training Accuracy= 0.530\n",
      "Epoch: 100, Loss= 0.6907, Training Accuracy= 0.535\n",
      "Epoch: 110, Loss= 0.6902, Training Accuracy= 0.535\n",
      "Epoch: 120, Loss= 0.6899, Training Accuracy= 0.528\n",
      "Epoch: 130, Loss= 0.6896, Training Accuracy= 0.531\n",
      "Epoch: 140, Loss= 0.6893, Training Accuracy= 0.533\n",
      "Epoch: 150, Loss= 0.6889, Training Accuracy= 0.531\n",
      "Epoch: 160, Loss= 0.6887, Training Accuracy= 0.533\n",
      "Epoch: 170, Loss= 0.6885, Training Accuracy= 0.534\n",
      "Epoch: 180, Loss= 0.6884, Training Accuracy= 0.540\n",
      "Epoch: 190, Loss= 0.6890, Training Accuracy= 0.542\n",
      "Epoch: 200, Loss= 0.6894, Training Accuracy= 0.540\n",
      "Epoch: 210, Loss= 0.6897, Training Accuracy= 0.538\n",
      "Epoch: 220, Loss= 0.6890, Training Accuracy= 0.540\n",
      "Epoch: 230, Loss= 0.6896, Training Accuracy= 0.542\n",
      "Epoch: 240, Loss= 0.6882, Training Accuracy= 0.545\n",
      "Epoch: 250, Loss= 0.6906, Training Accuracy= 0.542\n",
      "Epoch: 260, Loss= 0.7202, Training Accuracy= 0.509\n",
      "Epoch: 270, Loss= 0.7241, Training Accuracy= 0.506\n",
      "Epoch: 280, Loss= 0.7112, Training Accuracy= 0.506\n",
      "Epoch: 290, Loss= 0.7063, Training Accuracy= 0.506\n",
      "Epoch: 300, Loss= 0.7035, Training Accuracy= 0.506\n",
      "Epoch: 310, Loss= 0.7015, Training Accuracy= 0.506\n",
      "Epoch: 320, Loss= 0.7000, Training Accuracy= 0.507\n",
      "Epoch: 330, Loss= 0.6988, Training Accuracy= 0.508\n",
      "Epoch: 340, Loss= 0.6978, Training Accuracy= 0.512\n",
      "Epoch: 350, Loss= 0.6969, Training Accuracy= 0.513\n",
      "Epoch: 360, Loss= 0.6961, Training Accuracy= 0.515\n",
      "Epoch: 370, Loss= 0.6954, Training Accuracy= 0.518\n",
      "Epoch: 380, Loss= 0.6948, Training Accuracy= 0.520\n",
      "Epoch: 390, Loss= 0.6942, Training Accuracy= 0.523\n",
      "Epoch: 400, Loss= 0.6936, Training Accuracy= 0.523\n",
      "Epoch: 410, Loss= 0.6930, Training Accuracy= 0.525\n",
      "Epoch: 420, Loss= 0.6926, Training Accuracy= 0.525\n",
      "Epoch: 430, Loss= 0.6921, Training Accuracy= 0.524\n",
      "Epoch: 440, Loss= 0.6917, Training Accuracy= 0.523\n",
      "Epoch: 450, Loss= 0.6912, Training Accuracy= 0.523\n",
      "Epoch: 460, Loss= 0.6908, Training Accuracy= 0.524\n",
      "Epoch: 470, Loss= 0.6904, Training Accuracy= 0.529\n",
      "Epoch: 480, Loss= 0.6900, Training Accuracy= 0.529\n",
      "Epoch: 490, Loss= 0.6897, Training Accuracy= 0.531\n",
      "Epoch: 500, Loss= 0.6894, Training Accuracy= 0.533\n",
      "Epoch: 510, Loss= 0.6890, Training Accuracy= 0.534\n",
      "Epoch: 520, Loss= 0.6887, Training Accuracy= 0.535\n",
      "Epoch: 530, Loss= 0.6884, Training Accuracy= 0.536\n",
      "Epoch: 540, Loss= 0.6880, Training Accuracy= 0.538\n",
      "Epoch: 550, Loss= 0.6877, Training Accuracy= 0.541\n",
      "Epoch: 560, Loss= 0.6873, Training Accuracy= 0.544\n",
      "Epoch: 570, Loss= 0.6868, Training Accuracy= 0.546\n",
      "Epoch: 580, Loss= 0.6863, Training Accuracy= 0.547\n",
      "Epoch: 590, Loss= 0.6857, Training Accuracy= 0.547\n",
      "Epoch: 600, Loss= 0.6851, Training Accuracy= 0.549\n",
      "Epoch: 610, Loss= 0.6844, Training Accuracy= 0.551\n",
      "Epoch: 620, Loss= 0.6838, Training Accuracy= 0.556\n",
      "Epoch: 630, Loss= 0.6831, Training Accuracy= 0.558\n",
      "Epoch: 640, Loss= 0.6824, Training Accuracy= 0.558\n",
      "Epoch: 650, Loss= 0.6819, Training Accuracy= 0.562\n",
      "Epoch: 660, Loss= 0.6806, Training Accuracy= 0.563\n",
      "Epoch: 670, Loss= 0.6783, Training Accuracy= 0.563\n",
      "Epoch: 680, Loss= 0.6779, Training Accuracy= 0.565\n",
      "Epoch: 690, Loss= 0.6793, Training Accuracy= 0.564\n",
      "Epoch: 700, Loss= 0.6739, Training Accuracy= 0.572\n",
      "Epoch: 710, Loss= 0.6762, Training Accuracy= 0.568\n",
      "Epoch: 720, Loss= 0.6732, Training Accuracy= 0.576\n",
      "Epoch: 730, Loss= 0.6795, Training Accuracy= 0.571\n",
      "Epoch: 740, Loss= 0.6729, Training Accuracy= 0.577\n",
      "Epoch: 750, Loss= 0.6747, Training Accuracy= 0.575\n",
      "Epoch: 760, Loss= 0.6708, Training Accuracy= 0.580\n",
      "Epoch: 770, Loss= 0.6813, Training Accuracy= 0.570\n",
      "Epoch: 780, Loss= 0.6690, Training Accuracy= 0.585\n",
      "Epoch: 790, Loss= 0.6687, Training Accuracy= 0.589\n",
      "Epoch: 800, Loss= 0.6689, Training Accuracy= 0.585\n",
      "Epoch: 810, Loss= 0.6765, Training Accuracy= 0.571\n",
      "Epoch: 820, Loss= 0.6671, Training Accuracy= 0.592\n",
      "Epoch: 830, Loss= 0.6699, Training Accuracy= 0.586\n",
      "Epoch: 840, Loss= 0.6694, Training Accuracy= 0.589\n",
      "Epoch: 850, Loss= 0.6692, Training Accuracy= 0.593\n",
      "Epoch: 860, Loss= 0.6669, Training Accuracy= 0.595\n",
      "Epoch: 870, Loss= 0.6682, Training Accuracy= 0.588\n",
      "Epoch: 880, Loss= 0.6739, Training Accuracy= 0.582\n",
      "Epoch: 890, Loss= 0.6679, Training Accuracy= 0.586\n",
      "Epoch: 900, Loss= 0.6691, Training Accuracy= 0.587\n",
      "Epoch: 910, Loss= 0.6754, Training Accuracy= 0.577\n",
      "Epoch: 920, Loss= 0.6725, Training Accuracy= 0.585\n",
      "Epoch: 930, Loss= 0.6669, Training Accuracy= 0.588\n",
      "Epoch: 940, Loss= 0.6681, Training Accuracy= 0.587\n",
      "Epoch: 950, Loss= 0.6647, Training Accuracy= 0.594\n",
      "Epoch: 960, Loss= 0.6749, Training Accuracy= 0.581\n",
      "Epoch: 970, Loss= 0.6764, Training Accuracy= 0.575\n",
      "Epoch: 980, Loss= 0.6651, Training Accuracy= 0.591\n",
      "Epoch: 990, Loss= 0.6683, Training Accuracy= 0.587\n",
      "Epoch: 1000, Loss= 0.6703, Training Accuracy= 0.585\n",
      "Epoch: 1010, Loss= 0.6791, Training Accuracy= 0.579\n",
      "Epoch: 1020, Loss= 0.6633, Training Accuracy= 0.595\n",
      "Epoch: 1030, Loss= 0.6672, Training Accuracy= 0.593\n",
      "Epoch: 1040, Loss= 0.6632, Training Accuracy= 0.593\n",
      "Epoch: 1050, Loss= 0.6631, Training Accuracy= 0.597\n",
      "Epoch: 1060, Loss= 0.6721, Training Accuracy= 0.581\n",
      "Epoch: 1070, Loss= 0.6953, Training Accuracy= 0.505\n",
      "Epoch: 1080, Loss= 0.6926, Training Accuracy= 0.516\n",
      "Epoch: 1090, Loss= 0.6921, Training Accuracy= 0.518\n",
      "Epoch: 1100, Loss= 0.6918, Training Accuracy= 0.522\n",
      "Epoch: 1110, Loss= 0.6916, Training Accuracy= 0.519\n",
      "Epoch: 1120, Loss= 0.6914, Training Accuracy= 0.521\n",
      "Epoch: 1130, Loss= 0.6915, Training Accuracy= 0.521\n",
      "Epoch: 1140, Loss= 0.6909, Training Accuracy= 0.524\n",
      "Epoch: 1150, Loss= 0.6911, Training Accuracy= 0.523\n",
      "Epoch: 1160, Loss= 0.6906, Training Accuracy= 0.519\n",
      "Epoch: 1170, Loss= 0.6904, Training Accuracy= 0.521\n",
      "Epoch: 1180, Loss= 0.6905, Training Accuracy= 0.520\n",
      "Epoch: 1190, Loss= 0.6916, Training Accuracy= 0.521\n",
      "Epoch: 1200, Loss= 0.6907, Training Accuracy= 0.526\n",
      "Epoch: 1210, Loss= 0.6909, Training Accuracy= 0.525\n",
      "Epoch: 1220, Loss= 0.6915, Training Accuracy= 0.522\n",
      "Epoch: 1230, Loss= 0.6896, Training Accuracy= 0.526\n",
      "Epoch: 1240, Loss= 0.6901, Training Accuracy= 0.528\n",
      "Epoch: 1250, Loss= 0.6903, Training Accuracy= 0.528\n",
      "Epoch: 1260, Loss= 0.6897, Training Accuracy= 0.535\n",
      "Epoch: 1270, Loss= 0.6899, Training Accuracy= 0.531\n",
      "Epoch: 1280, Loss= 0.6885, Training Accuracy= 0.532\n",
      "Epoch: 1290, Loss= 0.6877, Training Accuracy= 0.540\n",
      "Epoch: 1300, Loss= 0.6882, Training Accuracy= 0.536\n",
      "Epoch: 1310, Loss= 0.6871, Training Accuracy= 0.536\n",
      "Epoch: 1320, Loss= 0.6880, Training Accuracy= 0.534\n",
      "Epoch: 1330, Loss= 0.6868, Training Accuracy= 0.541\n",
      "Epoch: 1340, Loss= 0.6966, Training Accuracy= 0.504\n",
      "Epoch: 1350, Loss= 0.6944, Training Accuracy= 0.506\n",
      "Epoch: 1360, Loss= 0.6942, Training Accuracy= 0.506\n",
      "Epoch: 1370, Loss= 0.6940, Training Accuracy= 0.506\n",
      "Epoch: 1380, Loss= 0.6939, Training Accuracy= 0.506\n",
      "Epoch: 1390, Loss= 0.6938, Training Accuracy= 0.507\n",
      "Epoch: 1400, Loss= 0.6936, Training Accuracy= 0.505\n",
      "Epoch: 1410, Loss= 0.6934, Training Accuracy= 0.507\n",
      "Epoch: 1420, Loss= 0.6931, Training Accuracy= 0.509\n",
      "Epoch: 1430, Loss= 0.6928, Training Accuracy= 0.512\n",
      "Epoch: 1440, Loss= 0.6926, Training Accuracy= 0.515\n",
      "Epoch: 1450, Loss= 0.6925, Training Accuracy= 0.514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1460, Loss= 0.6923, Training Accuracy= 0.517\n",
      "Epoch: 1470, Loss= 0.6922, Training Accuracy= 0.520\n",
      "Epoch: 1480, Loss= 0.6921, Training Accuracy= 0.521\n",
      "Epoch: 1490, Loss= 0.6920, Training Accuracy= 0.518\n",
      "Epoch: 1500, Loss= 0.6919, Training Accuracy= 0.520\n",
      "Epoch: 1510, Loss= 0.6918, Training Accuracy= 0.521\n",
      "Epoch: 1520, Loss= 0.6917, Training Accuracy= 0.522\n",
      "Epoch: 1530, Loss= 0.6915, Training Accuracy= 0.521\n",
      "Epoch: 1540, Loss= 0.6914, Training Accuracy= 0.519\n",
      "Epoch: 1550, Loss= 0.6913, Training Accuracy= 0.521\n",
      "Epoch: 1560, Loss= 0.6912, Training Accuracy= 0.521\n",
      "Epoch: 1570, Loss= 0.6911, Training Accuracy= 0.522\n",
      "Epoch: 1580, Loss= 0.6910, Training Accuracy= 0.520\n",
      "Epoch: 1590, Loss= 0.6910, Training Accuracy= 0.522\n",
      "Epoch: 1600, Loss= 0.6909, Training Accuracy= 0.524\n",
      "Epoch: 1610, Loss= 0.6908, Training Accuracy= 0.525\n",
      "Epoch: 1620, Loss= 0.6907, Training Accuracy= 0.526\n",
      "Epoch: 1630, Loss= 0.6906, Training Accuracy= 0.527\n",
      "Epoch: 1640, Loss= 0.6906, Training Accuracy= 0.527\n",
      "Epoch: 1650, Loss= 0.6905, Training Accuracy= 0.528\n",
      "Epoch: 1660, Loss= 0.6905, Training Accuracy= 0.527\n",
      "Epoch: 1670, Loss= 0.6904, Training Accuracy= 0.527\n",
      "Epoch: 1680, Loss= 0.6904, Training Accuracy= 0.529\n",
      "Epoch: 1690, Loss= 0.6903, Training Accuracy= 0.528\n",
      "Epoch: 1700, Loss= 0.6902, Training Accuracy= 0.529\n",
      "Epoch: 1710, Loss= 0.6901, Training Accuracy= 0.530\n",
      "Epoch: 1720, Loss= 0.6900, Training Accuracy= 0.530\n",
      "Epoch: 1730, Loss= 0.6898, Training Accuracy= 0.531\n",
      "Epoch: 1740, Loss= 0.6897, Training Accuracy= 0.530\n",
      "Epoch: 1750, Loss= 0.6895, Training Accuracy= 0.531\n",
      "Epoch: 1760, Loss= 0.6894, Training Accuracy= 0.533\n",
      "Epoch: 1770, Loss= 0.6893, Training Accuracy= 0.533\n",
      "Epoch: 1780, Loss= 0.6892, Training Accuracy= 0.535\n",
      "Epoch: 1790, Loss= 0.6891, Training Accuracy= 0.536\n",
      "Epoch: 1800, Loss= 0.6890, Training Accuracy= 0.538\n",
      "Epoch: 1810, Loss= 0.6888, Training Accuracy= 0.539\n",
      "Epoch: 1820, Loss= 0.6886, Training Accuracy= 0.539\n",
      "Epoch: 1830, Loss= 0.6882, Training Accuracy= 0.540\n",
      "Epoch: 1840, Loss= 0.6879, Training Accuracy= 0.544\n",
      "Epoch: 1850, Loss= 0.6878, Training Accuracy= 0.546\n",
      "Epoch: 1860, Loss= 0.6877, Training Accuracy= 0.544\n",
      "Epoch: 1870, Loss= 0.6876, Training Accuracy= 0.546\n",
      "Epoch: 1880, Loss= 0.6875, Training Accuracy= 0.545\n",
      "Epoch: 1890, Loss= 0.6873, Training Accuracy= 0.545\n",
      "Epoch: 1900, Loss= 0.6872, Training Accuracy= 0.545\n",
      "Epoch: 1910, Loss= 0.6873, Training Accuracy= 0.542\n",
      "Epoch: 1920, Loss= 0.6872, Training Accuracy= 0.541\n",
      "Epoch: 1930, Loss= 0.6880, Training Accuracy= 0.544\n",
      "Epoch: 1940, Loss= 0.6879, Training Accuracy= 0.542\n",
      "Epoch: 1950, Loss= 0.6880, Training Accuracy= 0.545\n",
      "Epoch: 1960, Loss= 0.6875, Training Accuracy= 0.545\n",
      "Epoch: 1970, Loss= 0.6871, Training Accuracy= 0.542\n",
      "Epoch: 1980, Loss= 0.6935, Training Accuracy= 0.506\n",
      "Epoch: 1990, Loss= 0.6932, Training Accuracy= 0.513\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5007\n",
      "Replication: 9: \n",
      "Epoch: 0, Loss= 0.7145, Training Accuracy= 0.504\n",
      "Epoch: 10, Loss= 0.7028, Training Accuracy= 0.504\n",
      "Epoch: 20, Loss= 0.6991, Training Accuracy= 0.504\n",
      "Epoch: 30, Loss= 0.6976, Training Accuracy= 0.508\n",
      "Epoch: 40, Loss= 0.6964, Training Accuracy= 0.511\n",
      "Epoch: 50, Loss= 0.6954, Training Accuracy= 0.511\n",
      "Epoch: 60, Loss= 0.6937, Training Accuracy= 0.520\n",
      "Epoch: 70, Loss= 0.6933, Training Accuracy= 0.522\n",
      "Epoch: 80, Loss= 0.6928, Training Accuracy= 0.521\n",
      "Epoch: 90, Loss= 0.6920, Training Accuracy= 0.523\n",
      "Epoch: 100, Loss= 0.6913, Training Accuracy= 0.532\n",
      "Epoch: 110, Loss= 0.6910, Training Accuracy= 0.533\n",
      "Epoch: 120, Loss= 0.6913, Training Accuracy= 0.533\n",
      "Epoch: 130, Loss= 0.6924, Training Accuracy= 0.529\n",
      "Epoch: 140, Loss= 0.6923, Training Accuracy= 0.534\n",
      "Epoch: 150, Loss= 0.6925, Training Accuracy= 0.530\n",
      "Epoch: 160, Loss= 0.6927, Training Accuracy= 0.530\n",
      "Epoch: 170, Loss= 0.6931, Training Accuracy= 0.526\n",
      "Epoch: 180, Loss= 0.6909, Training Accuracy= 0.531\n",
      "Epoch: 190, Loss= 0.6898, Training Accuracy= 0.535\n",
      "Epoch: 200, Loss= 0.6898, Training Accuracy= 0.540\n",
      "Epoch: 210, Loss= 0.6903, Training Accuracy= 0.535\n",
      "Epoch: 220, Loss= 0.6881, Training Accuracy= 0.542\n",
      "Epoch: 230, Loss= 0.7034, Training Accuracy= 0.509\n",
      "Epoch: 240, Loss= 0.6859, Training Accuracy= 0.551\n",
      "Epoch: 250, Loss= 0.6867, Training Accuracy= 0.546\n",
      "Epoch: 260, Loss= 0.6918, Training Accuracy= 0.535\n",
      "Epoch: 270, Loss= 0.6879, Training Accuracy= 0.545\n",
      "Epoch: 280, Loss= 0.6880, Training Accuracy= 0.549\n",
      "Epoch: 290, Loss= 0.6911, Training Accuracy= 0.544\n",
      "Epoch: 300, Loss= 0.6928, Training Accuracy= 0.542\n",
      "Epoch: 310, Loss= 0.6937, Training Accuracy= 0.535\n",
      "Epoch: 320, Loss= 0.6848, Training Accuracy= 0.559\n",
      "Epoch: 330, Loss= 0.7055, Training Accuracy= 0.499\n",
      "Epoch: 340, Loss= 0.7039, Training Accuracy= 0.499\n",
      "Epoch: 350, Loss= 0.7025, Training Accuracy= 0.503\n",
      "Epoch: 360, Loss= 0.7044, Training Accuracy= 0.502\n",
      "Epoch: 370, Loss= 0.7040, Training Accuracy= 0.504\n",
      "Epoch: 380, Loss= 0.7035, Training Accuracy= 0.505\n",
      "Epoch: 390, Loss= 0.7027, Training Accuracy= 0.504\n",
      "Epoch: 400, Loss= 0.7019, Training Accuracy= 0.504\n",
      "Epoch: 410, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 420, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 430, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 440, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 450, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 460, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 470, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 480, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 490, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 500, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 510, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 520, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 530, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 540, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 550, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 560, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 570, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 580, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 590, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 600, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 610, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 620, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 630, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 640, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 650, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 660, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 670, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 680, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 690, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 700, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 710, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 720, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 730, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 740, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 750, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 760, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 770, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 780, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 790, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 800, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 810, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 820, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 830, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 840, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 850, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 860, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 870, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 880, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 890, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 900, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 910, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 920, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 930, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 940, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 950, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 960, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 970, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 980, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 990, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1000, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1010, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1020, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1030, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1040, Loss= 0.6999, Training Accuracy= 0.504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1050, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1060, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1070, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1080, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1090, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1100, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1110, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1120, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1130, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1140, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1150, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1160, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1170, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1180, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1190, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1200, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1210, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1220, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1230, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1240, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1250, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1260, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1270, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1280, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1290, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1300, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1310, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1320, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1330, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1340, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1350, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1360, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1370, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1380, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1390, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1400, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1410, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1420, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1430, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1440, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1450, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1460, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1470, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1480, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1490, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1500, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1510, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1520, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1530, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1540, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1550, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1560, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1570, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1580, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1590, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1600, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1610, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1620, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1630, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1640, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1650, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1660, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1670, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1680, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1690, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1700, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1710, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1720, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1730, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1740, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1750, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1760, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1770, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1780, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1790, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1800, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1810, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1820, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1830, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1840, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1850, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1860, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1870, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1880, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1890, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1900, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1910, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1920, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1930, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1940, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1950, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1960, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1970, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1980, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Epoch: 1990, Loss= 0.6999, Training Accuracy= 0.504\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.03\n",
    "batch_size = 128\n",
    "display_step = batch_size * 100\n",
    "\n",
    "#batch_steps = 10000 / batch_size\n",
    "epochs = 2000\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1 # \n",
    "timesteps = N # timesteps\n",
    "num_hidden = H # hidden layer num of features\n",
    "num_classes = 2 # 0 or 1\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a RNN cell with tensorflow\n",
    "    rnn_cell = rnn.BasicRNNCell(num_hidden)\n",
    "\n",
    "    # Get RNN cell output\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = RNN(X, weights, biases)\n",
    "#prediction = tf.nn.softmax(logits)\n",
    "prediction = tf.tanh(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "test_accuracies_10replications = []\n",
    "losses_1st_replication = [] #epoch as unit\n",
    "test_accuracies_1st_replication = [] #epoch as unit\n",
    "train_accuracies_1st_replication = [] #epoch as unit\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Run 10 replications\n",
    "    for replication in range(10):\n",
    "        \n",
    "        print(\"Replication: %d: \" % replication)\n",
    "        \n",
    "        # Initialize random weights\n",
    "        train_data = generate_parity_sequences(N, 10000)\n",
    "        train_data_x = train_data[0]\n",
    "        train_data_y = train_data[1]\n",
    "        test_data = generate_parity_sequences(N, 10000)\n",
    "        test_data_x = test_data[0]\n",
    "        test_data_y = test_data[1]\n",
    "        \n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "    \n",
    "        for epoch in range(epochs):\n",
    "            batch_index = 0\n",
    "            while batch_index < 10000:\n",
    "\n",
    "                train_data_batch_x = []\n",
    "                train_data_batch_y = []\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_x[batch_index : batch_index + batch_size]\n",
    "                    train_data_batch_y = train_data_y[batch_index : batch_index + batch_size]\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_x[batch_index : ]\n",
    "                    train_data_batch_y = train_data_y[batch_index : ]\n",
    "\n",
    "                #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "                # Reshape data to get 28 seq of 28 elements\n",
    "                #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                #train_data_x = train_data_x.reshape((10000, timesteps, num_input))\n",
    "                #print(\"train_data_batch_x.shape:  \" , train_data_batch_x.shape)\n",
    "                if batch_index + batch_size < 10000: \n",
    "                    train_data_batch_x = train_data_batch_x.reshape((batch_size, timesteps, num_input))\n",
    "                else:\n",
    "                    train_data_batch_x = train_data_batch_x.reshape((10000 % batch_size, timesteps, num_input))\n",
    "                # Run optimization op (backprop)\n",
    "                #sess.run(train_op, feed_dict={X: train_data_x, \n",
    "                 #                             Y: train_data_y})\n",
    "                sess.run(train_op, feed_dict={X: train_data_batch_x, \n",
    "                                              Y: train_data_batch_y})\n",
    "\n",
    "                batch_index += batch_size\n",
    "\n",
    "            if replication == 0:\n",
    "                loss, train_accuracy = sess.run([loss_op, accuracy], feed_dict={X: train_data_x, Y: train_data_y})\n",
    "                test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "                losses_1st_replication.append(loss)\n",
    "                train_accuracies_1st_replication.append(train_accuracy)\n",
    "                test_accuracies_1st_replication.append(test_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: train_data_x,\n",
    "                                                                         Y: train_data_y})\n",
    "                print(\"Epoch: \" + str(epoch) + \\\n",
    "                          \", Loss= \" + \\\n",
    "                          \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                          \"{:.3f}\".format(acc))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        test_data_x = test_data_x.reshape((-1, timesteps, num_input))\n",
    "        test_data_y = test_data_y\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={X: test_data_x, Y: test_data_y})\n",
    "        test_accuracies_10replications.append(test_accuracy)\n",
    "        print(\"Testing Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accuracies_10replications:  [0.4991, 0.49160001, 0.49759999, 0.49489999, 0.4966, 0.50370002, 0.49270001, 0.50010002, 0.5007, 0.5]\n",
      "mean of test_accuracies_10replications:  0.4977\n",
      "standard deviation of test_accuracies_10replications_std_mean:  3.59555473551e-05\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "test_accuracies_10replications_std = np.std(test_accuracies_10replications, axis=0)\n",
    "test_accuracies_10replications_std_mean = test_accuracies_10replications_std / np.square(10)\n",
    "print(\"test_accuracies_10replications: \", test_accuracies_10replications)\n",
    "print(\"mean of test_accuracies_10replications: \", np.mean(test_accuracies_10replications))\n",
    "print(\"standard deviation of test_accuracies_10replications_std_mean: \", test_accuracies_10replications_std_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGGOMCYklHMYYY0JiCYcxxpiQ/D95CocBAK0MvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5219d8e790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_1st_replication\n",
    "plt.plot(losses_1st_replication, color='green', linewidth=5)\n",
    "plt.plot(train_accuracies_1st_replication, color='blue', linewidth=7)\n",
    "plt.plot(test_accuracies_1st_replication, color='red', linewidth=3)\n",
    "plt.xlim(0, epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.ylabel(\"train acc in blue, test acc in red, loss in green\")\n",
    "plt.title(\"1st replication\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
