{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference: \"3_mnist_from_scratch from\", \"docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\"\n",
    "https://github.com/michael-iuzzolino/CIFAR_reader\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "from CIFAR_reader import CIFAR_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CIFAR data...\n",
      "Extracting Data...\n",
      "Unpacking data...\n",
      "Loading training batch 1 of 5...\n",
      "Loading training batch 2 of 5...\n",
      "Loading training batch 3 of 5...\n",
      "Loading training batch 4 of 5...\n",
      "Loading training batch 5 of 5...\n",
      "Loading testing batch 1 of 1...\n"
     ]
    }
   ],
   "source": [
    "cifar = CIFAR_reader(one_hot=True, verbose=True, img_size=32, num_classes=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 128\n",
    "N_CHANNELS = 3\n",
    "N_LABELS = 10\n",
    "SEED = 32\n",
    "\n",
    "training_data = cifar.train\n",
    "training_labels = cifar.labels\n",
    "train_data = training_data['data']\n",
    "train_labels = training_data['labels']\n",
    "\n",
    "testing_data = cifar.test\n",
    "test_data = testing_data['data']\n",
    "test_data = np.float32(test_data)\n",
    "test_labels = testing_data['labels']\n",
    "test_labels = np.float32(test_labels)\n",
    "\n",
    "# convert train and test data values from [0, 255] to [-0.5, 0.5]\n",
    "N_TRAIN_IMAGE = len(train_data)\n",
    "train_data = (train_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "train_data = train_data.reshape(N_TRAIN_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "N_TEST_IMAGE = len(test_data)\n",
    "test_data = (test_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "test_data = test_data.reshape(N_TEST_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "#print(\"train_data[0]:\", train_data[0])\n",
    "#print(\"test_data[0]:\", test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1137b9b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar.preview_data(data_set=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape (40000, 32, 32, 3)\n",
      "validation_data.shape (10000, 32, 32, 3)\n",
      "train_data size: 40000\n",
      "validation_data size: 10000\n",
      "N_TRAIN_IMAGE:  40000\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_data = np.float32(train_data)\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = len(train_data)\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_data = np.float32(validation_data)\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "validation_size = len(validation_data)\n",
    "\n",
    "print('train_data.shape', train_data.shape)\n",
    "print('validation_data.shape', validation_data.shape)\n",
    "print('train_data size:', train_size)\n",
    "print('validation_data size:', validation_size)\n",
    "#print('validation_data:', validation_data)\n",
    "#print('validation_labels: ', validation_labels)\n",
    "\n",
    "N_TRAIN_IMAGE = train_size\n",
    "print(\"N_TRAIN_IMAGE: \", N_TRAIN_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, N_LABELS))\n",
    "\n",
    "train_all_data_node = tf.constant(train_data)\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, N_CHANNELS, 32],  # 5x5 kernel, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "\n",
    "print('Variables Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training computation: logits + cross-entropy loss done\n"
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# compute only by {eval()} method.\n",
    "train_all_data_prediction = tf.nn.softmax(model(train_all_data_node))\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Training computation: logits + cross-entropy loss done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.as_default()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 3)\n",
      "(128, 10)\n",
      "Run graph done.\n"
     ]
    }
   ],
   "source": [
    "# Grab the first BATCH_SIZE examples and labels.\n",
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# This dictionary maps the batch data (as a numpy array) to the\n",
    "# node in the graph it should be fed to.\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = sess.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_labels.shape)\n",
    "print('Run graph done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.16982765e-01   1.42972870e-03   1.97592890e-05   3.99296312e-07\n",
      "   8.81408807e-04   1.68743473e-03   1.40850125e-05   8.78815353e-01\n",
      "   1.66529047e-04   2.52190534e-06]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prediction 7\n",
      "(128, 10)\n",
      "All predictions [7 5 5 1 5 1 6 0 0 7 5 4 2 1 5 4 5 2 0 1 0 0 5 5 5 1 1 5 4 5 7 5 1 0 2 0 5\n",
      " 0 5 5 5 5 0 5 5 5 5 5 1 7 0 5 1 5 5 4 1 1 5 1 5 5 5 5 5 0 5 5 5 2 0 7 0 5\n",
      " 7 0 1 5 5 7 9 7 4 5 5 9 1 5 5 0 5 5 5 5 5 5 5 1 5 5 0 5 4 0 5 5 4 0 5 1 4\n",
      " 4 4 2 0 5 5 1 0 5 1 0 5 7 5 4 5 7]\n"
     ]
    }
   ],
   "source": [
    "# The highest probability in the first entry.\n",
    "print('First prediction', np.argmax(predictions[0]))\n",
    "\n",
    "# But, predictions is actually a list of BATCH_SIZE probability vectors.\n",
    "print(predictions.shape)\n",
    "\n",
    "# So, we'll take the highest probability for each vector.\n",
    "print('All predictions', np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels [1 6 6 8 8 3 4 6 0 6 0 3 6 6 5 4 8 3 2 6 0 3 1 4 0 6 6 2 7 6 9 0 4 5 7 1 6\n",
      " 7 9 1 7 7 8 0 3 7 4 7 3 1 0 4 6 6 1 4 9 2 6 4 5 0 4 6 0 8 3 4 8 8 3 9 5 7\n",
      " 1 9 4 7 9 1 9 7 5 2 7 3 4 8 8 2 1 5 9 2 7 8 8 6 8 8 1 3 8 8 5 4 7 1 6 6 1\n",
      " 6 1 6 7 0 4 6 9 5 8 7 1 9 0 3 3 7]\n"
     ]
    }
   ],
   "source": [
    "print('Batch labels', np.argmax(batch_labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1015625\n"
     ]
    }
   ],
   "source": [
    "correct = np.sum(np.argmax(predictions, 1) == np.argmax(batch_labels, 1))\n",
    "total = predictions.shape[0]\n",
    "\n",
    "print(float(correct) / float(total))\n",
    "\n",
    "confusions = np.zeros([10, 10], np.float32)\n",
    "bundled = zip(np.argmax(predictions, 1), np.argmax(batch_labels, 1))\n",
    "for predicted, actual in bundled:\n",
    "  confusions[predicted, actual] += 1\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(np.arange(N_LABELS))\n",
    "plt.yticks(np.arange(N_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_accuracy defined\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(predictions, labels):\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    accuracy = float(correct) / float(total)\n",
    "    accuracy_fig = \"\"\n",
    "    accuracy_fig = str(correct)\n",
    "    accuracy_fig += (\" of \")\n",
    "    accuracy_fig += str(total)\n",
    "    return accuracy, accuracy_fig\n",
    "\n",
    "print('get_accuracy defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 2000\n",
      "Validation accuracy: 10.940000% (1094 of 10000), Mini-batch loss: 8.96989, Learning rate: 0.01000\n",
      "Step 5 of 2000\n",
      "Validation accuracy: 12.410000% (1241 of 10000), Mini-batch loss: 7.36864, Learning rate: 0.01000\n",
      "Step 10 of 2000\n",
      "Validation accuracy: 16.630000% (1663 of 10000), Mini-batch loss: 6.51252, Learning rate: 0.01000\n",
      "Step 15 of 2000\n",
      "Validation accuracy: 18.060000% (1806 of 10000), Mini-batch loss: 6.34707, Learning rate: 0.01000\n",
      "Step 20 of 2000\n",
      "Validation accuracy: 21.690000% (2169 of 10000), Mini-batch loss: 6.22292, Learning rate: 0.01000\n",
      "Step 25 of 2000\n",
      "Validation accuracy: 24.370000% (2437 of 10000), Mini-batch loss: 6.30681, Learning rate: 0.01000\n",
      "Step 30 of 2000\n",
      "Validation accuracy: 26.820000% (2682 of 10000), Mini-batch loss: 6.17990, Learning rate: 0.01000\n",
      "Step 35 of 2000\n",
      "Validation accuracy: 26.730000% (2673 of 10000), Mini-batch loss: 6.16192, Learning rate: 0.01000\n",
      "Step 40 of 2000\n",
      "Validation accuracy: 28.280000% (2828 of 10000), Mini-batch loss: 6.14630, Learning rate: 0.01000\n",
      "Step 45 of 2000\n",
      "Validation accuracy: 31.110000% (3111 of 10000), Mini-batch loss: 6.17709, Learning rate: 0.01000\n",
      "Step 50 of 2000\n",
      "Validation accuracy: 31.730000% (3173 of 10000), Mini-batch loss: 6.05039, Learning rate: 0.01000\n",
      "Step 55 of 2000\n",
      "Validation accuracy: 34.400000% (3440 of 10000), Mini-batch loss: 6.09848, Learning rate: 0.01000\n",
      "Step 60 of 2000\n",
      "Validation accuracy: 35.110000% (3511 of 10000), Mini-batch loss: 6.09632, Learning rate: 0.01000\n",
      "Step 65 of 2000\n",
      "Validation accuracy: 35.550000% (3555 of 10000), Mini-batch loss: 6.07622, Learning rate: 0.01000\n",
      "Step 70 of 2000\n",
      "Validation accuracy: 36.460000% (3646 of 10000), Mini-batch loss: 6.07799, Learning rate: 0.01000\n",
      "Step 75 of 2000\n",
      "Validation accuracy: 37.970000% (3797 of 10000), Mini-batch loss: 5.94417, Learning rate: 0.01000\n",
      "Step 80 of 2000\n",
      "Validation accuracy: 37.830000% (3783 of 10000), Mini-batch loss: 5.99907, Learning rate: 0.01000\n",
      "Step 85 of 2000\n",
      "Validation accuracy: 39.170000% (3917 of 10000), Mini-batch loss: 5.80841, Learning rate: 0.01000\n",
      "Step 90 of 2000\n",
      "Validation accuracy: 38.370000% (3837 of 10000), Mini-batch loss: 5.79098, Learning rate: 0.01000\n",
      "Step 95 of 2000\n",
      "Validation accuracy: 39.150000% (3915 of 10000), Mini-batch loss: 5.93034, Learning rate: 0.01000\n",
      "Step 100 of 2000\n",
      "Validation accuracy: 39.790000% (3979 of 10000), Mini-batch loss: 5.96398, Learning rate: 0.01000\n",
      "Step 105 of 2000\n",
      "Validation accuracy: 39.410000% (3941 of 10000), Mini-batch loss: 5.88129, Learning rate: 0.01000\n",
      "Step 110 of 2000\n",
      "Validation accuracy: 39.250000% (3925 of 10000), Mini-batch loss: 5.67161, Learning rate: 0.01000\n",
      "Step 115 of 2000\n",
      "Validation accuracy: 40.250000% (4025 of 10000), Mini-batch loss: 5.87665, Learning rate: 0.01000\n",
      "Step 120 of 2000\n",
      "Validation accuracy: 39.520000% (3952 of 10000), Mini-batch loss: 5.82046, Learning rate: 0.01000\n",
      "Step 125 of 2000\n",
      "Validation accuracy: 39.830000% (3983 of 10000), Mini-batch loss: 5.76174, Learning rate: 0.01000\n",
      "Step 130 of 2000\n",
      "Validation accuracy: 38.340000% (3834 of 10000), Mini-batch loss: 5.61680, Learning rate: 0.01000\n",
      "Step 135 of 2000\n",
      "Validation accuracy: 39.020000% (3902 of 10000), Mini-batch loss: 5.81321, Learning rate: 0.01000\n",
      "Step 140 of 2000\n",
      "Validation accuracy: 39.640000% (3964 of 10000), Mini-batch loss: 5.93203, Learning rate: 0.01000\n",
      "Step 145 of 2000\n",
      "Validation accuracy: 40.160000% (4016 of 10000), Mini-batch loss: 5.73809, Learning rate: 0.01000\n",
      "Step 150 of 2000\n",
      "Validation accuracy: 41.330000% (4133 of 10000), Mini-batch loss: 5.62235, Learning rate: 0.01000\n",
      "Step 155 of 2000\n",
      "Validation accuracy: 41.690000% (4169 of 10000), Mini-batch loss: 5.65676, Learning rate: 0.01000\n",
      "Step 160 of 2000\n",
      "Validation accuracy: 42.350000% (4235 of 10000), Mini-batch loss: 5.65894, Learning rate: 0.01000\n",
      "Step 165 of 2000\n",
      "Validation accuracy: 42.540000% (4254 of 10000), Mini-batch loss: 5.84835, Learning rate: 0.01000\n",
      "Step 170 of 2000\n",
      "Validation accuracy: 40.850000% (4085 of 10000), Mini-batch loss: 5.70908, Learning rate: 0.01000\n",
      "Step 175 of 2000\n",
      "Validation accuracy: 41.270000% (4127 of 10000), Mini-batch loss: 5.64826, Learning rate: 0.01000\n",
      "Step 180 of 2000\n",
      "Validation accuracy: 40.790000% (4079 of 10000), Mini-batch loss: 5.67380, Learning rate: 0.01000\n",
      "Step 185 of 2000\n",
      "Validation accuracy: 43.300000% (4330 of 10000), Mini-batch loss: 5.74801, Learning rate: 0.01000\n",
      "Step 190 of 2000\n",
      "Validation accuracy: 43.300000% (4330 of 10000), Mini-batch loss: 5.63927, Learning rate: 0.01000\n",
      "Step 195 of 2000\n",
      "Validation accuracy: 43.730000% (4373 of 10000), Mini-batch loss: 5.71847, Learning rate: 0.01000\n",
      "Step 200 of 2000\n",
      "Validation accuracy: 42.310000% (4231 of 10000), Mini-batch loss: 5.68738, Learning rate: 0.01000\n",
      "Step 205 of 2000\n",
      "Validation accuracy: 43.840000% (4384 of 10000), Mini-batch loss: 5.69730, Learning rate: 0.01000\n",
      "Step 210 of 2000\n",
      "Validation accuracy: 44.590000% (4459 of 10000), Mini-batch loss: 5.76528, Learning rate: 0.01000\n",
      "Step 215 of 2000\n",
      "Validation accuracy: 44.020000% (4402 of 10000), Mini-batch loss: 5.76116, Learning rate: 0.01000\n",
      "Step 220 of 2000\n",
      "Validation accuracy: 43.390000% (4339 of 10000), Mini-batch loss: 5.82886, Learning rate: 0.01000\n",
      "Step 225 of 2000\n",
      "Validation accuracy: 43.400000% (4340 of 10000), Mini-batch loss: 5.55730, Learning rate: 0.01000\n",
      "Step 230 of 2000\n",
      "Validation accuracy: 44.140000% (4414 of 10000), Mini-batch loss: 5.57283, Learning rate: 0.01000\n",
      "Step 235 of 2000\n",
      "Validation accuracy: 43.110000% (4311 of 10000), Mini-batch loss: 5.73609, Learning rate: 0.01000\n",
      "Step 240 of 2000\n",
      "Validation accuracy: 42.380000% (4238 of 10000), Mini-batch loss: 5.67618, Learning rate: 0.01000\n",
      "Step 245 of 2000\n",
      "Validation accuracy: 43.370000% (4337 of 10000), Mini-batch loss: 5.57000, Learning rate: 0.01000\n",
      "Step 250 of 2000\n",
      "Validation accuracy: 45.200000% (4520 of 10000), Mini-batch loss: 5.55429, Learning rate: 0.01000\n",
      "Step 255 of 2000\n",
      "Validation accuracy: 45.770000% (4577 of 10000), Mini-batch loss: 5.54629, Learning rate: 0.01000\n",
      "Step 260 of 2000\n",
      "Validation accuracy: 45.790000% (4579 of 10000), Mini-batch loss: 5.56820, Learning rate: 0.01000\n",
      "Step 265 of 2000\n",
      "Validation accuracy: 45.770000% (4577 of 10000), Mini-batch loss: 5.58077, Learning rate: 0.01000\n",
      "Step 270 of 2000\n",
      "Validation accuracy: 44.960000% (4496 of 10000), Mini-batch loss: 5.60260, Learning rate: 0.01000\n",
      "Step 275 of 2000\n",
      "Validation accuracy: 44.730000% (4473 of 10000), Mini-batch loss: 5.62400, Learning rate: 0.01000\n",
      "Step 280 of 2000\n",
      "Validation accuracy: 45.440000% (4544 of 10000), Mini-batch loss: 5.58665, Learning rate: 0.01000\n",
      "Step 285 of 2000\n",
      "Validation accuracy: 45.580000% (4558 of 10000), Mini-batch loss: 5.67546, Learning rate: 0.01000\n",
      "Step 290 of 2000\n",
      "Validation accuracy: 45.230000% (4523 of 10000), Mini-batch loss: 5.38736, Learning rate: 0.01000\n",
      "Step 295 of 2000\n",
      "Validation accuracy: 45.280000% (4528 of 10000), Mini-batch loss: 5.68458, Learning rate: 0.01000\n",
      "Step 300 of 2000\n",
      "Validation accuracy: 46.920000% (4692 of 10000), Mini-batch loss: 5.55603, Learning rate: 0.01000\n",
      "Step 305 of 2000\n",
      "Validation accuracy: 45.930000% (4593 of 10000), Mini-batch loss: 5.47892, Learning rate: 0.01000\n",
      "Step 310 of 2000\n",
      "Validation accuracy: 46.970000% (4697 of 10000), Mini-batch loss: 5.56919, Learning rate: 0.01000\n",
      "Step 315 of 2000\n",
      "Validation accuracy: 46.530000% (4653 of 10000), Mini-batch loss: 5.61769, Learning rate: 0.00950\n",
      "Step 320 of 2000\n",
      "Validation accuracy: 46.080000% (4608 of 10000), Mini-batch loss: 5.49066, Learning rate: 0.00950\n",
      "Step 325 of 2000\n",
      "Validation accuracy: 47.110000% (4711 of 10000), Mini-batch loss: 5.42617, Learning rate: 0.00950\n",
      "Step 330 of 2000\n",
      "Validation accuracy: 46.910000% (4691 of 10000), Mini-batch loss: 5.34435, Learning rate: 0.00950\n",
      "Step 335 of 2000\n",
      "Validation accuracy: 47.960000% (4796 of 10000), Mini-batch loss: 5.52140, Learning rate: 0.00950\n",
      "Step 340 of 2000\n",
      "Validation accuracy: 48.190000% (4819 of 10000), Mini-batch loss: 5.58779, Learning rate: 0.00950\n",
      "Step 345 of 2000\n",
      "Validation accuracy: 47.360000% (4736 of 10000), Mini-batch loss: 5.54390, Learning rate: 0.00950\n",
      "Step 350 of 2000\n",
      "Validation accuracy: 47.990000% (4799 of 10000), Mini-batch loss: 5.51085, Learning rate: 0.00950\n",
      "Step 355 of 2000\n",
      "Validation accuracy: 48.330000% (4833 of 10000), Mini-batch loss: 5.52453, Learning rate: 0.00950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 360 of 2000\n",
      "Validation accuracy: 48.680000% (4868 of 10000), Mini-batch loss: 5.48614, Learning rate: 0.00950\n",
      "Step 365 of 2000\n",
      "Validation accuracy: 49.120000% (4912 of 10000), Mini-batch loss: 5.34098, Learning rate: 0.00950\n",
      "Step 370 of 2000\n",
      "Validation accuracy: 49.950000% (4995 of 10000), Mini-batch loss: 5.37984, Learning rate: 0.00950\n",
      "Step 375 of 2000\n",
      "Validation accuracy: 50.100000% (5010 of 10000), Mini-batch loss: 5.36856, Learning rate: 0.00950\n",
      "Step 380 of 2000\n",
      "Validation accuracy: 49.300000% (4930 of 10000), Mini-batch loss: 5.45897, Learning rate: 0.00950\n",
      "Step 385 of 2000\n",
      "Validation accuracy: 49.440000% (4944 of 10000), Mini-batch loss: 5.32027, Learning rate: 0.00950\n",
      "Step 390 of 2000\n",
      "Validation accuracy: 50.180000% (5018 of 10000), Mini-batch loss: 5.36415, Learning rate: 0.00950\n",
      "Step 395 of 2000\n",
      "Validation accuracy: 49.950000% (4995 of 10000), Mini-batch loss: 5.51847, Learning rate: 0.00950\n",
      "Step 400 of 2000\n",
      "Validation accuracy: 49.760000% (4976 of 10000), Mini-batch loss: 5.46389, Learning rate: 0.00950\n",
      "Step 405 of 2000\n",
      "Validation accuracy: 49.910000% (4991 of 10000), Mini-batch loss: 5.43567, Learning rate: 0.00950\n",
      "Step 410 of 2000\n",
      "Validation accuracy: 50.530000% (5053 of 10000), Mini-batch loss: 5.53805, Learning rate: 0.00950\n",
      "Step 415 of 2000\n",
      "Validation accuracy: 50.610000% (5061 of 10000), Mini-batch loss: 5.22060, Learning rate: 0.00950\n",
      "Step 420 of 2000\n",
      "Validation accuracy: 50.220000% (5022 of 10000), Mini-batch loss: 5.42620, Learning rate: 0.00950\n",
      "Step 425 of 2000\n",
      "Validation accuracy: 50.420000% (5042 of 10000), Mini-batch loss: 5.37271, Learning rate: 0.00950\n",
      "Step 430 of 2000\n",
      "Validation accuracy: 50.450000% (5045 of 10000), Mini-batch loss: 5.44106, Learning rate: 0.00950\n",
      "Step 435 of 2000\n",
      "Validation accuracy: 50.970000% (5097 of 10000), Mini-batch loss: 5.41091, Learning rate: 0.00950\n",
      "Step 440 of 2000\n",
      "Validation accuracy: 50.170000% (5017 of 10000), Mini-batch loss: 5.26849, Learning rate: 0.00950\n",
      "Step 445 of 2000\n",
      "Validation accuracy: 50.560000% (5056 of 10000), Mini-batch loss: 5.32833, Learning rate: 0.00950\n",
      "Step 450 of 2000\n",
      "Validation accuracy: 49.160000% (4916 of 10000), Mini-batch loss: 5.28021, Learning rate: 0.00950\n",
      "Step 455 of 2000\n",
      "Validation accuracy: 49.100000% (4910 of 10000), Mini-batch loss: 5.40201, Learning rate: 0.00950\n",
      "Step 460 of 2000\n",
      "Validation accuracy: 49.310000% (4931 of 10000), Mini-batch loss: 5.52653, Learning rate: 0.00950\n",
      "Step 465 of 2000\n",
      "Validation accuracy: 50.780000% (5078 of 10000), Mini-batch loss: 5.36069, Learning rate: 0.00950\n",
      "Step 470 of 2000\n",
      "Validation accuracy: 49.990000% (4999 of 10000), Mini-batch loss: 5.27545, Learning rate: 0.00950\n",
      "Step 475 of 2000\n",
      "Validation accuracy: 51.620000% (5162 of 10000), Mini-batch loss: 5.29266, Learning rate: 0.00950\n",
      "Step 480 of 2000\n",
      "Validation accuracy: 50.490000% (5049 of 10000), Mini-batch loss: 5.26180, Learning rate: 0.00950\n",
      "Step 485 of 2000\n",
      "Validation accuracy: 50.550000% (5055 of 10000), Mini-batch loss: 5.54325, Learning rate: 0.00950\n",
      "Step 490 of 2000\n",
      "Validation accuracy: 49.870000% (4987 of 10000), Mini-batch loss: 5.49375, Learning rate: 0.00950\n",
      "Step 495 of 2000\n",
      "Validation accuracy: 50.210000% (5021 of 10000), Mini-batch loss: 5.46700, Learning rate: 0.00950\n",
      "Step 500 of 2000\n",
      "Validation accuracy: 50.690000% (5069 of 10000), Mini-batch loss: 5.22791, Learning rate: 0.00950\n",
      "Step 505 of 2000\n",
      "Validation accuracy: 51.590000% (5159 of 10000), Mini-batch loss: 5.32141, Learning rate: 0.00950\n",
      "Step 510 of 2000\n",
      "Validation accuracy: 51.010000% (5101 of 10000), Mini-batch loss: 5.32706, Learning rate: 0.00950\n",
      "Step 515 of 2000\n",
      "Validation accuracy: 51.520000% (5152 of 10000), Mini-batch loss: 5.41505, Learning rate: 0.00950\n",
      "Step 520 of 2000\n",
      "Validation accuracy: 51.160000% (5116 of 10000), Mini-batch loss: 5.29523, Learning rate: 0.00950\n",
      "Step 525 of 2000\n",
      "Validation accuracy: 52.040000% (5204 of 10000), Mini-batch loss: 5.33905, Learning rate: 0.00950\n",
      "Step 530 of 2000\n",
      "Validation accuracy: 51.840000% (5184 of 10000), Mini-batch loss: 5.43288, Learning rate: 0.00950\n",
      "Step 535 of 2000\n",
      "Validation accuracy: 50.750000% (5075 of 10000), Mini-batch loss: 5.43728, Learning rate: 0.00950\n",
      "Step 540 of 2000\n",
      "Validation accuracy: 51.380000% (5138 of 10000), Mini-batch loss: 5.38879, Learning rate: 0.00950\n",
      "Step 545 of 2000\n",
      "Validation accuracy: 51.770000% (5177 of 10000), Mini-batch loss: 5.27999, Learning rate: 0.00950\n",
      "Step 550 of 2000\n",
      "Validation accuracy: 53.160000% (5316 of 10000), Mini-batch loss: 5.21792, Learning rate: 0.00950\n",
      "Step 555 of 2000\n",
      "Validation accuracy: 51.710000% (5171 of 10000), Mini-batch loss: 5.28840, Learning rate: 0.00950\n",
      "Step 560 of 2000\n",
      "Validation accuracy: 52.290000% (5229 of 10000), Mini-batch loss: 5.13789, Learning rate: 0.00950\n",
      "Step 565 of 2000\n",
      "Validation accuracy: 52.530000% (5253 of 10000), Mini-batch loss: 5.41413, Learning rate: 0.00950\n",
      "Step 570 of 2000\n",
      "Validation accuracy: 52.270000% (5227 of 10000), Mini-batch loss: 5.19417, Learning rate: 0.00950\n",
      "Step 575 of 2000\n",
      "Validation accuracy: 52.260000% (5226 of 10000), Mini-batch loss: 5.47375, Learning rate: 0.00950\n",
      "Step 580 of 2000\n",
      "Validation accuracy: 52.470000% (5247 of 10000), Mini-batch loss: 5.29254, Learning rate: 0.00950\n",
      "Step 585 of 2000\n",
      "Validation accuracy: 51.840000% (5184 of 10000), Mini-batch loss: 5.14993, Learning rate: 0.00950\n",
      "Step 590 of 2000\n",
      "Validation accuracy: 52.990000% (5299 of 10000), Mini-batch loss: 5.22396, Learning rate: 0.00950\n",
      "Step 595 of 2000\n",
      "Validation accuracy: 52.550000% (5255 of 10000), Mini-batch loss: 5.31286, Learning rate: 0.00950\n",
      "Step 600 of 2000\n",
      "Validation accuracy: 51.770000% (5177 of 10000), Mini-batch loss: 5.21714, Learning rate: 0.00950\n",
      "Step 605 of 2000\n",
      "Validation accuracy: 50.760000% (5076 of 10000), Mini-batch loss: 5.15321, Learning rate: 0.00950\n",
      "Step 610 of 2000\n",
      "Validation accuracy: 52.590000% (5259 of 10000), Mini-batch loss: 5.21768, Learning rate: 0.00950\n",
      "Step 615 of 2000\n",
      "Validation accuracy: 52.160000% (5216 of 10000), Mini-batch loss: 5.19271, Learning rate: 0.00950\n",
      "Step 620 of 2000\n",
      "Validation accuracy: 51.290000% (5129 of 10000), Mini-batch loss: 5.19122, Learning rate: 0.00950\n",
      "Step 625 of 2000\n",
      "Validation accuracy: 53.120000% (5312 of 10000), Mini-batch loss: 5.25464, Learning rate: 0.00902\n",
      "Step 630 of 2000\n",
      "Validation accuracy: 52.830000% (5283 of 10000), Mini-batch loss: 5.19657, Learning rate: 0.00902\n",
      "Step 635 of 2000\n",
      "Validation accuracy: 52.310000% (5231 of 10000), Mini-batch loss: 5.13698, Learning rate: 0.00902\n",
      "Step 640 of 2000\n",
      "Validation accuracy: 51.050000% (5105 of 10000), Mini-batch loss: 5.29103, Learning rate: 0.00902\n",
      "Step 645 of 2000\n",
      "Validation accuracy: 53.630000% (5363 of 10000), Mini-batch loss: 5.16143, Learning rate: 0.00902\n",
      "Step 650 of 2000\n",
      "Validation accuracy: 52.850000% (5285 of 10000), Mini-batch loss: 5.10817, Learning rate: 0.00902\n",
      "Step 655 of 2000\n",
      "Validation accuracy: 52.670000% (5267 of 10000), Mini-batch loss: 5.20240, Learning rate: 0.00902\n",
      "Step 660 of 2000\n",
      "Validation accuracy: 53.670000% (5367 of 10000), Mini-batch loss: 5.16100, Learning rate: 0.00902\n",
      "Step 665 of 2000\n",
      "Validation accuracy: 54.100000% (5410 of 10000), Mini-batch loss: 5.37530, Learning rate: 0.00902\n",
      "Step 670 of 2000\n",
      "Validation accuracy: 53.800000% (5380 of 10000), Mini-batch loss: 5.18248, Learning rate: 0.00902\n",
      "Step 675 of 2000\n",
      "Validation accuracy: 54.400000% (5440 of 10000), Mini-batch loss: 5.01393, Learning rate: 0.00902\n",
      "Step 680 of 2000\n",
      "Validation accuracy: 54.060000% (5406 of 10000), Mini-batch loss: 5.11897, Learning rate: 0.00902\n",
      "Step 685 of 2000\n",
      "Validation accuracy: 55.510000% (5551 of 10000), Mini-batch loss: 5.08104, Learning rate: 0.00902\n",
      "Step 690 of 2000\n",
      "Validation accuracy: 54.260000% (5426 of 10000), Mini-batch loss: 5.10408, Learning rate: 0.00902\n",
      "Step 695 of 2000\n",
      "Validation accuracy: 54.340000% (5434 of 10000), Mini-batch loss: 5.24562, Learning rate: 0.00902\n",
      "Step 700 of 2000\n",
      "Validation accuracy: 53.700000% (5370 of 10000), Mini-batch loss: 5.26987, Learning rate: 0.00902\n",
      "Step 705 of 2000\n",
      "Validation accuracy: 55.270000% (5527 of 10000), Mini-batch loss: 5.22530, Learning rate: 0.00902\n",
      "Step 710 of 2000\n",
      "Validation accuracy: 54.020000% (5402 of 10000), Mini-batch loss: 5.05948, Learning rate: 0.00902\n",
      "Step 715 of 2000\n",
      "Validation accuracy: 54.170000% (5417 of 10000), Mini-batch loss: 5.24631, Learning rate: 0.00902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 720 of 2000\n",
      "Validation accuracy: 54.890000% (5489 of 10000), Mini-batch loss: 5.15841, Learning rate: 0.00902\n",
      "Step 725 of 2000\n",
      "Validation accuracy: 55.840000% (5584 of 10000), Mini-batch loss: 5.12576, Learning rate: 0.00902\n",
      "Step 730 of 2000\n",
      "Validation accuracy: 54.680000% (5468 of 10000), Mini-batch loss: 5.14149, Learning rate: 0.00902\n",
      "Step 735 of 2000\n",
      "Validation accuracy: 55.380000% (5538 of 10000), Mini-batch loss: 5.10324, Learning rate: 0.00902\n",
      "Step 740 of 2000\n",
      "Validation accuracy: 54.700000% (5470 of 10000), Mini-batch loss: 5.14010, Learning rate: 0.00902\n",
      "Step 745 of 2000\n",
      "Validation accuracy: 55.430000% (5543 of 10000), Mini-batch loss: 5.23573, Learning rate: 0.00902\n",
      "Step 750 of 2000\n",
      "Validation accuracy: 54.820000% (5482 of 10000), Mini-batch loss: 5.16326, Learning rate: 0.00902\n",
      "Step 755 of 2000\n",
      "Validation accuracy: 56.200000% (5620 of 10000), Mini-batch loss: 5.20812, Learning rate: 0.00902\n",
      "Step 760 of 2000\n",
      "Validation accuracy: 55.640000% (5564 of 10000), Mini-batch loss: 5.07639, Learning rate: 0.00902\n",
      "Step 765 of 2000\n",
      "Validation accuracy: 55.150000% (5515 of 10000), Mini-batch loss: 5.08051, Learning rate: 0.00902\n",
      "Step 770 of 2000\n",
      "Validation accuracy: 54.280000% (5428 of 10000), Mini-batch loss: 5.01195, Learning rate: 0.00902\n",
      "Step 775 of 2000\n",
      "Validation accuracy: 55.390000% (5539 of 10000), Mini-batch loss: 5.18023, Learning rate: 0.00902\n",
      "Step 780 of 2000\n",
      "Validation accuracy: 55.300000% (5530 of 10000), Mini-batch loss: 5.19058, Learning rate: 0.00902\n",
      "Step 785 of 2000\n",
      "Validation accuracy: 55.890000% (5589 of 10000), Mini-batch loss: 5.28231, Learning rate: 0.00902\n",
      "Step 790 of 2000\n",
      "Validation accuracy: 54.780000% (5478 of 10000), Mini-batch loss: 5.28414, Learning rate: 0.00902\n",
      "Step 795 of 2000\n",
      "Validation accuracy: 55.100000% (5510 of 10000), Mini-batch loss: 5.22911, Learning rate: 0.00902\n",
      "Step 800 of 2000\n",
      "Validation accuracy: 54.550000% (5455 of 10000), Mini-batch loss: 5.20084, Learning rate: 0.00902\n",
      "Step 805 of 2000\n",
      "Validation accuracy: 54.150000% (5415 of 10000), Mini-batch loss: 5.24834, Learning rate: 0.00902\n",
      "Step 810 of 2000\n",
      "Validation accuracy: 55.520000% (5552 of 10000), Mini-batch loss: 5.21144, Learning rate: 0.00902\n",
      "Step 815 of 2000\n",
      "Validation accuracy: 54.160000% (5416 of 10000), Mini-batch loss: 5.08631, Learning rate: 0.00902\n",
      "Step 820 of 2000\n",
      "Validation accuracy: 56.780000% (5678 of 10000), Mini-batch loss: 5.21898, Learning rate: 0.00902\n",
      "Step 825 of 2000\n",
      "Validation accuracy: 55.650000% (5565 of 10000), Mini-batch loss: 5.10185, Learning rate: 0.00902\n",
      "Step 830 of 2000\n",
      "Validation accuracy: 56.340000% (5634 of 10000), Mini-batch loss: 5.06339, Learning rate: 0.00902\n",
      "Step 835 of 2000\n",
      "Validation accuracy: 56.050000% (5605 of 10000), Mini-batch loss: 5.12153, Learning rate: 0.00902\n",
      "Step 840 of 2000\n",
      "Validation accuracy: 56.010000% (5601 of 10000), Mini-batch loss: 5.01237, Learning rate: 0.00902\n",
      "Step 845 of 2000\n",
      "Validation accuracy: 54.700000% (5470 of 10000), Mini-batch loss: 5.03540, Learning rate: 0.00902\n",
      "Step 850 of 2000\n",
      "Validation accuracy: 56.950000% (5695 of 10000), Mini-batch loss: 5.08364, Learning rate: 0.00902\n",
      "Step 855 of 2000\n",
      "Validation accuracy: 56.370000% (5637 of 10000), Mini-batch loss: 5.11396, Learning rate: 0.00902\n",
      "Step 860 of 2000\n",
      "Validation accuracy: 56.010000% (5601 of 10000), Mini-batch loss: 5.22637, Learning rate: 0.00902\n",
      "Step 865 of 2000\n",
      "Validation accuracy: 56.930000% (5693 of 10000), Mini-batch loss: 5.12529, Learning rate: 0.00902\n",
      "Step 870 of 2000\n",
      "Validation accuracy: 56.510000% (5651 of 10000), Mini-batch loss: 5.00098, Learning rate: 0.00902\n",
      "Step 875 of 2000\n",
      "Validation accuracy: 56.850000% (5685 of 10000), Mini-batch loss: 5.08924, Learning rate: 0.00902\n",
      "Step 880 of 2000\n",
      "Validation accuracy: 56.140000% (5614 of 10000), Mini-batch loss: 5.32581, Learning rate: 0.00902\n",
      "Step 885 of 2000\n",
      "Validation accuracy: 55.960000% (5596 of 10000), Mini-batch loss: 5.08384, Learning rate: 0.00902\n",
      "Step 890 of 2000\n",
      "Validation accuracy: 56.460000% (5646 of 10000), Mini-batch loss: 5.27876, Learning rate: 0.00902\n",
      "Step 895 of 2000\n",
      "Validation accuracy: 56.250000% (5625 of 10000), Mini-batch loss: 5.22088, Learning rate: 0.00902\n",
      "Step 900 of 2000\n",
      "Validation accuracy: 56.950000% (5695 of 10000), Mini-batch loss: 5.00633, Learning rate: 0.00902\n",
      "Step 905 of 2000\n",
      "Validation accuracy: 56.690000% (5669 of 10000), Mini-batch loss: 4.93536, Learning rate: 0.00902\n",
      "Step 910 of 2000\n",
      "Validation accuracy: 55.510000% (5551 of 10000), Mini-batch loss: 4.99992, Learning rate: 0.00902\n",
      "Step 915 of 2000\n",
      "Validation accuracy: 55.200000% (5520 of 10000), Mini-batch loss: 5.18902, Learning rate: 0.00902\n",
      "Step 920 of 2000\n",
      "Validation accuracy: 56.210000% (5621 of 10000), Mini-batch loss: 5.24238, Learning rate: 0.00902\n",
      "Step 925 of 2000\n",
      "Validation accuracy: 55.660000% (5566 of 10000), Mini-batch loss: 5.11490, Learning rate: 0.00902\n",
      "Step 930 of 2000\n",
      "Validation accuracy: 56.680000% (5668 of 10000), Mini-batch loss: 5.19600, Learning rate: 0.00902\n",
      "Step 935 of 2000\n",
      "Validation accuracy: 56.370000% (5637 of 10000), Mini-batch loss: 5.09826, Learning rate: 0.00902\n",
      "Step 940 of 2000\n",
      "Validation accuracy: 56.740000% (5674 of 10000), Mini-batch loss: 4.91063, Learning rate: 0.00857\n",
      "Step 945 of 2000\n",
      "Validation accuracy: 57.450000% (5745 of 10000), Mini-batch loss: 4.99992, Learning rate: 0.00857\n",
      "Step 950 of 2000\n",
      "Validation accuracy: 56.030000% (5603 of 10000), Mini-batch loss: 5.09097, Learning rate: 0.00857\n",
      "Step 955 of 2000\n",
      "Validation accuracy: 56.850000% (5685 of 10000), Mini-batch loss: 5.10332, Learning rate: 0.00857\n",
      "Step 960 of 2000\n",
      "Validation accuracy: 57.800000% (5780 of 10000), Mini-batch loss: 5.02778, Learning rate: 0.00857\n",
      "Step 965 of 2000\n",
      "Validation accuracy: 57.470000% (5747 of 10000), Mini-batch loss: 5.03125, Learning rate: 0.00857\n",
      "Step 970 of 2000\n",
      "Validation accuracy: 57.900000% (5790 of 10000), Mini-batch loss: 4.84527, Learning rate: 0.00857\n",
      "Step 975 of 2000\n",
      "Validation accuracy: 57.670000% (5767 of 10000), Mini-batch loss: 5.03220, Learning rate: 0.00857\n",
      "Step 980 of 2000\n",
      "Validation accuracy: 57.590000% (5759 of 10000), Mini-batch loss: 5.28506, Learning rate: 0.00857\n",
      "Step 985 of 2000\n",
      "Validation accuracy: 57.790000% (5779 of 10000), Mini-batch loss: 4.93955, Learning rate: 0.00857\n",
      "Step 990 of 2000\n",
      "Validation accuracy: 57.540000% (5754 of 10000), Mini-batch loss: 4.95159, Learning rate: 0.00857\n",
      "Step 995 of 2000\n",
      "Validation accuracy: 58.360000% (5836 of 10000), Mini-batch loss: 4.97381, Learning rate: 0.00857\n",
      "Step 1000 of 2000\n",
      "Validation accuracy: 57.900000% (5790 of 10000), Mini-batch loss: 5.07236, Learning rate: 0.00857\n",
      "Step 1005 of 2000\n",
      "Validation accuracy: 58.480000% (5848 of 10000), Mini-batch loss: 4.99893, Learning rate: 0.00857\n",
      "Step 1010 of 2000\n",
      "Validation accuracy: 58.360000% (5836 of 10000), Mini-batch loss: 5.18961, Learning rate: 0.00857\n",
      "Step 1015 of 2000\n",
      "Validation accuracy: 59.260000% (5926 of 10000), Mini-batch loss: 4.95382, Learning rate: 0.00857\n",
      "Step 1020 of 2000\n",
      "Validation accuracy: 58.050000% (5805 of 10000), Mini-batch loss: 4.93052, Learning rate: 0.00857\n",
      "Step 1025 of 2000\n",
      "Validation accuracy: 58.610000% (5861 of 10000), Mini-batch loss: 4.88942, Learning rate: 0.00857\n",
      "Step 1030 of 2000\n",
      "Validation accuracy: 58.960000% (5896 of 10000), Mini-batch loss: 4.92403, Learning rate: 0.00857\n",
      "Step 1035 of 2000\n",
      "Validation accuracy: 58.490000% (5849 of 10000), Mini-batch loss: 5.04853, Learning rate: 0.00857\n",
      "Step 1040 of 2000\n",
      "Validation accuracy: 58.770000% (5877 of 10000), Mini-batch loss: 4.99813, Learning rate: 0.00857\n",
      "Step 1045 of 2000\n",
      "Validation accuracy: 57.200000% (5720 of 10000), Mini-batch loss: 5.07635, Learning rate: 0.00857\n",
      "Step 1050 of 2000\n",
      "Validation accuracy: 59.180000% (5918 of 10000), Mini-batch loss: 5.06407, Learning rate: 0.00857\n",
      "Step 1055 of 2000\n",
      "Validation accuracy: 58.630000% (5863 of 10000), Mini-batch loss: 5.01199, Learning rate: 0.00857\n",
      "Step 1060 of 2000\n",
      "Validation accuracy: 56.460000% (5646 of 10000), Mini-batch loss: 4.83270, Learning rate: 0.00857\n",
      "Step 1065 of 2000\n",
      "Validation accuracy: 57.920000% (5792 of 10000), Mini-batch loss: 4.96277, Learning rate: 0.00857\n",
      "Step 1070 of 2000\n",
      "Validation accuracy: 57.650000% (5765 of 10000), Mini-batch loss: 4.95429, Learning rate: 0.00857\n",
      "Step 1075 of 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 57.780000% (5778 of 10000), Mini-batch loss: 4.93815, Learning rate: 0.00857\n",
      "Step 1080 of 2000\n",
      "Validation accuracy: 57.790000% (5779 of 10000), Mini-batch loss: 4.89727, Learning rate: 0.00857\n",
      "Step 1085 of 2000\n",
      "Validation accuracy: 58.210000% (5821 of 10000), Mini-batch loss: 4.81702, Learning rate: 0.00857\n",
      "Step 1090 of 2000\n",
      "Validation accuracy: 56.420000% (5642 of 10000), Mini-batch loss: 4.79848, Learning rate: 0.00857\n",
      "Step 1095 of 2000\n",
      "Validation accuracy: 58.490000% (5849 of 10000), Mini-batch loss: 4.91615, Learning rate: 0.00857\n",
      "Step 1100 of 2000\n",
      "Validation accuracy: 58.510000% (5851 of 10000), Mini-batch loss: 4.89431, Learning rate: 0.00857\n",
      "Step 1105 of 2000\n",
      "Validation accuracy: 57.440000% (5744 of 10000), Mini-batch loss: 4.96129, Learning rate: 0.00857\n",
      "Step 1110 of 2000\n",
      "Validation accuracy: 58.410000% (5841 of 10000), Mini-batch loss: 4.79846, Learning rate: 0.00857\n",
      "Step 1115 of 2000\n",
      "Validation accuracy: 58.020000% (5802 of 10000), Mini-batch loss: 4.88550, Learning rate: 0.00857\n",
      "Step 1120 of 2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-de402f4c4aad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step %d of %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         validation_accuracy, validation_accuracy_fig = get_accuracy(\n\u001b[0;32m---> 13\u001b[0;31m               validation_prediction.eval(), validation_labels)\n\u001b[0m\u001b[1;32m     14\u001b[0m         print('Validation accuracy: %.6f%% (%s), Mini-batch loss: %.5f, Learning rate: %.5f' % \n\u001b[1;32m     15\u001b[0m               (validation_accuracy * 100, validation_accuracy_fig, l, lr))\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \"\"\"\n\u001b[0;32m--> 569\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3739\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3740\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3741\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#steps = train_size\n",
    "steps = 2000\n",
    "for step in range(steps):\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    feed_dict = {train_data_node: batch_data, train_labels_node: batch_labels}\n",
    "    _, l, lr, predictions = sess.run([optimizer, loss, learning_rate, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print('Step %d of %d' % (step, steps))\n",
    "        validation_accuracy, validation_accuracy_fig = get_accuracy(\n",
    "              validation_prediction.eval(), validation_labels)\n",
    "        print('Validation accuracy: %.6f%% (%s), Mini-batch loss: %.5f, Learning rate: %.5f' % \n",
    "              (validation_accuracy * 100, validation_accuracy_fig, l, lr))\n",
    "        #feed_train_all_data_dict = {train_all_data_node: train_data}\n",
    "        #train_accuracy, train_accuracy_fig = get_accuracy(\n",
    "         #     train_all_data_prediction.eval(feed_dict=feed_train_all_data_dict), \n",
    "         #   train_labels)\n",
    "        \n",
    "        #train_accuracy, train_accuracy_fig = get_accuracy(\n",
    "        #      train_all_data_prediction.eval(), train_labels)\n",
    "        \n",
    "        #print('Train accuracy: %.6f%% (%s), Mini-batch loss: %.5f, Learning rate: %.5f' % \n",
    "         #     (train_accuracy * 100, train_accuracy_fig, l, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, test_accuracy_fig = get_accuracy(test_prediction.eval(), test_labels)\n",
    "print('Test accuracy: %.8f (%s)' % (test_accuracy, test_accuracy_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_accuracy, train_accuracy_fig = get_accuracy(train_all_data_prediction.eval(), train_labels)\n",
    "print('Train accuracy: %.8f (%s)' % (train_accuracy, train_accuracy_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
