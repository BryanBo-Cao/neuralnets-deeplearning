{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference: \"3_mnist_from_scratch from\", \"docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\"\n",
    "https://github.com/michael-iuzzolino/CIFAR_reader\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "from CIFAR_reader import CIFAR_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CIFAR data...\n",
      "Extracting Data...\n",
      "Unpacking data...\n",
      "Loading training batch 1 of 5...\n",
      "Loading training batch 2 of 5...\n",
      "Loading training batch 3 of 5...\n",
      "Loading training batch 4 of 5...\n",
      "Loading training batch 5 of 5...\n",
      "Loading testing batch 1 of 1...\n"
     ]
    }
   ],
   "source": [
    "cifar = CIFAR_reader(one_hot=True, verbose=True, img_size=32, num_classes=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_TRAIN_IMAGE,  50000\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 32\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 128\n",
    "N_CHANNELS = 3\n",
    "N_LABELS = 10\n",
    "SEED = 32\n",
    "\n",
    "training_data = cifar.train\n",
    "training_labels = cifar.labels\n",
    "train_data = training_data['data']\n",
    "train_labels = training_data['labels']\n",
    "\n",
    "testing_data = cifar.test\n",
    "test_data = testing_data['data']\n",
    "test_data = np.float32(test_data)\n",
    "test_labels = testing_data['labels']\n",
    "test_labels = np.float32(test_labels)\n",
    "\n",
    "# convert train and test data values from [0, 255] to [-0.5, 0.5]\n",
    "N_TRAIN_IMAGE = len(train_data)\n",
    "train_data = (train_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "train_data = train_data.reshape(N_TRAIN_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "print('N_TRAIN_IMAGE, ', N_TRAIN_IMAGE)\n",
    "\n",
    "N_TEST_IMAGE = len(test_data)\n",
    "test_data = (test_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "test_data = test_data.reshape(N_TEST_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "#print(\"train_data[0]:\", train_data[0])\n",
    "#print(\"test_data[0]:\", test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvxlU28SjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105e22b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar.preview_data(data_set=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_data[0][0][0]: \t [-0.26862746 -0.25686276 -0.25294119]\n",
      "validation_data[0][0][1]: \t [-0.33137256 -0.31960785 -0.32352942]\n",
      "train_data[0][0][0]: \t [-0.36274511 -0.40196079 -0.39803922]\n",
      "train_data[0][0][1]: \t [-0.39411765 -0.41764706 -0.41764706]\n",
      "train_data.shape\t (40000, 32, 32, 3)\n",
      "validation_data.shape\t (10000, 32, 32, 3)\n",
      "train_data size:\t 40000\n",
      "validation_data size:\t 10000\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = len(train_data) / 5\n",
    "\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_data = np.float32(validation_data)\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "validation_size = len(validation_data)\n",
    "\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_data = np.float32(train_data)\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = len(train_data)\n",
    "\n",
    "# to verify validation and train data are splitted properly\n",
    "print('validation_data[0][0][0]: \\t', validation_data[0][0][0])\n",
    "print('validation_data[0][0][1]: \\t', validation_data[0][0][1])\n",
    "print('train_data[0][0][0]: \\t', train_data[0][0][0])\n",
    "print('train_data[0][0][1]: \\t', train_data[0][0][1])\n",
    "\n",
    "print('train_data.shape\\t', train_data.shape)\n",
    "print('validation_data.shape\\t', validation_data.shape)\n",
    "print('train_data size:\\t', train_size)\n",
    "print('validation_data size:\\t', validation_size)\n",
    "#print('validation_data:', validation_data)\n",
    "#print('validation_labels: ', validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_all_data_labels0:  [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "train_all_data_labels3:  [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_ALL_DATA_OFFSET = 0\n",
    "TRAIN_ALL_DATA_SIZE = 10000\n",
    "train_all_data_labels0 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_labels1 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_labels2 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_labels3 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "print(\"train_all_data_labels0: \", train_all_data_labels0)\n",
    "print(\"train_all_data_labels3: \", train_all_data_labels3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, N_LABELS))\n",
    "\n",
    "TRAIN_ALL_DATA_OFFSET = 0\n",
    "TRAIN_ALL_DATA_SIZE = 10000\n",
    "train_all_data_node0 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_node1 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_node2 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_node3 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([10, 10, N_CHANNELS, 64],  # 10x10 kernel, depth 64.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([64]))\n",
    "\n",
    "'''\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([10, 10, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "'''\n",
    "\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([512, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "\n",
    "'''\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "'''\n",
    "\n",
    "print('Variables Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1], #[image index, y, x, depth]\n",
    "                        padding='SAME')\n",
    "\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 4, 4, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    '''\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    '''\n",
    "\n",
    "    # Fully connected layers\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the \n",
    "    # '+' operation automatically broadcasts the biases.\n",
    "    #hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "    #if train:\n",
    "        #hidden = tf.nn.dropout(hidden, 0.5, seed=SEED) # drop out rate 50%\n",
    "    #return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "    return tf.matmul(hidden, fc1_weights) + fc1_biases\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 16384 and 512 for 'MatMul' (op: 'MatMul') with input shapes: [128,16384], [512,10].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5bd281f15068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training computation: logits + cross-entropy loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n\u001b[1;32m      5\u001b[0m   labels=train_labels_node, logits=logits))\n",
      "\u001b[0;32m<ipython-input-9-33e0b918ce6c>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(data, train)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Fully connected layer. Note that the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# '+' operation automatically broadcasts the biases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc1_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfc1_biases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1799\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1801\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   1261\u001b[0m   \"\"\"\n\u001b[1;32m   1262\u001b[0m   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\n\u001b[0;32m-> 1263\u001b[0;31m                                 transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1264\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 16384 and 512 for 'MatMul' (op: 'MatMul') with input shapes: [128,16384], [512,10]."
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# compute only by {eval()} method.\n",
    "train_all_data_prediction0 = tf.nn.softmax(model(train_all_data_node0))\n",
    "train_all_data_prediction1 = tf.nn.softmax(model(train_all_data_node1))\n",
    "train_all_data_prediction2 = tf.nn.softmax(model(train_all_data_node2))\n",
    "train_all_data_prediction3 = tf.nn.softmax(model(train_all_data_node3))\n",
    "\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Training computation: logits + cross-entropy loss done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.as_default()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# batch data\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = sess.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_labels.shape)\n",
    "print('Run graph done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The highest probability in the first entry.\n",
    "print('First prediction', np.argmax(predictions[0]))\n",
    "print(predictions.shape)\n",
    "print('All predictions', np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Batch labels', np.argmax(batch_labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions, labels):\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    accuracy = float(correct) / float(total)\n",
    "    accuracy_fig = \"\"\n",
    "    accuracy_fig = str(correct)\n",
    "    accuracy_fig += (\" of \")\n",
    "    accuracy_fig += str(total)\n",
    "    return accuracy, accuracy_fig\n",
    "\n",
    "print('get_accuracy() defined')\n",
    "\n",
    "def get_all_train_data_accuracy(\n",
    "    train_all_data_prediction0, train_all_data_labels0,\n",
    "    train_all_data_prediction1, train_all_data_labels1,\n",
    "    train_all_data_prediction2, train_all_data_labels2,\n",
    "    train_all_data_prediction3, train_all_data_labels3):\n",
    "    \n",
    "    correct0 = np.sum(np.argmax(train_all_data_prediction0, 1) == np.argmax(train_all_data_labels0, 1))\n",
    "    total0 = train_all_data_prediction0.shape[0]\n",
    "    print(\"correct0: %d, total0: %d\", (correct0, total0))\n",
    "    \n",
    "    correct1 = np.sum(np.argmax(train_all_data_prediction1, 1) == np.argmax(train_all_data_labels1, 1))\n",
    "    total1 = train_all_data_prediction1.shape[0]\n",
    "    print(\"correct1: %d, total1: %d\", (correct1, total1))\n",
    "    \n",
    "    correct2 = np.sum(np.argmax(train_all_data_prediction2, 1) == np.argmax(train_all_data_labels2, 1))\n",
    "    total2 = train_all_data_prediction2.shape[0]\n",
    "    print(\"correct2: %d, total2: %d\", (correct2, total2))\n",
    "    \n",
    "    correct3 = np.sum(np.argmax(train_all_data_prediction3, 1) == np.argmax(train_all_data_labels3, 1))\n",
    "    total3 = train_all_data_prediction3.shape[0]\n",
    "    print(\"correct3: %d, total3: %d\", (correct3, total3))\n",
    "\n",
    "    correct = correct0 + correct1 + correct2 + correct3\n",
    "    total = total0 + total1 + total2 + total3\n",
    "    accuracy = float(correct) / float(total)\n",
    "    accuracy_fig = \"\"\n",
    "    accuracy_fig = str(correct)\n",
    "    accuracy_fig += (\" of \")\n",
    "    accuracy_fig += str(total)\n",
    "    return accuracy, accuracy_fig\n",
    "\n",
    "print('get_all_train_data_accuracy() defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy, validation_accuracy_fig = get_accuracy(\n",
    "    validation_prediction.eval(), validation_labels)\n",
    "print('validation_prediction.shape', validation_prediction.shape)\n",
    "print('Validation accuracy: %.4f%% (%s)' % (validation_accuracy * 100, validation_accuracy_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train_accuracy, train_accuracy_fig = get_all_train_data_accuracy(\n",
    "    train_all_data_prediction0.eval(), train_all_data_labels0,\n",
    "    train_all_data_prediction1.eval(), train_all_data_labels1,\n",
    "    train_all_data_prediction2.eval(), train_all_data_labels2,\n",
    "    train_all_data_prediction3.eval(), train_all_data_labels3)\n",
    "print('Train accuracy: %.4f%% (%s)' % (train_accuracy * 100, train_accuracy_fig))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#steps = train_size\n",
    "steps = 2000\n",
    "for step in range(steps):\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    feed_dict = {train_data_node: batch_data, train_labels_node: batch_labels}\n",
    "    _, l, lr, predictions = sess.run([optimizer, loss, learning_rate, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print('Step %d of %d' % (step, steps))\n",
    "        validation_accuracy, validation_accuracy_fig = get_accuracy(\n",
    "              validation_prediction.eval(), validation_labels)\n",
    "        print('Validation accuracy: %.6f%% (%s), Mini-batch loss: %.5f, Learning rate: %.5f' % \n",
    "              (validation_accuracy * 100, validation_accuracy_fig, l, lr))\n",
    "        \n",
    "        '''\n",
    "        train_accuracy, train_accuracy_fig = get_all_train_data_accuracy(\n",
    "            train_all_data_prediction0.eval(), train_all_data_labels0,\n",
    "            train_all_data_prediction1.eval(), train_all_data_labels1,\n",
    "            train_all_data_prediction2.eval(), train_all_data_labels2,\n",
    "            train_all_data_prediction3.eval(), train_all_data_labels3)\n",
    "        print('Train accuracy: %.4f%% (%s)' % (train_accuracy * 100, train_accuracy_fig))\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accuracy, test_accuracy_fig = get_accuracy(\n",
    "    test_prediction.eval(), test_labels)\n",
    "print('Test accuracy: %.4f%% (%s)' % (test_accuracy * 100, test_accuracy_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
