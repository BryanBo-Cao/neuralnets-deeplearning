{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    \"3_mnist_from_scratch from\", \"docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\"\n",
    "    https://github.com/michael-iuzzolino/CIFAR_reader\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "from CIFAR_reader import CIFAR_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CIFAR data...\n",
      "Extracting Data...\n",
      "Unpacking data...\n",
      "Loading training batch 1 of 5...\n",
      "Loading training batch 2 of 5...\n",
      "Loading training batch 3 of 5...\n",
      "Loading training batch 4 of 5...\n",
      "Loading training batch 5 of 5...\n",
      "Loading testing batch 1 of 1...\n"
     ]
    }
   ],
   "source": [
    "cifar = CIFAR_reader(one_hot=True, verbose=True, img_size=32, num_classes=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_TRAIN_IMAGE,  50000\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 32\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 128\n",
    "N_CHANNELS = 3\n",
    "N_LABELS = 10\n",
    "SEED = 32\n",
    "\n",
    "training_data = cifar.train\n",
    "training_labels = cifar.labels\n",
    "train_data = training_data['data']\n",
    "train_labels = training_data['labels']\n",
    "\n",
    "testing_data = cifar.test\n",
    "test_data = testing_data['data']\n",
    "test_data = np.float32(test_data)\n",
    "test_labels = testing_data['labels']\n",
    "test_labels = np.float32(test_labels)\n",
    "\n",
    "# convert train and test data values from [0, 255] to [-0.5, 0.5]\n",
    "N_TRAIN_IMAGE = len(train_data)\n",
    "train_data = (train_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "train_data = train_data.reshape(N_TRAIN_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "print('N_TRAIN_IMAGE, ', N_TRAIN_IMAGE)\n",
    "\n",
    "N_TEST_IMAGE = len(test_data)\n",
    "test_data = (test_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "test_data = test_data.reshape(N_TEST_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "#print(\"train_data[0]:\", train_data[0])\n",
    "#print(\"test_data[0]:\", test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1146a7e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar.preview_data(data_set=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_data[0][0][0]: \t [-0.26862746 -0.25686276 -0.25294119]\n",
      "validation_data[0][0][1]: \t [-0.33137256 -0.31960785 -0.32352942]\n",
      "train_data[0][0][0]: \t [ 0.5  0.5  0.5]\n",
      "train_data[0][0][1]: \t [ 0.5  0.5  0.5]\n",
      "train_data.shape\t (25600, 32, 32, 3)\n",
      "validation_data.shape\t (6400, 32, 32, 3)\n",
      "train_data size:\t 25600\n",
      "validation_data size:\t 6400\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = len(train_data) / 5\n",
    "\n",
    "VALIDATION_OFFSET = 0\n",
    "\n",
    "validation_data = train_data[VALIDATION_OFFSET : VALIDATION_OFFSET + VALIDATION_SIZE, :, :, :]\n",
    "validation_data = np.float32(validation_data)\n",
    "validation_labels = train_labels[VALIDATION_OFFSET : VALIDATION_OFFSET + VALIDATION_SIZE]\n",
    "validation_size = len(validation_data)\n",
    "\n",
    "train_data = np.append(train_data[ : VALIDATION_OFFSET, :, :, :], train_data[VALIDATION_OFFSET + VALIDATION_SIZE : , :, :, :], axis = 0)\n",
    "train_data = np.float32(train_data)\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = len(train_data)\n",
    "\n",
    "# to verify validation and train data are splitted properly\n",
    "print('validation_data[0][0][0]: \\t', validation_data[0][0][0])\n",
    "print('validation_data[0][0][1]: \\t', validation_data[0][0][1])\n",
    "print('train_data[0][0][0]: \\t', train_data[0][0][0])\n",
    "print('train_data[0][0][1]: \\t', train_data[0][0][1])\n",
    "\n",
    "print('train_data.shape\\t', train_data.shape)\n",
    "print('validation_data.shape\\t', validation_data.shape)\n",
    "print('train_data size:\\t', train_size)\n",
    "print('validation_data size:\\t', validation_size)\n",
    "#print('validation_data:', validation_data)\n",
    "#print('validation_labels: ', validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ALL_DATA_OFFSET = 0\n",
    "TRAIN_ALL_DATA_SIZE = 10000\n",
    "train_all_data_labels0 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_labels1 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_labels2 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_labels3 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "print(\"train_all_data_labels0: \", train_all_data_labels0)\n",
    "print(\"train_all_data_labels3: \", train_all_data_labels3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, N_LABELS))\n",
    "\n",
    "TRAIN_ALL_DATA_OFFSET = 0\n",
    "TRAIN_ALL_DATA_SIZE = 10000\n",
    "train_all_data_node0 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_node1 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_node2 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_node3 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([10, 10, N_CHANNELS, 16],  # 10x10 kernel, depth 16.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([16]))\n",
    "\n",
    "fc1_weights = tf.Variable(  # fully connected, depth N_LABELS.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "\n",
    "print('Variables Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1], #[image index, y, x, depth]\n",
    "                        padding='SAME')\n",
    "\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 4, 4, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    # Fully connected layers\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the \n",
    "    # '+' operation automatically broadcasts the biases.\n",
    "    return tf.matmul(reshape, fc1_weights) + fc1_biases\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training computation: logits + cross-entropy loss done\n"
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases))\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# compute only by {eval()} method.\n",
    "train_all_data_prediction0 = tf.nn.softmax(model(train_all_data_node0))\n",
    "train_all_data_prediction1 = tf.nn.softmax(model(train_all_data_node1))\n",
    "train_all_data_prediction2 = tf.nn.softmax(model(train_all_data_node2))\n",
    "train_all_data_prediction3 = tf.nn.softmax(model(train_all_data_node3))\n",
    "\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Training computation: logits + cross-entropy loss done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.as_default()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 3)\n",
      "(128, 10)\n",
      "Run graph done.\n"
     ]
    }
   ],
   "source": [
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# batch data\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = sess.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_labels.shape)\n",
    "print('Run graph done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00545826  0.14673056  0.00062576  0.03628518  0.62372977  0.04597657\n",
      "  0.11217882  0.00206118  0.00350966  0.02344421]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prediction 4\n",
      "(128, 10)\n",
      "All predictions [4 6 9 4 6 6 4 4 6 6 6 6 4 6 4 4 7 6 6 6 6 4 4 9 6 4 6 6 4 4 4 8 1 1 4 9 6\n",
      " 1 1 4 4 6 1 6 3 6 6 1 4 1 6 6 4 4 9 9 1 9 4 4 4 1 6 1 1 1 4 4 6 4 6 4 1 7\n",
      " 9 6 4 0 8 1 6 4 6 1 6 7 9 6 6 1 4 4 6 6 6 1 6 4 6 6 6 9 9 6 6 4 4 6 1 4 4\n",
      " 6 4 9 4 6 4 1 1 4 6 4 1 6 6 7 4 0]\n"
     ]
    }
   ],
   "source": [
    "# The highest probability in the first entry.\n",
    "print('First prediction', np.argmax(predictions[0]))\n",
    "print(predictions.shape)\n",
    "print('All predictions', np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels [1 6 6 8 8 3 4 6 0 6 0 3 6 6 5 4 8 3 2 6 0 3 1 4 0 6 6 2 7 6 9 0 4 5 7 1 6\n",
      " 7 9 1 7 7 8 0 3 7 4 7 3 1 0 4 6 6 1 4 9 2 6 4 5 0 4 6 0 8 3 4 8 8 3 9 5 7\n",
      " 1 9 4 7 9 1 9 7 5 2 7 3 4 8 8 2 1 5 9 2 7 8 8 6 8 8 1 3 8 8 5 4 7 1 6 6 1\n",
      " 6 1 6 7 0 4 6 9 5 8 7 1 9 0 3 3 7]\n"
     ]
    }
   ],
   "source": [
    "print('Batch labels', np.argmax(batch_labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_accuracy() defined\n",
      "get_all_train_data_accuracy() defined\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(predictions, labels):\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    accuracy = float(correct) / float(total)\n",
    "    accuracy_fig = \"\"\n",
    "    accuracy_fig = str(correct)\n",
    "    accuracy_fig += (\" of \")\n",
    "    accuracy_fig += str(total)\n",
    "    return accuracy, accuracy_fig\n",
    "\n",
    "print('get_accuracy() defined')\n",
    "\n",
    "def get_all_train_data_accuracy(\n",
    "    train_all_data_prediction0, train_all_data_labels0,\n",
    "    train_all_data_prediction1, train_all_data_labels1,\n",
    "    train_all_data_prediction2, train_all_data_labels2,\n",
    "    train_all_data_prediction3, train_all_data_labels3):\n",
    "    \n",
    "    correct0 = np.sum(np.argmax(train_all_data_prediction0, 1) == np.argmax(train_all_data_labels0, 1))\n",
    "    total0 = train_all_data_prediction0.shape[0]\n",
    "    print(\"correct0: %d, total0: %d\", (correct0, total0))\n",
    "    \n",
    "    correct1 = np.sum(np.argmax(train_all_data_prediction1, 1) == np.argmax(train_all_data_labels1, 1))\n",
    "    total1 = train_all_data_prediction1.shape[0]\n",
    "    print(\"correct1: %d, total1: %d\", (correct1, total1))\n",
    "    \n",
    "    correct2 = np.sum(np.argmax(train_all_data_prediction2, 1) == np.argmax(train_all_data_labels2, 1))\n",
    "    total2 = train_all_data_prediction2.shape[0]\n",
    "    print(\"correct2: %d, total2: %d\", (correct2, total2))\n",
    "    \n",
    "    correct3 = np.sum(np.argmax(train_all_data_prediction3, 1) == np.argmax(train_all_data_labels3, 1))\n",
    "    total3 = train_all_data_prediction3.shape[0]\n",
    "    print(\"correct3: %d, total3: %d\" % (correct3, total3))\n",
    "\n",
    "    correct = correct0 + correct1 + correct2 + correct3\n",
    "    total = total0 + total1 + total2 + total3\n",
    "    accuracy = float(correct) / float(total)\n",
    "    accuracy_fig = \"\"\n",
    "    accuracy_fig = str(correct)\n",
    "    accuracy_fig += (\" of \")\n",
    "    accuracy_fig += str(total)\n",
    "    return accuracy, accuracy_fig\n",
    "\n",
    "print('get_all_train_data_accuracy() defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 40000\n",
      "Validation accuracy: 12.900000% (1290 of 10000), Mini-batch loss: 3.29991, Learning rate: 0.01000\n",
      "Step 1000 of 40000\n",
      "Validation accuracy: 58.400000% (5840 of 10000), Mini-batch loss: 1.33052, Learning rate: 0.00857\n",
      "Step 2000 of 40000\n",
      "Validation accuracy: 62.220000% (6222 of 10000), Mini-batch loss: 1.21205, Learning rate: 0.00735\n",
      "Step 3000 of 40000\n",
      "Validation accuracy: 63.550000% (6355 of 10000), Mini-batch loss: 1.00598, Learning rate: 0.00630\n",
      "Step 4000 of 40000\n",
      "Validation accuracy: 64.330000% (6433 of 10000), Mini-batch loss: 0.97928, Learning rate: 0.00540\n",
      "Step 5000 of 40000\n",
      "Validation accuracy: 64.060000% (6406 of 10000), Mini-batch loss: 0.96670, Learning rate: 0.00440\n",
      "Step 6000 of 40000\n",
      "Validation accuracy: 64.380000% (6438 of 10000), Mini-batch loss: 0.84760, Learning rate: 0.00377\n",
      "Step 7000 of 40000\n",
      "Validation accuracy: 64.800000% (6480 of 10000), Mini-batch loss: 0.75450, Learning rate: 0.00324\n",
      "Step 8000 of 40000\n",
      "Validation accuracy: 64.680000% (6468 of 10000), Mini-batch loss: 0.82930, Learning rate: 0.00277\n",
      "Step 9000 of 40000\n",
      "Validation accuracy: 64.380000% (6438 of 10000), Mini-batch loss: 0.84117, Learning rate: 0.00238\n",
      "Step 10000 of 40000\n",
      "Validation accuracy: 65.480000% (6548 of 10000), Mini-batch loss: 0.81638, Learning rate: 0.00194\n",
      "Step 11000 of 40000\n",
      "Validation accuracy: 65.250000% (6525 of 10000), Mini-batch loss: 0.82717, Learning rate: 0.00166\n",
      "Step 12000 of 40000\n",
      "Validation accuracy: 65.030000% (6503 of 10000), Mini-batch loss: 0.75637, Learning rate: 0.00142\n",
      "Step 13000 of 40000\n",
      "Validation accuracy: 65.430000% (6543 of 10000), Mini-batch loss: 0.74461, Learning rate: 0.00122\n",
      "Step 14000 of 40000\n",
      "Validation accuracy: 65.520000% (6552 of 10000), Mini-batch loss: 0.78732, Learning rate: 0.00105\n",
      "Step 15000 of 40000\n",
      "Validation accuracy: 65.800000% (6580 of 10000), Mini-batch loss: 0.78647, Learning rate: 0.00085\n",
      "Step 16000 of 40000\n",
      "Validation accuracy: 65.420000% (6542 of 10000), Mini-batch loss: 0.66057, Learning rate: 0.00073\n",
      "Step 17000 of 40000\n",
      "Validation accuracy: 65.470000% (6547 of 10000), Mini-batch loss: 0.73215, Learning rate: 0.00063\n",
      "Step 18000 of 40000\n",
      "Validation accuracy: 65.900000% (6590 of 10000), Mini-batch loss: 0.68782, Learning rate: 0.00054\n",
      "Step 19000 of 40000\n",
      "Validation accuracy: 65.940000% (6594 of 10000), Mini-batch loss: 0.79165, Learning rate: 0.00046\n",
      "Step 20000 of 40000\n",
      "Validation accuracy: 65.990000% (6599 of 10000), Mini-batch loss: 0.79366, Learning rate: 0.00038\n",
      "Step 21000 of 40000\n",
      "Validation accuracy: 65.730000% (6573 of 10000), Mini-batch loss: 0.68241, Learning rate: 0.00032\n",
      "Step 22000 of 40000\n",
      "Validation accuracy: 66.000000% (6600 of 10000), Mini-batch loss: 0.84310, Learning rate: 0.00028\n",
      "Step 23000 of 40000\n",
      "Validation accuracy: 65.880000% (6588 of 10000), Mini-batch loss: 0.59629, Learning rate: 0.00024\n",
      "Step 24000 of 40000\n",
      "Validation accuracy: 65.940000% (6594 of 10000), Mini-batch loss: 0.70632, Learning rate: 0.00020\n",
      "Step 25000 of 40000\n",
      "Validation accuracy: 65.950000% (6595 of 10000), Mini-batch loss: 0.76906, Learning rate: 0.00017\n",
      "Step 26000 of 40000\n",
      "Validation accuracy: 65.900000% (6590 of 10000), Mini-batch loss: 0.75682, Learning rate: 0.00014\n",
      "Step 27000 of 40000\n",
      "Validation accuracy: 65.900000% (6590 of 10000), Mini-batch loss: 0.72488, Learning rate: 0.00012\n",
      "Step 28000 of 40000\n",
      "Validation accuracy: 65.900000% (6590 of 10000), Mini-batch loss: 0.57469, Learning rate: 0.00010\n",
      "Step 29000 of 40000\n",
      "Validation accuracy: 65.880000% (6588 of 10000), Mini-batch loss: 0.75461, Learning rate: 0.00009\n",
      "Step 30000 of 40000\n",
      "Validation accuracy: 65.930000% (6593 of 10000), Mini-batch loss: 0.82183, Learning rate: 0.00007\n",
      "Step 31000 of 40000\n",
      "Validation accuracy: 66.030000% (6603 of 10000), Mini-batch loss: 0.76993, Learning rate: 0.00006\n",
      "Step 32000 of 40000\n",
      "Validation accuracy: 65.880000% (6588 of 10000), Mini-batch loss: 0.73142, Learning rate: 0.00005\n",
      "Step 33000 of 40000\n",
      "Validation accuracy: 65.850000% (6585 of 10000), Mini-batch loss: 0.76326, Learning rate: 0.00005\n",
      "Step 34000 of 40000\n",
      "Validation accuracy: 65.920000% (6592 of 10000), Mini-batch loss: 0.70223, Learning rate: 0.00004\n",
      "Step 35000 of 40000\n",
      "Validation accuracy: 65.880000% (6588 of 10000), Mini-batch loss: 0.88056, Learning rate: 0.00003\n",
      "Step 36000 of 40000\n",
      "Validation accuracy: 66.020000% (6602 of 10000), Mini-batch loss: 0.74646, Learning rate: 0.00003\n",
      "Step 37000 of 40000\n",
      "Validation accuracy: 65.930000% (6593 of 10000), Mini-batch loss: 0.62416, Learning rate: 0.00002\n",
      "Step 38000 of 40000\n",
      "Validation accuracy: 66.000000% (6600 of 10000), Mini-batch loss: 0.75388, Learning rate: 0.00002\n",
      "Step 39000 of 40000\n",
      "Validation accuracy: 65.950000% (6595 of 10000), Mini-batch loss: 0.67959, Learning rate: 0.00002\n"
     ]
    }
   ],
   "source": [
    "epochs = 1 # due to the hardware constraints of my MacBook, I could only finish 1 epoch training given the time I have :(\n",
    "steps = train_size\n",
    "for epoch in range(epochs):\n",
    "    # 5-fold cross verfication\n",
    "    folds = 5\n",
    "    for fold = in range(folds):\n",
    "        # assign train data and validation data\n",
    "        VALIDATION_SIZE = len(train_data) / 5\n",
    "        VALIDATION_OFFSET = fold * VALIDATION_SIZE\n",
    "\n",
    "        validation_data = train_data[VALIDATION_OFFSET : VALIDATION_OFFSET + VALIDATION_SIZE, :, :, :]\n",
    "        validation_data = np.float32(validation_data)\n",
    "        validation_labels = train_labels[VALIDATION_OFFSET : VALIDATION_OFFSET + VALIDATION_SIZE]\n",
    "        validation_size = len(validation_data)\n",
    "\n",
    "        train_data = np.append(train_data[ : VALIDATION_OFFSET, :, :, :], train_data[VALIDATION_OFFSET + VALIDATION_SIZE : , :, :, :], axis = 0)\n",
    "        train_data = np.float32(train_data)\n",
    "        train_labels = train_labels[VALIDATION_SIZE:]\n",
    "        train_size = len(train_data)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "            batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "            feed_dict = {train_data_node: batch_data, train_labels_node: batch_labels}\n",
    "            _, l, lr, predictions = sess.run([optimizer, loss, learning_rate, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                print('Epoch %d of %d, Step %d of %d, Fold %d of %d' % (epoch, epochs, step, steps, fold, folds))\n",
    "                validation_accuracy, validation_accuracy_fig = get_accuracy(\n",
    "                      validation_prediction.eval(), validation_labels)\n",
    "                print('Validation accuracy: %.6f%% (%s), Mini-batch loss: %.5f, Learning rate: %.5f' % \n",
    "                      (validation_accuracy * 100, validation_accuracy_fig, l, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct0: %d, total0: %d (7801, 10000)\n",
      "correct1: %d, total1: %d (7797, 10000)\n",
      "correct2: %d, total2: %d (7714, 10000)\n",
      "correct3: %d, total3: %d (7773, 10000)\n",
      "Train accuracy: 77.7125% (31085 of 40000)\n"
     ]
    }
   ],
   "source": [
    "train_accuracy, train_accuracy_fig = get_all_train_data_accuracy(\n",
    "    train_all_data_prediction0.eval(), train_all_data_labels0,\n",
    "    train_all_data_prediction1.eval(), train_all_data_labels1,\n",
    "    train_all_data_prediction2.eval(), train_all_data_labels2,\n",
    "    train_all_data_prediction3.eval(), train_all_data_labels3)\n",
    "print('Train accuracy: %.4f%% (%s)' % (train_accuracy * 100, train_accuracy_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 66.0400% (6604 of 10000)\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_accuracy_fig = get_accuracy(\n",
    "    test_prediction.eval(), test_labels)\n",
    "print('Test accuracy: %.4f%% (%s)' % (test_accuracy * 100, test_accuracy_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
