{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Reference:\n",
    "    \"3_mnist_from_scratch from\", \"docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\"\n",
    "    https://github.com/michael-iuzzolino/CIFAR_reader\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "from CIFAR_reader import CIFAR_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CIFAR data...\n",
      "Extracting Data...\n",
      "Unpacking data...\n",
      "Loading training batch 1 of 5...\n",
      "Loading training batch 2 of 5...\n",
      "Loading training batch 3 of 5...\n",
      "Loading training batch 4 of 5...\n",
      "Loading training batch 5 of 5...\n",
      "Loading testing batch 1 of 1...\n"
     ]
    }
   ],
   "source": [
    "cifar = CIFAR_reader(one_hot=True, verbose=True, img_size=32, num_classes=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_TRAIN_IMAGE,  50000\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 32\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 128\n",
    "N_CHANNELS = 3\n",
    "N_LABELS = 10\n",
    "SEED = 32\n",
    "\n",
    "training_data = cifar.train\n",
    "training_labels = cifar.labels\n",
    "train_data = training_data['data']\n",
    "train_labels = training_data['labels']\n",
    "\n",
    "testing_data = cifar.test\n",
    "test_data = testing_data['data']\n",
    "test_data = np.float32(test_data)\n",
    "test_labels = testing_data['labels']\n",
    "test_labels = np.float32(test_labels)\n",
    "\n",
    "# convert train and test data values from [0, 255] to [-0.5, 0.5]\n",
    "N_TRAIN_IMAGE = len(train_data)\n",
    "train_data = (train_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "train_data = train_data.reshape(N_TRAIN_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "print('N_TRAIN_IMAGE, ', N_TRAIN_IMAGE)\n",
    "\n",
    "N_TEST_IMAGE = len(test_data)\n",
    "test_data = (test_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "test_data = test_data.reshape(N_TEST_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "#print(\"train_data[0]:\", train_data[0])\n",
    "#print(\"test_data[0]:\", test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fd7ff90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar.preview_data(data_set=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_data[0][0][0]: \t [-0.26862746 -0.25686276 -0.25294119]\n",
      "validation_data[0][0][1]: \t [-0.33137256 -0.31960785 -0.32352942]\n",
      "train_data[0][0][0]: \t [-0.36274511 -0.40196079 -0.39803922]\n",
      "train_data[0][0][1]: \t [-0.39411765 -0.41764706 -0.41764706]\n",
      "train_data.shape\t (40000, 32, 32, 3)\n",
      "validation_data.shape\t (10000, 32, 32, 3)\n",
      "train_data size:\t 40000\n",
      "validation_data size:\t 10000\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = len(train_data) / 5\n",
    "\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_data = np.float32(validation_data)\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "validation_size = len(validation_data)\n",
    "\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_data = np.float32(train_data)\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = len(train_data)\n",
    "\n",
    "# to verify validation and train data are splitted properly\n",
    "print('validation_data[0][0][0]: \\t', validation_data[0][0][0])\n",
    "print('validation_data[0][0][1]: \\t', validation_data[0][0][1])\n",
    "print('train_data[0][0][0]: \\t', train_data[0][0][0])\n",
    "print('train_data[0][0][1]: \\t', train_data[0][0][1])\n",
    "\n",
    "print('train_data.shape\\t', train_data.shape)\n",
    "print('validation_data.shape\\t', validation_data.shape)\n",
    "print('train_data size:\\t', train_size)\n",
    "print('validation_data size:\\t', validation_size)\n",
    "#print('validation_data:', validation_data)\n",
    "#print('validation_labels: ', validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_all_data_labels0:  [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "train_all_data_labels3:  [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  1.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_ALL_DATA_OFFSET = 0\n",
    "TRAIN_ALL_DATA_SIZE = 10000\n",
    "train_all_data_labels0 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_labels1 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_labels2 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_labels3 = train_labels[TRAIN_ALL_DATA_OFFSET : TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE]\n",
    "print(\"train_all_data_labels0: \", train_all_data_labels0)\n",
    "print(\"train_all_data_labels3: \", train_all_data_labels3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, N_LABELS))\n",
    "\n",
    "TRAIN_ALL_DATA_OFFSET = 0\n",
    "TRAIN_ALL_DATA_SIZE = 10000\n",
    "train_all_data_node0 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_node1 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_node2 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "TRAIN_ALL_DATA_OFFSET += TRAIN_ALL_DATA_SIZE\n",
    "train_all_data_node3 = tf.constant(train_data[TRAIN_ALL_DATA_OFFSET: TRAIN_ALL_DATA_OFFSET + TRAIN_ALL_DATA_SIZE])\n",
    "\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([10, 10, N_CHANNELS, 64],  # 10x10 kernel, depth 64.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([64]))\n",
    "\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([10, 10, 64, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "\n",
    "conv3_weights = tf.Variable(\n",
    "  tf.truncated_normal([10, 10, 64, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv3_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "\n",
    "conv4_weights = tf.Variable(\n",
    "  tf.truncated_normal([10, 10, 64, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv4_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 4, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([256, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "fc3_weights = tf.Variable(\n",
    "  tf.truncated_normal([256, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc3_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "\n",
    "fc4_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc4_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "\n",
    "print('Variables Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "\n",
    "    ### group layer1\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1], #[image index, y, x, depth]\n",
    "                        padding='SAME')\n",
    "\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 4, 4, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    \n",
    "    ### group layer2\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    \n",
    "    ### group layer3\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv3_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    \n",
    "    ### group layer4\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv4_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    \n",
    "\n",
    "    # Fully connected layers\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the \n",
    "    # '+' operation automatically broadcasts the biases.\n",
    "    hidden1 = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "    if train:\n",
    "        hidden1 = tf.nn.dropout(hidden1, 0.5, seed=SEED) # drop out rate 50%\n",
    "    hidden2 = tf.nn.relu(tf.matmul(reshape, fc2_weights) + fc2_biases)\n",
    "    if train:\n",
    "        hidden2 = tf.nn.dropout(hidden2, 0.5, seed=SEED) # drop out rate 50%\n",
    "    hidden3 = tf.nn.relu(tf.matmul(reshape, fc3_weights) + fc3_biases)\n",
    "    if train:\n",
    "        hidden3 = tf.nn.dropout(hidden3, 0.5, seed=SEED) # drop out rate 50%\n",
    "    return tf.matmul(hidden3, fc4_weights) + fc4_biases\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training computation: logits + cross-entropy loss done\n"
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases) +\n",
    "                tf.nn.l2_loss(fc3_weights) + tf.nn.l2_loss(fc3_biases) +\n",
    "                tf.nn.l2_loss(fc4_weights) + tf.nn.l2_loss(fc4_biases))\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# compute only by {eval()} method.\n",
    "train_all_data_prediction0 = tf.nn.softmax(model(train_all_data_node0))\n",
    "train_all_data_prediction1 = tf.nn.softmax(model(train_all_data_node1))\n",
    "train_all_data_prediction2 = tf.nn.softmax(model(train_all_data_node2))\n",
    "train_all_data_prediction3 = tf.nn.softmax(model(train_all_data_node3))\n",
    "\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Training computation: logits + cross-entropy loss done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.as_default()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 3)\n",
      "(128, 10)\n",
      "Run graph done.\n"
     ]
    }
   ],
   "source": [
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# batch data\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = sess.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_labels.shape)\n",
    "print('Run graph done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   7.06086315e-37   2.27244345e-05   0.00000000e+00   9.99977231e-01\n",
      "   0.00000000e+00   7.95310739e-30]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prediction 7\n",
      "(128, 10)\n",
      "All predictions [7 5 5 5 1 5 5 2 6 2 5 2 5 6 2 5 5 2 5 9 5 4 9 1 5 5 5 5 5 5 5 1 9 5 5 9 4\n",
      " 9 5 5 5 4 5 4 5 9 4 5 2 2 5 5 5 5 5 4 1 5 5 5 4 4 0 5 5 6 5 5 5 9 4 5 1 5\n",
      " 5 0 9 5 4 7 5 5 9 5 9 9 9 5 5 4 5 5 5 4 4 1 6 6 2 5 5 5 5 9 5 5 5 5 4 5 5\n",
      " 9 5 2 5 6 5 2 5 4 9 5 4 4 2 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "# The highest probability in the first entry.\n",
    "print('First prediction', np.argmax(predictions[0]))\n",
    "print(predictions.shape)\n",
    "print('All predictions', np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels [1 6 6 8 8 3 4 6 0 6 0 3 6 6 5 4 8 3 2 6 0 3 1 4 0 6 6 2 7 6 9 0 4 5 7 1 6\n",
      " 7 9 1 7 7 8 0 3 7 4 7 3 1 0 4 6 6 1 4 9 2 6 4 5 0 4 6 0 8 3 4 8 8 3 9 5 7\n",
      " 1 9 4 7 9 1 9 7 5 2 7 3 4 8 8 2 1 5 9 2 7 8 8 6 8 8 1 3 8 8 5 4 7 1 6 6 1\n",
      " 6 1 6 7 0 4 6 9 5 8 7 1 9 0 3 3 7]\n"
     ]
    }
   ],
   "source": [
    "print('Batch labels', np.argmax(batch_labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_accuracy() defined\n",
      "get_all_train_data_accuracy() defined\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(predictions, labels):\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    accuracy = float(correct) / float(total)\n",
    "    accuracy_fig = \"\"\n",
    "    accuracy_fig = str(correct)\n",
    "    accuracy_fig += (\" of \")\n",
    "    accuracy_fig += str(total)\n",
    "    return accuracy, accuracy_fig\n",
    "\n",
    "print('get_accuracy() defined')\n",
    "\n",
    "def get_all_train_data_accuracy(\n",
    "    train_all_data_prediction0, train_all_data_labels0,\n",
    "    train_all_data_prediction1, train_all_data_labels1,\n",
    "    train_all_data_prediction2, train_all_data_labels2,\n",
    "    train_all_data_prediction3, train_all_data_labels3):\n",
    "    \n",
    "    correct0 = np.sum(np.argmax(train_all_data_prediction0, 1) == np.argmax(train_all_data_labels0, 1))\n",
    "    total0 = train_all_data_prediction0.shape[0]\n",
    "    print(\"correct0: %d, total0: %d\", (correct0, total0))\n",
    "    \n",
    "    correct1 = np.sum(np.argmax(train_all_data_prediction1, 1) == np.argmax(train_all_data_labels1, 1))\n",
    "    total1 = train_all_data_prediction1.shape[0]\n",
    "    print(\"correct1: %d, total1: %d\", (correct1, total1))\n",
    "    \n",
    "    correct2 = np.sum(np.argmax(train_all_data_prediction2, 1) == np.argmax(train_all_data_labels2, 1))\n",
    "    total2 = train_all_data_prediction2.shape[0]\n",
    "    print(\"correct2: %d, total2: %d\", (correct2, total2))\n",
    "    \n",
    "    correct3 = np.sum(np.argmax(train_all_data_prediction3, 1) == np.argmax(train_all_data_labels3, 1))\n",
    "    total3 = train_all_data_prediction3.shape[0]\n",
    "    print(\"correct3: %d, total3: %d\", (correct3, total3))\n",
    "\n",
    "    correct = correct0 + correct1 + correct2 + correct3\n",
    "    total = total0 + total1 + total2 + total3\n",
    "    accuracy = float(correct) / float(total)\n",
    "    accuracy_fig = \"\"\n",
    "    accuracy_fig = str(correct)\n",
    "    accuracy_fig += (\" of \")\n",
    "    accuracy_fig += str(total)\n",
    "    return accuracy, accuracy_fig\n",
    "\n",
    "print('get_all_train_data_accuracy() defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 50, Step 0 of 40000\n",
      "Validation accuracy: 9.900000% (990 of 10000), Mini-batch loss: 155.53656, Learning rate: 0.01000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-62145d95f634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtrain_data_node\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_node\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "steps = train_size\n",
    "for epoch in range(epochs):\n",
    "    for step in range(steps):\n",
    "        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "        batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "        feed_dict = {train_data_node: batch_data, train_labels_node: batch_labels}\n",
    "        _, l, lr, predictions = sess.run([optimizer, loss, learning_rate, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('Epoch %d of %d, Step %d of %d' % (epoch, epochs, step, steps))\n",
    "            validation_accuracy, validation_accuracy_fig = get_accuracy(\n",
    "                  validation_prediction.eval(), validation_labels)\n",
    "            print('Validation accuracy: %.6f%% (%s), Mini-batch loss: %.5f, Learning rate: %.5f' % \n",
    "                  (validation_accuracy * 100, validation_accuracy_fig, l, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_accuracy, train_accuracy_fig = get_all_train_data_accuracy(\n",
    "    train_all_data_prediction0.eval(), train_all_data_labels0,\n",
    "    train_all_data_prediction1.eval(), train_all_data_labels1,\n",
    "    train_all_data_prediction2.eval(), train_all_data_labels2,\n",
    "    train_all_data_prediction3.eval(), train_all_data_labels3)\n",
    "print('Train accuracy: %.4f%% (%s)' % (train_accuracy * 100, train_accuracy_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_accuracy, test_accuracy_fig = get_accuracy(\n",
    "    test_prediction.eval(), test_labels)\n",
    "print('Test accuracy: %.4f%% (%s)' % (test_accuracy * 100, test_accuracy_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
