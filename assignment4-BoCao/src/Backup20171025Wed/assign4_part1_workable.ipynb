{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Some of the code is modified from \"3_mnist_from_scratch from\", \"docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\"\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "from CIFAR_reader import CIFAR_reader # Reference: https://github.com/michael-iuzzolino/CIFAR_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CIFAR data...\n",
      "- Download progress: 100.0%\n",
      "\n",
      "Extracting Data...\n",
      "Unpacking data...\n",
      "Loading training batch 1 of 5...\n",
      "Loading training batch 2 of 5...\n",
      "Loading training batch 3 of 5...\n",
      "Loading training batch 4 of 5...\n",
      "Loading training batch 5 of 5...\n",
      "Loading testing batch 1 of 1...\n"
     ]
    }
   ],
   "source": [
    "cifar = CIFAR_reader(one_hot=True, verbose=True, img_size=32, num_classes=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 128\n",
    "N_CHANNELS = 3\n",
    "N_LABELS = 10\n",
    "SEED = 32\n",
    "\n",
    "training_data = cifar.train\n",
    "training_labels = cifar.labels\n",
    "train_data = training_data['data']\n",
    "train_labels = training_data['labels']\n",
    "\n",
    "testing_data = cifar.test\n",
    "test_data = testing_data['data']\n",
    "test_data = np.float32(test_data)\n",
    "test_labels = testing_data['labels']\n",
    "test_labels = np.float32(test_labels)\n",
    "\n",
    "N_IMAGE = len(train_data)\n",
    "train_data = (train_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "train_data = train_data.reshape(N_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "N_IMAGE = len(test_data)\n",
    "test_data = (test_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "test_data = test_data.reshape(N_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "#print(\"train_data[0]:\", train_data[0])\n",
    "#print(\"test_data[0]:\", test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118281d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar.preview_data(data_set=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape (40000, 32, 32, 3)\n",
      "validation_data.shape (10000, 32, 32, 3)\n",
      "train_data size: 40000\n",
      "validation_data size: 10000\n",
      "validation_data: [[[[-0.36274511 -0.40196079 -0.39803922]\n",
      "   [-0.39411765 -0.41764706 -0.41764706]\n",
      "   [-0.40196079 -0.42156863 -0.4254902 ]\n",
      "   ..., \n",
      "   [ 0.01764706  0.00588235  0.00588235]\n",
      "   [ 0.02156863 -0.0254902  -0.04509804]\n",
      "   [-0.00588235 -0.04901961 -0.05686275]]\n",
      "\n",
      "  [[-0.25294119 -0.28431374 -0.30392158]\n",
      "   [-0.37450981 -0.39411765 -0.41764706]\n",
      "   [-0.43333334 -0.44901961 -0.46862745]\n",
      "   ..., \n",
      "   [-0.1        -0.12745099 -0.15490197]\n",
      "   [-0.0882353  -0.1509804  -0.20196079]\n",
      "   [-0.10784314 -0.1627451  -0.19803922]]\n",
      "\n",
      "  [[-0.11176471 -0.14313726 -0.17058824]\n",
      "   [-0.30784315 -0.32352942 -0.35490197]\n",
      "   [-0.44117647 -0.45294118 -0.48039216]\n",
      "   ..., \n",
      "   [-0.31960785 -0.33137256 -0.34705883]\n",
      "   [-0.29607844 -0.3392157  -0.36666667]\n",
      "   [-0.29607844 -0.32745099 -0.3392157 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.1509804   0.14705883  0.17058824]\n",
      "   [ 0.14313726  0.13921569  0.1509804 ]\n",
      "   [ 0.14313726  0.14313726  0.14705883]\n",
      "   ..., \n",
      "   [ 0.17843138  0.17450981  0.16666667]\n",
      "   [ 0.16666667  0.16666667  0.15882353]\n",
      "   [ 0.15490197  0.15490197  0.15490197]]\n",
      "\n",
      "  [[ 0.1627451   0.15882353  0.19019608]\n",
      "   [ 0.1627451   0.15882353  0.17843138]\n",
      "   [ 0.15882353  0.15882353  0.17058824]\n",
      "   ..., \n",
      "   [ 0.17450981  0.17058824  0.16666667]\n",
      "   [ 0.15882353  0.15490197  0.15490197]\n",
      "   [ 0.14705883  0.14705883  0.1509804 ]]\n",
      "\n",
      "  [[ 0.17843138  0.17450981  0.20196079]\n",
      "   [ 0.18627451  0.18235295  0.19803922]\n",
      "   [ 0.17843138  0.17843138  0.18627451]\n",
      "   ..., \n",
      "   [ 0.16666667  0.15882353  0.1627451 ]\n",
      "   [ 0.15882353  0.15490197  0.15882353]\n",
      "   [ 0.1509804   0.1509804   0.15882353]]]\n",
      "\n",
      "\n",
      " [[[-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42941177 -0.44901961 -0.46078432]\n",
      "   ..., \n",
      "   [-0.42156863 -0.43725491 -0.43725491]\n",
      "   [-0.41764706 -0.43725491 -0.44509804]\n",
      "   [-0.41764706 -0.43725491 -0.44901961]]\n",
      "\n",
      "  [[-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42941177 -0.44901961 -0.46078432]\n",
      "   ..., \n",
      "   [-0.42156863 -0.43725491 -0.44117647]\n",
      "   [-0.41764706 -0.43725491 -0.44901961]\n",
      "   [-0.41764706 -0.43725491 -0.44901961]]\n",
      "\n",
      "  [[-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42156863 -0.44117647 -0.45294118]\n",
      "   [-0.42941177 -0.44901961 -0.46078432]\n",
      "   ..., \n",
      "   [-0.42156863 -0.43725491 -0.44509804]\n",
      "   [-0.41764706 -0.43725491 -0.44901961]\n",
      "   [-0.41764706 -0.43725491 -0.44901961]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.24117647 -0.28823531 -0.3392157 ]\n",
      "   [-0.18627451 -0.2372549  -0.29215688]\n",
      "   [-0.31568629 -0.36274511 -0.4254902 ]\n",
      "   ..., \n",
      "   [ 0.0254902   0.0254902  -0.10784314]\n",
      "   [-0.06862745 -0.05686275 -0.19803922]\n",
      "   [-0.11568628 -0.1        -0.24117647]]\n",
      "\n",
      "  [[-0.2647059  -0.31176472 -0.37058824]\n",
      "   [-0.28431374 -0.33137256 -0.39411765]\n",
      "   [-0.30392158 -0.3509804  -0.4137255 ]\n",
      "   ..., \n",
      "   [-0.01764706 -0.00980392 -0.17450981]\n",
      "   [-0.19019608 -0.18235295 -0.33529413]\n",
      "   [-0.21764706 -0.20980392 -0.3509804 ]]\n",
      "\n",
      "  [[-0.24901961 -0.28823531 -0.3509804 ]\n",
      "   [-0.28431374 -0.32352942 -0.38627452]\n",
      "   [-0.31176472 -0.3509804  -0.41764706]\n",
      "   ..., \n",
      "   [ 0.10784314  0.11568628 -0.06470589]\n",
      "   [ 0.03333334  0.0372549  -0.11960784]\n",
      "   [-0.15490197 -0.1509804  -0.30000001]]]\n",
      "\n",
      "\n",
      " [[[-0.04509804 -0.09607843 -0.28039217]\n",
      "   [-0.04901961 -0.0882353  -0.26862746]\n",
      "   [ 0.10784314  0.00196078 -0.17843138]\n",
      "   ..., \n",
      "   [ 0.18627451  0.01764706 -0.19803922]\n",
      "   [ 0.1627451   0.02156863 -0.21764706]\n",
      "   [ 0.05686275 -0.03333334 -0.29215688]]\n",
      "\n",
      "  [[-0.04509804 -0.1        -0.27254903]\n",
      "   [-0.02156863 -0.07647059 -0.24509804]\n",
      "   [ 0.1        -0.0254902  -0.19019608]\n",
      "   ..., \n",
      "   [ 0.0882353  -0.06470589 -0.2764706 ]\n",
      "   [ 0.06862745 -0.0254902  -0.2647059 ]\n",
      "   [ 0.02156863 -0.01764706 -0.28823531]]\n",
      "\n",
      "  [[-0.12745099 -0.1627451  -0.3392157 ]\n",
      "   [-0.11568628 -0.17058824 -0.32745099]\n",
      "   [ 0.05294118 -0.08431373 -0.2372549 ]\n",
      "   ..., \n",
      "   [ 0.06862745 -0.06078431 -0.27254903]\n",
      "   [-0.00588235 -0.06470589 -0.30000001]\n",
      "   [-0.00196078 -0.00980392 -0.25686276]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.19803922 -0.25294119 -0.38627452]\n",
      "   [-0.15490197 -0.21764706 -0.35490197]\n",
      "   [-0.2254902  -0.26862746 -0.39411765]\n",
      "   ..., \n",
      "   [-0.31176472 -0.34705883 -0.42156863]\n",
      "   [-0.04509804 -0.07647059 -0.17058824]\n",
      "   [ 0.12352941  0.05686275 -0.02156863]]\n",
      "\n",
      "  [[-0.28431374 -0.35490197 -0.43725491]\n",
      "   [-0.24509804 -0.31960785 -0.40588236]\n",
      "   [-0.23333333 -0.29215688 -0.38235295]\n",
      "   ..., \n",
      "   [-0.33529413 -0.38235295 -0.44901961]\n",
      "   [-0.00588235 -0.05294118 -0.14705883]\n",
      "   [ 0.12745099  0.07647059 -0.00980392]]\n",
      "\n",
      "  [[-0.19411765 -0.2764706  -0.35490197]\n",
      "   [-0.21764706 -0.30392158 -0.38235295]\n",
      "   [-0.2372549  -0.30392158 -0.37450981]\n",
      "   ..., \n",
      "   [-0.29607844 -0.35490197 -0.42941177]\n",
      "   [-0.01372549 -0.06862745 -0.17058824]\n",
      "   [ 0.10784314  0.06470589 -0.01372549]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[-0.00196078  0.12745099  0.10784314]\n",
      "   [ 0.04509804  0.17058824  0.17058824]\n",
      "   [ 0.10784314  0.24901961  0.22156863]\n",
      "   ..., \n",
      "   [-0.06862745  0.0882353   0.12745099]\n",
      "   [-0.02941176  0.11568628  0.14313726]\n",
      "   [-0.02941176  0.11568628  0.13921569]]\n",
      "\n",
      "  [[ 0.06078431  0.17058824  0.16666667]\n",
      "   [ 0.08431373  0.19411765  0.19411765]\n",
      "   [ 0.10784314  0.21764706  0.20196079]\n",
      "   ..., \n",
      "   [-0.01764706  0.15490197  0.17843138]\n",
      "   [ 0.0372549   0.17058824  0.19019608]\n",
      "   [ 0.06078431  0.19019608  0.20588236]]\n",
      "\n",
      "  [[ 0.07647059  0.1627451   0.17058824]\n",
      "   [ 0.05294118  0.13137256  0.13529412]\n",
      "   [ 0.05294118  0.12352941  0.11568628]\n",
      "   ..., \n",
      "   [-0.0254902   0.1627451   0.1627451 ]\n",
      "   [ 0.00588235  0.17058824  0.1627451 ]\n",
      "   [ 0.01372549  0.17450981  0.16666667]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.36274511  0.34705883  0.3509804 ]\n",
      "   [ 0.35882354  0.34313726  0.34705883]\n",
      "   [ 0.35882354  0.34313726  0.34705883]\n",
      "   ..., \n",
      "   [ 0.32745099  0.29607844  0.30784315]\n",
      "   [ 0.30784315  0.29607844  0.30000001]\n",
      "   [ 0.30392158  0.28823531  0.29215688]]\n",
      "\n",
      "  [[ 0.36274511  0.34705883  0.3509804 ]\n",
      "   [ 0.35490197  0.3392157   0.34313726]\n",
      "   [ 0.35882354  0.34313726  0.34705883]\n",
      "   ..., \n",
      "   [ 0.32352942  0.29215688  0.30392158]\n",
      "   [ 0.29215688  0.28039217  0.28431374]\n",
      "   [ 0.29607844  0.28039217  0.28431374]]\n",
      "\n",
      "  [[ 0.3509804   0.33529413  0.3392157 ]\n",
      "   [ 0.34705883  0.33137256  0.33529413]\n",
      "   [ 0.34705883  0.33137256  0.33529413]\n",
      "   ..., \n",
      "   [ 0.29215688  0.2647059   0.27254903]\n",
      "   [ 0.26078433  0.24901961  0.25294119]\n",
      "   [ 0.25686276  0.24509804  0.24901961]]]\n",
      "\n",
      "\n",
      " [[[ 0.24509804  0.21764706  0.19019608]\n",
      "   [ 0.28431374  0.24509804  0.19411765]\n",
      "   [ 0.31568629  0.27254903  0.20588236]\n",
      "   ..., \n",
      "   [ 0.28431374  0.25294119  0.14705883]\n",
      "   [ 0.32352942  0.30392158  0.21764706]\n",
      "   [ 0.30784315  0.30000001  0.24901961]]\n",
      "\n",
      "  [[ 0.17843138  0.1509804   0.11176471]\n",
      "   [ 0.24509804  0.19803922  0.13137256]\n",
      "   [ 0.30784315  0.24901961  0.15882353]\n",
      "   ..., \n",
      "   [ 0.28823531  0.23333333  0.09607843]\n",
      "   [ 0.31960785  0.28039217  0.17450981]\n",
      "   [ 0.30392158  0.28039217  0.22156863]]\n",
      "\n",
      "  [[ 0.17843138  0.15882353  0.11960784]\n",
      "   [ 0.24901961  0.20980392  0.14313726]\n",
      "   [ 0.31568629  0.26078433  0.17450981]\n",
      "   ..., \n",
      "   [ 0.31568629  0.24901961  0.13137256]\n",
      "   [ 0.3392157   0.28431374  0.19411765]\n",
      "   [ 0.31176472  0.28039217  0.23333333]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.15490197  0.13921569  0.11176471]\n",
      "   [ 0.19411765  0.16666667  0.12352941]\n",
      "   [ 0.23333333  0.20196079  0.14705883]\n",
      "   ..., \n",
      "   [ 0.20980392  0.20588236  0.17058824]\n",
      "   [ 0.25294119  0.25294119  0.2254902 ]\n",
      "   [ 0.2647059   0.2647059   0.24901961]]\n",
      "\n",
      "  [[ 0.15490197  0.13921569  0.11568628]\n",
      "   [ 0.21372549  0.17843138  0.13529412]\n",
      "   [ 0.2647059   0.22156863  0.1627451 ]\n",
      "   ..., \n",
      "   [ 0.22941177  0.20588236  0.1627451 ]\n",
      "   [ 0.25686276  0.25294119  0.21764706]\n",
      "   [ 0.2647059   0.2647059   0.24901961]]\n",
      "\n",
      "  [[ 0.1627451   0.15490197  0.13529412]\n",
      "   [ 0.20588236  0.18235295  0.14705883]\n",
      "   [ 0.24509804  0.21764706  0.17450981]\n",
      "   ..., \n",
      "   [ 0.19803922  0.17843138  0.13921569]\n",
      "   [ 0.25294119  0.24509804  0.21372549]\n",
      "   [ 0.26862746  0.26862746  0.25294119]]]\n",
      "\n",
      "\n",
      " [[[ 0.19411765  0.12745099  0.02941176]\n",
      "   [ 0.18235295  0.12352941  0.0254902 ]\n",
      "   [ 0.21372549  0.14705883  0.04901961]\n",
      "   ..., \n",
      "   [ 0.29215688  0.2254902   0.12745099]\n",
      "   [ 0.28431374  0.20980392  0.12745099]\n",
      "   [ 0.28039217  0.19411765  0.11176471]]\n",
      "\n",
      "  [[ 0.19803922  0.13529412  0.0372549 ]\n",
      "   [ 0.19803922  0.13137256  0.0372549 ]\n",
      "   [ 0.20588236  0.14313726  0.04509804]\n",
      "   ..., \n",
      "   [ 0.32352942  0.23333333  0.13529412]\n",
      "   [ 0.34313726  0.22941177  0.1509804 ]\n",
      "   [ 0.35490197  0.2372549   0.15882353]]\n",
      "\n",
      "  [[ 0.2372549   0.18235295  0.08039216]\n",
      "   [ 0.24117647  0.17058824  0.07647059]\n",
      "   [ 0.22941177  0.1627451   0.06862745]\n",
      "   ..., \n",
      "   [ 0.37058824  0.25294119  0.17058824]\n",
      "   [ 0.39411765  0.25294119  0.17843138]\n",
      "   [ 0.38235295  0.2372549   0.1627451 ]]\n",
      "\n",
      "  ..., \n",
      "  [[-0.20196079 -0.30392158 -0.2764706 ]\n",
      "   [-0.22941177 -0.31960785 -0.30784315]\n",
      "   [-0.20196079 -0.28823531 -0.26078433]\n",
      "   ..., \n",
      "   [ 0.19019608  0.1509804   0.03333334]\n",
      "   [ 0.17843138  0.11960784  0.01372549]\n",
      "   [ 0.22156863  0.13921569  0.0372549 ]]\n",
      "\n",
      "  [[-0.02941176 -0.1509804  -0.13137256]\n",
      "   [-0.06862745 -0.17450981 -0.14705883]\n",
      "   [-0.08039216 -0.17843138 -0.13529412]\n",
      "   ..., \n",
      "   [ 0.1509804   0.09607843 -0.00588235]\n",
      "   [ 0.12745099  0.06470589 -0.02941176]\n",
      "   [ 0.14313726  0.07254902 -0.01764706]]\n",
      "\n",
      "  [[ 0.00588235 -0.1627451  -0.1627451 ]\n",
      "   [-0.00980392 -0.15490197 -0.12745099]\n",
      "   [-0.02156863 -0.14705883 -0.11568628]\n",
      "   ..., \n",
      "   [ 0.11960784  0.05294118 -0.03333334]\n",
      "   [ 0.14313726  0.07254902 -0.00196078]\n",
      "   [ 0.17058824  0.11176471  0.03333334]]]]\n",
      "validation_labels:  [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = len(train_data)\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_data = np.float32(validation_data)\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "validation_size = len(validation_data)\n",
    "\n",
    "print('train_data.shape', train_data.shape)\n",
    "print('validation_data.shape', validation_data.shape)\n",
    "print('train_data size:', train_size)\n",
    "print('validation_data size:', validation_size)\n",
    "print('validation_data:', validation_data)\n",
    "print('validation_labels: ', validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, N_LABELS))\n",
    "\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, N_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "\n",
    "print('Variables Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training computation: logits + cross-entropy loss done\n"
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Training computation: logits + cross-entropy loss done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.as_default()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 3)\n",
      "(128, 10)\n",
      "Run graph done.\n"
     ]
    }
   ],
   "source": [
    "# Grab the first BATCH_SIZE examples and labels.\n",
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# This dictionary maps the batch data (as a numpy array) to the\n",
    "# node in the graph it should be fed to.\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = s.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_labels.shape)\n",
    "print('Run graph done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.16982765e-01   1.42972870e-03   1.97592890e-05   3.99296312e-07\n",
      "   8.81408807e-04   1.68743473e-03   1.40850125e-05   8.78815353e-01\n",
      "   1.66529047e-04   2.52190534e-06]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prediction 7\n",
      "(128, 10)\n",
      "All predictions [7 5 5 1 5 1 6 0 0 7 5 4 2 1 5 4 5 2 0 1 0 0 5 5 5 1 1 5 4 5 7 5 1 0 2 0 5\n",
      " 0 5 5 5 5 0 5 5 5 5 5 1 7 0 5 1 5 5 4 1 1 5 1 5 5 5 5 5 0 5 5 5 2 0 7 0 5\n",
      " 7 0 1 5 5 7 9 7 4 5 5 9 1 5 5 0 5 5 5 5 5 5 5 1 5 5 0 5 4 0 5 5 4 0 5 1 4\n",
      " 4 4 2 0 5 5 1 0 5 1 0 5 7 5 4 5 7]\n"
     ]
    }
   ],
   "source": [
    "# The highest probability in the first entry.\n",
    "print('First prediction', np.argmax(predictions[0]))\n",
    "\n",
    "# But, predictions is actually a list of BATCH_SIZE probability vectors.\n",
    "print(predictions.shape)\n",
    "\n",
    "# So, we'll take the highest probability for each vector.\n",
    "print('All predictions', np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels [1 6 6 8 8 3 4 6 0 6 0 3 6 6 5 4 8 3 2 6 0 3 1 4 0 6 6 2 7 6 9 0 4 5 7 1 6\n",
      " 7 9 1 7 7 8 0 3 7 4 7 3 1 0 4 6 6 1 4 9 2 6 4 5 0 4 6 0 8 3 4 8 8 3 9 5 7\n",
      " 1 9 4 7 9 1 9 7 5 2 7 3 4 8 8 2 1 5 9 2 7 8 8 6 8 8 1 3 8 8 5 4 7 1 6 6 1\n",
      " 6 1 6 7 0 4 6 9 5 8 7 1 9 0 3 3 7]\n"
     ]
    }
   ],
   "source": [
    "print('Batch labels', np.argmax(batch_labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1015625\n"
     ]
    }
   ],
   "source": [
    "correct = np.sum(np.argmax(predictions, 1) == np.argmax(batch_labels, 1))\n",
    "total = predictions.shape[0]\n",
    "\n",
    "print(float(correct) / float(total))\n",
    "\n",
    "confusions = np.zeros([10, 10], np.float32)\n",
    "bundled = zip(np.argmax(predictions, 1), np.argmax(batch_labels, 1))\n",
    "for predicted, actual in bundled:\n",
    "  confusions[predicted, actual] += 1\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(np.arange(N_LABELS))\n",
    "plt.yticks(np.arange(N_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate defined\n"
     ]
    }
   ],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate and confusions.\"\"\"\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "    confusions = np.zeros([10, 10], np.float32)\n",
    "    bundled = zip(np.argmax(predictions, 1), np.argmax(labels, 1))\n",
    "    for predicted, actual in bundled:\n",
    "        confusions[predicted, actual] += 1\n",
    "    \n",
    "    return error, confusions\n",
    "\n",
    "print('Error rate defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 40000\n",
      "Mini-batch loss: 8.96989 Error: 87.50000 Learning rate: 0.01000\n",
      "Validation error: 89.1%\n",
      "Step 10 of 40000\n",
      "Mini-batch loss: 6.51252 Error: 82.81250 Learning rate: 0.01000\n",
      "Validation error: 83.4%\n",
      "Step 20 of 40000\n",
      "Mini-batch loss: 6.22292 Error: 79.68750 Learning rate: 0.01000\n",
      "Validation error: 78.3%\n",
      "Step 30 of 40000\n",
      "Mini-batch loss: 6.17990 Error: 76.56250 Learning rate: 0.01000\n",
      "Validation error: 73.2%\n",
      "Step 40 of 40000\n",
      "Mini-batch loss: 6.14630 Error: 78.12500 Learning rate: 0.01000\n",
      "Validation error: 71.7%\n",
      "Step 50 of 40000\n",
      "Mini-batch loss: 6.05039 Error: 74.21875 Learning rate: 0.01000\n",
      "Validation error: 68.3%\n",
      "Step 60 of 40000\n",
      "Mini-batch loss: 6.09632 Error: 70.31250 Learning rate: 0.01000\n",
      "Validation error: 64.9%\n",
      "Step 70 of 40000\n",
      "Mini-batch loss: 6.07799 Error: 76.56250 Learning rate: 0.01000\n",
      "Validation error: 63.5%\n",
      "Step 80 of 40000\n",
      "Mini-batch loss: 5.99907 Error: 69.53125 Learning rate: 0.01000\n",
      "Validation error: 62.2%\n",
      "Step 90 of 40000\n",
      "Mini-batch loss: 5.79098 Error: 65.62500 Learning rate: 0.01000\n",
      "Validation error: 61.6%\n",
      "Step 100 of 40000\n",
      "Mini-batch loss: 5.96398 Error: 71.87500 Learning rate: 0.01000\n",
      "Validation error: 60.2%\n",
      "Step 110 of 40000\n",
      "Mini-batch loss: 5.67161 Error: 62.50000 Learning rate: 0.01000\n",
      "Validation error: 60.8%\n",
      "Step 120 of 40000\n",
      "Mini-batch loss: 5.82046 Error: 69.53125 Learning rate: 0.01000\n",
      "Validation error: 60.5%\n",
      "Step 130 of 40000\n",
      "Mini-batch loss: 5.61680 Error: 54.68750 Learning rate: 0.01000\n",
      "Validation error: 61.7%\n",
      "Step 140 of 40000\n",
      "Mini-batch loss: 5.93203 Error: 66.40625 Learning rate: 0.01000\n",
      "Validation error: 60.4%\n",
      "Step 150 of 40000\n",
      "Mini-batch loss: 5.62235 Error: 59.37500 Learning rate: 0.01000\n",
      "Validation error: 58.7%\n",
      "Step 160 of 40000\n",
      "Mini-batch loss: 5.65894 Error: 60.15625 Learning rate: 0.01000\n",
      "Validation error: 57.6%\n",
      "Step 170 of 40000\n",
      "Mini-batch loss: 5.70908 Error: 61.71875 Learning rate: 0.01000\n",
      "Validation error: 59.1%\n",
      "Step 180 of 40000\n",
      "Mini-batch loss: 5.67380 Error: 59.37500 Learning rate: 0.01000\n",
      "Validation error: 59.2%\n",
      "Step 190 of 40000\n",
      "Mini-batch loss: 5.63927 Error: 60.15625 Learning rate: 0.01000\n",
      "Validation error: 56.7%\n",
      "Step 200 of 40000\n",
      "Mini-batch loss: 5.68738 Error: 60.15625 Learning rate: 0.01000\n",
      "Validation error: 57.7%\n",
      "Step 210 of 40000\n",
      "Mini-batch loss: 5.76528 Error: 68.75000 Learning rate: 0.01000\n",
      "Validation error: 55.4%\n",
      "Step 220 of 40000\n",
      "Mini-batch loss: 5.82886 Error: 65.62500 Learning rate: 0.01000\n",
      "Validation error: 56.6%\n",
      "Step 230 of 40000\n",
      "Mini-batch loss: 5.57283 Error: 60.15625 Learning rate: 0.01000\n",
      "Validation error: 55.9%\n",
      "Step 240 of 40000\n",
      "Mini-batch loss: 5.67618 Error: 64.06250 Learning rate: 0.01000\n",
      "Validation error: 57.6%\n",
      "Step 250 of 40000\n",
      "Mini-batch loss: 5.55429 Error: 56.25000 Learning rate: 0.01000\n",
      "Validation error: 54.8%\n",
      "Step 260 of 40000\n",
      "Mini-batch loss: 5.56820 Error: 53.12500 Learning rate: 0.01000\n",
      "Validation error: 54.2%\n",
      "Step 270 of 40000\n",
      "Mini-batch loss: 5.60260 Error: 58.59375 Learning rate: 0.01000\n",
      "Validation error: 55.0%\n",
      "Step 280 of 40000\n",
      "Mini-batch loss: 5.58665 Error: 60.15625 Learning rate: 0.01000\n",
      "Validation error: 54.6%\n",
      "Step 290 of 40000\n",
      "Mini-batch loss: 5.38736 Error: 53.90625 Learning rate: 0.01000\n",
      "Validation error: 54.8%\n",
      "Step 300 of 40000\n",
      "Mini-batch loss: 5.55603 Error: 60.93750 Learning rate: 0.01000\n",
      "Validation error: 53.1%\n",
      "Step 310 of 40000\n",
      "Mini-batch loss: 5.56919 Error: 58.59375 Learning rate: 0.01000\n",
      "Validation error: 53.0%\n",
      "Step 320 of 40000\n",
      "Mini-batch loss: 5.49066 Error: 58.59375 Learning rate: 0.00950\n",
      "Validation error: 53.9%\n",
      "Step 330 of 40000\n",
      "Mini-batch loss: 5.34435 Error: 47.65625 Learning rate: 0.00950\n",
      "Validation error: 53.1%\n",
      "Step 340 of 40000\n",
      "Mini-batch loss: 5.58779 Error: 63.28125 Learning rate: 0.00950\n",
      "Validation error: 51.8%\n",
      "Step 350 of 40000\n",
      "Mini-batch loss: 5.51085 Error: 57.03125 Learning rate: 0.00950\n",
      "Validation error: 52.0%\n",
      "Step 360 of 40000\n",
      "Mini-batch loss: 5.48614 Error: 53.90625 Learning rate: 0.00950\n",
      "Validation error: 51.3%\n",
      "Step 370 of 40000\n",
      "Mini-batch loss: 5.37984 Error: 48.43750 Learning rate: 0.00950\n",
      "Validation error: 50.0%\n",
      "Step 380 of 40000\n",
      "Mini-batch loss: 5.45897 Error: 55.46875 Learning rate: 0.00950\n",
      "Validation error: 50.7%\n",
      "Step 390 of 40000\n",
      "Mini-batch loss: 5.36415 Error: 57.03125 Learning rate: 0.00950\n",
      "Validation error: 49.8%\n",
      "Step 400 of 40000\n",
      "Mini-batch loss: 5.46389 Error: 61.71875 Learning rate: 0.00950\n",
      "Validation error: 50.2%\n",
      "Step 410 of 40000\n",
      "Mini-batch loss: 5.53805 Error: 57.03125 Learning rate: 0.00950\n",
      "Validation error: 49.5%\n",
      "Step 420 of 40000\n",
      "Mini-batch loss: 5.42620 Error: 57.03125 Learning rate: 0.00950\n",
      "Validation error: 49.8%\n",
      "Step 430 of 40000\n",
      "Mini-batch loss: 5.44106 Error: 54.68750 Learning rate: 0.00950\n",
      "Validation error: 49.5%\n",
      "Step 440 of 40000\n",
      "Mini-batch loss: 5.26849 Error: 53.90625 Learning rate: 0.00950\n",
      "Validation error: 49.8%\n",
      "Step 450 of 40000\n",
      "Mini-batch loss: 5.28021 Error: 53.12500 Learning rate: 0.00950\n",
      "Validation error: 50.8%\n",
      "Step 460 of 40000\n",
      "Mini-batch loss: 5.52653 Error: 64.84375 Learning rate: 0.00950\n",
      "Validation error: 50.7%\n",
      "Step 470 of 40000\n",
      "Mini-batch loss: 5.27545 Error: 50.00000 Learning rate: 0.00950\n",
      "Validation error: 50.0%\n",
      "Step 480 of 40000\n",
      "Mini-batch loss: 5.26180 Error: 57.81250 Learning rate: 0.00950\n",
      "Validation error: 49.5%\n",
      "Step 490 of 40000\n",
      "Mini-batch loss: 5.49375 Error: 61.71875 Learning rate: 0.00950\n",
      "Validation error: 50.1%\n",
      "Step 500 of 40000\n",
      "Mini-batch loss: 5.22791 Error: 46.87500 Learning rate: 0.00950\n",
      "Validation error: 49.3%\n",
      "Step 510 of 40000\n",
      "Mini-batch loss: 5.32706 Error: 52.34375 Learning rate: 0.00950\n",
      "Validation error: 49.0%\n",
      "Step 520 of 40000\n",
      "Mini-batch loss: 5.29523 Error: 51.56250 Learning rate: 0.00950\n",
      "Validation error: 48.8%\n",
      "Step 530 of 40000\n",
      "Mini-batch loss: 5.43288 Error: 52.34375 Learning rate: 0.00950\n",
      "Validation error: 48.2%\n",
      "Step 540 of 40000\n",
      "Mini-batch loss: 5.38879 Error: 57.03125 Learning rate: 0.00950\n",
      "Validation error: 48.6%\n",
      "Step 550 of 40000\n",
      "Mini-batch loss: 5.21792 Error: 51.56250 Learning rate: 0.00950\n",
      "Validation error: 46.8%\n",
      "Step 560 of 40000\n",
      "Mini-batch loss: 5.13789 Error: 44.53125 Learning rate: 0.00950\n",
      "Validation error: 47.7%\n",
      "Step 570 of 40000\n",
      "Mini-batch loss: 5.19417 Error: 50.00000 Learning rate: 0.00950\n",
      "Validation error: 47.7%\n",
      "Step 580 of 40000\n",
      "Mini-batch loss: 5.29254 Error: 51.56250 Learning rate: 0.00950\n",
      "Validation error: 47.5%\n",
      "Step 590 of 40000\n",
      "Mini-batch loss: 5.22396 Error: 58.59375 Learning rate: 0.00950\n",
      "Validation error: 47.0%\n",
      "Step 600 of 40000\n",
      "Mini-batch loss: 5.21714 Error: 53.12500 Learning rate: 0.00950\n",
      "Validation error: 48.2%\n",
      "Step 610 of 40000\n",
      "Mini-batch loss: 5.21768 Error: 53.12500 Learning rate: 0.00950\n",
      "Validation error: 47.4%\n",
      "Step 620 of 40000\n",
      "Mini-batch loss: 5.19122 Error: 47.65625 Learning rate: 0.00950\n",
      "Validation error: 48.7%\n",
      "Step 630 of 40000\n",
      "Mini-batch loss: 5.19657 Error: 49.21875 Learning rate: 0.00902\n",
      "Validation error: 47.2%\n",
      "Step 640 of 40000\n",
      "Mini-batch loss: 5.29103 Error: 51.56250 Learning rate: 0.00902\n",
      "Validation error: 49.0%\n",
      "Step 650 of 40000\n",
      "Mini-batch loss: 5.10817 Error: 39.84375 Learning rate: 0.00902\n",
      "Validation error: 47.1%\n",
      "Step 660 of 40000\n",
      "Mini-batch loss: 5.16100 Error: 43.75000 Learning rate: 0.00902\n",
      "Validation error: 46.3%\n",
      "Step 670 of 40000\n",
      "Mini-batch loss: 5.18248 Error: 46.09375 Learning rate: 0.00902\n",
      "Validation error: 46.2%\n",
      "Step 680 of 40000\n",
      "Mini-batch loss: 5.11897 Error: 46.87500 Learning rate: 0.00902\n",
      "Validation error: 45.9%\n",
      "Step 690 of 40000\n",
      "Mini-batch loss: 5.10408 Error: 42.18750 Learning rate: 0.00902\n",
      "Validation error: 45.7%\n",
      "Step 700 of 40000\n",
      "Mini-batch loss: 5.26987 Error: 48.43750 Learning rate: 0.00902\n",
      "Validation error: 46.3%\n",
      "Step 710 of 40000\n",
      "Mini-batch loss: 5.05948 Error: 43.75000 Learning rate: 0.00902\n",
      "Validation error: 46.0%\n",
      "Step 720 of 40000\n",
      "Mini-batch loss: 5.15841 Error: 45.31250 Learning rate: 0.00902\n",
      "Validation error: 45.1%\n",
      "Step 730 of 40000\n",
      "Mini-batch loss: 5.14149 Error: 46.87500 Learning rate: 0.00902\n",
      "Validation error: 45.3%\n",
      "Step 740 of 40000\n",
      "Mini-batch loss: 5.14010 Error: 49.21875 Learning rate: 0.00902\n",
      "Validation error: 45.3%\n",
      "Step 750 of 40000\n",
      "Mini-batch loss: 5.16326 Error: 51.56250 Learning rate: 0.00902\n",
      "Validation error: 45.2%\n",
      "Step 760 of 40000\n",
      "Mini-batch loss: 5.07639 Error: 45.31250 Learning rate: 0.00902\n",
      "Validation error: 44.4%\n",
      "Step 770 of 40000\n",
      "Mini-batch loss: 5.01195 Error: 46.87500 Learning rate: 0.00902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 45.7%\n",
      "Step 780 of 40000\n",
      "Mini-batch loss: 5.19058 Error: 53.12500 Learning rate: 0.00902\n",
      "Validation error: 44.7%\n",
      "Step 790 of 40000\n",
      "Mini-batch loss: 5.28414 Error: 51.56250 Learning rate: 0.00902\n",
      "Validation error: 45.2%\n",
      "Step 800 of 40000\n",
      "Mini-batch loss: 5.20084 Error: 48.43750 Learning rate: 0.00902\n",
      "Validation error: 45.5%\n",
      "Step 810 of 40000\n",
      "Mini-batch loss: 5.21144 Error: 51.56250 Learning rate: 0.00902\n",
      "Validation error: 44.5%\n",
      "Step 820 of 40000\n",
      "Mini-batch loss: 5.21898 Error: 56.25000 Learning rate: 0.00902\n",
      "Validation error: 43.2%\n",
      "Step 830 of 40000\n",
      "Mini-batch loss: 5.06339 Error: 50.78125 Learning rate: 0.00902\n",
      "Validation error: 43.7%\n",
      "Step 840 of 40000\n",
      "Mini-batch loss: 5.01237 Error: 42.96875 Learning rate: 0.00902\n",
      "Validation error: 44.0%\n",
      "Step 850 of 40000\n",
      "Mini-batch loss: 5.08364 Error: 46.87500 Learning rate: 0.00902\n",
      "Validation error: 43.0%\n",
      "Step 860 of 40000\n",
      "Mini-batch loss: 5.22637 Error: 51.56250 Learning rate: 0.00902\n",
      "Validation error: 44.0%\n",
      "Step 870 of 40000\n",
      "Mini-batch loss: 5.00098 Error: 43.75000 Learning rate: 0.00902\n",
      "Validation error: 43.5%\n",
      "Step 880 of 40000\n",
      "Mini-batch loss: 5.32581 Error: 57.03125 Learning rate: 0.00902\n",
      "Validation error: 43.9%\n",
      "Step 890 of 40000\n",
      "Mini-batch loss: 5.27876 Error: 53.12500 Learning rate: 0.00902\n",
      "Validation error: 43.5%\n",
      "Step 900 of 40000\n",
      "Mini-batch loss: 5.00633 Error: 50.78125 Learning rate: 0.00902\n",
      "Validation error: 43.0%\n",
      "Step 910 of 40000\n",
      "Mini-batch loss: 4.99992 Error: 46.87500 Learning rate: 0.00902\n",
      "Validation error: 44.5%\n",
      "Step 920 of 40000\n",
      "Mini-batch loss: 5.24238 Error: 57.81250 Learning rate: 0.00902\n",
      "Validation error: 43.8%\n",
      "Step 930 of 40000\n",
      "Mini-batch loss: 5.19600 Error: 54.68750 Learning rate: 0.00902\n",
      "Validation error: 43.3%\n",
      "Step 940 of 40000\n",
      "Mini-batch loss: 4.91063 Error: 37.50000 Learning rate: 0.00857\n",
      "Validation error: 43.3%\n",
      "Step 950 of 40000\n",
      "Mini-batch loss: 5.09097 Error: 50.00000 Learning rate: 0.00857\n",
      "Validation error: 44.0%\n",
      "Step 960 of 40000\n",
      "Mini-batch loss: 5.02778 Error: 50.78125 Learning rate: 0.00857\n",
      "Validation error: 42.2%\n",
      "Step 970 of 40000\n",
      "Mini-batch loss: 4.84527 Error: 38.28125 Learning rate: 0.00857\n",
      "Validation error: 42.1%\n",
      "Step 980 of 40000\n",
      "Mini-batch loss: 5.28506 Error: 50.78125 Learning rate: 0.00857\n",
      "Validation error: 42.4%\n",
      "Step 990 of 40000\n",
      "Mini-batch loss: 4.95159 Error: 39.84375 Learning rate: 0.00857\n",
      "Validation error: 42.5%\n",
      "Step 1000 of 40000\n",
      "Mini-batch loss: 5.07236 Error: 48.43750 Learning rate: 0.00857\n",
      "Validation error: 42.1%\n",
      "Step 1010 of 40000\n",
      "Mini-batch loss: 5.18961 Error: 56.25000 Learning rate: 0.00857\n",
      "Validation error: 41.6%\n",
      "Step 1020 of 40000\n",
      "Mini-batch loss: 4.93052 Error: 41.40625 Learning rate: 0.00857\n",
      "Validation error: 42.0%\n",
      "Step 1030 of 40000\n",
      "Mini-batch loss: 4.92403 Error: 44.53125 Learning rate: 0.00857\n",
      "Validation error: 41.0%\n",
      "Step 1040 of 40000\n",
      "Mini-batch loss: 4.99813 Error: 41.40625 Learning rate: 0.00857\n",
      "Validation error: 41.2%\n",
      "Step 1050 of 40000\n",
      "Mini-batch loss: 5.06407 Error: 53.90625 Learning rate: 0.00857\n",
      "Validation error: 40.8%\n",
      "Step 1060 of 40000\n",
      "Mini-batch loss: 4.83270 Error: 39.84375 Learning rate: 0.00857\n",
      "Validation error: 43.5%\n",
      "Step 1070 of 40000\n",
      "Mini-batch loss: 4.95429 Error: 49.21875 Learning rate: 0.00857\n",
      "Validation error: 42.4%\n",
      "Step 1080 of 40000\n",
      "Mini-batch loss: 4.89727 Error: 46.87500 Learning rate: 0.00857\n",
      "Validation error: 42.2%\n",
      "Step 1090 of 40000\n",
      "Mini-batch loss: 4.79848 Error: 39.06250 Learning rate: 0.00857\n",
      "Validation error: 43.6%\n",
      "Step 1100 of 40000\n",
      "Mini-batch loss: 4.89431 Error: 40.62500 Learning rate: 0.00857\n",
      "Validation error: 41.5%\n",
      "Step 1110 of 40000\n",
      "Mini-batch loss: 4.79846 Error: 39.06250 Learning rate: 0.00857\n",
      "Validation error: 41.6%\n",
      "Step 1120 of 40000\n",
      "Mini-batch loss: 4.94924 Error: 45.31250 Learning rate: 0.00857\n",
      "Validation error: 42.1%\n",
      "Step 1130 of 40000\n",
      "Mini-batch loss: 5.01441 Error: 50.78125 Learning rate: 0.00857\n",
      "Validation error: 42.6%\n",
      "Step 1140 of 40000\n",
      "Mini-batch loss: 4.98500 Error: 46.09375 Learning rate: 0.00857\n",
      "Validation error: 41.0%\n",
      "Step 1150 of 40000\n",
      "Mini-batch loss: 4.95016 Error: 40.62500 Learning rate: 0.00857\n",
      "Validation error: 41.8%\n",
      "Step 1160 of 40000\n",
      "Mini-batch loss: 4.81962 Error: 44.53125 Learning rate: 0.00857\n",
      "Validation error: 40.9%\n",
      "Step 1170 of 40000\n",
      "Mini-batch loss: 5.15409 Error: 50.78125 Learning rate: 0.00857\n",
      "Validation error: 41.5%\n",
      "Step 1180 of 40000\n",
      "Mini-batch loss: 4.92031 Error: 53.12500 Learning rate: 0.00857\n",
      "Validation error: 40.9%\n",
      "Step 1190 of 40000\n",
      "Mini-batch loss: 4.96820 Error: 45.31250 Learning rate: 0.00857\n",
      "Validation error: 40.7%\n",
      "Step 1200 of 40000\n",
      "Mini-batch loss: 4.81239 Error: 38.28125 Learning rate: 0.00857\n",
      "Validation error: 41.2%\n",
      "Step 1210 of 40000\n",
      "Mini-batch loss: 4.81789 Error: 42.18750 Learning rate: 0.00857\n",
      "Validation error: 41.9%\n",
      "Step 1220 of 40000\n",
      "Mini-batch loss: 5.01313 Error: 50.00000 Learning rate: 0.00857\n",
      "Validation error: 41.4%\n",
      "Step 1230 of 40000\n",
      "Mini-batch loss: 5.11554 Error: 53.90625 Learning rate: 0.00857\n",
      "Validation error: 42.9%\n",
      "Step 1240 of 40000\n",
      "Mini-batch loss: 4.84514 Error: 44.53125 Learning rate: 0.00857\n",
      "Validation error: 42.2%\n",
      "Step 1250 of 40000\n",
      "Mini-batch loss: 5.02568 Error: 53.90625 Learning rate: 0.00815\n",
      "Validation error: 41.4%\n",
      "Step 1260 of 40000\n",
      "Mini-batch loss: 4.85464 Error: 39.84375 Learning rate: 0.00815\n",
      "Validation error: 40.8%\n",
      "Step 1270 of 40000\n",
      "Mini-batch loss: 4.88256 Error: 43.75000 Learning rate: 0.00815\n",
      "Validation error: 40.2%\n",
      "Step 1280 of 40000\n",
      "Mini-batch loss: 4.83321 Error: 46.87500 Learning rate: 0.00815\n",
      "Validation error: 40.1%\n",
      "Step 1290 of 40000\n",
      "Mini-batch loss: 4.88879 Error: 41.40625 Learning rate: 0.00815\n",
      "Validation error: 39.2%\n",
      "Step 1300 of 40000\n",
      "Mini-batch loss: 4.64819 Error: 35.15625 Learning rate: 0.00815\n",
      "Validation error: 40.9%\n",
      "Step 1310 of 40000\n",
      "Mini-batch loss: 4.78925 Error: 42.18750 Learning rate: 0.00815\n",
      "Validation error: 39.1%\n",
      "Step 1320 of 40000\n",
      "Mini-batch loss: 4.89518 Error: 46.09375 Learning rate: 0.00815\n",
      "Validation error: 38.6%\n",
      "Step 1330 of 40000\n",
      "Mini-batch loss: 4.85088 Error: 43.75000 Learning rate: 0.00815\n",
      "Validation error: 39.0%\n",
      "Step 1340 of 40000\n",
      "Mini-batch loss: 4.71773 Error: 47.65625 Learning rate: 0.00815\n",
      "Validation error: 38.6%\n",
      "Step 1350 of 40000\n",
      "Mini-batch loss: 4.72976 Error: 42.96875 Learning rate: 0.00815\n",
      "Validation error: 39.0%\n",
      "Step 1360 of 40000\n",
      "Mini-batch loss: 4.72391 Error: 46.09375 Learning rate: 0.00815\n",
      "Validation error: 39.4%\n",
      "Step 1370 of 40000\n",
      "Mini-batch loss: 4.78469 Error: 43.75000 Learning rate: 0.00815\n",
      "Validation error: 38.6%\n",
      "Step 1380 of 40000\n",
      "Mini-batch loss: 4.88363 Error: 50.00000 Learning rate: 0.00815\n",
      "Validation error: 40.3%\n",
      "Step 1390 of 40000\n",
      "Mini-batch loss: 4.91737 Error: 46.87500 Learning rate: 0.00815\n",
      "Validation error: 39.1%\n",
      "Step 1400 of 40000\n",
      "Mini-batch loss: 4.83585 Error: 45.31250 Learning rate: 0.00815\n",
      "Validation error: 39.2%\n",
      "Step 1410 of 40000\n",
      "Mini-batch loss: 4.67404 Error: 40.62500 Learning rate: 0.00815\n",
      "Validation error: 39.8%\n",
      "Step 1420 of 40000\n",
      "Mini-batch loss: 4.88792 Error: 42.18750 Learning rate: 0.00815\n",
      "Validation error: 39.5%\n",
      "Step 1430 of 40000\n",
      "Mini-batch loss: 4.97490 Error: 46.87500 Learning rate: 0.00815\n",
      "Validation error: 39.6%\n",
      "Step 1440 of 40000\n",
      "Mini-batch loss: 4.71700 Error: 41.40625 Learning rate: 0.00815\n",
      "Validation error: 39.5%\n",
      "Step 1450 of 40000\n",
      "Mini-batch loss: 4.79311 Error: 45.31250 Learning rate: 0.00815\n",
      "Validation error: 38.9%\n",
      "Step 1460 of 40000\n",
      "Mini-batch loss: 4.87515 Error: 52.34375 Learning rate: 0.00815\n",
      "Validation error: 40.2%\n",
      "Step 1470 of 40000\n",
      "Mini-batch loss: 4.71959 Error: 40.62500 Learning rate: 0.00815\n",
      "Validation error: 38.3%\n",
      "Step 1480 of 40000\n",
      "Mini-batch loss: 4.61186 Error: 40.62500 Learning rate: 0.00815\n",
      "Validation error: 38.9%\n",
      "Step 1490 of 40000\n",
      "Mini-batch loss: 4.75329 Error: 42.18750 Learning rate: 0.00815\n",
      "Validation error: 38.5%\n",
      "Step 1500 of 40000\n",
      "Mini-batch loss: 4.89183 Error: 47.65625 Learning rate: 0.00815\n",
      "Validation error: 39.3%\n",
      "Step 1510 of 40000\n",
      "Mini-batch loss: 4.82058 Error: 45.31250 Learning rate: 0.00815\n",
      "Validation error: 38.6%\n",
      "Step 1520 of 40000\n",
      "Mini-batch loss: 4.86718 Error: 48.43750 Learning rate: 0.00815\n",
      "Validation error: 38.8%\n",
      "Step 1530 of 40000\n",
      "Mini-batch loss: 4.77676 Error: 45.31250 Learning rate: 0.00815\n",
      "Validation error: 38.9%\n",
      "Step 1540 of 40000\n",
      "Mini-batch loss: 4.74665 Error: 45.31250 Learning rate: 0.00815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 41.2%\n",
      "Step 1550 of 40000\n",
      "Mini-batch loss: 4.84265 Error: 47.65625 Learning rate: 0.00815\n",
      "Validation error: 39.7%\n",
      "Step 1560 of 40000\n",
      "Mini-batch loss: 4.86643 Error: 43.75000 Learning rate: 0.00815\n",
      "Validation error: 38.7%\n",
      "Step 1570 of 40000\n",
      "Mini-batch loss: 4.70841 Error: 42.18750 Learning rate: 0.00774\n",
      "Validation error: 37.5%\n",
      "Step 1580 of 40000\n",
      "Mini-batch loss: 4.82459 Error: 50.78125 Learning rate: 0.00774\n",
      "Validation error: 38.6%\n",
      "Step 1590 of 40000\n",
      "Mini-batch loss: 4.78211 Error: 46.09375 Learning rate: 0.00774\n",
      "Validation error: 39.5%\n",
      "Step 1600 of 40000\n",
      "Mini-batch loss: 4.65333 Error: 39.06250 Learning rate: 0.00774\n",
      "Validation error: 37.2%\n",
      "Step 1610 of 40000\n",
      "Mini-batch loss: 4.69833 Error: 41.40625 Learning rate: 0.00774\n",
      "Validation error: 37.5%\n",
      "Step 1620 of 40000\n",
      "Mini-batch loss: 4.67073 Error: 39.84375 Learning rate: 0.00774\n",
      "Validation error: 37.3%\n",
      "Step 1630 of 40000\n",
      "Mini-batch loss: 4.59948 Error: 36.71875 Learning rate: 0.00774\n",
      "Validation error: 36.4%\n",
      "Step 1640 of 40000\n",
      "Mini-batch loss: 4.65732 Error: 40.62500 Learning rate: 0.00774\n",
      "Validation error: 36.1%\n",
      "Step 1650 of 40000\n",
      "Mini-batch loss: 4.63667 Error: 46.09375 Learning rate: 0.00774\n",
      "Validation error: 37.0%\n",
      "Step 1660 of 40000\n",
      "Mini-batch loss: 4.66536 Error: 40.62500 Learning rate: 0.00774\n",
      "Validation error: 36.5%\n",
      "Step 1670 of 40000\n",
      "Mini-batch loss: 4.54875 Error: 35.93750 Learning rate: 0.00774\n",
      "Validation error: 38.1%\n",
      "Step 1680 of 40000\n",
      "Mini-batch loss: 4.76546 Error: 49.21875 Learning rate: 0.00774\n",
      "Validation error: 37.3%\n",
      "Step 1690 of 40000\n",
      "Mini-batch loss: 4.79968 Error: 45.31250 Learning rate: 0.00774\n",
      "Validation error: 37.8%\n",
      "Step 1700 of 40000\n",
      "Mini-batch loss: 4.68411 Error: 46.09375 Learning rate: 0.00774\n",
      "Validation error: 37.3%\n",
      "Step 1710 of 40000\n",
      "Mini-batch loss: 4.67283 Error: 42.96875 Learning rate: 0.00774\n",
      "Validation error: 37.0%\n",
      "Step 1720 of 40000\n",
      "Mini-batch loss: 4.71178 Error: 44.53125 Learning rate: 0.00774\n",
      "Validation error: 38.1%\n",
      "Step 1730 of 40000\n",
      "Mini-batch loss: 4.59046 Error: 40.62500 Learning rate: 0.00774\n",
      "Validation error: 37.5%\n",
      "Step 1740 of 40000\n",
      "Mini-batch loss: 4.72308 Error: 45.31250 Learning rate: 0.00774\n",
      "Validation error: 38.2%\n",
      "Step 1750 of 40000\n",
      "Mini-batch loss: 4.74238 Error: 47.65625 Learning rate: 0.00774\n",
      "Validation error: 37.5%\n",
      "Step 1760 of 40000\n",
      "Mini-batch loss: 4.71519 Error: 49.21875 Learning rate: 0.00774\n",
      "Validation error: 37.4%\n",
      "Step 1770 of 40000\n",
      "Mini-batch loss: 4.55916 Error: 43.75000 Learning rate: 0.00774\n",
      "Validation error: 36.8%\n",
      "Step 1780 of 40000\n",
      "Mini-batch loss: 4.51585 Error: 37.50000 Learning rate: 0.00774\n",
      "Validation error: 37.5%\n",
      "Step 1790 of 40000\n",
      "Mini-batch loss: 4.74440 Error: 44.53125 Learning rate: 0.00774\n",
      "Validation error: 36.8%\n",
      "Step 1800 of 40000\n",
      "Mini-batch loss: 4.65828 Error: 38.28125 Learning rate: 0.00774\n",
      "Validation error: 37.2%\n",
      "Step 1810 of 40000\n",
      "Mini-batch loss: 4.44210 Error: 33.59375 Learning rate: 0.00774\n",
      "Validation error: 37.1%\n",
      "Step 1820 of 40000\n",
      "Mini-batch loss: 4.38206 Error: 28.12500 Learning rate: 0.00774\n",
      "Validation error: 37.1%\n",
      "Step 1830 of 40000\n",
      "Mini-batch loss: 4.61435 Error: 46.87500 Learning rate: 0.00774\n",
      "Validation error: 37.1%\n",
      "Step 1840 of 40000\n",
      "Mini-batch loss: 4.53829 Error: 35.15625 Learning rate: 0.00774\n",
      "Validation error: 36.8%\n",
      "Step 1850 of 40000\n",
      "Mini-batch loss: 4.71981 Error: 38.28125 Learning rate: 0.00774\n",
      "Validation error: 38.3%\n",
      "Step 1860 of 40000\n",
      "Mini-batch loss: 4.55085 Error: 39.84375 Learning rate: 0.00774\n",
      "Validation error: 36.5%\n",
      "Step 1870 of 40000\n",
      "Mini-batch loss: 4.80838 Error: 45.31250 Learning rate: 0.00774\n",
      "Validation error: 36.5%\n",
      "Step 1880 of 40000\n",
      "Mini-batch loss: 4.70903 Error: 46.09375 Learning rate: 0.00735\n",
      "Validation error: 35.7%\n",
      "Step 1890 of 40000\n",
      "Mini-batch loss: 4.61772 Error: 35.15625 Learning rate: 0.00735\n",
      "Validation error: 36.4%\n",
      "Step 1900 of 40000\n",
      "Mini-batch loss: 4.73347 Error: 46.09375 Learning rate: 0.00735\n",
      "Validation error: 37.7%\n",
      "Step 1910 of 40000\n",
      "Mini-batch loss: 4.51528 Error: 39.06250 Learning rate: 0.00735\n",
      "Validation error: 35.8%\n",
      "Step 1920 of 40000\n",
      "Mini-batch loss: 4.61352 Error: 46.87500 Learning rate: 0.00735\n",
      "Validation error: 35.3%\n",
      "Step 1930 of 40000\n",
      "Mini-batch loss: 4.60555 Error: 42.18750 Learning rate: 0.00735\n",
      "Validation error: 35.0%\n",
      "Step 1940 of 40000\n",
      "Mini-batch loss: 4.49041 Error: 32.81250 Learning rate: 0.00735\n",
      "Validation error: 34.5%\n",
      "Step 1950 of 40000\n",
      "Mini-batch loss: 4.59705 Error: 40.62500 Learning rate: 0.00735\n",
      "Validation error: 34.4%\n",
      "Step 1960 of 40000\n",
      "Mini-batch loss: 4.58123 Error: 44.53125 Learning rate: 0.00735\n",
      "Validation error: 35.1%\n",
      "Step 1970 of 40000\n",
      "Mini-batch loss: 4.43263 Error: 31.25000 Learning rate: 0.00735\n",
      "Validation error: 34.3%\n",
      "Step 1980 of 40000\n",
      "Mini-batch loss: 4.48052 Error: 35.93750 Learning rate: 0.00735\n",
      "Validation error: 36.9%\n",
      "Step 1990 of 40000\n",
      "Mini-batch loss: 4.56311 Error: 36.71875 Learning rate: 0.00735\n",
      "Validation error: 35.1%\n",
      "Step 2000 of 40000\n",
      "Mini-batch loss: 4.60681 Error: 43.75000 Learning rate: 0.00735\n",
      "Validation error: 34.5%\n",
      "Step 2010 of 40000\n",
      "Mini-batch loss: 4.56633 Error: 36.71875 Learning rate: 0.00735\n",
      "Validation error: 35.7%\n",
      "Step 2020 of 40000\n",
      "Mini-batch loss: 4.41425 Error: 39.84375 Learning rate: 0.00735\n",
      "Validation error: 35.1%\n",
      "Step 2030 of 40000\n",
      "Mini-batch loss: 4.75614 Error: 45.31250 Learning rate: 0.00735\n",
      "Validation error: 35.5%\n",
      "Step 2040 of 40000\n",
      "Mini-batch loss: 4.45091 Error: 35.15625 Learning rate: 0.00735\n",
      "Validation error: 35.6%\n",
      "Step 2050 of 40000\n",
      "Mini-batch loss: 4.62186 Error: 41.40625 Learning rate: 0.00735\n",
      "Validation error: 35.0%\n",
      "Step 2060 of 40000\n",
      "Mini-batch loss: 4.58641 Error: 42.18750 Learning rate: 0.00735\n",
      "Validation error: 35.4%\n",
      "Step 2070 of 40000\n",
      "Mini-batch loss: 4.55643 Error: 39.06250 Learning rate: 0.00735\n",
      "Validation error: 35.4%\n",
      "Step 2080 of 40000\n",
      "Mini-batch loss: 4.49640 Error: 39.84375 Learning rate: 0.00735\n",
      "Validation error: 34.9%\n",
      "Step 2090 of 40000\n",
      "Mini-batch loss: 4.51154 Error: 39.06250 Learning rate: 0.00735\n",
      "Validation error: 36.6%\n",
      "Step 2100 of 40000\n",
      "Mini-batch loss: 4.75923 Error: 49.21875 Learning rate: 0.00735\n",
      "Validation error: 36.7%\n",
      "Step 2110 of 40000\n",
      "Mini-batch loss: 4.64745 Error: 42.18750 Learning rate: 0.00735\n",
      "Validation error: 35.2%\n",
      "Step 2120 of 40000\n",
      "Mini-batch loss: 4.55617 Error: 42.18750 Learning rate: 0.00735\n",
      "Validation error: 35.4%\n",
      "Step 2130 of 40000\n",
      "Mini-batch loss: 4.53081 Error: 43.75000 Learning rate: 0.00735\n",
      "Validation error: 34.9%\n",
      "Step 2140 of 40000\n",
      "Mini-batch loss: 4.45628 Error: 35.93750 Learning rate: 0.00735\n",
      "Validation error: 35.0%\n",
      "Step 2150 of 40000\n",
      "Mini-batch loss: 4.46926 Error: 39.06250 Learning rate: 0.00735\n",
      "Validation error: 35.2%\n",
      "Step 2160 of 40000\n",
      "Mini-batch loss: 4.34800 Error: 35.15625 Learning rate: 0.00735\n",
      "Validation error: 35.9%\n",
      "Step 2170 of 40000\n",
      "Mini-batch loss: 4.44751 Error: 38.28125 Learning rate: 0.00735\n",
      "Validation error: 35.5%\n",
      "Step 2180 of 40000\n",
      "Mini-batch loss: 4.37700 Error: 35.93750 Learning rate: 0.00735\n",
      "Validation error: 35.4%\n",
      "Step 2190 of 40000\n",
      "Mini-batch loss: 4.48602 Error: 42.18750 Learning rate: 0.00698\n",
      "Validation error: 35.5%\n",
      "Step 2200 of 40000\n",
      "Mini-batch loss: 4.65425 Error: 46.09375 Learning rate: 0.00698\n",
      "Validation error: 35.8%\n",
      "Step 2210 of 40000\n",
      "Mini-batch loss: 4.38011 Error: 33.59375 Learning rate: 0.00698\n",
      "Validation error: 34.1%\n",
      "Step 2220 of 40000\n",
      "Mini-batch loss: 4.56077 Error: 42.96875 Learning rate: 0.00698\n",
      "Validation error: 34.3%\n",
      "Step 2230 of 40000\n",
      "Mini-batch loss: 4.41585 Error: 33.59375 Learning rate: 0.00698\n",
      "Validation error: 34.2%\n",
      "Step 2240 of 40000\n",
      "Mini-batch loss: 4.48541 Error: 39.84375 Learning rate: 0.00698\n",
      "Validation error: 33.4%\n",
      "Step 2250 of 40000\n",
      "Mini-batch loss: 4.43799 Error: 39.84375 Learning rate: 0.00698\n",
      "Validation error: 33.4%\n",
      "Step 2260 of 40000\n",
      "Mini-batch loss: 4.58392 Error: 44.53125 Learning rate: 0.00698\n",
      "Validation error: 33.5%\n",
      "Step 2270 of 40000\n",
      "Mini-batch loss: 4.42656 Error: 35.93750 Learning rate: 0.00698\n",
      "Validation error: 33.1%\n",
      "Step 2280 of 40000\n",
      "Mini-batch loss: 4.46183 Error: 41.40625 Learning rate: 0.00698\n",
      "Validation error: 34.0%\n",
      "Step 2290 of 40000\n",
      "Mini-batch loss: 4.31084 Error: 36.71875 Learning rate: 0.00698\n",
      "Validation error: 34.3%\n",
      "Step 2300 of 40000\n",
      "Mini-batch loss: 4.40038 Error: 35.93750 Learning rate: 0.00698\n",
      "Validation error: 33.9%\n",
      "Step 2310 of 40000\n",
      "Mini-batch loss: 4.41473 Error: 39.84375 Learning rate: 0.00698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 33.4%\n",
      "Step 2320 of 40000\n",
      "Mini-batch loss: 4.56361 Error: 39.84375 Learning rate: 0.00698\n",
      "Validation error: 33.6%\n",
      "Step 2330 of 40000\n",
      "Mini-batch loss: 4.44115 Error: 42.96875 Learning rate: 0.00698\n",
      "Validation error: 33.2%\n",
      "Step 2340 of 40000\n",
      "Mini-batch loss: 4.43509 Error: 36.71875 Learning rate: 0.00698\n",
      "Validation error: 34.8%\n",
      "Step 2350 of 40000\n",
      "Mini-batch loss: 4.37973 Error: 37.50000 Learning rate: 0.00698\n",
      "Validation error: 33.6%\n",
      "Step 2360 of 40000\n",
      "Mini-batch loss: 4.44749 Error: 41.40625 Learning rate: 0.00698\n",
      "Validation error: 33.9%\n",
      "Step 2370 of 40000\n",
      "Mini-batch loss: 4.42756 Error: 41.40625 Learning rate: 0.00698\n",
      "Validation error: 33.2%\n",
      "Step 2380 of 40000\n",
      "Mini-batch loss: 4.47142 Error: 46.09375 Learning rate: 0.00698\n",
      "Validation error: 33.5%\n",
      "Step 2390 of 40000\n",
      "Mini-batch loss: 4.54940 Error: 44.53125 Learning rate: 0.00698\n",
      "Validation error: 34.9%\n",
      "Step 2400 of 40000\n",
      "Mini-batch loss: 4.47464 Error: 34.37500 Learning rate: 0.00698\n",
      "Validation error: 33.3%\n",
      "Step 2410 of 40000\n",
      "Mini-batch loss: 4.35290 Error: 35.93750 Learning rate: 0.00698\n",
      "Validation error: 35.6%\n",
      "Step 2420 of 40000\n",
      "Mini-batch loss: 4.22072 Error: 35.15625 Learning rate: 0.00698\n",
      "Validation error: 34.3%\n",
      "Step 2430 of 40000\n",
      "Mini-batch loss: 4.37252 Error: 39.84375 Learning rate: 0.00698\n",
      "Validation error: 33.3%\n",
      "Step 2440 of 40000\n",
      "Mini-batch loss: 4.56396 Error: 49.21875 Learning rate: 0.00698\n",
      "Validation error: 33.4%\n",
      "Step 2450 of 40000\n",
      "Mini-batch loss: 4.31909 Error: 37.50000 Learning rate: 0.00698\n",
      "Validation error: 33.4%\n",
      "Step 2460 of 40000\n",
      "Mini-batch loss: 4.40691 Error: 36.71875 Learning rate: 0.00698\n",
      "Validation error: 33.6%\n",
      "Step 2470 of 40000\n",
      "Mini-batch loss: 4.30344 Error: 35.15625 Learning rate: 0.00698\n",
      "Validation error: 34.8%\n",
      "Step 2480 of 40000\n",
      "Mini-batch loss: 4.31784 Error: 33.59375 Learning rate: 0.00698\n",
      "Validation error: 33.8%\n",
      "Step 2490 of 40000\n",
      "Mini-batch loss: 4.49827 Error: 45.31250 Learning rate: 0.00698\n",
      "Validation error: 33.7%\n",
      "Step 2500 of 40000\n",
      "Mini-batch loss: 4.57338 Error: 49.21875 Learning rate: 0.00663\n",
      "Validation error: 33.5%\n",
      "Step 2510 of 40000\n",
      "Mini-batch loss: 4.51238 Error: 45.31250 Learning rate: 0.00663\n",
      "Validation error: 34.8%\n",
      "Step 2520 of 40000\n",
      "Mini-batch loss: 4.48418 Error: 39.84375 Learning rate: 0.00663\n",
      "Validation error: 33.4%\n",
      "Step 2530 of 40000\n",
      "Mini-batch loss: 4.44002 Error: 42.96875 Learning rate: 0.00663\n",
      "Validation error: 32.8%\n",
      "Step 2540 of 40000\n",
      "Mini-batch loss: 4.38128 Error: 38.28125 Learning rate: 0.00663\n",
      "Validation error: 33.1%\n",
      "Step 2550 of 40000\n",
      "Mini-batch loss: 4.30572 Error: 36.71875 Learning rate: 0.00663\n",
      "Validation error: 31.9%\n",
      "Step 2560 of 40000\n",
      "Mini-batch loss: 4.36072 Error: 37.50000 Learning rate: 0.00663\n",
      "Validation error: 31.5%\n",
      "Step 2570 of 40000\n",
      "Mini-batch loss: 4.39355 Error: 38.28125 Learning rate: 0.00663\n",
      "Validation error: 31.4%\n",
      "Step 2580 of 40000\n",
      "Mini-batch loss: 4.35545 Error: 39.84375 Learning rate: 0.00663\n",
      "Validation error: 31.9%\n",
      "Step 2590 of 40000\n",
      "Mini-batch loss: 4.27664 Error: 35.15625 Learning rate: 0.00663\n",
      "Validation error: 30.9%\n",
      "Step 2600 of 40000\n",
      "Mini-batch loss: 4.23552 Error: 37.50000 Learning rate: 0.00663\n",
      "Validation error: 32.2%\n",
      "Step 2610 of 40000\n",
      "Mini-batch loss: 4.31146 Error: 37.50000 Learning rate: 0.00663\n",
      "Validation error: 31.1%\n",
      "Step 2620 of 40000\n",
      "Mini-batch loss: 4.41020 Error: 37.50000 Learning rate: 0.00663\n",
      "Validation error: 32.9%\n",
      "Step 2630 of 40000\n",
      "Mini-batch loss: 4.28863 Error: 38.28125 Learning rate: 0.00663\n",
      "Validation error: 33.6%\n",
      "Step 2640 of 40000\n",
      "Mini-batch loss: 4.33722 Error: 36.71875 Learning rate: 0.00663\n",
      "Validation error: 32.0%\n",
      "Step 2650 of 40000\n",
      "Mini-batch loss: 4.21637 Error: 28.90625 Learning rate: 0.00663\n",
      "Validation error: 33.9%\n",
      "Step 2660 of 40000\n",
      "Mini-batch loss: 4.36674 Error: 39.84375 Learning rate: 0.00663\n",
      "Validation error: 32.3%\n",
      "Step 2670 of 40000\n",
      "Mini-batch loss: 4.35977 Error: 41.40625 Learning rate: 0.00663\n",
      "Validation error: 31.9%\n",
      "Step 2680 of 40000\n",
      "Mini-batch loss: 4.17499 Error: 31.25000 Learning rate: 0.00663\n",
      "Validation error: 32.6%\n",
      "Step 2690 of 40000\n",
      "Mini-batch loss: 4.37486 Error: 37.50000 Learning rate: 0.00663\n",
      "Validation error: 32.7%\n",
      "Step 2700 of 40000\n",
      "Mini-batch loss: 4.23413 Error: 39.06250 Learning rate: 0.00663\n",
      "Validation error: 32.0%\n",
      "Step 2710 of 40000\n",
      "Mini-batch loss: 4.41941 Error: 41.40625 Learning rate: 0.00663\n",
      "Validation error: 32.1%\n",
      "Step 2720 of 40000\n",
      "Mini-batch loss: 4.21125 Error: 36.71875 Learning rate: 0.00663\n",
      "Validation error: 31.9%\n",
      "Step 2730 of 40000\n",
      "Mini-batch loss: 4.26551 Error: 33.59375 Learning rate: 0.00663\n",
      "Validation error: 33.2%\n",
      "Step 2740 of 40000\n",
      "Mini-batch loss: 4.21452 Error: 36.71875 Learning rate: 0.00663\n",
      "Validation error: 32.3%\n",
      "Step 2750 of 40000\n",
      "Mini-batch loss: 4.17073 Error: 37.50000 Learning rate: 0.00663\n",
      "Validation error: 31.8%\n",
      "Step 2760 of 40000\n",
      "Mini-batch loss: 4.37345 Error: 39.06250 Learning rate: 0.00663\n",
      "Validation error: 33.4%\n",
      "Step 2770 of 40000\n",
      "Mini-batch loss: 4.26415 Error: 36.71875 Learning rate: 0.00663\n",
      "Validation error: 31.8%\n",
      "Step 2780 of 40000\n",
      "Mini-batch loss: 4.31597 Error: 42.18750 Learning rate: 0.00663\n",
      "Validation error: 33.1%\n",
      "Step 2790 of 40000\n",
      "Mini-batch loss: 4.20959 Error: 29.68750 Learning rate: 0.00663\n",
      "Validation error: 32.8%\n",
      "Step 2800 of 40000\n",
      "Mini-batch loss: 4.10207 Error: 25.78125 Learning rate: 0.00663\n",
      "Validation error: 32.6%\n",
      "Step 2810 of 40000\n",
      "Mini-batch loss: 4.24377 Error: 36.71875 Learning rate: 0.00663\n",
      "Validation error: 32.0%\n",
      "Step 2820 of 40000\n",
      "Mini-batch loss: 4.23779 Error: 34.37500 Learning rate: 0.00630\n",
      "Validation error: 32.8%\n",
      "Step 2830 of 40000\n",
      "Mini-batch loss: 4.21540 Error: 35.15625 Learning rate: 0.00630\n",
      "Validation error: 32.0%\n",
      "Step 2840 of 40000\n",
      "Mini-batch loss: 4.31432 Error: 39.84375 Learning rate: 0.00630\n",
      "Validation error: 32.2%\n",
      "Step 2850 of 40000\n",
      "Mini-batch loss: 4.19848 Error: 29.68750 Learning rate: 0.00630\n",
      "Validation error: 30.8%\n",
      "Step 2860 of 40000\n",
      "Mini-batch loss: 4.25367 Error: 37.50000 Learning rate: 0.00630\n",
      "Validation error: 30.8%\n",
      "Step 2870 of 40000\n",
      "Mini-batch loss: 4.23868 Error: 33.59375 Learning rate: 0.00630\n",
      "Validation error: 30.2%\n",
      "Step 2880 of 40000\n",
      "Mini-batch loss: 4.30608 Error: 39.06250 Learning rate: 0.00630\n",
      "Validation error: 30.0%\n",
      "Step 2890 of 40000\n",
      "Mini-batch loss: 4.08458 Error: 31.25000 Learning rate: 0.00630\n",
      "Validation error: 30.2%\n",
      "Step 2900 of 40000\n",
      "Mini-batch loss: 4.23727 Error: 33.59375 Learning rate: 0.00630\n",
      "Validation error: 29.7%\n",
      "Step 2910 of 40000\n",
      "Mini-batch loss: 4.12214 Error: 33.59375 Learning rate: 0.00630\n",
      "Validation error: 30.6%\n",
      "Step 2920 of 40000\n",
      "Mini-batch loss: 4.15146 Error: 35.15625 Learning rate: 0.00630\n",
      "Validation error: 30.0%\n",
      "Step 2930 of 40000\n",
      "Mini-batch loss: 4.13230 Error: 34.37500 Learning rate: 0.00630\n",
      "Validation error: 30.4%\n",
      "Step 2940 of 40000\n",
      "Mini-batch loss: 4.32962 Error: 41.40625 Learning rate: 0.00630\n",
      "Validation error: 30.7%\n",
      "Step 2950 of 40000\n",
      "Mini-batch loss: 4.12707 Error: 34.37500 Learning rate: 0.00630\n",
      "Validation error: 30.4%\n",
      "Step 2960 of 40000\n",
      "Mini-batch loss: 4.24417 Error: 33.59375 Learning rate: 0.00630\n",
      "Validation error: 32.0%\n",
      "Step 2970 of 40000\n",
      "Mini-batch loss: 4.25017 Error: 39.84375 Learning rate: 0.00630\n",
      "Validation error: 31.5%\n",
      "Step 2980 of 40000\n",
      "Mini-batch loss: 4.24763 Error: 38.28125 Learning rate: 0.00630\n",
      "Validation error: 30.8%\n",
      "Step 2990 of 40000\n",
      "Mini-batch loss: 4.44988 Error: 42.18750 Learning rate: 0.00630\n",
      "Validation error: 31.9%\n",
      "Step 3000 of 40000\n",
      "Mini-batch loss: 4.15976 Error: 33.59375 Learning rate: 0.00630\n",
      "Validation error: 30.7%\n",
      "Step 3010 of 40000\n",
      "Mini-batch loss: 4.17819 Error: 31.25000 Learning rate: 0.00630\n",
      "Validation error: 30.7%\n",
      "Step 3020 of 40000\n",
      "Mini-batch loss: 4.21601 Error: 44.53125 Learning rate: 0.00630\n",
      "Validation error: 31.2%\n",
      "Step 3030 of 40000\n",
      "Mini-batch loss: 4.32922 Error: 39.84375 Learning rate: 0.00630\n",
      "Validation error: 31.9%\n",
      "Step 3040 of 40000\n",
      "Mini-batch loss: 4.24679 Error: 35.93750 Learning rate: 0.00630\n",
      "Validation error: 30.4%\n",
      "Step 3050 of 40000\n",
      "Mini-batch loss: 4.25690 Error: 35.93750 Learning rate: 0.00630\n",
      "Validation error: 30.6%\n",
      "Step 3060 of 40000\n",
      "Mini-batch loss: 4.23516 Error: 38.28125 Learning rate: 0.00630\n",
      "Validation error: 31.1%\n",
      "Step 3070 of 40000\n",
      "Mini-batch loss: 4.29809 Error: 34.37500 Learning rate: 0.00630\n",
      "Validation error: 31.1%\n",
      "Step 3080 of 40000\n",
      "Mini-batch loss: 4.04353 Error: 38.28125 Learning rate: 0.00630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 31.3%\n",
      "Step 3090 of 40000\n",
      "Mini-batch loss: 3.99159 Error: 26.56250 Learning rate: 0.00630\n",
      "Validation error: 31.3%\n",
      "Step 3100 of 40000\n",
      "Mini-batch loss: 4.42768 Error: 46.87500 Learning rate: 0.00630\n",
      "Validation error: 32.5%\n",
      "Step 3110 of 40000\n",
      "Mini-batch loss: 4.09216 Error: 35.15625 Learning rate: 0.00630\n",
      "Validation error: 31.9%\n",
      "Step 3120 of 40000\n",
      "Mini-batch loss: 4.05829 Error: 27.34375 Learning rate: 0.00630\n",
      "Validation error: 31.0%\n",
      "Step 3130 of 40000\n",
      "Mini-batch loss: 4.12341 Error: 35.15625 Learning rate: 0.00599\n",
      "Validation error: 30.6%\n",
      "Step 3140 of 40000\n",
      "Mini-batch loss: 4.26798 Error: 37.50000 Learning rate: 0.00599\n",
      "Validation error: 30.8%\n",
      "Step 3150 of 40000\n",
      "Mini-batch loss: 4.10279 Error: 34.37500 Learning rate: 0.00599\n",
      "Validation error: 31.2%\n",
      "Step 3160 of 40000\n",
      "Mini-batch loss: 4.20023 Error: 36.71875 Learning rate: 0.00599\n",
      "Validation error: 28.8%\n",
      "Step 3170 of 40000\n",
      "Mini-batch loss: 4.08064 Error: 32.03125 Learning rate: 0.00599\n",
      "Validation error: 29.7%\n",
      "Step 3180 of 40000\n",
      "Mini-batch loss: 4.08936 Error: 31.25000 Learning rate: 0.00599\n",
      "Validation error: 28.7%\n",
      "Step 3190 of 40000\n",
      "Mini-batch loss: 4.22082 Error: 42.18750 Learning rate: 0.00599\n",
      "Validation error: 29.2%\n",
      "Step 3200 of 40000\n",
      "Mini-batch loss: 4.03829 Error: 31.25000 Learning rate: 0.00599\n",
      "Validation error: 29.0%\n",
      "Step 3210 of 40000\n",
      "Mini-batch loss: 4.08243 Error: 33.59375 Learning rate: 0.00599\n",
      "Validation error: 28.7%\n",
      "Step 3220 of 40000\n",
      "Mini-batch loss: 4.22396 Error: 39.06250 Learning rate: 0.00599\n",
      "Validation error: 29.0%\n",
      "Step 3230 of 40000\n",
      "Mini-batch loss: 4.22626 Error: 38.28125 Learning rate: 0.00599\n",
      "Validation error: 29.1%\n",
      "Step 3240 of 40000\n",
      "Mini-batch loss: 4.06419 Error: 32.81250 Learning rate: 0.00599\n",
      "Validation error: 29.1%\n",
      "Step 3250 of 40000\n",
      "Mini-batch loss: 4.03536 Error: 29.68750 Learning rate: 0.00599\n",
      "Validation error: 29.7%\n",
      "Step 3260 of 40000\n",
      "Mini-batch loss: 4.17408 Error: 39.06250 Learning rate: 0.00599\n",
      "Validation error: 29.4%\n",
      "Step 3270 of 40000\n",
      "Mini-batch loss: 4.12529 Error: 32.03125 Learning rate: 0.00599\n",
      "Validation error: 29.4%\n",
      "Step 3280 of 40000\n",
      "Mini-batch loss: 4.22537 Error: 36.71875 Learning rate: 0.00599\n",
      "Validation error: 30.0%\n",
      "Step 3290 of 40000\n",
      "Mini-batch loss: 4.08860 Error: 39.84375 Learning rate: 0.00599\n",
      "Validation error: 30.8%\n",
      "Step 3300 of 40000\n",
      "Mini-batch loss: 3.95805 Error: 28.12500 Learning rate: 0.00599\n",
      "Validation error: 30.1%\n",
      "Step 3310 of 40000\n",
      "Mini-batch loss: 4.18675 Error: 38.28125 Learning rate: 0.00599\n",
      "Validation error: 29.8%\n",
      "Step 3320 of 40000\n",
      "Mini-batch loss: 4.08688 Error: 30.46875 Learning rate: 0.00599\n",
      "Validation error: 30.5%\n",
      "Step 3330 of 40000\n",
      "Mini-batch loss: 4.05987 Error: 32.81250 Learning rate: 0.00599\n",
      "Validation error: 29.8%\n",
      "Step 3340 of 40000\n",
      "Mini-batch loss: 3.96662 Error: 32.81250 Learning rate: 0.00599\n",
      "Validation error: 30.5%\n",
      "Step 3350 of 40000\n",
      "Mini-batch loss: 4.20437 Error: 36.71875 Learning rate: 0.00599\n",
      "Validation error: 30.7%\n",
      "Step 3360 of 40000\n",
      "Mini-batch loss: 4.03670 Error: 28.12500 Learning rate: 0.00599\n",
      "Validation error: 29.6%\n",
      "Step 3370 of 40000\n",
      "Mini-batch loss: 4.01538 Error: 30.46875 Learning rate: 0.00599\n",
      "Validation error: 29.8%\n",
      "Step 3380 of 40000\n",
      "Mini-batch loss: 4.08367 Error: 33.59375 Learning rate: 0.00599\n",
      "Validation error: 29.8%\n",
      "Step 3390 of 40000\n",
      "Mini-batch loss: 4.11300 Error: 33.59375 Learning rate: 0.00599\n",
      "Validation error: 30.0%\n",
      "Step 3400 of 40000\n",
      "Mini-batch loss: 4.39222 Error: 43.75000 Learning rate: 0.00599\n",
      "Validation error: 30.3%\n",
      "Step 3410 of 40000\n",
      "Mini-batch loss: 4.14418 Error: 36.71875 Learning rate: 0.00599\n",
      "Validation error: 31.0%\n",
      "Step 3420 of 40000\n",
      "Mini-batch loss: 4.02699 Error: 33.59375 Learning rate: 0.00599\n",
      "Validation error: 29.8%\n",
      "Step 3430 of 40000\n",
      "Mini-batch loss: 4.07192 Error: 32.03125 Learning rate: 0.00599\n",
      "Validation error: 29.2%\n",
      "Step 3440 of 40000\n",
      "Mini-batch loss: 4.18196 Error: 37.50000 Learning rate: 0.00569\n",
      "Validation error: 28.8%\n",
      "Step 3450 of 40000\n",
      "Mini-batch loss: 4.13353 Error: 35.93750 Learning rate: 0.00569\n",
      "Validation error: 29.9%\n",
      "Step 3460 of 40000\n",
      "Mini-batch loss: 4.22478 Error: 35.15625 Learning rate: 0.00569\n",
      "Validation error: 29.7%\n",
      "Step 3470 of 40000\n",
      "Mini-batch loss: 4.18081 Error: 32.81250 Learning rate: 0.00569\n",
      "Validation error: 28.6%\n",
      "Step 3480 of 40000\n",
      "Mini-batch loss: 4.03945 Error: 32.03125 Learning rate: 0.00569\n",
      "Validation error: 28.7%\n",
      "Step 3490 of 40000\n",
      "Mini-batch loss: 4.01373 Error: 39.84375 Learning rate: 0.00569\n",
      "Validation error: 27.6%\n",
      "Step 3500 of 40000\n",
      "Mini-batch loss: 3.98690 Error: 28.12500 Learning rate: 0.00569\n",
      "Validation error: 28.6%\n",
      "Step 3510 of 40000\n",
      "Mini-batch loss: 4.15191 Error: 35.93750 Learning rate: 0.00569\n",
      "Validation error: 27.6%\n",
      "Step 3520 of 40000\n",
      "Mini-batch loss: 3.95278 Error: 32.03125 Learning rate: 0.00569\n",
      "Validation error: 27.2%\n",
      "Step 3530 of 40000\n",
      "Mini-batch loss: 3.93280 Error: 28.90625 Learning rate: 0.00569\n",
      "Validation error: 28.2%\n",
      "Step 3540 of 40000\n",
      "Mini-batch loss: 3.94491 Error: 36.71875 Learning rate: 0.00569\n",
      "Validation error: 28.9%\n",
      "Step 3550 of 40000\n",
      "Mini-batch loss: 4.14458 Error: 35.15625 Learning rate: 0.00569\n",
      "Validation error: 29.4%\n",
      "Step 3560 of 40000\n",
      "Mini-batch loss: 4.01743 Error: 34.37500 Learning rate: 0.00569\n",
      "Validation error: 28.8%\n",
      "Step 3570 of 40000\n",
      "Mini-batch loss: 3.89885 Error: 27.34375 Learning rate: 0.00569\n",
      "Validation error: 28.3%\n",
      "Step 3580 of 40000\n",
      "Mini-batch loss: 4.00944 Error: 32.03125 Learning rate: 0.00569\n",
      "Validation error: 28.4%\n",
      "Step 3590 of 40000\n",
      "Mini-batch loss: 3.94123 Error: 28.90625 Learning rate: 0.00569\n",
      "Validation error: 28.8%\n",
      "Step 3600 of 40000\n",
      "Mini-batch loss: 4.06222 Error: 36.71875 Learning rate: 0.00569\n",
      "Validation error: 29.0%\n",
      "Step 3610 of 40000\n",
      "Mini-batch loss: 4.22322 Error: 38.28125 Learning rate: 0.00569\n",
      "Validation error: 28.9%\n",
      "Step 3620 of 40000\n",
      "Mini-batch loss: 4.07943 Error: 31.25000 Learning rate: 0.00569\n",
      "Validation error: 29.0%\n",
      "Step 3630 of 40000\n",
      "Mini-batch loss: 3.98766 Error: 29.68750 Learning rate: 0.00569\n",
      "Validation error: 28.8%\n",
      "Step 3640 of 40000\n",
      "Mini-batch loss: 4.09914 Error: 34.37500 Learning rate: 0.00569\n",
      "Validation error: 28.3%\n",
      "Step 3650 of 40000\n",
      "Mini-batch loss: 4.04412 Error: 39.06250 Learning rate: 0.00569\n",
      "Validation error: 29.2%\n",
      "Step 3660 of 40000\n",
      "Mini-batch loss: 4.08899 Error: 37.50000 Learning rate: 0.00569\n",
      "Validation error: 29.8%\n",
      "Step 3670 of 40000\n",
      "Mini-batch loss: 3.97184 Error: 32.81250 Learning rate: 0.00569\n",
      "Validation error: 29.1%\n",
      "Step 3680 of 40000\n",
      "Mini-batch loss: 4.11233 Error: 44.53125 Learning rate: 0.00569\n",
      "Validation error: 28.5%\n",
      "Step 3690 of 40000\n",
      "Mini-batch loss: 4.19213 Error: 42.96875 Learning rate: 0.00569\n",
      "Validation error: 28.5%\n",
      "Step 3700 of 40000\n",
      "Mini-batch loss: 3.86187 Error: 28.12500 Learning rate: 0.00569\n",
      "Validation error: 28.6%\n",
      "Step 3710 of 40000\n",
      "Mini-batch loss: 4.22845 Error: 39.06250 Learning rate: 0.00569\n",
      "Validation error: 29.5%\n",
      "Step 3720 of 40000\n",
      "Mini-batch loss: 3.85271 Error: 27.34375 Learning rate: 0.00569\n",
      "Validation error: 29.2%\n",
      "Step 3730 of 40000\n",
      "Mini-batch loss: 3.98754 Error: 31.25000 Learning rate: 0.00569\n",
      "Validation error: 29.0%\n",
      "Step 3740 of 40000\n",
      "Mini-batch loss: 4.01071 Error: 32.03125 Learning rate: 0.00569\n",
      "Validation error: 29.8%\n",
      "Step 3750 of 40000\n",
      "Mini-batch loss: 4.02109 Error: 34.37500 Learning rate: 0.00540\n",
      "Validation error: 28.6%\n",
      "Step 3760 of 40000\n",
      "Mini-batch loss: 4.00190 Error: 35.93750 Learning rate: 0.00540\n",
      "Validation error: 29.9%\n",
      "Step 3770 of 40000\n",
      "Mini-batch loss: 4.04938 Error: 34.37500 Learning rate: 0.00540\n",
      "Validation error: 27.8%\n",
      "Step 3780 of 40000\n",
      "Mini-batch loss: 4.03575 Error: 30.46875 Learning rate: 0.00540\n",
      "Validation error: 28.2%\n",
      "Step 3790 of 40000\n",
      "Mini-batch loss: 3.96840 Error: 32.81250 Learning rate: 0.00540\n",
      "Validation error: 27.4%\n",
      "Step 3800 of 40000\n",
      "Mini-batch loss: 3.96685 Error: 31.25000 Learning rate: 0.00540\n",
      "Validation error: 26.5%\n",
      "Step 3810 of 40000\n",
      "Mini-batch loss: 4.07528 Error: 35.93750 Learning rate: 0.00540\n",
      "Validation error: 26.6%\n",
      "Step 3820 of 40000\n",
      "Mini-batch loss: 4.00984 Error: 30.46875 Learning rate: 0.00540\n",
      "Validation error: 26.7%\n",
      "Step 3830 of 40000\n",
      "Mini-batch loss: 4.01627 Error: 32.03125 Learning rate: 0.00540\n",
      "Validation error: 26.8%\n",
      "Step 3840 of 40000\n",
      "Mini-batch loss: 3.95863 Error: 30.46875 Learning rate: 0.00540\n",
      "Validation error: 27.0%\n",
      "Step 3850 of 40000\n",
      "Mini-batch loss: 4.08038 Error: 31.25000 Learning rate: 0.00540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 28.1%\n",
      "Step 3860 of 40000\n",
      "Mini-batch loss: 4.02951 Error: 34.37500 Learning rate: 0.00540\n",
      "Validation error: 27.8%\n",
      "Step 3870 of 40000\n",
      "Mini-batch loss: 4.02499 Error: 36.71875 Learning rate: 0.00540\n",
      "Validation error: 27.4%\n",
      "Step 3880 of 40000\n",
      "Mini-batch loss: 3.83591 Error: 29.68750 Learning rate: 0.00540\n",
      "Validation error: 27.3%\n",
      "Step 3890 of 40000\n",
      "Mini-batch loss: 3.80823 Error: 25.78125 Learning rate: 0.00540\n",
      "Validation error: 27.8%\n",
      "Step 3900 of 40000\n",
      "Mini-batch loss: 4.07789 Error: 35.15625 Learning rate: 0.00540\n",
      "Validation error: 28.0%\n",
      "Step 3910 of 40000\n",
      "Mini-batch loss: 3.97411 Error: 31.25000 Learning rate: 0.00540\n",
      "Validation error: 27.8%\n",
      "Step 3920 of 40000\n",
      "Mini-batch loss: 4.02512 Error: 33.59375 Learning rate: 0.00540\n",
      "Validation error: 28.9%\n",
      "Step 3930 of 40000\n",
      "Mini-batch loss: 3.90312 Error: 30.46875 Learning rate: 0.00540\n",
      "Validation error: 28.2%\n",
      "Step 3940 of 40000\n",
      "Mini-batch loss: 3.87006 Error: 32.81250 Learning rate: 0.00540\n",
      "Validation error: 28.3%\n",
      "Step 3950 of 40000\n",
      "Mini-batch loss: 3.94575 Error: 33.59375 Learning rate: 0.00540\n",
      "Validation error: 27.3%\n",
      "Step 3960 of 40000\n",
      "Mini-batch loss: 3.93639 Error: 31.25000 Learning rate: 0.00540\n",
      "Validation error: 27.2%\n",
      "Step 3970 of 40000\n",
      "Mini-batch loss: 4.05885 Error: 36.71875 Learning rate: 0.00540\n",
      "Validation error: 27.7%\n",
      "Step 3980 of 40000\n",
      "Mini-batch loss: 3.92285 Error: 33.59375 Learning rate: 0.00540\n",
      "Validation error: 27.6%\n",
      "Step 3990 of 40000\n",
      "Mini-batch loss: 3.85797 Error: 28.90625 Learning rate: 0.00540\n",
      "Validation error: 27.7%\n",
      "Step 4000 of 40000\n",
      "Mini-batch loss: 3.85078 Error: 30.46875 Learning rate: 0.00540\n",
      "Validation error: 27.5%\n",
      "Step 4010 of 40000\n",
      "Mini-batch loss: 4.05644 Error: 37.50000 Learning rate: 0.00540\n",
      "Validation error: 28.3%\n",
      "Step 4020 of 40000\n",
      "Mini-batch loss: 3.76263 Error: 21.87500 Learning rate: 0.00540\n",
      "Validation error: 27.5%\n",
      "Step 4030 of 40000\n",
      "Mini-batch loss: 4.08301 Error: 36.71875 Learning rate: 0.00540\n",
      "Validation error: 28.8%\n",
      "Step 4040 of 40000\n",
      "Mini-batch loss: 4.04907 Error: 32.81250 Learning rate: 0.00540\n",
      "Validation error: 27.6%\n",
      "Step 4050 of 40000\n",
      "Mini-batch loss: 3.92481 Error: 31.25000 Learning rate: 0.00540\n",
      "Validation error: 27.7%\n",
      "Step 4060 of 40000\n",
      "Mini-batch loss: 3.80924 Error: 27.34375 Learning rate: 0.00540\n",
      "Validation error: 28.0%\n",
      "Step 4070 of 40000\n",
      "Mini-batch loss: 4.04850 Error: 30.46875 Learning rate: 0.00513\n",
      "Validation error: 28.4%\n",
      "Step 4080 of 40000\n",
      "Mini-batch loss: 4.09662 Error: 37.50000 Learning rate: 0.00513\n",
      "Validation error: 27.3%\n",
      "Step 4090 of 40000\n",
      "Mini-batch loss: 4.01417 Error: 37.50000 Learning rate: 0.00513\n",
      "Validation error: 26.8%\n",
      "Step 4100 of 40000\n",
      "Mini-batch loss: 3.88803 Error: 35.15625 Learning rate: 0.00513\n",
      "Validation error: 26.6%\n",
      "Step 4110 of 40000\n",
      "Mini-batch loss: 3.99052 Error: 34.37500 Learning rate: 0.00513\n",
      "Validation error: 25.9%\n",
      "Step 4120 of 40000\n",
      "Mini-batch loss: 3.88985 Error: 30.46875 Learning rate: 0.00513\n",
      "Validation error: 25.7%\n",
      "Step 4130 of 40000\n",
      "Mini-batch loss: 3.99061 Error: 38.28125 Learning rate: 0.00513\n",
      "Validation error: 26.1%\n",
      "Step 4140 of 40000\n",
      "Mini-batch loss: 3.77348 Error: 28.12500 Learning rate: 0.00513\n",
      "Validation error: 26.0%\n",
      "Step 4150 of 40000\n",
      "Mini-batch loss: 4.01213 Error: 35.93750 Learning rate: 0.00513\n",
      "Validation error: 25.7%\n",
      "Step 4160 of 40000\n",
      "Mini-batch loss: 3.81267 Error: 25.00000 Learning rate: 0.00513\n",
      "Validation error: 26.0%\n",
      "Step 4170 of 40000\n",
      "Mini-batch loss: 4.00656 Error: 35.15625 Learning rate: 0.00513\n",
      "Validation error: 26.8%\n",
      "Step 4180 of 40000\n",
      "Mini-batch loss: 3.85034 Error: 33.59375 Learning rate: 0.00513\n",
      "Validation error: 26.3%\n",
      "Step 4190 of 40000\n",
      "Mini-batch loss: 3.82571 Error: 31.25000 Learning rate: 0.00513\n",
      "Validation error: 26.9%\n",
      "Step 4200 of 40000\n",
      "Mini-batch loss: 3.72348 Error: 28.12500 Learning rate: 0.00513\n",
      "Validation error: 26.3%\n",
      "Step 4210 of 40000\n",
      "Mini-batch loss: 3.88153 Error: 28.90625 Learning rate: 0.00513\n",
      "Validation error: 28.3%\n",
      "Step 4220 of 40000\n",
      "Mini-batch loss: 3.96503 Error: 33.59375 Learning rate: 0.00513\n",
      "Validation error: 27.3%\n",
      "Step 4230 of 40000\n",
      "Mini-batch loss: 3.76160 Error: 28.90625 Learning rate: 0.00513\n",
      "Validation error: 26.8%\n",
      "Step 4240 of 40000\n",
      "Mini-batch loss: 3.96313 Error: 32.81250 Learning rate: 0.00513\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-af64c6d39ec6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mini-batch loss: %.5f Error: %.5f Learning rate: %.5f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         print('Validation error: %.1f%%' % error_rate(\n\u001b[0;32m---> 25\u001b[0;31m               validation_prediction.eval(), validation_labels)[0])\n\u001b[0m",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \"\"\"\n\u001b[0;32m--> 569\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3739\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3740\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3741\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/GundamOO/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train over the first 1/4th of our training set.\n",
    "#steps = train_size // BATCH_SIZE\n",
    "steps = train_size\n",
    "for step in range(steps):\n",
    "    # Compute the offset of the current minibatch in the data.\n",
    "    # Note that we could use better randomization across epochs.\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    # This dictionary maps the batch data (as a numpy array) to the\n",
    "    # node in the graph it should be fed to.\n",
    "    feed_dict = {train_data_node: batch_data,\n",
    "                 train_labels_node: batch_labels}\n",
    "    # Run the graph and fetch some of the nodes.\n",
    "    _, l, lr, predictions = s.run(\n",
    "      [optimizer, loss, learning_rate, train_prediction],\n",
    "      feed_dict=feed_dict)\n",
    "    \n",
    "    # Print out the loss periodically.\n",
    "    if step % 10 == 0:\n",
    "        error, _ = error_rate(predictions, batch_labels)\n",
    "        print('Step %d of %d' % (step, steps))\n",
    "        print('Mini-batch loss: %.5f Error: %.5f Learning rate: %.5f' % (l, error, lr))\n",
    "        print('Validation error: %.1f%%' % error_rate(\n",
    "              validation_prediction.eval(), validation_labels)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
