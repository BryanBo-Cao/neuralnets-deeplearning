{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Some of the code is modified from \"3_mnist_from_scratch from\", \"docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\"\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "from CIFAR_reader import CIFAR_reader # Reference: https://github.com/michael-iuzzolino/CIFAR_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CIFAR data...\n",
      "Extracting Data...\n",
      "Unpacking data...\n",
      "Loading training batch 1 of 5...\n",
      "Loading training batch 2 of 5...\n",
      "Loading training batch 3 of 5...\n",
      "Loading training batch 4 of 5...\n",
      "Loading training batch 5 of 5...\n",
      "Loading testing batch 1 of 1...\n"
     ]
    }
   ],
   "source": [
    "cifar = CIFAR_reader(one_hot=True, verbose=True, img_size=32, num_classes=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 128\n",
    "N_CHANNELS = 3\n",
    "N_LABELS = 10\n",
    "SEED = 32\n",
    "\n",
    "training_data = cifar.train\n",
    "training_labels = cifar.labels\n",
    "train_data = training_data['data']\n",
    "train_labels = training_data['labels']\n",
    "\n",
    "testing_data = cifar.test\n",
    "test_data = testing_data['data']\n",
    "test_data = np.float32(test_data)\n",
    "test_labels = testing_data['labels']\n",
    "test_labels = np.float32(test_labels)\n",
    "\n",
    "N_TRAIN_IMAGE = len(train_data)\n",
    "train_data = (train_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "train_data = train_data.reshape(N_TRAIN_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "N_TEST_IMAGE = len(test_data)\n",
    "test_data = (test_data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "test_data = test_data.reshape(N_TEST_IMAGE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "#print(\"train_data[0]:\", train_data[0])\n",
    "#print(\"test_data[0]:\", test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAEICAYAAAByNDmmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1093be5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar.preview_data(data_set=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape (40000, 32, 32, 3)\n",
      "validation_data.shape (10000, 32, 32, 3)\n",
      "train_data size: 40000\n",
      "validation_data size: 10000\n",
      "N_TRAIN_IMAGE:  40000\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_data = np.float32(train_data)\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = len(train_data)\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_data = np.float32(validation_data)\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "validation_size = len(validation_data)\n",
    "\n",
    "print('train_data.shape', train_data.shape)\n",
    "print('validation_data.shape', validation_data.shape)\n",
    "print('train_data size:', train_size)\n",
    "print('validation_data size:', validation_size)\n",
    "#print('validation_data:', validation_data)\n",
    "#print('validation_labels: ', validation_labels)\n",
    "\n",
    "N_TRAIN_IMAGE = train_size\n",
    "print(\"N_TRAIN_IMAGE: \", N_TRAIN_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, N_LABELS))\n",
    "\n",
    "train_all_data_node = tf.constant(train_data)\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, N_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "\n",
    "print('Variables Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training computation: logits + cross-entropy loss done\n"
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# compute only by {eval()} method.\n",
    "train_all_data_prediction = tf.nn.softmax(model(train_all_data_node))\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Training computation: logits + cross-entropy loss done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.as_default()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 3)\n",
      "(128, 10)\n",
      "Run graph done.\n"
     ]
    }
   ],
   "source": [
    "# Grab the first BATCH_SIZE examples and labels.\n",
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# This dictionary maps the batch data (as a numpy array) to the\n",
    "# node in the graph it should be fed to.\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = sess.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_labels.shape)\n",
    "print('Run graph done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.16982765e-01   1.42972870e-03   1.97592890e-05   3.99296312e-07\n",
      "   8.81408807e-04   1.68743473e-03   1.40850125e-05   8.78815353e-01\n",
      "   1.66529047e-04   2.52190534e-06]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prediction 7\n",
      "(128, 10)\n",
      "All predictions [7 5 5 1 5 1 6 0 0 7 5 4 2 1 5 4 5 2 0 1 0 0 5 5 5 1 1 5 4 5 7 5 1 0 2 0 5\n",
      " 0 5 5 5 5 0 5 5 5 5 5 1 7 0 5 1 5 5 4 1 1 5 1 5 5 5 5 5 0 5 5 5 2 0 7 0 5\n",
      " 7 0 1 5 5 7 9 7 4 5 5 9 1 5 5 0 5 5 5 5 5 5 5 1 5 5 0 5 4 0 5 5 4 0 5 1 4\n",
      " 4 4 2 0 5 5 1 0 5 1 0 5 7 5 4 5 7]\n"
     ]
    }
   ],
   "source": [
    "# The highest probability in the first entry.\n",
    "print('First prediction', np.argmax(predictions[0]))\n",
    "\n",
    "# But, predictions is actually a list of BATCH_SIZE probability vectors.\n",
    "print(predictions.shape)\n",
    "\n",
    "# So, we'll take the highest probability for each vector.\n",
    "print('All predictions', np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels [1 6 6 8 8 3 4 6 0 6 0 3 6 6 5 4 8 3 2 6 0 3 1 4 0 6 6 2 7 6 9 0 4 5 7 1 6\n",
      " 7 9 1 7 7 8 0 3 7 4 7 3 1 0 4 6 6 1 4 9 2 6 4 5 0 4 6 0 8 3 4 8 8 3 9 5 7\n",
      " 1 9 4 7 9 1 9 7 5 2 7 3 4 8 8 2 1 5 9 2 7 8 8 6 8 8 1 3 8 8 5 4 7 1 6 6 1\n",
      " 6 1 6 7 0 4 6 9 5 8 7 1 9 0 3 3 7]\n"
     ]
    }
   ],
   "source": [
    "print('Batch labels', np.argmax(batch_labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1015625\n"
     ]
    }
   ],
   "source": [
    "correct = np.sum(np.argmax(predictions, 1) == np.argmax(batch_labels, 1))\n",
    "total = predictions.shape[0]\n",
    "\n",
    "print(float(correct) / float(total))\n",
    "\n",
    "confusions = np.zeros([10, 10], np.float32)\n",
    "bundled = zip(np.argmax(predictions, 1), np.argmax(batch_labels, 1))\n",
    "for predicted, actual in bundled:\n",
    "  confusions[predicted, actual] += 1\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(np.arange(N_LABELS))\n",
    "plt.yticks(np.arange(N_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate defined\n",
      "get_accuracy defined\n"
     ]
    }
   ],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "    return error\n",
    "\n",
    "print('Error rate defined')\n",
    "\n",
    "def get_accuracy(predictions, labels):\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    accuracy = float(correct) / float(total)\n",
    "    accuracy_fig = \"\"\n",
    "    accuracy_fig = str(correct)\n",
    "    accuracy_fig += (\" of \")\n",
    "    accuracy_fig += str(total)\n",
    "    return accuracy, accuracy_fig\n",
    "\n",
    "print('get_accuracy defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 500\n",
      "Validation accuracy: 10.940000% (1094 of 10000), Mini-batch loss: 8.96989, Learning rate: 0.01000\n",
      "Step 5 of 500\n",
      "Validation accuracy: 12.410000% (1241 of 10000), Mini-batch loss: 7.36864, Learning rate: 0.01000\n",
      "Step 10 of 500\n",
      "Validation accuracy: 16.630000% (1663 of 10000), Mini-batch loss: 6.51252, Learning rate: 0.01000\n",
      "Step 15 of 500\n",
      "Validation accuracy: 18.060000% (1806 of 10000), Mini-batch loss: 6.34707, Learning rate: 0.01000\n",
      "Step 20 of 500\n",
      "Validation accuracy: 21.690000% (2169 of 10000), Mini-batch loss: 6.22292, Learning rate: 0.01000\n",
      "Step 25 of 500\n",
      "Validation accuracy: 24.370000% (2437 of 10000), Mini-batch loss: 6.30681, Learning rate: 0.01000\n",
      "Step 30 of 500\n",
      "Validation accuracy: 26.820000% (2682 of 10000), Mini-batch loss: 6.17990, Learning rate: 0.01000\n",
      "Step 35 of 500\n",
      "Validation accuracy: 26.730000% (2673 of 10000), Mini-batch loss: 6.16192, Learning rate: 0.01000\n",
      "Step 40 of 500\n",
      "Validation accuracy: 28.280000% (2828 of 10000), Mini-batch loss: 6.14630, Learning rate: 0.01000\n",
      "Step 45 of 500\n",
      "Validation accuracy: 31.110000% (3111 of 10000), Mini-batch loss: 6.17709, Learning rate: 0.01000\n",
      "Step 50 of 500\n",
      "Validation accuracy: 31.730000% (3173 of 10000), Mini-batch loss: 6.05039, Learning rate: 0.01000\n",
      "Step 55 of 500\n",
      "Validation accuracy: 34.400000% (3440 of 10000), Mini-batch loss: 6.09848, Learning rate: 0.01000\n",
      "Step 60 of 500\n",
      "Validation accuracy: 35.110000% (3511 of 10000), Mini-batch loss: 6.09632, Learning rate: 0.01000\n",
      "Step 65 of 500\n",
      "Validation accuracy: 35.550000% (3555 of 10000), Mini-batch loss: 6.07622, Learning rate: 0.01000\n",
      "Step 70 of 500\n",
      "Validation accuracy: 36.460000% (3646 of 10000), Mini-batch loss: 6.07799, Learning rate: 0.01000\n",
      "Step 75 of 500\n",
      "Validation accuracy: 37.970000% (3797 of 10000), Mini-batch loss: 5.94417, Learning rate: 0.01000\n",
      "Step 80 of 500\n",
      "Validation accuracy: 37.830000% (3783 of 10000), Mini-batch loss: 5.99907, Learning rate: 0.01000\n",
      "Step 85 of 500\n",
      "Validation accuracy: 39.170000% (3917 of 10000), Mini-batch loss: 5.80841, Learning rate: 0.01000\n",
      "Step 90 of 500\n",
      "Validation accuracy: 38.370000% (3837 of 10000), Mini-batch loss: 5.79098, Learning rate: 0.01000\n",
      "Step 95 of 500\n",
      "Validation accuracy: 39.150000% (3915 of 10000), Mini-batch loss: 5.93034, Learning rate: 0.01000\n",
      "Step 100 of 500\n",
      "Validation accuracy: 39.790000% (3979 of 10000), Mini-batch loss: 5.96398, Learning rate: 0.01000\n",
      "Step 105 of 500\n",
      "Validation accuracy: 39.410000% (3941 of 10000), Mini-batch loss: 5.88129, Learning rate: 0.01000\n",
      "Step 110 of 500\n",
      "Validation accuracy: 39.250000% (3925 of 10000), Mini-batch loss: 5.67161, Learning rate: 0.01000\n",
      "Step 115 of 500\n",
      "Validation accuracy: 40.250000% (4025 of 10000), Mini-batch loss: 5.87665, Learning rate: 0.01000\n",
      "Step 120 of 500\n",
      "Validation accuracy: 39.520000% (3952 of 10000), Mini-batch loss: 5.82046, Learning rate: 0.01000\n",
      "Step 125 of 500\n",
      "Validation accuracy: 39.830000% (3983 of 10000), Mini-batch loss: 5.76174, Learning rate: 0.01000\n",
      "Step 130 of 500\n",
      "Validation accuracy: 38.340000% (3834 of 10000), Mini-batch loss: 5.61680, Learning rate: 0.01000\n",
      "Step 135 of 500\n",
      "Validation accuracy: 39.020000% (3902 of 10000), Mini-batch loss: 5.81321, Learning rate: 0.01000\n",
      "Step 140 of 500\n",
      "Validation accuracy: 39.640000% (3964 of 10000), Mini-batch loss: 5.93203, Learning rate: 0.01000\n",
      "Step 145 of 500\n",
      "Validation accuracy: 40.160000% (4016 of 10000), Mini-batch loss: 5.73809, Learning rate: 0.01000\n",
      "Step 150 of 500\n",
      "Validation accuracy: 41.330000% (4133 of 10000), Mini-batch loss: 5.62235, Learning rate: 0.01000\n",
      "Step 155 of 500\n",
      "Validation accuracy: 41.690000% (4169 of 10000), Mini-batch loss: 5.65676, Learning rate: 0.01000\n",
      "Step 160 of 500\n",
      "Validation accuracy: 42.350000% (4235 of 10000), Mini-batch loss: 5.65894, Learning rate: 0.01000\n",
      "Step 165 of 500\n",
      "Validation accuracy: 42.540000% (4254 of 10000), Mini-batch loss: 5.84835, Learning rate: 0.01000\n",
      "Step 170 of 500\n",
      "Validation accuracy: 40.850000% (4085 of 10000), Mini-batch loss: 5.70908, Learning rate: 0.01000\n",
      "Step 175 of 500\n",
      "Validation accuracy: 41.270000% (4127 of 10000), Mini-batch loss: 5.64826, Learning rate: 0.01000\n",
      "Step 180 of 500\n",
      "Validation accuracy: 40.790000% (4079 of 10000), Mini-batch loss: 5.67380, Learning rate: 0.01000\n",
      "Step 185 of 500\n",
      "Validation accuracy: 43.300000% (4330 of 10000), Mini-batch loss: 5.74801, Learning rate: 0.01000\n",
      "Step 190 of 500\n",
      "Validation accuracy: 43.300000% (4330 of 10000), Mini-batch loss: 5.63927, Learning rate: 0.01000\n",
      "Step 195 of 500\n",
      "Validation accuracy: 43.730000% (4373 of 10000), Mini-batch loss: 5.71847, Learning rate: 0.01000\n",
      "Step 200 of 500\n",
      "Validation accuracy: 42.310000% (4231 of 10000), Mini-batch loss: 5.68738, Learning rate: 0.01000\n",
      "Step 205 of 500\n",
      "Validation accuracy: 43.840000% (4384 of 10000), Mini-batch loss: 5.69730, Learning rate: 0.01000\n",
      "Step 210 of 500\n",
      "Validation accuracy: 44.590000% (4459 of 10000), Mini-batch loss: 5.76528, Learning rate: 0.01000\n",
      "Step 215 of 500\n",
      "Validation accuracy: 44.020000% (4402 of 10000), Mini-batch loss: 5.76116, Learning rate: 0.01000\n",
      "Step 220 of 500\n",
      "Validation accuracy: 43.390000% (4339 of 10000), Mini-batch loss: 5.82886, Learning rate: 0.01000\n",
      "Step 225 of 500\n",
      "Validation accuracy: 43.400000% (4340 of 10000), Mini-batch loss: 5.55730, Learning rate: 0.01000\n",
      "Step 230 of 500\n",
      "Validation accuracy: 44.140000% (4414 of 10000), Mini-batch loss: 5.57283, Learning rate: 0.01000\n",
      "Step 235 of 500\n",
      "Validation accuracy: 43.110000% (4311 of 10000), Mini-batch loss: 5.73609, Learning rate: 0.01000\n",
      "Step 240 of 500\n",
      "Validation accuracy: 42.380000% (4238 of 10000), Mini-batch loss: 5.67618, Learning rate: 0.01000\n",
      "Step 245 of 500\n",
      "Validation accuracy: 43.370000% (4337 of 10000), Mini-batch loss: 5.57000, Learning rate: 0.01000\n",
      "Step 250 of 500\n",
      "Validation accuracy: 45.200000% (4520 of 10000), Mini-batch loss: 5.55429, Learning rate: 0.01000\n",
      "Step 255 of 500\n",
      "Validation accuracy: 45.770000% (4577 of 10000), Mini-batch loss: 5.54629, Learning rate: 0.01000\n",
      "Step 260 of 500\n",
      "Validation accuracy: 45.790000% (4579 of 10000), Mini-batch loss: 5.56820, Learning rate: 0.01000\n",
      "Step 265 of 500\n",
      "Validation accuracy: 45.770000% (4577 of 10000), Mini-batch loss: 5.58077, Learning rate: 0.01000\n",
      "Step 270 of 500\n",
      "Validation accuracy: 44.960000% (4496 of 10000), Mini-batch loss: 5.60260, Learning rate: 0.01000\n",
      "Step 275 of 500\n",
      "Validation accuracy: 44.730000% (4473 of 10000), Mini-batch loss: 5.62400, Learning rate: 0.01000\n",
      "Step 280 of 500\n",
      "Validation accuracy: 45.440000% (4544 of 10000), Mini-batch loss: 5.58665, Learning rate: 0.01000\n",
      "Step 285 of 500\n",
      "Validation accuracy: 45.580000% (4558 of 10000), Mini-batch loss: 5.67546, Learning rate: 0.01000\n",
      "Step 290 of 500\n",
      "Validation accuracy: 45.230000% (4523 of 10000), Mini-batch loss: 5.38736, Learning rate: 0.01000\n",
      "Step 295 of 500\n",
      "Validation accuracy: 45.280000% (4528 of 10000), Mini-batch loss: 5.68458, Learning rate: 0.01000\n",
      "Step 300 of 500\n",
      "Validation accuracy: 46.920000% (4692 of 10000), Mini-batch loss: 5.55603, Learning rate: 0.01000\n",
      "Step 305 of 500\n",
      "Validation accuracy: 45.930000% (4593 of 10000), Mini-batch loss: 5.47892, Learning rate: 0.01000\n",
      "Step 310 of 500\n",
      "Validation accuracy: 46.970000% (4697 of 10000), Mini-batch loss: 5.56919, Learning rate: 0.01000\n",
      "Step 315 of 500\n",
      "Validation accuracy: 46.530000% (4653 of 10000), Mini-batch loss: 5.61769, Learning rate: 0.00950\n",
      "Step 320 of 500\n",
      "Validation accuracy: 46.080000% (4608 of 10000), Mini-batch loss: 5.49066, Learning rate: 0.00950\n",
      "Step 325 of 500\n",
      "Validation accuracy: 47.110000% (4711 of 10000), Mini-batch loss: 5.42617, Learning rate: 0.00950\n",
      "Step 330 of 500\n",
      "Validation accuracy: 46.910000% (4691 of 10000), Mini-batch loss: 5.34435, Learning rate: 0.00950\n",
      "Step 335 of 500\n",
      "Validation accuracy: 47.960000% (4796 of 10000), Mini-batch loss: 5.52140, Learning rate: 0.00950\n",
      "Step 340 of 500\n",
      "Validation accuracy: 48.190000% (4819 of 10000), Mini-batch loss: 5.58779, Learning rate: 0.00950\n",
      "Step 345 of 500\n",
      "Validation accuracy: 47.360000% (4736 of 10000), Mini-batch loss: 5.54390, Learning rate: 0.00950\n",
      "Step 350 of 500\n",
      "Validation accuracy: 47.990000% (4799 of 10000), Mini-batch loss: 5.51085, Learning rate: 0.00950\n",
      "Step 355 of 500\n",
      "Validation accuracy: 48.330000% (4833 of 10000), Mini-batch loss: 5.52453, Learning rate: 0.00950\n",
      "Step 360 of 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 48.680000% (4868 of 10000), Mini-batch loss: 5.48614, Learning rate: 0.00950\n",
      "Step 365 of 500\n",
      "Validation accuracy: 49.120000% (4912 of 10000), Mini-batch loss: 5.34098, Learning rate: 0.00950\n",
      "Step 370 of 500\n",
      "Validation accuracy: 49.950000% (4995 of 10000), Mini-batch loss: 5.37984, Learning rate: 0.00950\n",
      "Step 375 of 500\n",
      "Validation accuracy: 50.100000% (5010 of 10000), Mini-batch loss: 5.36856, Learning rate: 0.00950\n",
      "Step 380 of 500\n",
      "Validation accuracy: 49.300000% (4930 of 10000), Mini-batch loss: 5.45897, Learning rate: 0.00950\n",
      "Step 385 of 500\n",
      "Validation accuracy: 49.440000% (4944 of 10000), Mini-batch loss: 5.32027, Learning rate: 0.00950\n",
      "Step 390 of 500\n",
      "Validation accuracy: 50.180000% (5018 of 10000), Mini-batch loss: 5.36415, Learning rate: 0.00950\n",
      "Step 395 of 500\n",
      "Validation accuracy: 49.950000% (4995 of 10000), Mini-batch loss: 5.51847, Learning rate: 0.00950\n",
      "Step 400 of 500\n",
      "Validation accuracy: 49.760000% (4976 of 10000), Mini-batch loss: 5.46389, Learning rate: 0.00950\n",
      "Step 405 of 500\n",
      "Validation accuracy: 49.910000% (4991 of 10000), Mini-batch loss: 5.43567, Learning rate: 0.00950\n",
      "Step 410 of 500\n",
      "Validation accuracy: 50.530000% (5053 of 10000), Mini-batch loss: 5.53805, Learning rate: 0.00950\n",
      "Step 415 of 500\n",
      "Validation accuracy: 50.610000% (5061 of 10000), Mini-batch loss: 5.22060, Learning rate: 0.00950\n",
      "Step 420 of 500\n",
      "Validation accuracy: 50.220000% (5022 of 10000), Mini-batch loss: 5.42620, Learning rate: 0.00950\n",
      "Step 425 of 500\n",
      "Validation accuracy: 50.420000% (5042 of 10000), Mini-batch loss: 5.37271, Learning rate: 0.00950\n",
      "Step 430 of 500\n",
      "Validation accuracy: 50.450000% (5045 of 10000), Mini-batch loss: 5.44106, Learning rate: 0.00950\n",
      "Step 435 of 500\n",
      "Validation accuracy: 50.970000% (5097 of 10000), Mini-batch loss: 5.41091, Learning rate: 0.00950\n",
      "Step 440 of 500\n",
      "Validation accuracy: 50.170000% (5017 of 10000), Mini-batch loss: 5.26849, Learning rate: 0.00950\n",
      "Step 445 of 500\n",
      "Validation accuracy: 50.560000% (5056 of 10000), Mini-batch loss: 5.32833, Learning rate: 0.00950\n",
      "Step 450 of 500\n",
      "Validation accuracy: 49.160000% (4916 of 10000), Mini-batch loss: 5.28021, Learning rate: 0.00950\n",
      "Step 455 of 500\n",
      "Validation accuracy: 49.100000% (4910 of 10000), Mini-batch loss: 5.40201, Learning rate: 0.00950\n",
      "Step 460 of 500\n",
      "Validation accuracy: 49.310000% (4931 of 10000), Mini-batch loss: 5.52653, Learning rate: 0.00950\n",
      "Step 465 of 500\n",
      "Validation accuracy: 50.780000% (5078 of 10000), Mini-batch loss: 5.36069, Learning rate: 0.00950\n",
      "Step 470 of 500\n",
      "Validation accuracy: 49.990000% (4999 of 10000), Mini-batch loss: 5.27545, Learning rate: 0.00950\n",
      "Step 475 of 500\n",
      "Validation accuracy: 51.620000% (5162 of 10000), Mini-batch loss: 5.29266, Learning rate: 0.00950\n",
      "Step 480 of 500\n",
      "Validation accuracy: 50.490000% (5049 of 10000), Mini-batch loss: 5.26180, Learning rate: 0.00950\n",
      "Step 485 of 500\n",
      "Validation accuracy: 50.550000% (5055 of 10000), Mini-batch loss: 5.54325, Learning rate: 0.00950\n",
      "Step 490 of 500\n",
      "Validation accuracy: 49.870000% (4987 of 10000), Mini-batch loss: 5.49375, Learning rate: 0.00950\n",
      "Step 495 of 500\n",
      "Validation accuracy: 50.210000% (5021 of 10000), Mini-batch loss: 5.46700, Learning rate: 0.00950\n"
     ]
    }
   ],
   "source": [
    "#steps = train_size\n",
    "steps = 500\n",
    "for step in range(steps):\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    feed_dict = {train_data_node: batch_data, train_labels_node: batch_labels}\n",
    "    _, l, lr, predictions = sess.run([optimizer, loss, learning_rate, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if step % 5 == 0:\n",
    "        print('Step %d of %d' % (step, steps))\n",
    "        validation_accuracy, validation_accuracy_fig = get_accuracy(\n",
    "              validation_prediction.eval(), validation_labels)\n",
    "        print('Validation accuracy: %.6f%% (%s), Mini-batch loss: %.5f, Learning rate: %.5f' % \n",
    "              (validation_accuracy * 100, validation_accuracy_fig, l, lr))\n",
    "        #feed_train_all_data_dict = {train_all_data_node: train_data}\n",
    "        #train_accuracy, train_accuracy_fig = get_accuracy(\n",
    "         #     train_all_data_prediction.eval(feed_dict=feed_train_all_data_dict), \n",
    "         #   train_labels)\n",
    "        \n",
    "        #train_accuracy, train_accuracy_fig = get_accuracy(\n",
    "        #      train_all_data_prediction.eval(), train_labels)\n",
    "        \n",
    "        #print('Train accuracy: %.6f%% (%s), Mini-batch loss: %.5f, Learning rate: %.5f' % \n",
    "         #     (train_accuracy * 100, train_accuracy_fig, l, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.50180000 (5018 of 10000)\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, test_accuracy_fig = get_accuracy(test_prediction.eval(), test_labels)\n",
    "print('Test accuracy: %.8f (%s)' % (test_accuracy, test_accuracy_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_accuracy, train_accuracy_fig = get_accuracy(train_all_data_prediction.eval(), train_labels)\n",
    "print('Train accuracy: %.8f (%s)' % (train_accuracy, train_accuracy_fig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
