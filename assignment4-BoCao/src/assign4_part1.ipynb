{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Some of the code is modified from \"3_mnist_from_scratch from\", \"docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\"\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "from CIFAR_reader import CIFAR_reader # Reference: https://github.com/michael-iuzzolino/CIFAR_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CIFAR data...\n",
      "Extracting Data...\n",
      "Unpacking data...\n",
      "Loading training batch 1 of 5...\n",
      "Loading training batch 2 of 5...\n",
      "Loading training batch 3 of 5...\n",
      "Loading training batch 4 of 5...\n",
      "Loading training batch 5 of 5...\n",
      "Loading testing batch 1 of 1...\n"
     ]
    }
   ],
   "source": [
    "cifar = CIFAR_reader(one_hot=True, verbose=True, img_size=32, num_classes=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = cifar.train\n",
    "training_labels = cifar.labels\n",
    "train_data = training_data['data']\n",
    "train_labels = training_data['labels']\n",
    "\n",
    "testing_data = cifar.test\n",
    "test_data = testing_data['data']\n",
    "test_labels = testing_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJH4hgvK/1BzWWYV1EUoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa7bd222d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar.preview_data(data_set=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape (40000, 32, 32, 3)\n",
      "validation_data.shape (10000, 32, 32, 3)\n",
      "train_data size: 40000\n",
      "validation_data size: 10000\n",
      "validation_data: [[[[  35.   25.   26.]\n",
      "   [  27.   21.   21.]\n",
      "   [  25.   20.   19.]\n",
      "   ..., \n",
      "   [ 132.  129.  129.]\n",
      "   [ 133.  121.  116.]\n",
      "   [ 126.  115.  113.]]\n",
      "\n",
      "  [[  63.   55.   50.]\n",
      "   [  32.   27.   21.]\n",
      "   [  17.   13.    8.]\n",
      "   ..., \n",
      "   [ 102.   95.   88.]\n",
      "   [ 105.   89.   76.]\n",
      "   [ 100.   86.   77.]]\n",
      "\n",
      "  [[  99.   91.   84.]\n",
      "   [  49.   45.   37.]\n",
      "   [  15.   12.    5.]\n",
      "   ..., \n",
      "   [  46.   43.   39.]\n",
      "   [  52.   41.   34.]\n",
      "   [  52.   44.   41.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 166.  165.  171.]\n",
      "   [ 164.  163.  166.]\n",
      "   [ 164.  164.  165.]\n",
      "   ..., \n",
      "   [ 173.  172.  170.]\n",
      "   [ 170.  170.  168.]\n",
      "   [ 167.  167.  167.]]\n",
      "\n",
      "  [[ 169.  168.  176.]\n",
      "   [ 169.  168.  173.]\n",
      "   [ 168.  168.  171.]\n",
      "   ..., \n",
      "   [ 172.  171.  170.]\n",
      "   [ 168.  167.  167.]\n",
      "   [ 165.  165.  166.]]\n",
      "\n",
      "  [[ 173.  172.  179.]\n",
      "   [ 175.  174.  178.]\n",
      "   [ 173.  173.  175.]\n",
      "   ..., \n",
      "   [ 170.  168.  169.]\n",
      "   [ 168.  167.  168.]\n",
      "   [ 166.  166.  168.]]]\n",
      "\n",
      "\n",
      " [[[  20.   15.   12.]\n",
      "   [  20.   15.   12.]\n",
      "   [  18.   13.   10.]\n",
      "   ..., \n",
      "   [  20.   16.   16.]\n",
      "   [  21.   16.   14.]\n",
      "   [  21.   16.   13.]]\n",
      "\n",
      "  [[  20.   15.   12.]\n",
      "   [  20.   15.   12.]\n",
      "   [  18.   13.   10.]\n",
      "   ..., \n",
      "   [  20.   16.   15.]\n",
      "   [  21.   16.   13.]\n",
      "   [  21.   16.   13.]]\n",
      "\n",
      "  [[  20.   15.   12.]\n",
      "   [  20.   15.   12.]\n",
      "   [  18.   13.   10.]\n",
      "   ..., \n",
      "   [  20.   16.   14.]\n",
      "   [  21.   16.   13.]\n",
      "   [  21.   16.   13.]]\n",
      "\n",
      "  ..., \n",
      "  [[  66.   54.   41.]\n",
      "   [  80.   67.   53.]\n",
      "   [  47.   35.   19.]\n",
      "   ..., \n",
      "   [ 134.  134.  100.]\n",
      "   [ 110.  113.   77.]\n",
      "   [  98.  102.   66.]]\n",
      "\n",
      "  [[  60.   48.   33.]\n",
      "   [  55.   43.   27.]\n",
      "   [  50.   38.   22.]\n",
      "   ..., \n",
      "   [ 123.  125.   83.]\n",
      "   [  79.   81.   42.]\n",
      "   [  72.   74.   38.]]\n",
      "\n",
      "  [[  64.   54.   38.]\n",
      "   [  55.   45.   29.]\n",
      "   [  48.   38.   21.]\n",
      "   ..., \n",
      "   [ 155.  157.  111.]\n",
      "   [ 136.  137.   97.]\n",
      "   [  88.   89.   51.]]]\n",
      "\n",
      "\n",
      " [[[ 116.  103.   56.]\n",
      "   [ 115.  105.   59.]\n",
      "   [ 155.  128.   82.]\n",
      "   ..., \n",
      "   [ 175.  132.   77.]\n",
      "   [ 169.  133.   72.]\n",
      "   [ 142.  119.   53.]]\n",
      "\n",
      "  [[ 116.  102.   58.]\n",
      "   [ 122.  108.   65.]\n",
      "   [ 153.  121.   79.]\n",
      "   ..., \n",
      "   [ 150.  111.   57.]\n",
      "   [ 145.  121.   60.]\n",
      "   [ 133.  123.   54.]]\n",
      "\n",
      "  [[  95.   86.   41.]\n",
      "   [  98.   84.   44.]\n",
      "   [ 141.  106.   67.]\n",
      "   ..., \n",
      "   [ 145.  112.   58.]\n",
      "   [ 126.  111.   51.]\n",
      "   [ 127.  125.   62.]]\n",
      "\n",
      "  ..., \n",
      "  [[  77.   63.   29.]\n",
      "   [  88.   72.   37.]\n",
      "   [  70.   59.   27.]\n",
      "   ..., \n",
      "   [  48.   39.   20.]\n",
      "   [ 116.  108.   84.]\n",
      "   [ 159.  142.  122.]]\n",
      "\n",
      "  [[  55.   37.   16.]\n",
      "   [  65.   46.   24.]\n",
      "   [  68.   53.   30.]\n",
      "   ..., \n",
      "   [  42.   30.   13.]\n",
      "   [ 126.  114.   90.]\n",
      "   [ 160.  147.  125.]]\n",
      "\n",
      "  [[  78.   57.   37.]\n",
      "   [  72.   50.   30.]\n",
      "   [  67.   50.   32.]\n",
      "   ..., \n",
      "   [  52.   37.   18.]\n",
      "   [ 124.  110.   84.]\n",
      "   [ 155.  144.  124.]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 127.  160.  155.]\n",
      "   [ 139.  171.  171.]\n",
      "   [ 155.  191.  184.]\n",
      "   ..., \n",
      "   [ 110.  150.  160.]\n",
      "   [ 120.  157.  164.]\n",
      "   [ 120.  157.  163.]]\n",
      "\n",
      "  [[ 143.  171.  170.]\n",
      "   [ 149.  177.  177.]\n",
      "   [ 155.  183.  179.]\n",
      "   ..., \n",
      "   [ 123.  167.  173.]\n",
      "   [ 137.  171.  176.]\n",
      "   [ 143.  176.  180.]]\n",
      "\n",
      "  [[ 147.  169.  171.]\n",
      "   [ 141.  161.  162.]\n",
      "   [ 141.  159.  157.]\n",
      "   ..., \n",
      "   [ 121.  169.  169.]\n",
      "   [ 129.  171.  169.]\n",
      "   [ 131.  172.  170.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 220.  216.  217.]\n",
      "   [ 219.  215.  216.]\n",
      "   [ 219.  215.  216.]\n",
      "   ..., \n",
      "   [ 211.  203.  206.]\n",
      "   [ 206.  203.  204.]\n",
      "   [ 205.  201.  202.]]\n",
      "\n",
      "  [[ 220.  216.  217.]\n",
      "   [ 218.  214.  215.]\n",
      "   [ 219.  215.  216.]\n",
      "   ..., \n",
      "   [ 210.  202.  205.]\n",
      "   [ 202.  199.  200.]\n",
      "   [ 203.  199.  200.]]\n",
      "\n",
      "  [[ 217.  213.  214.]\n",
      "   [ 216.  212.  213.]\n",
      "   [ 216.  212.  213.]\n",
      "   ..., \n",
      "   [ 202.  195.  197.]\n",
      "   [ 194.  191.  192.]\n",
      "   [ 193.  190.  191.]]]\n",
      "\n",
      "\n",
      " [[[ 190.  183.  176.]\n",
      "   [ 200.  190.  177.]\n",
      "   [ 208.  197.  180.]\n",
      "   ..., \n",
      "   [ 200.  192.  165.]\n",
      "   [ 210.  205.  183.]\n",
      "   [ 206.  204.  191.]]\n",
      "\n",
      "  [[ 173.  166.  156.]\n",
      "   [ 190.  178.  161.]\n",
      "   [ 206.  191.  168.]\n",
      "   ..., \n",
      "   [ 201.  187.  152.]\n",
      "   [ 209.  199.  172.]\n",
      "   [ 205.  199.  184.]]\n",
      "\n",
      "  [[ 173.  168.  158.]\n",
      "   [ 191.  181.  164.]\n",
      "   [ 208.  194.  172.]\n",
      "   ..., \n",
      "   [ 208.  191.  161.]\n",
      "   [ 214.  200.  177.]\n",
      "   [ 207.  199.  187.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 167.  163.  156.]\n",
      "   [ 177.  170.  159.]\n",
      "   [ 187.  179.  165.]\n",
      "   ..., \n",
      "   [ 181.  180.  171.]\n",
      "   [ 192.  192.  185.]\n",
      "   [ 195.  195.  191.]]\n",
      "\n",
      "  [[ 167.  163.  157.]\n",
      "   [ 182.  173.  162.]\n",
      "   [ 195.  184.  169.]\n",
      "   ..., \n",
      "   [ 186.  180.  169.]\n",
      "   [ 193.  192.  183.]\n",
      "   [ 195.  195.  191.]]\n",
      "\n",
      "  [[ 169.  167.  162.]\n",
      "   [ 180.  174.  165.]\n",
      "   [ 190.  183.  172.]\n",
      "   ..., \n",
      "   [ 178.  173.  163.]\n",
      "   [ 192.  190.  182.]\n",
      "   [ 196.  196.  192.]]]\n",
      "\n",
      "\n",
      " [[[ 177.  160.  135.]\n",
      "   [ 174.  159.  134.]\n",
      "   [ 182.  165.  140.]\n",
      "   ..., \n",
      "   [ 202.  185.  160.]\n",
      "   [ 200.  181.  160.]\n",
      "   [ 199.  177.  156.]]\n",
      "\n",
      "  [[ 178.  162.  137.]\n",
      "   [ 178.  161.  137.]\n",
      "   [ 180.  164.  139.]\n",
      "   ..., \n",
      "   [ 210.  187.  162.]\n",
      "   [ 215.  186.  166.]\n",
      "   [ 218.  188.  168.]]\n",
      "\n",
      "  [[ 188.  174.  148.]\n",
      "   [ 189.  171.  147.]\n",
      "   [ 186.  169.  145.]\n",
      "   ..., \n",
      "   [ 222.  192.  171.]\n",
      "   [ 228.  192.  173.]\n",
      "   [ 225.  188.  169.]]\n",
      "\n",
      "  ..., \n",
      "  [[  76.   50.   57.]\n",
      "   [  69.   46.   49.]\n",
      "   [  76.   54.   61.]\n",
      "   ..., \n",
      "   [ 176.  166.  136.]\n",
      "   [ 173.  158.  131.]\n",
      "   [ 184.  163.  137.]]\n",
      "\n",
      "  [[ 120.   89.   94.]\n",
      "   [ 110.   83.   90.]\n",
      "   [ 107.   82.   93.]\n",
      "   ..., \n",
      "   [ 166.  152.  126.]\n",
      "   [ 160.  144.  120.]\n",
      "   [ 164.  146.  123.]]\n",
      "\n",
      "  [[ 129.   86.   86.]\n",
      "   [ 125.   88.   95.]\n",
      "   [ 122.   90.   98.]\n",
      "   ..., \n",
      "   [ 158.  141.  119.]\n",
      "   [ 164.  146.  127.]\n",
      "   [ 171.  156.  136.]]]]\n",
      "validation_labels:  [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = len(train_data)\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_data = np.float32(validation_data)\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "validation_size = len(validation_data)\n",
    "\n",
    "print('train_data.shape', train_data.shape)\n",
    "print('validation_data.shape', validation_data.shape)\n",
    "print('train_data size:', train_size)\n",
    "print('validation_data size:', validation_size)\n",
    "print('validation_data:', validation_data)\n",
    "print('validation_labels: ', validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 100\n",
    "N_CHANNELS = 1\n",
    "N_LABELS = 10\n",
    "SEED = 32\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, N_LABELS))\n",
    "\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, N_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "\n",
    "print('Variables Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 3 and 1 for 'Conv2D_2' (op: 'Conv2D') with input shapes: [10000,32,32,3], [5,5,1,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f241c4e5257f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtrain_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# We'll compute them only once in a while by calling their {eval()} method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mvalidation_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mtest_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-115f18994b31>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(data, train)\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mconv1_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                         padding='SAME')\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Bias and rectified linear non-linearity.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.pyc\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, name)\u001b[0m\n\u001b[1;32m    395\u001b[0m                                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                                 \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                                 data_format=data_format, name=name)\n\u001b[0m\u001b[1;32m    398\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2630\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2631\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2632\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2633\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2634\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1909\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    593\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    594\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    657\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 3 and 1 for 'Conv2D_2' (op: 'Conv2D') with input shapes: [10000,32,32,3], [5,5,1,32]."
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Training computation: logits + cross-entropy loss done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
