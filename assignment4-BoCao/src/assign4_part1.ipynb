{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Some of the code is modified from \"3_mnist_from_scratch from\", \"docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\"\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "from CIFAR_reader import CIFAR_reader # Reference: https://github.com/michael-iuzzolino/CIFAR_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CIFAR data...\n",
      "Extracting Data...\n",
      "Unpacking data...\n",
      "Loading training batch 1 of 5...\n",
      "Loading training batch 2 of 5...\n",
      "Loading training batch 3 of 5...\n",
      "Loading training batch 4 of 5...\n",
      "Loading training batch 5 of 5...\n",
      "Loading testing batch 1 of 1...\n"
     ]
    }
   ],
   "source": [
    "cifar = CIFAR_reader(one_hot=True, verbose=True, img_size=32, num_classes=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = cifar.train\n",
    "training_labels = cifar.labels\n",
    "train_data = training_data['data']\n",
    "train_labels = training_data['labels']\n",
    "\n",
    "testing_data = cifar.test\n",
    "test_data = testing_data['data']\n",
    "test_data = np.float32(test_data)\n",
    "test_labels = testing_data['labels']\n",
    "test_labels = np.float32(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb9UggppALDexAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd03b948a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar.preview_data(data_set=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape (40000, 32, 32, 3)\n",
      "validation_data.shape (10000, 32, 32, 3)\n",
      "train_data size: 40000\n",
      "validation_data size: 10000\n",
      "validation_data: [[[[  35.   25.   26.]\n",
      "   [  27.   21.   21.]\n",
      "   [  25.   20.   19.]\n",
      "   ..., \n",
      "   [ 132.  129.  129.]\n",
      "   [ 133.  121.  116.]\n",
      "   [ 126.  115.  113.]]\n",
      "\n",
      "  [[  63.   55.   50.]\n",
      "   [  32.   27.   21.]\n",
      "   [  17.   13.    8.]\n",
      "   ..., \n",
      "   [ 102.   95.   88.]\n",
      "   [ 105.   89.   76.]\n",
      "   [ 100.   86.   77.]]\n",
      "\n",
      "  [[  99.   91.   84.]\n",
      "   [  49.   45.   37.]\n",
      "   [  15.   12.    5.]\n",
      "   ..., \n",
      "   [  46.   43.   39.]\n",
      "   [  52.   41.   34.]\n",
      "   [  52.   44.   41.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 166.  165.  171.]\n",
      "   [ 164.  163.  166.]\n",
      "   [ 164.  164.  165.]\n",
      "   ..., \n",
      "   [ 173.  172.  170.]\n",
      "   [ 170.  170.  168.]\n",
      "   [ 167.  167.  167.]]\n",
      "\n",
      "  [[ 169.  168.  176.]\n",
      "   [ 169.  168.  173.]\n",
      "   [ 168.  168.  171.]\n",
      "   ..., \n",
      "   [ 172.  171.  170.]\n",
      "   [ 168.  167.  167.]\n",
      "   [ 165.  165.  166.]]\n",
      "\n",
      "  [[ 173.  172.  179.]\n",
      "   [ 175.  174.  178.]\n",
      "   [ 173.  173.  175.]\n",
      "   ..., \n",
      "   [ 170.  168.  169.]\n",
      "   [ 168.  167.  168.]\n",
      "   [ 166.  166.  168.]]]\n",
      "\n",
      "\n",
      " [[[  20.   15.   12.]\n",
      "   [  20.   15.   12.]\n",
      "   [  18.   13.   10.]\n",
      "   ..., \n",
      "   [  20.   16.   16.]\n",
      "   [  21.   16.   14.]\n",
      "   [  21.   16.   13.]]\n",
      "\n",
      "  [[  20.   15.   12.]\n",
      "   [  20.   15.   12.]\n",
      "   [  18.   13.   10.]\n",
      "   ..., \n",
      "   [  20.   16.   15.]\n",
      "   [  21.   16.   13.]\n",
      "   [  21.   16.   13.]]\n",
      "\n",
      "  [[  20.   15.   12.]\n",
      "   [  20.   15.   12.]\n",
      "   [  18.   13.   10.]\n",
      "   ..., \n",
      "   [  20.   16.   14.]\n",
      "   [  21.   16.   13.]\n",
      "   [  21.   16.   13.]]\n",
      "\n",
      "  ..., \n",
      "  [[  66.   54.   41.]\n",
      "   [  80.   67.   53.]\n",
      "   [  47.   35.   19.]\n",
      "   ..., \n",
      "   [ 134.  134.  100.]\n",
      "   [ 110.  113.   77.]\n",
      "   [  98.  102.   66.]]\n",
      "\n",
      "  [[  60.   48.   33.]\n",
      "   [  55.   43.   27.]\n",
      "   [  50.   38.   22.]\n",
      "   ..., \n",
      "   [ 123.  125.   83.]\n",
      "   [  79.   81.   42.]\n",
      "   [  72.   74.   38.]]\n",
      "\n",
      "  [[  64.   54.   38.]\n",
      "   [  55.   45.   29.]\n",
      "   [  48.   38.   21.]\n",
      "   ..., \n",
      "   [ 155.  157.  111.]\n",
      "   [ 136.  137.   97.]\n",
      "   [  88.   89.   51.]]]\n",
      "\n",
      "\n",
      " [[[ 116.  103.   56.]\n",
      "   [ 115.  105.   59.]\n",
      "   [ 155.  128.   82.]\n",
      "   ..., \n",
      "   [ 175.  132.   77.]\n",
      "   [ 169.  133.   72.]\n",
      "   [ 142.  119.   53.]]\n",
      "\n",
      "  [[ 116.  102.   58.]\n",
      "   [ 122.  108.   65.]\n",
      "   [ 153.  121.   79.]\n",
      "   ..., \n",
      "   [ 150.  111.   57.]\n",
      "   [ 145.  121.   60.]\n",
      "   [ 133.  123.   54.]]\n",
      "\n",
      "  [[  95.   86.   41.]\n",
      "   [  98.   84.   44.]\n",
      "   [ 141.  106.   67.]\n",
      "   ..., \n",
      "   [ 145.  112.   58.]\n",
      "   [ 126.  111.   51.]\n",
      "   [ 127.  125.   62.]]\n",
      "\n",
      "  ..., \n",
      "  [[  77.   63.   29.]\n",
      "   [  88.   72.   37.]\n",
      "   [  70.   59.   27.]\n",
      "   ..., \n",
      "   [  48.   39.   20.]\n",
      "   [ 116.  108.   84.]\n",
      "   [ 159.  142.  122.]]\n",
      "\n",
      "  [[  55.   37.   16.]\n",
      "   [  65.   46.   24.]\n",
      "   [  68.   53.   30.]\n",
      "   ..., \n",
      "   [  42.   30.   13.]\n",
      "   [ 126.  114.   90.]\n",
      "   [ 160.  147.  125.]]\n",
      "\n",
      "  [[  78.   57.   37.]\n",
      "   [  72.   50.   30.]\n",
      "   [  67.   50.   32.]\n",
      "   ..., \n",
      "   [  52.   37.   18.]\n",
      "   [ 124.  110.   84.]\n",
      "   [ 155.  144.  124.]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 127.  160.  155.]\n",
      "   [ 139.  171.  171.]\n",
      "   [ 155.  191.  184.]\n",
      "   ..., \n",
      "   [ 110.  150.  160.]\n",
      "   [ 120.  157.  164.]\n",
      "   [ 120.  157.  163.]]\n",
      "\n",
      "  [[ 143.  171.  170.]\n",
      "   [ 149.  177.  177.]\n",
      "   [ 155.  183.  179.]\n",
      "   ..., \n",
      "   [ 123.  167.  173.]\n",
      "   [ 137.  171.  176.]\n",
      "   [ 143.  176.  180.]]\n",
      "\n",
      "  [[ 147.  169.  171.]\n",
      "   [ 141.  161.  162.]\n",
      "   [ 141.  159.  157.]\n",
      "   ..., \n",
      "   [ 121.  169.  169.]\n",
      "   [ 129.  171.  169.]\n",
      "   [ 131.  172.  170.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 220.  216.  217.]\n",
      "   [ 219.  215.  216.]\n",
      "   [ 219.  215.  216.]\n",
      "   ..., \n",
      "   [ 211.  203.  206.]\n",
      "   [ 206.  203.  204.]\n",
      "   [ 205.  201.  202.]]\n",
      "\n",
      "  [[ 220.  216.  217.]\n",
      "   [ 218.  214.  215.]\n",
      "   [ 219.  215.  216.]\n",
      "   ..., \n",
      "   [ 210.  202.  205.]\n",
      "   [ 202.  199.  200.]\n",
      "   [ 203.  199.  200.]]\n",
      "\n",
      "  [[ 217.  213.  214.]\n",
      "   [ 216.  212.  213.]\n",
      "   [ 216.  212.  213.]\n",
      "   ..., \n",
      "   [ 202.  195.  197.]\n",
      "   [ 194.  191.  192.]\n",
      "   [ 193.  190.  191.]]]\n",
      "\n",
      "\n",
      " [[[ 190.  183.  176.]\n",
      "   [ 200.  190.  177.]\n",
      "   [ 208.  197.  180.]\n",
      "   ..., \n",
      "   [ 200.  192.  165.]\n",
      "   [ 210.  205.  183.]\n",
      "   [ 206.  204.  191.]]\n",
      "\n",
      "  [[ 173.  166.  156.]\n",
      "   [ 190.  178.  161.]\n",
      "   [ 206.  191.  168.]\n",
      "   ..., \n",
      "   [ 201.  187.  152.]\n",
      "   [ 209.  199.  172.]\n",
      "   [ 205.  199.  184.]]\n",
      "\n",
      "  [[ 173.  168.  158.]\n",
      "   [ 191.  181.  164.]\n",
      "   [ 208.  194.  172.]\n",
      "   ..., \n",
      "   [ 208.  191.  161.]\n",
      "   [ 214.  200.  177.]\n",
      "   [ 207.  199.  187.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 167.  163.  156.]\n",
      "   [ 177.  170.  159.]\n",
      "   [ 187.  179.  165.]\n",
      "   ..., \n",
      "   [ 181.  180.  171.]\n",
      "   [ 192.  192.  185.]\n",
      "   [ 195.  195.  191.]]\n",
      "\n",
      "  [[ 167.  163.  157.]\n",
      "   [ 182.  173.  162.]\n",
      "   [ 195.  184.  169.]\n",
      "   ..., \n",
      "   [ 186.  180.  169.]\n",
      "   [ 193.  192.  183.]\n",
      "   [ 195.  195.  191.]]\n",
      "\n",
      "  [[ 169.  167.  162.]\n",
      "   [ 180.  174.  165.]\n",
      "   [ 190.  183.  172.]\n",
      "   ..., \n",
      "   [ 178.  173.  163.]\n",
      "   [ 192.  190.  182.]\n",
      "   [ 196.  196.  192.]]]\n",
      "\n",
      "\n",
      " [[[ 177.  160.  135.]\n",
      "   [ 174.  159.  134.]\n",
      "   [ 182.  165.  140.]\n",
      "   ..., \n",
      "   [ 202.  185.  160.]\n",
      "   [ 200.  181.  160.]\n",
      "   [ 199.  177.  156.]]\n",
      "\n",
      "  [[ 178.  162.  137.]\n",
      "   [ 178.  161.  137.]\n",
      "   [ 180.  164.  139.]\n",
      "   ..., \n",
      "   [ 210.  187.  162.]\n",
      "   [ 215.  186.  166.]\n",
      "   [ 218.  188.  168.]]\n",
      "\n",
      "  [[ 188.  174.  148.]\n",
      "   [ 189.  171.  147.]\n",
      "   [ 186.  169.  145.]\n",
      "   ..., \n",
      "   [ 222.  192.  171.]\n",
      "   [ 228.  192.  173.]\n",
      "   [ 225.  188.  169.]]\n",
      "\n",
      "  ..., \n",
      "  [[  76.   50.   57.]\n",
      "   [  69.   46.   49.]\n",
      "   [  76.   54.   61.]\n",
      "   ..., \n",
      "   [ 176.  166.  136.]\n",
      "   [ 173.  158.  131.]\n",
      "   [ 184.  163.  137.]]\n",
      "\n",
      "  [[ 120.   89.   94.]\n",
      "   [ 110.   83.   90.]\n",
      "   [ 107.   82.   93.]\n",
      "   ..., \n",
      "   [ 166.  152.  126.]\n",
      "   [ 160.  144.  120.]\n",
      "   [ 164.  146.  123.]]\n",
      "\n",
      "  [[ 129.   86.   86.]\n",
      "   [ 125.   88.   95.]\n",
      "   [ 122.   90.   98.]\n",
      "   ..., \n",
      "   [ 158.  141.  119.]\n",
      "   [ 164.  146.  127.]\n",
      "   [ 171.  156.  136.]]]]\n",
      "validation_labels:  [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = len(train_data)\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_data = np.float32(validation_data)\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "validation_size = len(validation_data)\n",
    "\n",
    "print('train_data.shape', train_data.shape)\n",
    "print('validation_data.shape', validation_data.shape)\n",
    "print('train_data size:', train_size)\n",
    "print('validation_data size:', validation_size)\n",
    "print('validation_data:', validation_data)\n",
    "print('validation_labels: ', validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 10\n",
    "N_CHANNELS = 3\n",
    "N_LABELS = 10\n",
    "SEED = 32\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, N_LABELS))\n",
    "\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, N_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "\n",
    "print('Variables Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training computation: logits + cross-entropy loss done\n"
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Training computation: logits + cross-entropy loss done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.as_default()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 32, 32, 3)\n",
      "(10, 10)\n",
      "Run graph done.\n"
     ]
    }
   ],
   "source": [
    "# Grab the first BATCH_SIZE examples and labels.\n",
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# This dictionary maps the batch data (as a numpy array) to the\n",
    "# node in the graph it should be fed to.\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = s.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_labels.shape)\n",
    "print('Run graph done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prediction 7\n",
      "(10, 10)\n",
      "All predictions [7 4 0 1 1 5 7 0 3 7]\n"
     ]
    }
   ],
   "source": [
    "# The highest probability in the first entry.\n",
    "print('First prediction', np.argmax(predictions[0]))\n",
    "\n",
    "# But, predictions is actually a list of BATCH_SIZE probability vectors.\n",
    "print(predictions.shape)\n",
    "\n",
    "# So, we'll take the highest probability for each vector.\n",
    "print('All predictions', np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels [1 6 6 8 8 3 4 6 0 6]\n"
     ]
    }
   ],
   "source": [
    "print('Batch labels', np.argmax(batch_labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "correct = np.sum(np.argmax(predictions, 1) == np.argmax(batch_labels, 1))\n",
    "total = predictions.shape[0]\n",
    "\n",
    "print(float(correct) / float(total))\n",
    "\n",
    "confusions = np.zeros([10, 10], np.float32)\n",
    "bundled = zip(np.argmax(predictions, 1), np.argmax(batch_labels, 1))\n",
    "for predicted, actual in bundled:\n",
    "  confusions[predicted, actual] += 1\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(np.arange(N_LABELS))\n",
    "plt.yticks(np.arange(N_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate defined\n"
     ]
    }
   ],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate and confusions.\"\"\"\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "    confusions = np.zeros([10, 10], np.float32)\n",
    "    bundled = zip(np.argmax(predictions, 1), np.argmax(labels, 1))\n",
    "    for predicted, actual in bundled:\n",
    "        confusions[predicted, actual] += 1\n",
    "    \n",
    "    return error, confusions\n",
    "\n",
    "print('Error rate defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 4000\n",
      "Mini-batch loss: 700806848.00000 Error: 60.00000 Learning rate: 0.01000\n"
     ]
    }
   ],
   "source": [
    "# Train over the first 1/4th of our training set.\n",
    "steps = train_size // BATCH_SIZE\n",
    "for step in range(steps):\n",
    "    # Compute the offset of the current minibatch in the data.\n",
    "    # Note that we could use better randomization across epochs.\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    # This dictionary maps the batch data (as a numpy array) to the\n",
    "    # node in the graph it should be fed to.\n",
    "    feed_dict = {train_data_node: batch_data,\n",
    "                 train_labels_node: batch_labels}\n",
    "    # Run the graph and fetch some of the nodes.\n",
    "    _, l, lr, predictions = s.run(\n",
    "      [optimizer, loss, learning_rate, train_prediction],\n",
    "      feed_dict=feed_dict)\n",
    "    \n",
    "    # Print out the loss periodically.\n",
    "    if step % 10 == 0:\n",
    "        error, _ = error_rate(predictions, batch_labels)\n",
    "        print('Step %d of %d' % (step, steps))\n",
    "        print('Mini-batch loss: %.5f Error: %.5f Learning rate: %.5f' % (l, error, lr))\n",
    "        print('Validation error: %.1f%%' % error_rate(\n",
    "              validation_prediction.eval(), validation_labels)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
