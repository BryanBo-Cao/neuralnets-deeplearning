{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Bryan Bo Cao\n",
    "Email: boca7588@colorado.edu or bo.cao-1@colorado.edu\n",
    "Github Repo: https://github.com/BryanBo-Cao/neuralnets-deeplearning\n",
    "Some of the code is modified from \"3_mnist_from_scratch from\", \"docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\"\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import import_module\n",
    "from CIFAR_reader import CIFAR_reader # Reference: https://github.com/michael-iuzzolino/CIFAR_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for CIFAR data...\n",
      "Extracting Data...\n",
      "Unpacking data...\n",
      "Loading training batch 1 of 5...\n",
      "Loading training batch 2 of 5...\n",
      "Loading training batch 3 of 5...\n",
      "Loading training batch 4 of 5...\n",
      "Loading training batch 5 of 5...\n",
      "Loading testing batch 1 of 1...\n"
     ]
    }
   ],
   "source": [
    "cifar = CIFAR_reader(one_hot=True, verbose=True, img_size=32, num_classes=10, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = cifar.train\n",
    "training_labels = cifar.labels\n",
    "train_data = training_data['data']\n",
    "train_labels = training_data['labels']\n",
    "\n",
    "testing_data = cifar.test\n",
    "test_data = testing_data['data']\n",
    "test_data = np.float32(test_data)\n",
    "test_labels = testing_data['labels']\n",
    "test_labels = np.float32(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAE99JREFUeJzt3XuMXdV1x/HfundmbI/tMcXG5k0DsSEldgwqr4RAA6nU\nQAJRm1ZtHoI/0iZKq/4BqVRVVUsrJa1aNY3aFBFVjdQqqCFR6SMNUQmlEAohCUpSQ5zY4WHANsYP\n/H7O3Lv7x1xXk+lZyzObO9eO1vcjoThnzz5nn3PP8rlzltfeVkoRgHxaJ3sAAE4Ogh9IiuAHkiL4\ngaQIfiApgh9IiuA/xZjZXWb2+ZM9Do+ZfdXMbnsd/R8xsw/3c0yoQ/CfBGb2fjN7yswOmNkrvYC6\n9mSP6zib9LyZrZ/eVkp5Vynl70/GuNBfBP+Amdkdkj4t6ZOSVkg6X9Ldkm49meOa5jpJyyVdaGZX\nzLSTmQ3N3ZDQbwT/AJnZEkl/LOk3Syn3l1IOllLGSylfLqX8jtPnS2a2zcz2mtnXzezSKW03mdl6\nM9tvZlvM7OO97cvM7N/NbI+ZvWZmj5nZbD7r2yT9q6QHen+eOp7/+9puZreb2eNm9pdmtkvSXVO2\nfaY35h+a2Y3OuV1kZg+b2S4z22lm95rZaVPaN5nZx81sXW9f95nZ/Cnt7zaz7/XO8wkzWzOLc0yP\n4B+sayTNl/TPs+jzVUkrNfkk/o6ke6e0/Z2kj5RSFkt6s6SHe9vvlLRZ0hma/Hbxe5KKJJnZ3WZ2\nt3cwMxuV9L7ece6V9KtmNhKM7ypJz/eO84kp256TtEzSH0q638xObzqcpD+RdLakN0k6T9Jd037m\nVyT9gqQ3SFoj6fbeOC+T9DlJH5G0VNJnJf2bmc0LxoopCP7BWippZyllYqYdSimfK6XsL6Uc1WRg\nvKX3DUKSxiX9jJmNlVJ2l1K+M2X7WZIu6H2zeKz0ijhKKR8rpXwsOOQvSjoq6UFJX5E0LOnm4Oe3\nllL+upQyUUo53Nu2XdKne8e+T9KGpn2UUp4tpXytlHK0lLJD0qckXT/tx/6qlLK1lPKapC9LWtvb\n/huSPltK+WYppdN7D3FU0tXBWDEFwT9YuyQtm+nvxmbWNrM/NbPnzGyfpE29pmW9//0lSTdJetHM\nHjWza3rb/1zSs5Ie7L24+91ZjPE2SV/sBfMRSf+kaV/9p3m5YduW43/Z9Lyoyaf7jzGzFWb2hd6v\nLPskfX7KuR23bcqfD0la1PvzBZLu7H3l32NmezT5zeH/HQfNCP7B+oYmn07vneHPv1+TLwLfKWmJ\npJ/ubTdJKqV8u5RyqyZ/JfgXSV/sbd9fSrmzlHKhpFsk3eH93j2VmZ0r6QZJH+y9Z9imyV8BbjKz\n6UF5XFNZ6DlmZlP+//mStjb83Cd7/VeXUsYkffD4uc3Ay5I+UUo5bcp/o6WUf5xh//QI/gEqpeyV\n9AeS/sbM3mtmo2Y2bGbvMrM/a+iyWJN/WeySNKrJYJEkmdmImX3AzJaUUsYl7ZPU7bW928ze2AvA\nvZI6x9tO4EOSNkq6WJNfr9dKWqXJ9we/NotTXS7pt3vn9sua/H3+Aef8Dkjaa2bnSGp86en4W0kf\nNbOreqnJhWZ2s5ktnsU+UiP4B6yU8heS7pD0+5J2aPIJ9luafHJP9w+a/Mq8RdJ6SU9Oa/+QpE29\nr8wflfSB3vaVkh7SZGB9Q9LdpZT/kiQzu8fM7nGGd1vvZ7dN/U/SPYq/+k/3zd4YdmryJeD7Sim7\nGn7ujyRdrsm/oL4i6f6ZHqCU8pSkX5f0GUm7Nflrzu2zGGN6xmQe6Cczu13Sh0spp8w/WkIznvxA\nUgQ/kBRf+4GkePIDSQ20EOOaK99e9TXjx1PGJ94uSdE3mlaQSm4H++w6++wGmelWy//7NRp/JDo3\nb49Ds/qn/XMnHHvl9YhymF6bReOYizEG+4zuH0+r6+/v0ScfmdEeT407AsDAEfxAUgQ/kBTBDyRF\n8ANJEfxAUgNN9dWmSTyl6yd5opxi7T9rctNUc5Cya0X77PN1HKR+3wOT+/Tb3Kdb0MlK0Ball4P7\nMTprNx0c3KhmMynSjPHkB5Ii+IGkCH4gKYIfSIrgB5Ii+IGkBprqi1Jb7RlP2jp1f35bK9hdu7LS\nruXkXkqUNvKHEZ5ztMBOJ8gBueOP0kZ+UzzrZ03aLkzLBUeL0mhR+s09cf/6RuccnXK73fb3Gd2s\nTlsUL/2YhYMnP5AUwQ8kRfADSRH8QFIEP5DUKVPYY9Gbb3+Hr29AjbuM3vY3t0Vv3yPR/H6Rmjn8\noiFGo++GmYXZZ03CgqugsR2kb+K5HJu3d8I36dE8jm7TCeb+8/vV3D39KJDiyQ8kRfADSRH8QFIE\nP5AUwQ8kRfADSQ001RcuXRX0M3fqvNkvrXWitjDf1O/UYuX+WhUpNu8ankiUvqpSub+oeCpKzbnX\nIzhW+ESM7qugW80nHabz+nAr8uQHkiL4gaQIfiApgh9IiuAHkiL4gaROnVRfRaVaN0h3hGmXqJou\nSqM5lWX+zG2xqrn4FKf6vIq/6FpFn0t0bmGqNZhzL9ij29IJy+miVN9sjyQNBfvrBHdWCSoPw+vh\nfGbx3ISvH09+ICmCH0iK4AeSIviBpAh+ICmCH0hqoKm+sKKrz8VjYTov4C/vJHeM4cSklZV7tft0\nJ87sd3XeHIgq9+o5VX1BWq4V3gLBMl9haers05FRn/A+nSGe/EBSBD+QFMEPJEXwA0kR/EBSBD+Q\n1GBTfeGCZbOvcAuyNQrrtqpTMrNPo4Xr6lWmtmr22Y+13WYzDk/1eo3Bhx2Nw6tYbIXr+wXXNyjO\ni+7HbnTebef+jtLVpeO3zRBPfiApgh9IiuAHkiL4gaQIfiCpwb7tD4Rz1lUW6XhK8Fa20+c32LX9\narME3lxxtYVCtQVBNdckKqhphZmA2d8f0VmFBUbBoVrRPINRiqndvNOoeKcfyRue/EBSBD+QFMEP\nJEXwA0kR/EBSBD+Q1EBTfXEBRrAwlNcvyHe0g/TPeLB0Uk1qq92uXbDL1+/0Yd3yWfXc8Uen1fGv\nfTT+mlRlN0ijRam+qCAoLE7ze7kjie/E15/r48kPJEXwA0kR/EBSBD+QFMEPJEXwA0kNNNXXjqrR\nSpR+c/YXTJoWLZ0UzQc3ZLO/JIOsfKs1F8fq+zJl0aMoWkIraPPG0Y526MypJ0nBbRoK6/3cakC/\nV0fM4QegEsEPJEXwA0kR/EBSBD+QFMEPJDXgVF9dpZ1XwFRbpxYm5iqX3qoRTUxaswRV1C/qM8iU\nYyiqzgu6RctkmdfRht0+E0E+z+8Vp5drKv76fb9Nx5MfSIrgB5Ii+IGkCH4gKYIfSIrgB5I6hSbw\nnH2apzZ99ZNehVezrmE0AWa/10KM1E4kGl6PYDJL77NePOz3uXB4xG3bePio23akG92P/U0h9+Ne\n5MkPJEXwA0kR/EBSBD+QFMEPJEXwA0kNNNUXVqNFa6c5qZDalF1tNV2/U31zkTocHx9v3D4cpK9K\niVJldRNFeucWpuyCj7NVuTbduLPPq85a7vZZu2ix2/bsDza6bZ3gBKJz884sTOn6u5sxnvxAUgQ/\nkBTBDyRF8ANJEfxAUqdMYU/0tr9mfz8JoiKXdrtd1e+SSy5p3D46usjt8/TTT7tttUU/5vQLi7vc\nZaukdjCO8YnmDIckLR8bbdz+1mAGyM1bXnLbDlbeckFCxc1kRPmNbrTDGeLJDyRF8ANJEfxAUgQ/\nkBTBDyRF8ANJDTTVFwvXOmruMQeFMTX7nItx1C7XtWrVqsbtu3fv7fuxokW0Smf2c/WFWbQgvdkN\n0nZXLPupxu0r9+xx+3xr+063bXykOXUoSUPRtQpO7mSlrHnyA0kR/EBSBD+QFMEPJEXwA0kR/EBS\ng63qC5bkUpTuqMiE1C7XVTOH38DnBAzSaEcOHWrcPjJv2O3THvYrCDUx4TaFoy/N/az416ptftvR\nrj+OFUvG3LZrJpqv1cjEMbfP0Bn+/H4jh/1+5dgRt03BuXm3SHh/UNUHoBbBDyRF8ANJEfxAUgQ/\nkBTBDyQ10FRftzLFVtNnkJVSnY6/pFU0MWmUIrQgldMNjjd+rDkVtWCxvwRVK0gpRZOFRumrTstL\nH/rHWjDun9f58/1U5TvOPMNtu2RkXuP2kTXN1Y+StNaa+0jSE098y217dds+t23YGYckybn+8T1M\nqg9AJYIfSIrgB5Ii+IGkCH4gKYIfSGqgqb5+T3RZu7+oX201oN+psq0dnFsnSgM2r1s3NORX7rWC\nir+DTupQkhYN+bfPec76eavkr6t3xRlL3LY3r73MbVv2jhvdtpecStINL21x+4zvP+C2rb64eS1E\nSTo27l+rA/v3u23mPYODWyBKm88UT34gKYIfSIrgB5Ii+IGkCH4gqcEu19XnWpv6ZabqeJmAcBxB\n9iCYOU/dY/6cdd3iF9t0nGKb0ZERt8/qxf5b9sVH/Hnp3rLULxZaee7ZjdvPWPlGt09nxVlu2+bR\n0922//jRZrftwYe/1rj9B99f5/Y521niS5LOP/c8t214nn+NJ/b598iQe4v0OfM0DU9+ICmCH0iK\n4AeSIviBpAh+ICmCH0hqwHP4+SmqQS6TFdfTVBT9BMfqBEfrjPjJvrFRfwmqs0f8j23NkeYCkje9\n5BfU3LziNH8cb1vttrUvXuO2vTjUnAZ8fL8/jo3Pb3Xbvv7IF9y2DeuecNv279vRuH1knn8Nj+zb\n47ZtfWWb2zayYL7bNn++3zbhLIlmwf3d7UONHE9+ICmCH0iK4AeSIviBpAh+ICmCH0hqoKm+2kok\nL6sRZzuC1iDlGC2FVZxUnwXHOj045dXBElQ/f5FfPXbBsN9vbGnz0lXHLjjf7fPa0uYKPEnaOO6n\nI7fu8Cv+Nmz4fuP2//n2426f/Xv8NNqWl19w28z8ufNWrGi+HlHRpwX1lkPBtW+5S5RJ3Qn/nnPv\nnzleco4nP5AUwQ8kRfADSRH8QFIEP5AUwQ8kNdgJPAPR30JtLxXSrUwdmt9vouun+pY6aZ4bFvoV\nW5cv8yfHXBUs/TQUpOZ2nu5PdPlMq7ma7oWd+9w++5/zl5IaCZYN27HtebftPx/4UuP2Xbu2u32s\n+JOWjo2Num0LF/oTbg57qbng1iklmDjT7xamfKtEqb7Ke38qnvxAUgQ/kBTBDyRF8ANJEfxAUgQ/\nkNQpk+orQeqitGrSGlEux28aHvbTNddf/bON229bu9btM3LRSrft+Za/ttszm/1JJJ/b4E90OTby\nWuP2889Z7vaZP+JPqlm6fsXcdjvktk10mtsWLpzn9jl9yQq3re2sQSidaLJWry1YQ7Hth0V0rG7X\nr9yL1FS7slYfgGoEP5AUwQ8kRfADSRH8QFKDfdsfvdGP3tg6bdaKCjD8tk7HLyC58rLL3bZLr7up\ncftTI/4b7B2bmpeLkqSN63/otj3zzDq3bftO/23/aactaNy+ZvWlbp8bbnyn27Zzx163rW3+nHW3\n3HRL4/ZNL/hz8b263Z/D7/ARP7OgaIk157ZqVy71Flb2VPKWgatZwm42ePIDSRH8QFIEP5AUwQ8k\nRfADSRH8QFIDTfVZZTGCl/KY6ASFFEEqJCoUOnDIL2T57rrvNm7fuau5mEaS1q9vXrZKkjZt+pHb\nduyYvxTW+IQ/xrdfe23j9ve851a3z/Iz/TkB77vvU27b0+u+57Zde/WVjdtXXfQGt89ZZ/rFR88+\n61+rHbv9Iqjh4eY0bDnmFzNFS7bVTtN3sop3Ijz5gaQIfiApgh9IiuAHkiL4gaQIfiApm+t0wlTX\nX/Vz7sH8udakdru5eqwTVT21g6qtoLrwyGG/euzQ4QPO/upSjq0hP9M6Njbmtg0NOUtQSbryyuYU\n21uvfZvb54nH/ttte+jBh9y2g4f8JcDmL2hewuzMIK14ycpVbtuSMX/ZMzt82G3bsvmVxu2bdux0\n+7Si6tNgqbdoBr9ofj+3ajVKVwf3/qNPPjKjhCRPfiApgh9IiuAHkiL4gaQIfiApgh9IaqCpvuuu\nuj6akdBtajspsSg9GFb1dYJ+Xb+ia8KtpvOPNW9+c8pLklptP2XXjlKVwWfmTU56OEiHWZDaWrCg\neUJQKa4uPOikTA8e8scxPOSf81nnnO22nbvYTwPufq254nJPUBFqQc4uipaorSbVF4nSgKT6AIQI\nfiApgh9IiuAHkiL4gaQIfiCpwa7VF81+GKy7N+6k31pBuiNKrQxFlXZOBaEkDQ8vatweLpsWLfsW\npBWjSxWtM9duN3+k86KquOAExif8dQ2HgkGOLWpOYy6YN+r2OXLUr6jc8tLLbtur7ebKvclxNH9m\nQ8Mjbp9u5TOx32lz1uoDMCcIfiApgh9IiuAHkiL4gaQIfiCpAaf6gskPg8qyrpPyKEG6I0yFmP93\nXpQi9LSC1Fs0DgtSZdE4ojb3eJVVZXG6yW2S120omLR04dBCt23+/OY198KDSWq3mlO34TmH9Xm+\n2gk3Txae/EBSBD+QFMEPJEXwA0kR/EBSA33b3wneokZLaHmvlYOVk2TB29Vu8YtVojfw3lvg6Lyi\ncUTjr+WNP3wzHxVIRZ9ZxRSK0fU182/HlvkFV9FAvJZ4Tr3ZZ3yk+rf9NRmmmj7T8eQHkiL4gaQI\nfiApgh9IiuAHkiL4gaQGmurzCnROxEughLU7URoqmiAv4O0yLNoImlq11yNKKTkHDM85SkMF3bpB\nSsxbAiy68vXz0s0+xRZ+ZJWfS22/mvPuR6EQT34gKYIfSIrgB5Ii+IGkCH4gKYIfSMpOxbnFAMw9\nnvxAUgQ/kBTBDyRF8ANJEfxAUgQ/kBTBDyRF8ANJEfxAUgQ/kBTBDyRF8ANJEfxAUgQ/kBTBDyRF\n8ANJEfxAUgQ/kBTBDyRF8ANJEfxAUgQ/kBTBDyT1vwZpmO6tR4JvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa252c08690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cifar.preview_data(data_set=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (train_labels.shape)\n",
    "print (test_data.shape)\n",
    "print (test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape (40000, 32, 32, 3)\n",
      "validation_data.shape (10000, 32, 32, 3)\n",
      "train_data size: 40000\n",
      "validation_data size: 10000\n",
      "validation_data: [[[[  35.   25.   26.]\n",
      "   [  27.   21.   21.]\n",
      "   [  25.   20.   19.]\n",
      "   ..., \n",
      "   [ 132.  129.  129.]\n",
      "   [ 133.  121.  116.]\n",
      "   [ 126.  115.  113.]]\n",
      "\n",
      "  [[  63.   55.   50.]\n",
      "   [  32.   27.   21.]\n",
      "   [  17.   13.    8.]\n",
      "   ..., \n",
      "   [ 102.   95.   88.]\n",
      "   [ 105.   89.   76.]\n",
      "   [ 100.   86.   77.]]\n",
      "\n",
      "  [[  99.   91.   84.]\n",
      "   [  49.   45.   37.]\n",
      "   [  15.   12.    5.]\n",
      "   ..., \n",
      "   [  46.   43.   39.]\n",
      "   [  52.   41.   34.]\n",
      "   [  52.   44.   41.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 166.  165.  171.]\n",
      "   [ 164.  163.  166.]\n",
      "   [ 164.  164.  165.]\n",
      "   ..., \n",
      "   [ 173.  172.  170.]\n",
      "   [ 170.  170.  168.]\n",
      "   [ 167.  167.  167.]]\n",
      "\n",
      "  [[ 169.  168.  176.]\n",
      "   [ 169.  168.  173.]\n",
      "   [ 168.  168.  171.]\n",
      "   ..., \n",
      "   [ 172.  171.  170.]\n",
      "   [ 168.  167.  167.]\n",
      "   [ 165.  165.  166.]]\n",
      "\n",
      "  [[ 173.  172.  179.]\n",
      "   [ 175.  174.  178.]\n",
      "   [ 173.  173.  175.]\n",
      "   ..., \n",
      "   [ 170.  168.  169.]\n",
      "   [ 168.  167.  168.]\n",
      "   [ 166.  166.  168.]]]\n",
      "\n",
      "\n",
      " [[[  20.   15.   12.]\n",
      "   [  20.   15.   12.]\n",
      "   [  18.   13.   10.]\n",
      "   ..., \n",
      "   [  20.   16.   16.]\n",
      "   [  21.   16.   14.]\n",
      "   [  21.   16.   13.]]\n",
      "\n",
      "  [[  20.   15.   12.]\n",
      "   [  20.   15.   12.]\n",
      "   [  18.   13.   10.]\n",
      "   ..., \n",
      "   [  20.   16.   15.]\n",
      "   [  21.   16.   13.]\n",
      "   [  21.   16.   13.]]\n",
      "\n",
      "  [[  20.   15.   12.]\n",
      "   [  20.   15.   12.]\n",
      "   [  18.   13.   10.]\n",
      "   ..., \n",
      "   [  20.   16.   14.]\n",
      "   [  21.   16.   13.]\n",
      "   [  21.   16.   13.]]\n",
      "\n",
      "  ..., \n",
      "  [[  66.   54.   41.]\n",
      "   [  80.   67.   53.]\n",
      "   [  47.   35.   19.]\n",
      "   ..., \n",
      "   [ 134.  134.  100.]\n",
      "   [ 110.  113.   77.]\n",
      "   [  98.  102.   66.]]\n",
      "\n",
      "  [[  60.   48.   33.]\n",
      "   [  55.   43.   27.]\n",
      "   [  50.   38.   22.]\n",
      "   ..., \n",
      "   [ 123.  125.   83.]\n",
      "   [  79.   81.   42.]\n",
      "   [  72.   74.   38.]]\n",
      "\n",
      "  [[  64.   54.   38.]\n",
      "   [  55.   45.   29.]\n",
      "   [  48.   38.   21.]\n",
      "   ..., \n",
      "   [ 155.  157.  111.]\n",
      "   [ 136.  137.   97.]\n",
      "   [  88.   89.   51.]]]\n",
      "\n",
      "\n",
      " [[[ 116.  103.   56.]\n",
      "   [ 115.  105.   59.]\n",
      "   [ 155.  128.   82.]\n",
      "   ..., \n",
      "   [ 175.  132.   77.]\n",
      "   [ 169.  133.   72.]\n",
      "   [ 142.  119.   53.]]\n",
      "\n",
      "  [[ 116.  102.   58.]\n",
      "   [ 122.  108.   65.]\n",
      "   [ 153.  121.   79.]\n",
      "   ..., \n",
      "   [ 150.  111.   57.]\n",
      "   [ 145.  121.   60.]\n",
      "   [ 133.  123.   54.]]\n",
      "\n",
      "  [[  95.   86.   41.]\n",
      "   [  98.   84.   44.]\n",
      "   [ 141.  106.   67.]\n",
      "   ..., \n",
      "   [ 145.  112.   58.]\n",
      "   [ 126.  111.   51.]\n",
      "   [ 127.  125.   62.]]\n",
      "\n",
      "  ..., \n",
      "  [[  77.   63.   29.]\n",
      "   [  88.   72.   37.]\n",
      "   [  70.   59.   27.]\n",
      "   ..., \n",
      "   [  48.   39.   20.]\n",
      "   [ 116.  108.   84.]\n",
      "   [ 159.  142.  122.]]\n",
      "\n",
      "  [[  55.   37.   16.]\n",
      "   [  65.   46.   24.]\n",
      "   [  68.   53.   30.]\n",
      "   ..., \n",
      "   [  42.   30.   13.]\n",
      "   [ 126.  114.   90.]\n",
      "   [ 160.  147.  125.]]\n",
      "\n",
      "  [[  78.   57.   37.]\n",
      "   [  72.   50.   30.]\n",
      "   [  67.   50.   32.]\n",
      "   ..., \n",
      "   [  52.   37.   18.]\n",
      "   [ 124.  110.   84.]\n",
      "   [ 155.  144.  124.]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 127.  160.  155.]\n",
      "   [ 139.  171.  171.]\n",
      "   [ 155.  191.  184.]\n",
      "   ..., \n",
      "   [ 110.  150.  160.]\n",
      "   [ 120.  157.  164.]\n",
      "   [ 120.  157.  163.]]\n",
      "\n",
      "  [[ 143.  171.  170.]\n",
      "   [ 149.  177.  177.]\n",
      "   [ 155.  183.  179.]\n",
      "   ..., \n",
      "   [ 123.  167.  173.]\n",
      "   [ 137.  171.  176.]\n",
      "   [ 143.  176.  180.]]\n",
      "\n",
      "  [[ 147.  169.  171.]\n",
      "   [ 141.  161.  162.]\n",
      "   [ 141.  159.  157.]\n",
      "   ..., \n",
      "   [ 121.  169.  169.]\n",
      "   [ 129.  171.  169.]\n",
      "   [ 131.  172.  170.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 220.  216.  217.]\n",
      "   [ 219.  215.  216.]\n",
      "   [ 219.  215.  216.]\n",
      "   ..., \n",
      "   [ 211.  203.  206.]\n",
      "   [ 206.  203.  204.]\n",
      "   [ 205.  201.  202.]]\n",
      "\n",
      "  [[ 220.  216.  217.]\n",
      "   [ 218.  214.  215.]\n",
      "   [ 219.  215.  216.]\n",
      "   ..., \n",
      "   [ 210.  202.  205.]\n",
      "   [ 202.  199.  200.]\n",
      "   [ 203.  199.  200.]]\n",
      "\n",
      "  [[ 217.  213.  214.]\n",
      "   [ 216.  212.  213.]\n",
      "   [ 216.  212.  213.]\n",
      "   ..., \n",
      "   [ 202.  195.  197.]\n",
      "   [ 194.  191.  192.]\n",
      "   [ 193.  190.  191.]]]\n",
      "\n",
      "\n",
      " [[[ 190.  183.  176.]\n",
      "   [ 200.  190.  177.]\n",
      "   [ 208.  197.  180.]\n",
      "   ..., \n",
      "   [ 200.  192.  165.]\n",
      "   [ 210.  205.  183.]\n",
      "   [ 206.  204.  191.]]\n",
      "\n",
      "  [[ 173.  166.  156.]\n",
      "   [ 190.  178.  161.]\n",
      "   [ 206.  191.  168.]\n",
      "   ..., \n",
      "   [ 201.  187.  152.]\n",
      "   [ 209.  199.  172.]\n",
      "   [ 205.  199.  184.]]\n",
      "\n",
      "  [[ 173.  168.  158.]\n",
      "   [ 191.  181.  164.]\n",
      "   [ 208.  194.  172.]\n",
      "   ..., \n",
      "   [ 208.  191.  161.]\n",
      "   [ 214.  200.  177.]\n",
      "   [ 207.  199.  187.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 167.  163.  156.]\n",
      "   [ 177.  170.  159.]\n",
      "   [ 187.  179.  165.]\n",
      "   ..., \n",
      "   [ 181.  180.  171.]\n",
      "   [ 192.  192.  185.]\n",
      "   [ 195.  195.  191.]]\n",
      "\n",
      "  [[ 167.  163.  157.]\n",
      "   [ 182.  173.  162.]\n",
      "   [ 195.  184.  169.]\n",
      "   ..., \n",
      "   [ 186.  180.  169.]\n",
      "   [ 193.  192.  183.]\n",
      "   [ 195.  195.  191.]]\n",
      "\n",
      "  [[ 169.  167.  162.]\n",
      "   [ 180.  174.  165.]\n",
      "   [ 190.  183.  172.]\n",
      "   ..., \n",
      "   [ 178.  173.  163.]\n",
      "   [ 192.  190.  182.]\n",
      "   [ 196.  196.  192.]]]\n",
      "\n",
      "\n",
      " [[[ 177.  160.  135.]\n",
      "   [ 174.  159.  134.]\n",
      "   [ 182.  165.  140.]\n",
      "   ..., \n",
      "   [ 202.  185.  160.]\n",
      "   [ 200.  181.  160.]\n",
      "   [ 199.  177.  156.]]\n",
      "\n",
      "  [[ 178.  162.  137.]\n",
      "   [ 178.  161.  137.]\n",
      "   [ 180.  164.  139.]\n",
      "   ..., \n",
      "   [ 210.  187.  162.]\n",
      "   [ 215.  186.  166.]\n",
      "   [ 218.  188.  168.]]\n",
      "\n",
      "  [[ 188.  174.  148.]\n",
      "   [ 189.  171.  147.]\n",
      "   [ 186.  169.  145.]\n",
      "   ..., \n",
      "   [ 222.  192.  171.]\n",
      "   [ 228.  192.  173.]\n",
      "   [ 225.  188.  169.]]\n",
      "\n",
      "  ..., \n",
      "  [[  76.   50.   57.]\n",
      "   [  69.   46.   49.]\n",
      "   [  76.   54.   61.]\n",
      "   ..., \n",
      "   [ 176.  166.  136.]\n",
      "   [ 173.  158.  131.]\n",
      "   [ 184.  163.  137.]]\n",
      "\n",
      "  [[ 120.   89.   94.]\n",
      "   [ 110.   83.   90.]\n",
      "   [ 107.   82.   93.]\n",
      "   ..., \n",
      "   [ 166.  152.  126.]\n",
      "   [ 160.  144.  120.]\n",
      "   [ 164.  146.  123.]]\n",
      "\n",
      "  [[ 129.   86.   86.]\n",
      "   [ 125.   88.   95.]\n",
      "   [ 122.   90.   98.]\n",
      "   ..., \n",
      "   [ 158.  141.  119.]\n",
      "   [ 164.  146.  127.]\n",
      "   [ 171.  156.  136.]]]]\n",
      "validation_labels:  [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_SIZE = 10000\n",
    "\n",
    "train_data = train_data[VALIDATION_SIZE:, :, :, :]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "train_size = len(train_data)\n",
    "validation_data = train_data[:VALIDATION_SIZE, :, :, :]\n",
    "validation_data = np.float32(validation_data)\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "validation_size = len(validation_data)\n",
    "\n",
    "print('train_data.shape', train_data.shape)\n",
    "print('validation_data.shape', validation_data.shape)\n",
    "print('train_data size:', train_size)\n",
    "print('validation_data size:', validation_size)\n",
    "print('validation_data:', validation_data)\n",
    "print('validation_labels: ', validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables Initialized\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_SIZE = 32\n",
    "PIXEL_DEPTH = 255\n",
    "BATCH_SIZE = 100\n",
    "N_CHANNELS = 3\n",
    "N_LABELS = 10\n",
    "SEED = 32\n",
    "\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, N_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, N_LABELS))\n",
    "\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, N_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, N_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[N_LABELS]))\n",
    "\n",
    "print('Variables Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined\n"
     ]
    }
   ],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "\n",
    "    # Max pooling. The kernel size spec ksize also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "  \n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "print('Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training computation: logits + cross-entropy loss done\n"
     ]
    }
   ],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0)\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.01,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.95,                # Decay rate.\n",
    "  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                       0.9).minimize(loss,\n",
    "                                                     global_step=batch)\n",
    "\n",
    "# Predictions for the minibatch, validation set and test set.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "# We'll compute them only once in a while by calling their {eval()} method.\n",
    "validation_prediction = tf.nn.softmax(model(validation_data_node))\n",
    "test_prediction = tf.nn.softmax(model(test_data_node))\n",
    "\n",
    "print('Training computation: logits + cross-entropy loss done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.as_default()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 32, 32, 3)\n",
      "(100, 10)\n",
      "Run graph done.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "# Grab the first BATCH_SIZE examples and labels.\n",
    "batch_data = train_data[:BATCH_SIZE, :, :, :]\n",
    "batch_labels = train_labels[:BATCH_SIZE]\n",
    "\n",
    "# This dictionary maps the batch data (as a numpy array) to the\n",
    "# node in the graph it should be fed to.\n",
    "feed_dict = {train_data_node: batch_data,\n",
    "             train_labels_node: batch_labels}\n",
    "\n",
    "# Run the graph and fetch some of the nodes.\n",
    "_, l, lr, predictions = s.run(\n",
    "  [optimizer, loss, learning_rate, train_prediction],\n",
    "  feed_dict=feed_dict)\n",
    "\n",
    "print(batch_data.shape)\n",
    "print(batch_labels.shape)\n",
    "print('Run graph done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First prediction 8\n",
      "(100, 10)\n",
      "All predictions [8 8 8 8 8 8 8 8 6 6 6 8 8 8 6 8 8 6 8 8 8 6 8 8 8 8 8 8 8 8 8 8 8 8 8 8 6\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 6 8 8 8 8 8 8 8 6 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 6 8 8 6 8 8 8 8 8 8 8 8 8 8 8 6 8 8 8 8 6]\n"
     ]
    }
   ],
   "source": [
    "# The highest probability in the first entry.\n",
    "print('First prediction', np.argmax(predictions[0]))\n",
    "\n",
    "# But, predictions is actually a list of BATCH_SIZE probability vectors.\n",
    "print(predictions.shape)\n",
    "\n",
    "# So, we'll take the highest probability for each vector.\n",
    "print('All predictions', np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch labels [1 6 6 8 8 3 4 6 0 6 0 3 6 6 5 4 8 3 2 6 0 3 1 4 0 6 6 2 7 6 9 0 4 5 7 1 6\n",
      " 7 9 1 7 7 8 0 3 7 4 7 3 1 0 4 6 6 1 4 9 2 6 4 5 0 4 6 0 8 3 4 8 8 3 9 5 7\n",
      " 1 9 4 7 9 1 9 7 5 2 7 3 4 8 8 2 1 5 9 2 7 8 8 6 8 8]\n"
     ]
    }
   ],
   "source": [
    "print('Batch labels', np.argmax(batch_labels, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14\n"
     ]
    }
   ],
   "source": [
    "correct = np.sum(np.argmax(predictions, 1) == np.argmax(batch_labels, 1))\n",
    "total = predictions.shape[0]\n",
    "\n",
    "print(float(correct) / float(total))\n",
    "\n",
    "confusions = np.zeros([10, 10], np.float32)\n",
    "bundled = zip(np.argmax(predictions, 1), np.argmax(batch_labels, 1))\n",
    "for predicted, actual in bundled:\n",
    "  confusions[predicted, actual] += 1\n",
    "\n",
    "plt.grid(False)\n",
    "plt.xticks(np.arange(N_LABELS))\n",
    "plt.yticks(np.arange(N_LABELS))\n",
    "plt.imshow(confusions, cmap=plt.cm.jet, interpolation='nearest');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate defined\n"
     ]
    }
   ],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate and confusions.\"\"\"\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "    confusions = np.zeros([10, 10], np.float32)\n",
    "    bundled = zip(np.argmax(predictions, 1), np.argmax(labels, 1))\n",
    "    for predicted, actual in bundled:\n",
    "        confusions[predicted, actual] += 1\n",
    "    \n",
    "    return error, confusions\n",
    "\n",
    "print('Error rate defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 of 400\n",
      "Mini-batch loss: 293523708995339420696576.00000 Error: 87.00000 Learning rate: 0.01000\n"
     ]
    }
   ],
   "source": [
    "# Train over the first 1/4th of our training set.\n",
    "steps = train_size // BATCH_SIZE\n",
    "for step in range(steps):\n",
    "    # Compute the offset of the current minibatch in the data.\n",
    "    # Note that we could use better randomization across epochs.\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_data = train_data[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "    # This dictionary maps the batch data (as a numpy array) to the\n",
    "    # node in the graph it should be fed to.\n",
    "    feed_dict = {train_data_node: batch_data,\n",
    "                 train_labels_node: batch_labels}\n",
    "    # Run the graph and fetch some of the nodes.\n",
    "    _, l, lr, predictions = s.run(\n",
    "      [optimizer, loss, learning_rate, train_prediction],\n",
    "      feed_dict=feed_dict)\n",
    "    \n",
    "    # Print out the loss periodically.\n",
    "    if step % 100 == 0:\n",
    "        error, _ = error_rate(predictions, batch_labels)\n",
    "        print('Step %d of %d' % (step, steps))\n",
    "        print('Mini-batch loss: %.5f Error: %.5f Learning rate: %.5f' % (l, error, lr))\n",
    "        print('Validation error: %.1f%%' % error_rate(\n",
    "              validation_prediction.eval(), validation_labels)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
